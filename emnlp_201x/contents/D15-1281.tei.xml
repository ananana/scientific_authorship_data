<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Verbal and Nonverbal Clues for Real-life Deception Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Linton</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Verbal and Nonverbal Clues for Real-life Deception Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deception detection has been receiving an increasing amount of attention from the computational linguistics, speech, and multimodal processing communities. One of the major challenges encountered in this task is the availability of data, and most of the research work to date has been conducted on acted or artificially collected data. The generated deception models are thus lacking real-world evidence. In this paper, we explore the use of multi-modal real-life data for the task of deception detection. We develop a new deception dataset consisting of videos from real-life scenarios, and build deception tools relying on verbal and nonverbal features. We achieve classification accuracies in the range of 77-82% when using a model that extracts and fuses features from the linguistic and visual modalities. We show that these results outperform the human capability of identifying deceit.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As deceptive behavior occurs on a daily basis in different areas of life <ref type="bibr" target="#b25">(Meyer, 2010;</ref><ref type="bibr" target="#b36">Smith et al., 2014</ref>), the need arises for automated methodolo- gies to detect deception in an efficient, yet reliable manner. There are many applications that can ben- efit from automatic deception identification, such as airport security screening, crime investigation and interrogation, interviews, advertisement, and others. In many of these settings, the polygraph test has been used as the main method to identify deceptive behavior. However, this method requires the use of skin-contact devices and human exper- tise, making it infeasible for large-scale applica- tions. Moreover, polygraph tests were shown to be misleading in multiple cases <ref type="bibr" target="#b41">(Vrij, 2001;</ref><ref type="bibr" target="#b13">Gannon et al., 2009)</ref>, as human judgment is often biased.</p><p>Given the difficulties associated with the use of polygraph-like methods, learning-based ap- proaches have been proposed to address the de- ception detection task using a number of modali- ties, including text <ref type="bibr" target="#b12">(Feng et al., 2012</ref>) and speech ( <ref type="bibr" target="#b16">Hirschberg et al., 2005;</ref><ref type="bibr" target="#b28">Newman et al., 2003)</ref>. Unlike the polygraph methods, learning-based methods for deception detection rely mainly on data collected from deceivers and truth-tellers. The data is usually elicited from human contrib- utors, in a lab setting or via crowdsourcing. An important problem identified in this data-driven re- search is the lack of real data. Because of the arti- ficial setting, the subjects may not be emotionally aroused, as they may not take the experiments seri- ously given the lack of motivation and/or penalty.</p><p>In this paper, we describe what we believe is a first attempt at building a multimodal system that detects deception in real-life settings. We collect a dataset consisting of 118 deceptive and truthful video clips, from real trials and live street inter- views aired in television shows. We use the tran- scription of these videos to extract several linguis- tic features, and we manually annotate the videos for the presence of several gestures that are used to extract nonverbal features. We then build a system that jointly uses the verbal and nonverbal modali- ties to automatically detect the presence of decep- tion. Our experiments show that the multimodal system can identify deception with an accuracy in the range of 77-82%, significantly improving over the baseline. In addition, we present a study on the human ability to detect deception in single or multimodal data streams, and show that our sys- tem outperforms humans on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset</head><p>Our goal is to build a multimodal collection of oc- currences of real deception, which will allow us to analyze both verbal and nonverbal behaviors in relation to deception. Starting at the top left-hand corner: deceptive interview with up gaze (Up), deceptive interview with side gaze (Side), deceptive trial with both hands (Both-H), truthful trial with forward head (Forward), truthful interview with side turn (Side-Turn), and truthful interview with single hand (Single-H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truthful</head><p>Deceptive I was sentenced to forty to sixty years in prison for this crime that I didn't commit. At the trial the judge had exceeded the sentence guidelines because he said I failed to show remorse. And I told him, you know, I felt terrible for what happen to this woman, shouldn't happen to anyone, but I can't show remorse for something I didn't do.</p><p>We had some drinks at the bar, maybe one ... two. um I got onto the dance floor myself as I ex- plained, um I have been a trained dancer for some time, going to be able to dance freely is like a ... release. I'm very much in my own space when I do that and so I got up, and I was dancing alone on the dance floor. It's difficult to pick just one but um I think Ten- der Mercies uh is ... really captured my imagi- nation um when I was in junior high. Had a lot to do with Robert Duval's performance certainly and that got me excited about the possibility of um .... pulling off an acting career for myself.</p><p>Yeah, yeah he was convincing as a wolf. Ahhh actually you know ahhh this is like crazy I'm ter- rified from wolves, it's my worst fear even though they don't exist but thats my worst fear, sharks and stuff like that. Yeah its my worst fear, I am being honest with you. <ref type="table" target="#tab_0">Table 1</ref>: Sample transcripts for deceptive and truthful clips. The first row presents transcripts from the Trials domain while the second shows transcripts corresponding to the Interviews domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Collection</head><p>To collect real deception data, we start by identi- fying online multimedia sources where deceptive behavior can be observed and verified. We specif- ically target videos of people, on which we en- force some of the constraints imposed by current data processing technologies: the person in the video should be in front of the camera; her face should be clearly visible; visual quality should be clear enough to identify the facial expressions; and finally, audio quality should be clear enough to hear the voices and understand what the per- son is saying. We collect video clips from pub- lic real trials and interviews aired during television shows, where the truth or falsehood of the partic- ipant's statements ends up being known. Video clips from trials consist of statements from wit- nesses and defendants in the same trial. In or- der to have a clear distinction between deceptive and truthful trial videos portraying defendants, the process of labeling the trial relies on the verdict. Thus, clips with a guilty verdict are considered deceptive whereas clips with a non-guilty verdict or exoneration are labeled as truthful. Clips con- taining witness testimonies are labeled as truth- ful if their statements are verified by police in- vestigations. Examples of trials included in our dataset are Jodi Arias, Andrea Sneiderman, and Amanda Hayes. Exoneree's statements were taken from "The Innocence Project" (http://www. innocenceproject.org).</p><p>Deceptive and truthful responses are also col- lected from TV shows and interviews. Examples of such shows are "Lie Witness," "Golden Balls," and the "American Film Institute" and "RevYOU" You-Tube channels. Deceptive videos portray sce- narios where interviewees' responses were known to be a lie. For example, the interviewer asks a ran- dom individual on his opinion on a non-existing film where the interviewee fabricates a story. On the other hand, truthful videos are collected from individuals asked on their opinions on real movies.</p><p>Given our goals and constraints, data collec- tion ended up being a lengthy and laborious pro- cess consisting of several iterations of Web min- ing, data processing and analysis, and content val- idation.</p><p>The final dataset includes 118 videos, includ- ing 59 that are labeled as deceptive and 59 la- beled as truthful. Among them, 62 belong to the TV street interviews and shows category (Inter- views) with 28 deceptive and 34 truthful video clips, and 56 belong to the trials category (Trials) with 31 deceptive and 25 truthful clips. The aver- age length of the videos in the dataset is 27.28 sec- onds, with an average length of 33.02 seconds for the truthful clips and 21.54 seconds for the decep- tive clips. Collected trial samples cover famous murder cases, while street interviews cover sev- eral topics such as movies, music, politics, and re- ligion. The dataset contains 23 unique female and 39 unique male speakers, with their ages ranging approximately between 16 and 60 years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transcriptions and Nonverbal Behavior Annotations</head><p>Our goal is to analyze both verbal and nonverbal behavior to understand their relation to deception. First, all the video clips were manually tran- scribed. The transcription was performed by two transcribers using the Elan software ( <ref type="bibr" target="#b43">Wittenburg et al., 2006</ref>). We asked transcribers to include word repetitions and fillers such as um, ah, and uh, as well as long pauses that were marked using three consecutive dots. The final set of transcrip- tions contain 7835 words, with an average of 66 words per transcript.  <ref type="table">Table 2</ref>: Gesture annotation agreement specifically focus on the annotation of facial dis- plays and hand movements, as they have been pre- viously found to correlate with deceptive behav- ior ( <ref type="bibr" target="#b8">Depaulo et al., 2003)</ref>. The gesture annotation is performed using the MUMIN coding scheme ( <ref type="bibr" target="#b2">Allwood et al., 2007</ref>).</p><p>In the MUMIN scheme, facial displays consist of several different facial expressions associated with eyebrows, eyes, gaze, and mouth. Smile, laughter, and scowl are also included, as well as general head and hand movements.</p><p>The multimodal annotation was performed by two annotators using the Elan software <ref type="bibr" target="#b43">(Wittenburg et al., 2006</ref>). We decided to perform the ges- ture annotations at video level, rather than at utter- ance level, because the overall judgment of truth- fulness and deceitfulness is based on the whole video content. During the annotation process, an- notators were allowed to watch each video clip as many times as they needed. They were asked to identify the facial displays and hand gestures that were most frequently observed or dominating dur- ing the entire clip duration. For each video clip, the annotators had to choose one label for each of the nine gestures listed in <ref type="table" target="#tab_2">Table 3</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the frequency counts associated with the nine gestures considered during the an- notation. Note that the counts under each gesture add up to 118, reflecting the fact that for every ges- ture, the annotators had to choose one label for ev- ery video clip. When none of the labels applied, the "Other" category was selected. In the case of gestures associated with hand movements, the "Other" label also accounted for those cases where the speaker's hands were not moving or were not visible.</p><p>After all the video clips were annotated for gestures, the inter-annotator agreement was mea-    <ref type="table">Table 2</ref> shows the observed annotation agreement between the two annotators, along with the Kappa statistic. The agreement measure rep- resents the percentage of times the two annotators agreed on the same label for each gesture category. For instance, 72.88% of the time the annotators agreed on the label assigned to the General Face category. On average, the observed agreement was measured at 75%, with a Kappa of 0.58 (macro- averaged over the nine categories), which reflects substantial agreement. Observed agreement for Head Movements and Gaze is noticeably lower than other categories, which can be attributed to a higher number of available gesture choices, as seen in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Features of Verbal and Nonverbal Behaviors</head><p>Given the multimodal nature of our dataset, we de- cided to focus on the linguistic and gesture compo- nents. In this section, we describe the sets of fea- tures extracted for each modality, which will then be used to build classifiers of deception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Verbal Features</head><p>We implement three types of features, consisting of unigrams, psycholinguistic features, and syn- tactic complexity features.</p><p>Unigrams. We extract unigrams derived from the bag-of-words representation of the video transcripts. The unigram features are en- coded as word frequencies and include all the words present in the transcripts. Syntactic Complexity. We also extract features to measure the syntactic complexity of the speech produced by the speakers in truth- ful and deceptive clips. This set of features is motivated by previous research that has suggested that deceivers' speech has lower complexity ( <ref type="bibr" target="#b8">Depaulo et al., 2003)</ref>. We use the tool described in <ref type="bibr" target="#b22">(Lu, 2010)</ref>, which gen- erates indexes of syntactic complexity, in- cluding general complexity metrics, length of production, and amount of coordination. The set of features consists of fourteen indexes including statistics related to T-units, which are linguistic units that include a main clause in addition to attached subordinate clauses. T-unit analysis is extensively used to ana- lyze syntactic complexity in speech and writ- ten content. The set of features includes the mean length of sentence, mean length of T- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Nonverbal Features</head><p>The nonverbal features are derived from the an- notations performed using the MUMIN coding scheme as described in section 2.2. We create a binary feature for each of the 40 available gesture labels. Each feature indicates the presence of a gesture only if it is observed during the majority of the interaction duration. The generated features represent nine different gesture categories cover- ing facial displays and hand movements.</p><p>Facial Displays. These are facial expressions or head movements displayed by the speaker during the deceptive or truthful interaction. They include all the behaviors listed in <ref type="table" target="#tab_2">Table  3</ref> under the General Facial Expressions, Eye- brows, Eyes, Mouth Openness, Mouth Lips, and Head Movements.</p><p>Hand Gestures. The second broad category cov- ers gestures made with the hands, and it in- cludes the Hand Movements and Hand Tra- jectories listed in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We start our experiments with an analysis of the nonverbal behaviors occurring in deceptive and truthful videos. We compare the percentage of each behavior as observed in each class. For in- stance, there is a total of 41 videos in the dataset  that include the Smile feature (as shown in Ta- ble 3), out of which 12 are part of the deceptive set of 59 videos, and 29 are part of the truthful set (again, of 59 videos). Hence, the percentages for this feature are 20.33% in the deceptive class, and 49.13% in the truthful class. <ref type="figure" target="#fig_2">Figure 2</ref> shows the percentages of all the nonverbal features for which we observe noticeable differences for the deceptive and truthful groups. As the figure sug- gests, facial displays seem to help differentiate be- tween the deceptive and truthful conditions. For instance, we can observe that truth-tellers smile (Smile) and blink more (Close-R). Interestingly deceivers seem to make more eye contact (Inter- locutor gaze) and nod (Side-Turn-R) more fre- quently than truth-tellers. This agrees with the findings in ( <ref type="bibr" target="#b8">Depaulo et al., 2003</ref>) that liars who are more motivated to get away with their lies (i.e., tri- als) are likely to increase their eye-contact behav- ior.</p><p>Motivated by these results, we proceed to con- duct further experiments to evaluate the perfor- mance of the extracted features using a machine learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVM All</head><p>77.11% -Hand gestures 74.57% -Facial displays 64.40% -Syntactic 76.27% -Semantic 72.03% -Unigrams 73.72% <ref type="table">Table 5</ref>: Feature ablation study.</p><p>We run our learning experiments on the real- deception dataset introduced earlier. Given the even distribution between deceptive and truthful clips, the baseline on this dataset is 50%. For each video clip, we create feature vectors formed by combinations of the verbal and nonverbal fea- tures described in the previous section. We build deception classifiers using three classification al- gorithms: Support Vector Machines (SVM), De- cision Trees (DT), and Random Forest (RF). <ref type="bibr">2</ref> We run several comparative experiments using leave- one-out cross-validation. <ref type="table" target="#tab_5">Table 4</ref> shows the accu- racy figures obtained by the three classifiers on the major feature groups described in Section 3. As shown in this table, the facial displays classifier achieves the highest accuracy among the individ- ual classifiers, followed by the unigrams classifier.</p><p>We also evaluate classifiers that rely on com- bined sets of features. The nonverbal features clearly outperform the verbal features, and the classifier that includes all the features improves over the classifiers that rely on all the verbal fea- tures or all the nonverbal features. Importantly, several of the classifiers improve significantly over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of Feature Contribution</head><p>To better understand the contribution of the dif- ferent feature sets to the overall classifier perfor- mance, we conduct an ablation study where we re- move one group of features at a time. Given that SVM had the best performance in our initial set of experiments, we run all our analysis experiments only using this classifier. <ref type="table">Table 5</ref> shows the accu- racies obtained when one feature group is removed and the deception classifier is built using the re- maining features. From this table, we can again observe that Facial Displays contribute the most to the classifier performance, while Syntactic Fea- tures show the lowest contribution. We also analyze the contribution of the lin- guistic features. Using the linguistic ethnogra- phy method <ref type="bibr" target="#b26">(Mihalcea and Pulman, 2009)</ref>, we ob- tain the most dominant LIWC word classes asso- ciated with deceptive and truthful transcripts ex- tracted from trials and interviews clips. Results are shown in <ref type="table" target="#tab_7">Table 6</ref>. Interestingly, the most dom- inant classes in truthful clips, regardless of being from interviews or trials, correspond to words re- lated to Family, Home, and Humans. This sug- gests that truth-tellers show similar word usage when interviewed on a real scenario. On the other hand, dominant classes associated to deceivers are less consistent as they discuss aspects related to the topic being discussed. For instance, while be- ing interviewed about a non-existing movie, de- ceivers talk about their Past, Assent, and use Mo- tion words in order to support their lies. In con- trast, while being on trial stating their (false) inno- cence, they use Anxiety, Anger, and negative emo-  tion words (class Negemo). In line with earlier ob- servations ( <ref type="bibr" target="#b27">Mihalcea and Strapparava, 2009</ref>), de- ceptive texts include more words that reflect cer- tainty (class Certain, with words such as com- pletely, truly, always) and more references to oth- ers (class Other, with words such as she, day, him).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Experiments</head><p>We perform three sets of experiments to determine the role played by the domain. The first set of ex- periments uses only the Interviews video clips (62 in total), and the results are shown in the left col- umn of <ref type="table" target="#tab_9">Table 7</ref>. The second set uses only the Tri- als instances (56 in total), with results shown in the right column of <ref type="table" target="#tab_9">Table 7</ref>. Finally, we also perform cross-domain experiments, with the training data drawn from one domain and the test data from the other. The results of these experiments are shown in <ref type="table">Table 8</ref>. Given the uneven distribution of the truthful and deceptive video clips in two domains, the baselines are 54.83% for the Interviews do- main (34 truthful, 28 deceptive), and 55.35% for the Trials domain (25 truthful, 31 deceptive). What we learn from these experiments is that the domain does matter. Despite the smaller dataset, the experiments run on one domain at a time lead to results that are higher than the ones obtained with more data but with a mix of do- mains. The cross-domain experiments also sup- port this argument, as the performance drops sig-    <ref type="table">Table 8</ref>: Cross-domain classification results using a SVM classifier trained on all the features nificantly when there is no overlap in domain be- tween the training and the test instances. Over- all, in all our machine learning experiments, the combined classifier that makes use of all the verbal and nonverbal features achieves the best trade-off between performance and robustness, as it always leads to the best or second best performance across all the experiments using individual or combined feature sets. While a classifier based on an individ- ual feature set can sometime lead to a better per- formance (e.g., the Facial Displays classifier has better performance when all the video clips are used), that same classifier may not perform well in another setting (e.g., the Facial Displays classifier is significantly below the All Features classifier in the domain experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Human Performance</head><p>An important remaining question is concerned with the human performance on the task of de- ception detection. An answer to this question can shed light on the difficulty of the task, and can also place our results in perspective.</p><p>We conduct a study where we evaluate the hu- man ability to identify deceit when exposed to four different modalities: Text, consisting of the lan- guage transcript; Audio, consisting of the audio track of the clip; Silent video, consisting of only the video with muted audio; and Full video, where    <ref type="table" target="#tab_0">Table 10</ref>: Performance of three annotators and the developed automatic system (Sys) on the real- deception dataset over four modalities.</p><p>audio and video are played simultaneously. We create an annotation interface that shows an anno- tator instances for each modality in random order, and ask him or her to select a label of either "De- ception" or "Truth" according to his or her percep- tion of truthfulness or falsehood.</p><p>To avoid annotation bias, we show the modal- ities in the following order: first we show either Text or Silent video, then we show Audio, followed by Full video. Note that apart from this constraint which is enforced over the four modalities belong- ing to each video clip, the order in which instances are presented to an annotator is random. Further- more, the annotators did not have access to any information that would reveal the true label of an instance. The only exception to this could have been the annotators' previous knowledge of some of the public trials in our dataset. A discussion with the annotators after the annotation took place indicated however that this was not the case.</p><p>Three annotators labeled all the 118 video clips in the dataset. Since four modalities were ex- tracted from each video, each annotator annotated a total of 412 instances. Annotators were not of- fered a monetary reward and we considered their judgments to be honest as they participated volun- tarily in this experiment. <ref type="table" target="#tab_11">Table 9</ref> shows the ob- served agreement and Kappa statistics among the three annotators for each modality. <ref type="bibr">3</ref> The agree- ment for most modalities is rather low and the Kappa scores range between slight to fair agree- ment. As noted before <ref type="bibr" target="#b31">(Ott et al., 2011</ref>), this low agreement can be interpreted as an indication that people are poor judges of deception.</p><p>We also determine each annotator's perfor- mance for each modality. The results, shown in <ref type="table" target="#tab_0">Table 10</ref>, additionally support the argument that human judges have difficulty performing the de- ception detection task. An interesting, yet perhaps unsurprising observation is that the human perfor- mance increases with the availability of modali- ties. The poorest accuracy is obtained in Silent video, followed by Text, Audio, and Full Video where the judges have the highest performance.</p><p>Overall, our study indicates that detecting de- ception is indeed a difficult task for humans and further verifies previous findings where human ability to spot liars was found to be slightly better than chance <ref type="bibr" target="#b0">(Aamodt and Custer, 2006</ref>). More- over, the performance of the human annotators ap- pears to be significantly below that of our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Verbal Deception Detection. To date, several re- search publications on verbal-based deception de- tection have explored the identification of decep- tive content in a variety of domains, including on- line dating websites ( <ref type="bibr" target="#b38">Toma and Hancock, 2010;</ref><ref type="bibr" target="#b14">Guadagno et al., 2012</ref>), forums ( <ref type="bibr" target="#b42">Warkentin et al., 2010;</ref><ref type="bibr" target="#b19">Joinson and Dietz-Uhler, 2002</ref>), social net- works ( <ref type="bibr" target="#b17">Ho and Hollister, 2013)</ref>, and consumer re- port websites <ref type="bibr" target="#b31">(Ott et al., 2011;</ref><ref type="bibr" target="#b20">Li et al., 2014)</ref>. Research findings have shown the effectiveness of features derived from text analysis, which fre- quently includes basic linguistic representations such as n-grams and sentence count statistics <ref type="bibr" target="#b27">(Mihalcea and Strapparava, 2009)</ref>, and also more complex linguistic features derived from syntac- tic CFG trees and part of speech tags <ref type="bibr" target="#b12">(Feng et al., 2012;</ref><ref type="bibr" target="#b44">Xu and Zhao, 2012)</ref>. Research work has also relied on the LIWC lexicon to build deception models using machine learning approaches <ref type="bibr" target="#b27">(Mihalcea and Strapparava, 2009;</ref><ref type="bibr" target="#b3">´ Angela Almela et al., 2012)</ref> and showed that the use of psycholin- guistic information is helpful for the automatic identification of deceit. Following the hypothe- sis that deceivers might create less complex sen- tences in an effort to conceal the truth and being able to recall their lies more easily, several re- searchers have also studied the relation between text syntactic complexity and deception <ref type="bibr" target="#b45">(Yancheva and Rudzicz, 2013)</ref>.</p><p>Nonverbal Deception Detection. Earlier ap- proaches to nonverbal deception detection relied on polygraph tests to detect deceptive behavior. These tests are mainly based on such physiolog- ical features such as heart rate, respiration rate, skin temperature. Several studies <ref type="bibr" target="#b41">(Vrij, 2001;</ref><ref type="bibr" target="#b13">Gannon et al., 2009;</ref><ref type="bibr" target="#b9">Derksen, 2012)</ref> indicated that relying solely on physiological measurements can be biased and misleading. <ref type="bibr" target="#b6">Chittaranjan et al. (Chittaranjan and Hung, 2010</ref>) created an au- dio visual recording of the "Are you a Werewolf?" game in order to detect deceptive behaviour us- ing non-verbal audio cues and to predict the sub- jects' decisions in the game. For hand gestures, blob analysis was used to detect deceit by track- ing the hand movements of the subjects ( <ref type="bibr" target="#b21">Lu et al., 2005;</ref><ref type="bibr" target="#b21">Tsechpenakis et al., 2005</ref>), or using geo- metric features related to the hand and head mo- tion ( ). <ref type="bibr" target="#b5">Caso et al. (Caso et al., 2006</ref>) identified particular hand gestures that can be related to the act of deception using data from simulated interviews. <ref type="bibr" target="#b7">Cohen et al. (2010)</ref> found that fewer iconic hand gestures were a sign of a deceptive narration, and <ref type="bibr" target="#b15">Hillman et al. (2012)</ref> determined that increased speech prompt- ing gestures were associated with deception while increased rhythmic pulsing gestures were associ- ated with truthful behavior. Also related is the taxonomy of hand gestures developed by (Mar- icchiolo et al., ) for deception and social behav- ior. Facial expressions also played a critical role in the identification of deception. <ref type="bibr" target="#b10">(Ekman, 2001</ref>) defined micro-expressions as relatively short in- voluntary expressions, which can be indicative of deceptive behavior. Moreover, these expressions were analyzed using smoothness and asymmetry measurements to further relate them to an act of deceit <ref type="bibr" target="#b11">(Ekman, 2003)</ref>. <ref type="bibr" target="#b37">Tian et al. (Tian et al., 2005</ref>) considered features such as face orienta- tion and facial expression intensity. <ref type="bibr" target="#b32">Owayjan et al. (Owayjan et al., 2012</ref>) extracted geometric- based features from facial expressions, and Pfis- ter and Pietikainen <ref type="bibr" target="#b35">(Pfister and Pietikäinen, 2012</ref>) developed a micro-expression dataset to identify expressions that are clues for deception. Recently, features from different modalities were integrated in order to find a combination of multimodal fea- tures with superior performance ( <ref type="bibr" target="#b4">Burgoon et al., 2009;</ref><ref type="bibr" target="#b18">Jensen et al., 2010)</ref>. A multimodal decep- tion dataset consisting of linguistic, thermal, and physiological features was introduced in <ref type="bibr">(PérezRosas et al., 2014)</ref>, which was then used to de- velop a multimodal deception detection system ( <ref type="bibr" target="#b1">Abouelenien et al., 2014</ref>). An extensive review of approaches for evaluating human credibility us- ing physiological, visual, acoustic, and linguistic features is available in <ref type="bibr">(Nunamaker et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we presented a study of multimodal deception detection using real-life occurrences of deceit. We introduced a novel dataset covering recordings from public real trials and street inter- views, and used this dataset to perform both qual- itative and quantitative experiments. Our analy- sis of nonverbal behaviors occurring in deceptive and truthful videos brought insight into the ges- tures that play a role in deception. We also built classifiers relying on individual or combined sets of verbal and nonverbal features, and showed that we can achieve accuracies in the range of 77-82%.</p><p>Additional analyses showed the role played by the various feature sets used in the experiments, and the importance of the domain. To place our re- sults in perspective and better understand the dif- ficulty of the task, we performed a study of hu- man ability to detect deception, which revealed high disagreement among the annotators. Our au- tomatic system outperforms the human detection of deceit by 6-15%.</p><p>To our knowledge this is the first work to auto- matically detect instances of deceit using both ver- bal and nonverbal features extracted from real de- ception data. In order to develop a fully automated deception deception system, our future work will address the use of automatic gesture and facial ex- pression identification and automated speech tran- scription. Our goal is to move forward towards a real-time deception detection system.</p><p>The dataset introduced in this paper is publicly available from http://lit.eecs.umich.edu.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample screenshots showing facial displays and hand gestures from real-life deception and truthful clips. Starting at the top left-hand corner: deceptive interview with up gaze (Up), deceptive interview with side gaze (Side), deceptive trial with both hands (Both-H), truthful trial with forward head (Forward), truthful interview with side turn (Side-Turn), and truthful interview with single hand (Single-H).</figDesc><graphic url="image-1.png" coords="2,133.29,62.86,330.88,141.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Label</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of nonverbal features for deceptive and truthful groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Weights of top nonverbal features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Modality</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>transcriptions 
of sample deceptive and truthful statements from 
both trials and reality shows. 
Second, we annotate the gestures 1 observed 
during the interactions in the video clips. We </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Frequency counts for nine facial displays and hand gestures sured.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Deception classifiers using individual and combined sets of verbal and nonverbal features.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc>LIWC word classes most strongly associ- ated with deception and truth.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Deception classifiers for the Interviews 
and Trials domains, using a SVM classifier trained 
on individual and combined sets of verbal and 
nonverbal features. 

Training 
Test 
SVM 
Trials 
Interviews 58.06% 
Interviews Trials 
58.92% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Agreement among three human annota-
tors on text, audio, silent video, and full video 
modalities. 

Text 
Audio Silent video Full video 
A1 
54.24% 58.47% 
50.85% 
63.00% 
A2 
55.93% 67.80% 
45.76% 
68.00% 
A3 
65.25% 70.34% 
55.93% 
71.00% 
Sys. 65.75% 
NA 
75.42% 
77.11% 

</table></figure>

			<note place="foot" n="1"> As done in the Human-Computer Interaction community, we use the term &quot;gesture&quot; to broadly refer to body movements, including facial expressions and hand gestures.</note>

			<note place="foot" n="2"> We use the implementation available in the Weka toolkit with the default parameters.</note>

			<note place="foot" n="3"> Inter-rater agreement with multiple raters and variables. https://mlnl.net/jg/software/ira/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based in part upon work sup-ported by National Science Foundation awards #1344257 and #1355633, by grant #48503 from the John Templeton Foundation, and by DARPA-BAA-12-47 DEFT grant #12475008. Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation, the John Tem-pleton Foundation, or the Defense Advanced Re-search Projects Agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Who can best catch a liar? a meta-analysis of individual differences in detecting deception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heather</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Custer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Examiner</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="11" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deception detection using a multimodal approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abouelenien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction</title>
		<meeting>the 16th International Conference on Multimodal Interaction<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mumin coding scheme for the annotation of feedback, turn management and sequencing phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Allwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loredana</forename><surname>Cerrato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristiina</forename><surname>Jokinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costanza</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrizia</forename><surname>Paggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="273" to="287" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Seeing through deception: A computational approach to deceit detection in written communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Almela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valencia-García</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascual</forename><surname>Cantos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Deception Detection</title>
		<meeting>the Workshop on Computational Approaches to Deception Detection<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting concealment of intent in transportation screening: A proof of concept</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Judee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">P</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">O</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">V</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The impact of deception and suspicion on different hand movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Letizia</forename><surname>Caso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fridanna</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marino</forename><surname>Bonaiuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldert</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samantha</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are you awerewolf? detecting deceptive roles and outcomes in a conversational role-playing game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Chittaranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayley</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="5334" to="5337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonverbal indicators of deception: How iconic gestures reveal thoughts that cannot be suppressed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Beattie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="issue">182</biblScope>
			<biblScope unit="page" from="133" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>and Heather Shovelton</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cues to deception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bella</forename><surname>Depaulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Malone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Muhlenbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="page" from="74" to="118" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Control and resistance in the psychology of lying</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maarten Derksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Psychology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Telling Lies: Clues to Deceit in the Marketplace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Politics and Marriage</title>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Norton, W.W. and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Annals of the New York Academy of Sciences, 1000(EMOTIONS INSIDE OUT: 130 Years after Darwin&apos;s The Expression of the Emotions in Man and Animals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="205" to="221" />
		</imprint>
	</monogr>
	<note>Darwin, deception, and facial expression</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Syntactic stylometry for deception detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="171" to="175" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Risk Assessment and the Polygraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Beech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Ward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley and Sons Ltd</publisher>
			<biblScope unit="page" from="129" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dating deception: Gender, online dating, and exaggerated self-presentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanna</forename><forename type="middle">E</forename><surname>Guadagno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><forename type="middle">M</forename><surname>Okdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="642" to="647" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Um they were wearing : The effect of deception on specific hand gestures. Legal and Criminological</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><surname>Hillman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldert</forename><surname>Vrij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samantha</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="345" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinguishing deceptive from non-deceptive speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Benus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Brenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Enos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Gilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Girand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graciarena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kathol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Michaelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech 2005-Eurospeech</title>
		<meeting>Interspeech 2005-Eurospeech</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1833" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guess who? an empirical study of gender deception and detection in computer-mediated communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Shuyuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hollister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Society for Information Science and Technology</title>
		<meeting>the American Society for Information Science and Technology</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic, multimodal evaluation of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judee</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Group Decision and Negotiation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explanations for the perpetration of and reactions to deception in a virtual community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Joinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietz-Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="289" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards a general rule for identifying deceptive opinion spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blob analysis of the head and hands: A method for deception detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Hawaii International Conference on System Sciences (HICSS&apos;05), HICSS &apos;05</title>
		<meeting>the 38th Annual Hawaii International Conference on System Sciences (HICSS&apos;05), HICSS &apos;05<address><addrLine>DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic analysis of syntactic complexity in second language writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Corpus Linguistics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="474" to="496" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fridanna</forename><surname>Maricchiolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augusto</forename><surname>Gnisci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marino</forename><surname>Bonaiuto</surname></persName>
		</author>
		<title level="m">Cognitive Behavioural Systems</title>
		<editor>Anna Esposito, AntoniettaM. Esposito, Alessandro Vinciarelli, Rdiger Hoffmann, and VincentC. Mller</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="405" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deception detection through automatic, unobtrusive analysis of nonverbal behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kruse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Douglas Twitchell, Gabriel Tsechpenakis, Judee Burgoon, Dimitris Metaxas, and Jay Nunamaker</title>
		<imprint>
			<date type="published" when="2005-09" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Liespotting: Proven Techniques to Detect Deception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Meyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>New York; St. Martin&apos;s</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linguistic ethnography: Identifying dominant word classes in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The lie detector: Explorations in the automatic recognition of deceptive language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL 2009)</title>
		<meeting>the Association for Computational Linguistics (ACL 2009)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lying words: Predicting deception from linguistic styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">S</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><forename type="middle">M</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Bulletin</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judee</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">W</forename><surname>Twyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Gainer</forename><surname>Proudfoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Establishing a foundation for automated human credibility screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">Scott</forename><surname>Schuetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giboney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Intelligence and Security Informatics (ISI)</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The design and development of a lie detection system using facial micro-expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Owayjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Kashour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">Al</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghinwa Al</forename><surname>Souki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 2nd International Conference on Advances in Computational Tools for Engineering Applications (ACTEA)</title>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: LIWC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Erlbaum Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A multimodal dataset for deception detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verónica</forename><surname>Pérez-Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Narvaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC-2014)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC-2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-26" />
			<biblScope unit="page" from="3118" to="3122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Electronic imaging &amp; signal processing automatic identification of facial clues to lies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Newsroom</title>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Everyday deception or a few prolific liars? the prevalence of lies in text messaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeline</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Birnholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="220" to="227" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Li</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Face Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="247" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reading between the lines: linguistic cues to deception in online dating profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalina</forename><forename type="middle">L</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM conference on Computer supported cooperative work, CSCW &apos;10</title>
		<meeting>the 2010 ACM conference on Computer supported cooperative work, CSCW &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Adkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judee</forename><forename type="middle">K</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hmmbased deception recognition from visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">P</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><forename type="middle">F</forename><surname>Deokar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nunamaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2005-07" />
			<biblScope unit="page" from="824" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Detecting Lies and Deceit: The Psychology of Lying and the Implications for Professional Practice. Wiley series in the psychology of crime, policing and law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldert</forename><surname>Vrij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Warrants and deception in computer mediated communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darcy</forename><surname>Warkentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Cormier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM conference on Computer supported cooperative work</title>
		<meeting>the 2010 ACM conference on Computer supported cooperative work</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Elan: a professional framework for multimodality research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wittenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hennie</forename><surname>Brugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Klassmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Sloetjes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using deep linguistic features for finding deceptive opinion spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters</title>
		<meeting>COLING 2012: Posters<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic detection of deception in child-produced speech using syntactic complexity features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Yancheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="944" to="953" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
