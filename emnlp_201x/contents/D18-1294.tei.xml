<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Syntax Encoding with Application in Authorship Attribution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2742</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
							<email>zhangrc@act.buaa.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Hu</surname></persName>
							<email>zhiyuan.hu.bj@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
							<email>hongyu.guo@nrc-cnrc.gc.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
							<email>ymao@uottawa.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Beijing University of Chemical Technology</orgName>
								<orgName type="institution" key="instit2">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Ottawa</orgName>
								<address>
									<settlement>Ottawa</settlement>
									<region>Ontario</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Syntax Encoding with Application in Authorship Attribution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2742" to="2753"/>
							<date type="published">October 31-November 4, 2018. 2018. 2742</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel strategy to encode the syntax parse tree of sentence into a learnable distributed representation. The proposed syntax encoding scheme is provably information-lossless. In specific, an embedding vector is constructed for each word in the sentence, encoding the path in the syntax tree corresponding to the word. The one-to-one correspondence between these &quot;syntax-embedding&quot; vectors and the words (hence their embedding vectors) in the sentence makes it easy to integrate such a representation with all word-level NLP models. We empirically show the benefits of the syntax embeddings on the Authorship Attribution domain, where our approach improves upon the prior art and achieves new performance records on five benchmarking data sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Syntactic parse information plays an essential role in interpreting natural languages because natural language sentences are typically structured in a linguistic grammar. As such, in many NLP ap- plications it is desirable to extract syntactic fea- tures from text or sentences, for which there exist a rich body of literature ( <ref type="bibr" target="#b2">Baayen et al., 1996;</ref><ref type="bibr" target="#b13">Hirst, 2007;</ref><ref type="bibr" target="#b17">Massung et al., 2013;</ref><ref type="bibr" target="#b46">Wang et al., 2015;</ref><ref type="bibr" target="#b36">Socher et al., 2011;</ref><ref type="bibr" target="#b48">Zhu et al., 2015b;</ref><ref type="bibr" target="#b43">Tai et al., 2015;</ref><ref type="bibr" target="#b47">Zhu et al., 2015a)</ref> To date, existing approaches to exploit syntac- tic information can be categorized into two cate- gories. The first category may be regarded as "syn- tactic feature engineering". In such approaches, certain properties or statistics are extracted from the syntax parse tree of a sentence as the syntacti- cal feature. For example, the extracted feature may include the depths of the tree, frequency of certain structural patterns in the tree and so on ( <ref type="bibr" target="#b17">Massung et al., 2013;</ref><ref type="bibr" target="#b46">Wang et al., 2015)</ref>. The advantage of such a method is that the extracted feature can be used for any kind of classifier, if the feature is deemed relevant to the classification task. The limitation of such an approach is however that rich structural information contained in the syntax tree is lost in the feature extraction process. Addition- ally, with such a strategy, the model designer is of- ten required to design syntactic feature extractors specific for his tasks.</p><p>The second category may be regarded as "syntax-assisted sentence coding". This category of approaches build upon neural network mod- els. Examples of such approaches include Tree- LSTM ( <ref type="bibr" target="#b43">Tai et al., 2015;</ref><ref type="bibr" target="#b48">Zhu et al., 2015b</ref>) and Re- cursive Neural Networks <ref type="bibr" target="#b36">(Socher et al., 2011</ref>), in which the networks are structured according to the syntax tree of the input sentence. The network, after being trained, is capable of encoding a se- quence of word embeddings, in a bottom-up man- ner, to a vector representing the entire sentence. It is worth noting that with these approaches, the en- coded feature vector, although containing syntac- tical information, mainly serves as a semantic rep- resentation of the input sentence, and the syntactic information exploited therein primarily serves to assist the semantic representation. Additionally, such an approach is not flexible enough to be inte- grated with another popular class of NLP models, CNN.</p><p>One motivation of this work is to develop a generic representation of the parse structure of sentences. Ideally, we would like the representa- tion to maximally preserve the syntactical infor- mation and can be integrated easily with any neu- ral network NLP models. This latter requirement is particularly desirable, in light of the competi- tive performance of CNN and their advantages in training efficiency.</p><p>Another motivation of this work is the applica- tion of Authorship Attribution (AA) <ref type="bibr" target="#b13">(Hirst, 2007;</ref><ref type="bibr" target="#b40">Stamatatos, 2009;</ref><ref type="bibr" target="#b19">Ouamour and Sayoud, 2012;</ref><ref type="bibr" target="#b41">Stamatatos, 2011;</ref><ref type="bibr" target="#b8">De Vel et al., 2001;</ref><ref type="bibr" target="#b25">Rocha et al., 2017b;</ref><ref type="bibr" target="#b3">Binongo, 2003;</ref><ref type="bibr" target="#b37">Sohn et al., 2015;</ref><ref type="bibr" target="#b35">Shrestha et al., 2017b)</ref>. In this application, one is to ex- tract information from a document so as to infer the document's author, from a given list of can- didates. There have been a variety of use cases of AA in practice, which include, amongst many others, plagiarism detection <ref type="bibr" target="#b42">(Stamatatos and Koppel, 2011</ref>), forensics ( <ref type="bibr" target="#b24">Rocha et al., 2017a</ref>), suicidal note verification <ref type="bibr" target="#b5">(Chaski, 2005)</ref>, and intelligent question answering <ref type="bibr" target="#b38">(Stamatatos, 2006</ref>). In this application, previous works have mostly focused on building a classifier based on content-level fea- tures. The hypothesis underlying our approach to AA in this paper is that syntactic information is a useful additional feature that can characterize an author. This is not only because that syntactical in- formation complements the document content in language understanding, it is also due to the fact that different authors may construct sentences with varying distributions of syntactical structures. In a sense, the "syntactical pattern" of an author may characterize in part the "writing style" of the au- thor, which is independent of the content he writes and the semantics of the document.</p><p>To that end, we propose a novel strategy to encode the syntax parse tree of sentence into a learnable distributed representation. Briefly, in this representation, an embedding vector is con- structed for each word in the sentence, encoding the path in the syntax tree corresponding to the word. The one-to-one correspondence between these "syntax-embedding" vectors and the words (hence their embedding vectors) in the sentence makes it easy to integrate such a representation with all word-level NLP models.</p><p>The proposed syntax encoding scheme has a remarkable property, namely that it is provably information-lossless, as long as the syntax embed- ding space has adequate dimension. This property also distinguishes the proposed scheme from other distributed representations of syntax.</p><p>We apply the proposed syntax encoding ap- proach to the state-of-the-art CNN-based AA model and evaluate its performance over five benchmarking datasets. Experimental study ver- ifies the effectiveness of the proposed syntax en- coding scheme. In fact, over all five datasets, syntax-augmented CNN model demonstrates new state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The aim of the related work here is twofold: syn- tax encoding and authorship attribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Syntax Encoding</head><p>Including syntactic parse information to benefit NLP models have been actively investigated in decades. Syntactic feature engineering refers to efforts to statically extract domain specific fea- tures from syntax parse tree of the given text <ref type="bibr" target="#b17">(Massung et al., 2013;</ref><ref type="bibr" target="#b46">Wang et al., 2015)</ref>. Recent attempts also include leveraging syntactic parse tree structure to recursively generate sentence rep- resentations bottom-up <ref type="bibr" target="#b36">(Socher et al., 2011;</ref><ref type="bibr" target="#b48">Zhu et al., 2015b;</ref><ref type="bibr" target="#b43">Tai et al., 2015;</ref><ref type="bibr" target="#b47">Zhu et al., 2015a</ref>).</p><p>Both the above two categories of methods have severe limitations. The former parse represen- tation typically fails to encode the parse tree structure, and the latter is constrained by the tree structures favored by the parser. Further- more, the recent distributed word embedding tech- niques, such as Glove ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>) and W2V ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref>, have been shown to encode limited syntax knowledge of the given corpus ( <ref type="bibr" target="#b0">Andreas and Klein, 2014)</ref>. This short- coming has also promoted recent research on cre- ating syntax-aware word embedding, which en- hances the distributed embedding vectors with po- sition information of the word within its surround- ing context <ref type="bibr" target="#b7">(Cheng and Kartsaklis, 2015)</ref>, which again encodes limited syntax information.</p><p>Our syntax embedding method overcomes the above mentioned limitations, as previously dis- cussed in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Authorship Attribution</head><p>Various AA models make use of SVM classifiers on some carefully engineered lexical or syntactic feature. These works include <ref type="bibr" target="#b22">(Pillay and Solorio, 2010;</ref><ref type="bibr" target="#b45">Varela et al., 2011;</ref><ref type="bibr" target="#b30">Segarra et al., 2013;</ref><ref type="bibr" target="#b31">Seker et al., 2013;</ref><ref type="bibr" target="#b32">Seroussi et al., 2010;</ref><ref type="bibr" target="#b4">Castillo et al., 2015)</ref>. In many of these models and among many others (e.g., ( <ref type="bibr" target="#b15">Koppel et al., 2009;</ref><ref type="bibr" target="#b20">Peng et al., 2003;</ref><ref type="bibr" target="#b1">Apté et al., 1994)</ref>), character n-grams are chosen as an important feature.</p><p>Recently, researchers have relied on CNN to ex- tract features automatically. In <ref type="bibr" target="#b11">(Ferracane et al., 2017)</ref>, for instance, a CNN is used on 2-gram em- beddings to learn the discourse information for the AA task. In ( <ref type="bibr" target="#b35">Shrestha et al., 2017b</ref>), a CNN is employed on n-gram embeddings to learn richer  Our proposed model, apart from syntax encod- ing, is the most close to the CNN architecture of (Shrestha et al., 2017b) where n-grams are used to extract global content information representing content features. The detailed model architecture will be given in a later section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Syntax Encoding</head><p>The syntactical structure of a given sentence can be uniquely represented by a tree, which we refer to as the syntax tree. An example of such a syntax tree is given in <ref type="figure" target="#fig_4">Figure 1</ref>. As seen in the example, a syntax tree has labeled nodes. Specifically, the la- bel of each node is a "syntax token", such as S, NP, VP, etc., representing the grammatical property of the word sequences covered by tree branches un- derneath the node. For example, the root of the tree is always labeled by S ("sentence"), and the branches underneath the tree cover the entire sen- tence. On the other hand, the labels for terminals or leafs of the tree, such as EX, VBP, JJ etc., cor- respond to "part-of-speech" tags of each word in this sentence. We will denote by T the set of all syntax tokens. Given this syntactic tree structure for a sentence s, each word w in sentence s has a unique path in the tree leaving the root and arriving at a terminal. Such a "syntax path" for the word w can then be represented by a sequence of node labels along the path. Some examples of syntax paths are given in <ref type="table" target="#tab_0">Table 1</ref>. The following lemma is easy to verify.</p><p>Lemma 1 Let a sentence s be written as a se- quence of words (w 1 , w 2 , . . . , w n ). For each word position i = 1, 2, . . . , n, let r(w i ) denote the syn- tax path of word w i . Let R := {(i, r(w i )) : i = 1, 2, . . . , n} be an (unordered) set containing pre- cisely all syntax paths for the words in s. Then the syntax tree of s can be uniquely recovered by R.</p><p>In the lemma, we note that R is an unordered set. That is, regardless of the ordering of the paths in R, one can always recover the syntax tree from R.</p><p>Let r(w) be the syntax path of a word w in the sentence s of interest. Specifically, r(w) can be written as the sequence (t 1 , t 2 , . . . , t L ), where L is the number of nodes in the path r, and each t i is a syntax token.</p><p>Let the Euclidean space R K be the embedding space which we will use to encode syntax. We now describe a method that encodes the path r(w) into a vector r(w) ∈ R K .</p><p>Let L max be maximum depth of the syntax trees in the corpus. For each i = 1, 2, . . . , L max , let p i be a vector in R K serving as the embedding for in- teger i. Here integer i is meant to indicate the lo- cation of a token in a syntax path. For each t ∈ T , let t also be a vector in R K , serving as the embed- ding for syntax token t. Let vector r(w) ∈ R K , the embedding of path r(w), be defined by</p><formula xml:id="formula_0">r(w) := t j ∈r(w) t j • p j (1)</formula><p>where • is the element-wise product operation. For example in <ref type="table" target="#tab_0">Table 1</ref>, the syntax path for word "there" will have embedding NP • p 1 + EX • p 2 ; the syntax path for word "no" will have embed-</p><formula xml:id="formula_1">ding VP • p 1 + NP • p 2 + DT • p 3 .</formula><p>Note that when embedding a syntax path, the beginning token S is removed from the path since it exists in every path.</p><p>Lemma 2 There exists a random assignment of the vectors {t : t ∈ T } and {p i : i = 1, 2, . . . , L} such that the syntax path r(w) can be recovered from its embedding r(w) almost surely for suffi- ciently large K.</p><p>This lemma (proof given in Appendix) sug- gests that as long as we choose a sufficiently large embedding dimension K, the above intro- duced encoding for syntax paths is essentially lossless. Thus, for a given n-word sentence s = (w 1 , w 2 , . . . , w n ), if we collect the embed- ding vectors r(w 1 ), r(w 2 ), . . . , r(w n ) of all syn- tax paths and list them as the columns of a K × n matrix, then by Lemmas 1 and 2, the syntax tree of s can be recovered from the matrix. Such a ma- trix is then an information lossless encoding of the syntax tree.</p><p>We note however that in practice, when the to- kens embeddings and the position (integer) em- beddings are learned, there is no longer a guar- anty that a syntax path can be recovered from its embedding. This is particularly the case for su- pervised tasks. During the training for such tasks, the information irrelevant to the training objective is necessarily "squeezed out", and the represen- tations of those syntax paths that provide no dis- tinguishing features are "pulled closer". This will cause these paths non-distinguishable (and hence non-recoverable) from their embeddings. This is also the reason that in practice there is no need to have very large embedding dimension K.</p><p>Nonetheless, since different supervised tasks may have distinct training objectives, a "lossy" syntax encoding suitable for one task may prove ineffective for other tasks. Thus it is still essential to adopt an information-lossless encoding frame- work, as we propose in this paper, that is univer- sally applicable.</p><p>Next, we will discuss the application of our syn- tax encoding approach to AA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Authorship Attribution Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Definition</head><p>The objective of Authorship Attribution is to de- velop a classifier that predicts the authors of un- seen documents based on a given set of documents and their corresponding authors. Despite the pre- vious successes in solving the AA problem dis- cussed in a previous section, we argue that the syntactic parse information in an author's writing can characterize in part the "writing style" of the author. Specifically, even when writing the same content, two authors may prefer using different syntax structure in constructing their sentences. Consider the following two sentences. A1: Take a left at the end of the street, you will see the house. A2: You will see the house, if you go down to the end of the street and take a left.</p><p>The two sentences have the same meaning and yet two different authors may favor different ones over the other. This provides opportunity for leveraging such syntactical information as an additional fea- ture, beyond the lexical and semantic features, to distinguish the two authors. Detecting the syntactical differences among au- thors suits well the application scope of the pro- posed syntax encoding scheme. As such, in this work, we will use the AA problem as an test bed to examine the effectiveness of our scheme. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Syntax-augmented CNN Model</head><p>The overall architecture of our model is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Our model takes advantage of the expressive power of convolutional neural net- works(CNNs) to learn the embeddings for both of the content-level features and the syntax-level features. The model consists of 5 types of lay- ers: Syntax-level feature embedding, Content-level feature embedding, Convolution, Max-pooling and Softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Overall Structure</head><p>The model takes both the context-level features and the syntax-level features as the input.</p><p>As the character n-gram features have been used successfully in both of the text classification tasks <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b6">Chen et al., 2017</ref>) and the AA tasks ( <ref type="bibr" target="#b35">Shrestha et al., 2017b;</ref><ref type="bibr" target="#b27">Sapkota et al., 2015;</ref><ref type="bibr" target="#b34">Shrestha et al., 2017a;</ref><ref type="bibr" target="#b28">Sari et al., 2017;</ref><ref type="bibr" target="#b26">Ruder et al., 2016)</ref>, in this study, we use the character n-grams to represent the content-level features.</p><p>We align in tandem the embedding vectors of consecutive n-grams of a document to form a content-level embedding matrix, and the consecu- tive syntax-path embeddings of each word to form the syntax-level embedding matrix. These embed- ding matrices are individually passed to 2 paral- lel CNNs having different filter lengths. The con- volutional layer outputs feature maps via convolu- tional filtering. The max-pool layer is then applied across different feature maps to form the content- level and syntax-level representation of a docu- ment respectively. These two representations are then concatenated to form the final feature vector characterizing the author of the document. Finally, this feature vector is passed to a learnable softmax layer. The number of outputs of the softmax layer is the number of authors, which the i th output is the probability that the document written by au- thor i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Convolution and Max-pool</head><p>In the convolutional layer, we capture local con- textual information using kernels, which combine the vectors within it's window as it slides over the embedding matrix. A linear transformation pro- cesses the output of the kernel.</p><p>We present a clean description for this opera- tion. Let E ∈ R k×d be the embedding matrix con- taining embeddings e 1:k , where e i ∈ R d is the i-th embedding in E, l is the number of filters and w is the filter length or window size. Let F ∈ R w×d be the filter matrix and a bias vector b. We define the vector g i ∈ R w×d as the concatenation of w embeddings in the i-th window, where</p><formula xml:id="formula_2">g i = e i−w+1:i 1 ≤ i ≤ k + w − 1<label>(2)</label></formula><p>The result of filter F across embedding matrix E outputs a feature map f j ∈ R d for the j-th ker- nel where the i-th value of f j is computed as</p><formula xml:id="formula_3">f ji = F ⊗ g i + b<label>(3)</label></formula><p>where ⊗ denotes the convolution operator. Let f ∈ R l×d denote the concatenation of the l fea- ture maps for compactness. Further to this, we apply a ReLU activation function before passing the output to the max-pool layer. Given l filters of different dimensions, we obtain l feature maps. Each feature map encodes different types of ab- stract contextual information globally for the ma- trix E. To obtain the most relevant features from each dimension of the feature maps, we max-over feature map dimensions expressed as:</p><formula xml:id="formula_4">m i = max(f (·, i)), 1 ≤ i ≤ d<label>(4)</label></formula><p>where m i represents the maximum value for di- mension i across the l feature maps. The feature vector m from the max-pool operation is the con- catenation of all m i . For our 2 CNN's we obtain a content-level vector representation and a syntax- level representation for an author's text. We de- note ¯ m ∈ R t as the concatenation of these 2 vec- tors representing an author's style. We obtain the confidence of an author's attribution to a text by feeding this final vector to a softmax layer. In the next section, we give a brief description of the softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Softmax Layer</head><p>Given a text input x and CNN parameters θ, this layer outputs a score for all n authors. The output of the softmax layer is a vector with dimension equal to the number of author labels. To compute the confidence of author's attribution, the author's style representation ¯ m is transformed by a trans- formation matrix W ∈ R n×t .</p><formula xml:id="formula_5">o = W ¯ m<label>(5)</label></formula><p>where the i-th component of o corresponds to the confidence score of author i. A softmax operation is called on o to compute the conditional proba- bility for each dimension. This is calculated by:</p><formula xml:id="formula_6">p(i|x, θ) = e o i n j=1 e o j<label>(6)</label></formula><p>To train our model, the log-likelihood of the probability should be maximized. To predict the attribution of authors, the label with the highest probability is selected. For the optimization of our model, we use SGD algorithm to solve the opti- mization problem.</p><p>We denote our syntax augmented CNN model as Syntax-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Studies</head><p>We first empirically show the predictive perfor- mance of the Syntax-CNN strategy, which es- tablishes new state-of-the-art accuracy for several Description CCAT10 CCAT50 IMDB62 blogs10 blogs50 <ref type="table" target="#tab_0">Avg # of words per doc  580  584  345  108.6  117.1  Avg # of chars per doc  3,089  3,010  1742  502.3  541.5  Docs per author  100  100  1000  2353.4  1470.1  Max # of chars  8,716  8,716  11617  30712  30712  Min # of chars  483  345  82  3  3   Table 2</ref>: Statistics of Datasets benchmarking data sets. We then provide ablation studies, aiming at better understanding the contri- butions of the syntax embeddings to the Syntax- CNN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We test the Syntax-CNN approach on several benchmarking datasets. Summary statistics of the datasets are in <ref type="table">Table 2</ref>. CCAT10: 100 newswire stories, written by 10 au- thors, in English taken from Reuters Corpus Vol- ume 1 (RCV1) <ref type="bibr" target="#b39">(Stamatatos, 2008)</ref>. CCAT50: Same as CCAT10 but with 100 news article written by 50 authors. IMDB62: 62,000 movie reviews and 17,550 mes- sage posts from 62 prolific authors obtained from Internet Movie Database(IMDb) ( <ref type="bibr" target="#b32">Seroussi et al., 2010)</ref>. Blogs10: The original data set contains 681,288 blog posts by 19,320 bloggers for blogger.com. Posts written by the top ten bloggers are selected for the Blogs10 data set ( <ref type="bibr" target="#b29">Schler et al., 2006</ref>). Blogs50: Same as Blogs50 but with the posts writ- ten by the top 50 bloggers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Compared Prior Art</head><p>We compare our method with various baseline ap- proaches, which represent the current art in the AA problem. They include SCAP ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Hyperparameters and Training</head><p>Our experimental setup follows that of the current state-of-the-art AA method ( <ref type="bibr" target="#b35">Shrestha et al., 2017b</ref>). In specific, the networks are trained using mini-batches with size of either 16 (for IMDB62,Blogs10 and Blogs50) or 32 (for CCAT10 and CCAT50). We use 3-gram with em- bedding size of 300 for character, and embedding size of 60 for syntax vector; these embeddings are randomly initialized. We apply <ref type="bibr">Adagrad (Duchi et al., 2011</ref>) with initial learning rate of 1e-4. For the CNN, we use filter sizes of 3, 4, and 5 with 50 feature maps for syntax embedding; 500 di- mensional character embedding for CCAT, IMDB, and Blogs50, and 200 for Blog10. We train for at most 300 epochs, with a dropout rate of 0.25. We deploy early stop strategy for the training using a validation data set, which contain 10% of the ran- domly selected samples from the training set. We stop the training when the validation loss goes up.</p><p>In our experiments, we use the Stanford CoreNLP parser. For documents contain more than one sentences, each sentence is parsed sepa- rately. The syntax encoding for each word is done according to its syntax path in syntax tree contain- ing the word. The syntax embeddings of all words in the document form the input matrix to the CNN responsible for extracting syntactical features. <ref type="table" target="#tab_1">Table 3</ref> presents the accuracy obtained by var- ious testing methods on the five benchmarking datasets, where the best result of each data set is highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predictive Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Accuracy Obtained by Syntax-CNN</head><p>Results in <ref type="table" target="#tab_1">Table 3</ref> indicate that the Syntax-CNN outperformed all the testing methods on the five benchmarking datasets, beating the current best records on these testing AA tasks. Our further Model CCAT10 CCAT50 IMDB62 Blogs10 Blogs50</p><formula xml:id="formula_7">SCAP (2007) # # 94.8</formula><p>48.6 41.6 SVM with most frequent 3-grams <ref type="bibr">(2008)</ref> 80.8 67 81.4 # # SVM with bag of local histogram <ref type="bibr">(2011)</ref> 86.4 # # # # Imposters <ref type="bibr">(2011)</ref> # # 76.9 35.4 22.6 LDAH-S <ref type="bibr">(2011)</ref> # # 72.0 52.5 18.3 SVM with affix+punctuation 3-grams <ref type="bibr">(2015)</ref> 78  <ref type="table">Table 4</ref>: Syntax correctness as measured by the Heminway Editor tool analysis also shows that, the predictive improve- ment on some datasets is large. For example, against the CCAT50 and Blogs50 datasets, the rel- ative error reductions over the current state-of-the- art result are 5.5% and 3.64% , respectively.</p><note type="other">.8 69.3 # # # CNN-char (2016) # # 91.7 61.2 49.4 Continuous n-gram representations (2017) 74.8 72.6 95.12 61.34 52.82 N-gram CNN (2017) 86.8 76.5 95.21 63.74 53.09 Syntax 22.8 10.08 83.48 48.64 42.91 Syntax-CNN 88.20 81.00 96.16 64.10 56.73</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Contribution of Syntax Encoding</head><p>We also include the accuracy obtained by using the syntax embeddings alone on the second last row in <ref type="table" target="#tab_1">Table 3</ref>. These results indicate that using only the syntax style embeddings achieved very low accu- racy on some of the datasets, for example, with only 10.08% on the CCAT50 dataset. As shown in <ref type="table">Table 4</ref>, the CCAT50 is, indeed, one of hard- est datasets, as judged by the Hemingway Editor tool 1 , which aims to measure the syntax correc- tion of a given piece of text. Nevertheless, such syntax style embeddings can bring significant ac- curacy gain to the CNN strategies. As shown in <ref type="table" target="#tab_1">Table 3</ref>, for the CCAT50 dataset, a relative error reduction of 5.5% (which represents the largest error reduction of the five testing datasets) was achieved by Syntax-CNN over the best performed CNN model, i.e., the N-gram CNN. Similar be- havior can also been observed for the CCAT10 dataset as shown in <ref type="table" target="#tab_1">Tables 4 and 3</ref>. These results suggest that the Syntax-CNN may favor sentences with syntax difficulties.</p><p>1 http://www.hemingwayapp.com</p><p>We also provide, in <ref type="table" target="#tab_2">Table 5</ref>, further information about the percentage of classifications corrected or mislabeled when enabling the syntax style em- beddings in the Syntax-CNN method on both the CCAT10 and CCAT50 datasets. <ref type="table" target="#tab_2">Table 5</ref> clearly indicates that with the syntax style embeddings deployed, the Syntax-CNN was able to, respec- tively for the CCAT10 and CCAT50 datasets, cor- rect 43.48% and 37.7% of the testing examples which were mislabeled when the syntax embed- dings was disabled in the Syntax-CNN. data set CCAT10 CCAT50 wrong-to-correct 43.48% 37.7% correct-to-wrong 4.87% 6.1% Percentage of classifications cor- rected (wrong-to-correct) or mislabeled (correct- to-wrong) when using the syntax style embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Syntactic Style Examples</head><p>The contributions of the syntax information is fur- ther justified by examining documents whose clas- sifications are corrected when enabling the syntax embedding of the Syntax-CNN. For example, the following two excerpts, extracted from documents written by two different authors, share great se- mantic similarity. But they show a difference in terms of syntax information contained. In particu- lar, the former is constructed by deep parse trees, but the later has a succinct parse tree structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>author1:</head><p>China's long-delayed bid to enter the World Trade Organization will fall un- der the spotlight when the world's rich- est nations meet to discuss its applica- tion this week. United States says Bei- jing must comply with a "road map" to open its markets and eliminate trade and non-trade barriers before it can win U.S. support for its entry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>author2:</head><p>China must make real changes to its economy if it wants to join the World Trade Organization and should replace anti-U.S. rhetoric with international co- operation.</p><p>Using the two documents containing these two excerpts, we perform a separate small experiment. When we remove syntax encoding and its cor- responding feature from Syntax-CNN, the model fails to classify the authors of the two documents. But when syntax encoding is included, the model can distinguish the two authors. Additionally, on the syntax encoding side, when we only keep the syntax path encoding for these two excerpts in their respective documents and remove all other sentences from the two documents, Syntax-CNN is still able to classify the two authors correctly.</p><p>This suggests that syntactic information is in- deed useful for authorship attribution, and Syntax- CNN is able to extract such information effec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Behaviors</head><p>This section aims to further evaluate the behaviors of the Syntax-CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Sensitivity to Syntax Embedding</head><p>Dimension To understand the impact of the embedding size of the syntax tree in the Syntax-CNN method, we conduct further experiments on the Blogs50 dataset. We vary the dimension of the syntax tree embedding, from 5 up to 150. The experimental results are reported in <ref type="table" target="#tab_3">Table 6</ref>. <ref type="table" target="#tab_3">Table 6</ref> shows that Syntax-CNN is robust to the dimension size of syntax style embedding. The dimension Syntax- <ref type="table" target="#tab_0">CNN  5  54.96  30  55.86  60  56.82  100  56.42  150</ref> 53.34  lowest accuracy was obtained by Syntax-CNN with an embedding dimension of 150. Although we have noted earlier that using a higher syntax embedding dimension allows the syntactical infor- mation to be better preserved. A downside of such a choice is however the risk of overfitting, which is expectedly the cause of the degradation of the performance at dimension 150. Note that even in this case, the performance of Syntax-CNN is still higher than the current state-of-the-art accuracy of 53.09% achieved by the N-gram CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Impact of Position Vector</head><p>We further evaluate the impact of the position em- bedding vectors (i.e., the p j vectors in Section 3 ) in syntax encoding. We conduct experiments on both the Blogs10 and Blogs50 datasets with and without the position embedding. In particular, by "without position", we mean that the element-wise multiplication with the p j is removed from equa- tion (1) in Section 3. The results are reported in <ref type="table" target="#tab_4">Table 7</ref>. <ref type="table" target="#tab_4">Table 7</ref> indicates that the position vectors play an important role in capturing syntactical informa- tion. In fact, it is possible to prove that without multiplying with the position vectors, in general the syntax tree is no longer recoverable from the encoding r(w) of syntax path r(w). Additionally, the results in <ref type="table" target="#tab_4">Table 7</ref> suggest that depth informa- tion in the syntax tree is an important feature for distinguishing the writing styles of the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel strategy to encode a syn- tax tree into a learnable distributed representation. This representation can be easily integrated into any NLP neural network model and entails no loss of information. Using this representation as an ad- ditional input, we extend the CNN architecture in- troduced in <ref type="bibr" target="#b35">(Shrestha et al., 2017b</ref>) for the author- ship attribution problem. Experimental evaluation of the extended model on the standard author attri- bution datasets demonstrates the effectiveness of the proposed syntax encoding approach. Record- breaking performances are obtained. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Proof of Lemma 2</head><p>We first establish some elementary results. 3. E(X 2 Y 2 ) = 1 and VAR(X 2 Y 2 ) = 8.</p><p>These results follow from the independence among (X, Y, Z, U ) and the fact that the square of a standard normal random variable is a Chi-Square random variable.</p><p>We now construct a random assignment scheme and a recovery scheme. Random Assignment: Generate each vector in {t : t ∈ T } and in {p i : i = 1, 2, . . . , L max } by as- signing independent values drawn from a standard normal distribution. Recovery Scheme: Let a path embedding r(w) be given. For each token-integer pair (u, i) ∈ T × {1, 2, . . . , L max }, compute vector a ∈ R K and scalar b by</p><formula xml:id="formula_8">a := r(w) • u • p i , b := 1 K K k=1 a[k]</formula><p>where a <ref type="bibr">[k]</ref> is the k th element of a. Choose an arbitrary positive value &lt; 1/2. If |b − 1| &lt; , claim the i th token on path r(w) is u. Now we prove that using this scheme, when the embedding dimension K is sufficiently large, one can recover the path r(w) with probability arbi- trarily close to 1. First a = t j ∈r(w)</p><formula xml:id="formula_9">t j • p j • u • p i</formula><p>Note that this summation contains summing of L K-vectors, which we will re-denote by c 1 , c 2 , . . . , c L . Also we denote</p><formula xml:id="formula_10">C l := 1 K K k=1 c l [k].</formula><p>Then we have a = L l=1 c l</p><p>and</p><formula xml:id="formula_12">b = L l=1 C l<label>(8)</label></formula><p>Each of c l terms in Equation <ref type="formula" target="#formula_11">(7)</ref> corresponds to one of the four cases below. For each of these cases, the distribution of C l can be made concentrated at its mean when K is made large enough (by invoking either the Weak Law of Large Number or the Central Limit Theo- rem). Therefore, if there exists a token in the path r(w) that makes Case 1 satisfied, the distribution of b is concentrated at 1, and our recovery scheme will detect, with probability close to 1, the token and its position in the path. On the other hand, if there is no such token, the distribution of b is con- centrated at 0, and with probability close to 0 our scheme will make a false detection.</p><p>2</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Syntax Tree Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Single Layer CNN Architecture</figDesc><graphic url="image-1.png" coords="4,307.28,283.80,218.27,213.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Frantzeskou et al., 2007), SVM with 2,500 most frequent 3-grams (Plakias and Stamatatos, 2008), SVM with bag of local histogram (Escalante et al., 2011), Imposters (Koppel et al., 2011), LDAH- S (Seroussi et al., 2011),SVM with affix and punc- tuation 3-grams (Sapkota et al., 2015), CNN- char (Ruder et al., 2016), Continuous n-gram representation (Sari et al., 2017), and N-gram CNN (Shrestha et al., 2017a). Except the last two methods, all the results reported in this paper were obtained from their respective papers. In all our experiments, we partitioned the datasets into train/dev/test in the same way as are used in the literature in order for fair comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma 3</head><label>3</label><figDesc>Suppose that random variables X, Y, Z, U are independent standard normal random variables. Then 1. E(XY ZU ) = 0 and VAR(XY ZU ) = 1. 2. E(X 2 Y Z) = 0 and VAR(X 2 Y Z) = 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Case 1 :</head><label>1</label><figDesc>i = j and u = t j . This corresponds to case 3 of Lemma 3, and we have E(c l [k]) = 1 and VAR(c[k]) = 8. It follows that E(C l ) = 1 and VAR(C l ) = 8/K. Case 2: i = j and u = t j . This corresponds to case 2 of Lemma 3, and we have E(c l [k]) = 0 and VAR(c[k]) = 3. It follows that E(C l ) = 0 and VAR(C l ) = 3/K. Case 3: i = j and u = t j . This also corresponds to case 2 of Lemma 3, and we also have E(C l ) = 0 and VAR(C l ) = 3/K. Case 4: i = j and u = t j . This corresponds to case 1 of Lemma 3, and we have E(c l [k]) = 0 and VAR(c[k]) = 1. It follows that E(C l ) = 0 and VAR(C l ) = 1/K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Syntax Path Example 

features. A study by (Tang, 2013) considers using 
SVM as an activation layer for the CNN, replacing 
the soft-max layer. Nevertheless, a small perfor-
mance advantage is demonstrated by the method 
at a high computation cost. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 : Accuracy obtained by the testing methods; best result for each data set is in bold.</head><label>3</label><figDesc></figDesc><table>Quality metrics CCAT10 CCAT50 IMDB62 Blogs10 Blogs50 
normal 
18.58% 
18.79% 
55.81% 
82.15% 82.13% 
hard 
5.96% 
6.3% 
19.05% 
8.61% 
8.9% 
very hard 
75.46% 
74.91% 
25.14% 
9.24% 
8.97% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracy obtained by Syntax-CNN on the 
Blogs50 while varying the syntax style embedding 
dimension. 

data set with without 
Blogs50 56.73 
55.05 
Blogs10 64.10 
62.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Accuracy obtained by Syntax-CNN with 
and without position vector. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How much do word embeddings encode about syntax? In ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="822" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards language independent automated learning of text categorization models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidanand</forename><surname>Apté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sholom</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 17th annual international ACM SIGIR conference on Research and development in information retrieval<address><addrLine>New York, Inc</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Halteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tweedie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who wrote the 15th book of oz? an application of multivariate analysis to authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Nilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Binongo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chance</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="17" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Author attribution using a graph based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darnes</forename><surname>Vilarino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofelia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pinto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Electronics, Communications and Computers (CONIELECOMP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Who&apos;s at the keyboard? authorship attribution in digital evidence investigations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><forename type="middle">E</forename><surname>Chaski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving sentiment analysis via sentence type classification using bilstm-crf and cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntaxaware multi-sense word embeddings for deep compositional models of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1531" to="1542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining email content for author identification forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Olivier De Vel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="55" to="64" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local histograms of character n-grams for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Hugo Jair Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Montes-Y Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="288" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leveraging discourse information effectively for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ferracane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="584" to="593" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identifying authorship by bytelevel n-grams: The source code author profile (scap) method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Frantzeskou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Gritzalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><forename type="middle">E</forename><surname>Chaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><forename type="middle">Stephen</forename><surname>Howald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Digital Evidence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bigrams of syntactic labels for authorship discrimination of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Literary and Linguistic Computing</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational methods in authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="26" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Authorship attribution in the wild. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="83" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structural parse tree features for text representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Massung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Seventh International Conference on Semantic Computing</title>
		<meeting><address><addrLine>Irvine, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-09-16" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Authorship attribution of ancient texts written by ten arabic travelers using a smo-svm classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siham</forename><surname>Ouamour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halim</forename><surname>Sayoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications and Information Technology (ICCIT), 2012 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="44" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated authorship attribution with character level language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchim</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlado</forename><surname>Keselj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2003)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Authorship attribution of web forum posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sangita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Pillay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">eCrime Researchers Summit (ECrime)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensor space models for authorship identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Plakias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence: Theories, Models and Applications</title>
		<imprint>
			<biblScope unit="page" from="239" to="249" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Authorship attribution for social media forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Forstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cavalcante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Theophilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R B</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Authorship attribution for social media forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><surname>Forstall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Cavalcante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyu</forename><surname>Theophilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Ariadne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="33" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Breslin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06686</idno>
		<title level="m">Character-level and multi-channel convolutional neural networks for large-scale authorship attribution</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Not all character n-grams are created equal: A study in authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Ygmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous n-gram representations for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunita</forename><surname>Sari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effects of age and gender on blogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI spring symposium: Computational approaches to analyzing weblogs</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="199" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Authorship attribution using function words adjacency networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Segarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Eisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5563" to="5567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Author attribution on streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Sadi Evren Seker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Latifur</forename><surname>Al-Naami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Reuse and Integration (IRI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="497" to="503" />
		</imprint>
	</monogr>
	<note>IEEE 14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Collaborative inference of sentiments from texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Bohnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UMAP</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Authorship attribution with latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanir</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Bohnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-23" />
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for authorship attribution of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasha</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="669" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for authorship attribution of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasha</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Sierra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">669</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff Chiung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A graph model based author attribution technique for singleclass e-mail classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Sun</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer and Information Science (ICIS), 2015 IEEE/ACIS 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Authorship attribution based on feature set subspacing ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="823" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Author identification: Using text sampling to handle the class imbalance problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey of modern authorship attribution methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="556" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Plagiarism detection using stopword n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2512" to="2527" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Plagiarism and authorship analysis: introduction to the special issue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.0239</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Selecting syntactic attributes for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Varela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edson</forename><surname>Justino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz S</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="167" to="172" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A re-ranking model for dependency parser with recursive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
