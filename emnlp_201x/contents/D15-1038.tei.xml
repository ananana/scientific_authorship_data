<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Traversing Knowledge Graphs in Vector Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
							<email>kguu@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
							<email>millerjp@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Traversing Knowledge Graphs in Vector Space</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Path queries on a knowledge graph can be used to answer compositional questions such as &quot;What languages are spoken by people living in Lisbon?&quot;. However, knowledge graphs often have missing facts (edges) which disrupts path queries. Recent models for knowledge base completion impute missing facts by embedding knowledge graphs in vector spaces. We show that these models can be recursively applied to answer path queries, but that they suffer from cascading errors. This motivates a new &quot;compositional&quot; training objective, which dramatically improves all models&apos; ability to answer path queries, in some cases more than doubling accuracy. On a standard knowledge base completion task, we also demonstrate that com-positional training acts as a novel form of structural regularization, reliably improving performance across all base models (reducing errors by up to 43%) and achieving new state-of-the-art results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Broad-coverage knowledge bases such as Free- base ( <ref type="bibr" target="#b2">Bollacker et al., 2008</ref>) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete cov- erage ( <ref type="bibr" target="#b12">Min et al., 2013</ref>). For example, as of May 2015, Freebase has an entity Tad Lincoln (Abra- ham Lincoln's son), but does not have his ethnic- ity. An elegant solution to incompleteness is using vector space representations: Controlling the di- mensionality of the vector space forces generaliza- tion to new facts ( <ref type="bibr" target="#b14">Nickel et al., 2011;</ref><ref type="bibr" target="#b15">Nickel et al., 2012;</ref><ref type="bibr" target="#b21">Socher et al., 2013;</ref><ref type="bibr" target="#b19">Riedel et al., 2013;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2015)</ref>. In the example, we would hope to infer Tad's ethnicity from the ethnicity of his parents. However, what is missing from these vector space models is the original strength of knowledge bases: the ability to support compositional queries <ref type="bibr" target="#b24">(Ullman, 1985)</ref>. For example, we might ask what the ethnicity of Abraham Lincoln's daugh- ter would be. This can be formulated as a path query on the knowledge graph, and we would like a method that can answer this efficiently, while generalizing over missing facts and even missing or hypothetical entities (Abraham Lincoln did not in fact have a daughter).</p><p>In this paper, we present a scheme to answer path queries on knowledge bases by "composi- tionalizing" a broad class of vector space mod- els that have been used for knowledge base com- pletion (see <ref type="figure" target="#fig_0">Figure 1)</ref>. At a high level, we inter- pret the base vector space model as implementing a soft edge traversal operator. This operator can then be recursively applied to predict paths. Our interpretation suggests a new compositional train- ing objective that encourages better modeling of paths. Our technique is applicable to a broad class of composable models that includes the bilinear model <ref type="bibr" target="#b14">(Nickel et al., 2011</ref>) and <ref type="bibr">TransE (Bordes et al., 2013)</ref>.</p><p>We have two key empirical findings: First, we show that compositional training enables us to answer path queries up to at least length 5 by substantially reducing cascading errors present in the base vector space model. Second, we find that somewhat surprisingly, compositional train- ing also improves upon state-of-the-art perfor- mance for knowledge base completion, which is a special case of answering unit length path queries. Therefore, compositional training can also be seen as a new form of structural regularization for ex- isting models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>We now give a formal definition of the task of an- swering path queries on a knowledge base. Let E be a set of entities and R be a set of binary relations. A knowledge graph G is defined as a set of triples of the form (s, r, t) where s, t ∈ E and r ∈ R. An example of a triple in Freebase is (tad lincoln, parents, abraham lincoln).</p><p>A path query q consists of an initial anchor en- tity, s, followed by a sequence of relations to be traversed, p = (r 1 , . . . , r k ). The answer or deno- tation of the query, q, is the set of all entities that can be reached from s by traversing p. Formally, this can be defined recursively:</p><formula xml:id="formula_0">s def = {s},<label>(1)</label></formula><formula xml:id="formula_1">q/r def = {t : ∃s ∈ q, (s, r, t) ∈ G} . (2)</formula><p>For example, tad lincoln/parents/location is a query q that asks: "Where did Tad Lincoln's par- ents live?". For evaluation (see Section 5 for details), we de- fine the set of candidate answers to a query C(q) as the set of all entities that "type match", namely those that participate in the final relation of q at least once; and let N (q) be the incorrect answers:</p><formula xml:id="formula_2">C (s/r 1 / · · · /r k ) def = {t | ∃e, (e, r k , t) ∈ G} (3) N (q) def = C (q) \q.<label>(4)</label></formula><p>Knowledge base completion. Knowledge base completion (KBC) is the task of predicting whether a given edge (s, r, t) belongs in the graph or not. This can be formulated as a path query q = s/r with candidate answer t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Compositionalization</head><p>In this section, we show how to compositional- ize existing KBC models to answer path queries. We start with a motivating example in Section 3.1, then present the general technique in Section 3.2. This suggests a new compositional training objec- tive, described in Section 3.3. Finally, we illus- trate the technique for several more models in Sec- tion 3.4, which we use in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Example</head><p>A common vector space model for knowledge base completion is the bilinear model <ref type="bibr" target="#b14">(Nickel et al., 2011</ref>). In this model, we learn a vector x e ∈ R d for each entity e ∈ E and a matrix W r ∈ R d×d for each relation r ∈ R. Given a query s/r (ask- ing for the set of entities connected to s via relation r), the bilinear model scores how likely t ∈ s/r holds using</p><formula xml:id="formula_3">score(s/r, t) = x s W r x t .<label>(5)</label></formula><p>To motivate our compositionalization tech- nique, take d = |E| and suppose W r is the ad- jacency matrix for relation r and entity vector x e is the indicator vector with a 1 in the entry corre- sponding to entity e. Then, to answer a path query q = s/r 1 / . . . /r k , we would then compute</p><formula xml:id="formula_4">score(q, t) = x s W r 1 . . . W r k x t .<label>(6)</label></formula><p>It is easy to verify that the score counts the number of unique paths between s and t following rela- tions r 1 / . . . /r k . Hence, any t with positive score is a correct answer (q = {t : score(q, t) &gt; 0}). Let us interpret (6) recursively. The model be- gins with an entity vector x s , and sequentially applies traversal operators T r i (v) = v W r i for each r i . Each traversal operation results in a new "set vector" representing the entities reached at that point in traversal (corresponding to the nonzero entries of the set vector). Finally, it ap- plies the membership operator M(v, x t ) = v x t to check if t ∈ s/r 1 / . . . /r k . Writing graph traversal in this way immediately suggests a useful generalization: take d much smaller than |E| and learn the parameters W r and x e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">General technique</head><p>The strategy used to extend the bilinear model of (5) to the compositional model in (6) can be ap- plied to any composable model: namely, one that has a scoring function of the form:</p><formula xml:id="formula_5">score(s/r, t) = M(T r (x s ), x t )<label>(7)</label></formula><p>for some choice of membership operator M :</p><formula xml:id="formula_6">R d × R d → R and traversal operator T r : R d → R d .</formula><p>We can now define the vector denotation of a query q V analogous to the definition of q in (1) and <ref type="formula">(2)</ref>:</p><formula xml:id="formula_7">s V def = x s ,<label>(8)</label></formula><formula xml:id="formula_8">q/r V def = T r (q V ) .<label>(9)</label></formula><p>The score function for a compositionalized model is then</p><formula xml:id="formula_9">score(q, t) = M(q V , t V ).<label>(10)</label></formula><p>We would like q V to approximately represent the set q in the sense that for every e ∈ q, M (q V , e V ) is larger than the values for e ∈ q. Of course it is not possible to represent all sets perfectly, but in the next section, we present a training objective that explicitly optimizes T and M to preserve path information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compositional training</head><p>The score function in (10) naturally suggests a new compositional training objective. Let</p><formula xml:id="formula_10">{(q i , t i )} N i=1</formula><p>denote a set of path query training examples with path lengths ranging from 1 to L. We minimize the following max-margin objective:</p><formula xml:id="formula_11">J(Θ) = N i=1 t ∈N (q i ) 1 − margin(q i , t i , t ) + , margin(q, t, t ) = score(q, t) − score(q, t ),</formula><p>where the parameters are the membership opera- tor, the traversal operators, and the entity vectors:</p><formula xml:id="formula_12">Θ = {M} ∪ {T r : r ∈ R} ∪ x e ∈ R d : e ∈ E .</formula><p>This objective encourages the construction of "set vectors": because there are path queries of different lengths and types, the model must learn to produce an accurate set vector q V after any sequence of traversals. Another perspective is that each traversal operator is trained such that its transformation preserves information in the set vector which might be needed in subsequent traversal steps.</p><p>In contrast, previously proposed training objec- tives for knowledge base completion only train on queries of path length 1. We will refer to this spe- cial case as single-edge training.</p><p>In Section 5, we show that compositional train- ing leads to substantially better results for both path query answering and knowledge base com- pletion. In Section 6, we provide insight into why.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other composable models</head><p>There are many possible candidates for T and M. For example, T could be one's favorite neural net- work mapping from R d to R d . Here, we focus on two composable models that were both recently shown to achieve state-of-the-art performance on knowledge base completion.</p><p>TransE. The TransE model of <ref type="bibr" target="#b3">Bordes et al. (2013)</ref> uses the scoring function</p><formula xml:id="formula_13">score(s/r, t) = −−x s + w r − x t 2 2 . (11)</formula><p>where x s , w r and x t are all d-dimensional vectors.</p><p>In this case, the model can be expressed using membership operator</p><formula xml:id="formula_14">M(v, x t ) = −−v − x t 2 2 (12)</formula><p>and traversal operator T r (x s ) = x s + w r . Hence, TransE can handle a path query q = s/r 1 /r 2 / · · · /r k using</p><formula xml:id="formula_15">score(q, t) = −−x s + w r 1 + · · · + w r k − x t 2 2 .</formula><p>We visualize the compositional TransE model in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>Bilinear-Diag. The Bilinear-Diag model of <ref type="bibr" target="#b25">Yang et al. (2015)</ref> is a special case of the bilinear model with the relation matrices constrained to be diagonal. Alternatively, the model can be viewed as a variant of TransE with multiplicative interac- tions between entity and relation vectors.</p><p>Not all models can be compositionalized. It is important to point out that some models are not naturally composable-for example, the latent feature model of <ref type="bibr" target="#b19">Riedel et al. (2013)</ref> and the neu- ral tensor network of <ref type="bibr" target="#b21">Socher et al. (2013)</ref>. These approaches have scoring functions which combine s, r and t in a way that does not involve an inter- mediate vector representing s/r alone without t, so they do not decompose according to <ref type="bibr">(7)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet Freebase</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation</head><p>We use AdaGrad ( <ref type="bibr" target="#b7">Duchi et al., 2010</ref>) to optimize J(Θ), which is in general non-convex. Initial- ization scale, mini-batch size and step size were cross-validated for all models. We initialize all parameters with i.i.d. Gaussians of variance 0.1 in every entry, use a mini-batch size of 300 examples, and a step size in [0.001, 0.1] (chosen via cross- validation) for all of the models. For each exam- ple q, we sample 10 negative entities t ∈ N (q).</p><p>During training, all of the entity vectors are con- strained to lie on the unit ball, and we clipped the gradients to the median of the observed gradients if the update exceeded 3 times the median. We first train on path queries of length 1 until convergence and then train on all path queries until convergence. This guarantees that the model mas- ters basic edges before composing them to form paths. When training on path queries, we explic- itly parameterize inverse relations. For the bilinear model, we initialize W r −1 with W r . For TransE, we initialize w r −1 with −w r . For Bilinear-Diag, we found initializing w r −1 with the exact inverse 1/w r is numerically unstable, so we instead ran- domly initialize w r −1 with i.i.d Gaussians of vari- ance 0.1 in every entry. Additionally, for the bi- linear model, we replaced the sum over N (q i ) in the objective with a max since it yielded slightly higher accuracy. Our models are implemented us- ing Theano ( <ref type="bibr" target="#b0">Bastien et al., 2012;</ref><ref type="bibr" target="#b1">Bergstra et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets</head><p>In Section 4.1, we describe two standard knowl- edge base completion datasets. These consist of single-edge queries, so we call them base datasets. In Section 4.2, we generate path query datasets from these base datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Base datasets</head><p>Our experiments are conducted using the sub- sets of WordNet and Freebase from <ref type="bibr" target="#b21">Socher et al. (2013)</ref>. The statistics of these datasets and their splits are given in <ref type="table">Table 1</ref>.</p><p>The WordNet and Freebase subsets exhibit sub- stantial differences that can influence model per- formance. The Freebase subset is almost bipartite with most of the edges taking the form (s, r, t) for some person s, relation r and property t. In Word- Net, both the source and target entities are arbi- trary words.</p><p>Both the raw WordNet and Freebase contain many relations that are almost perfectly correlated with an inverse relation. For example, WordNet contains both has part and part of, and Freebase contains both parents and children. At test time, a query on an edge (s, r, t) is easy to answer if the inverse triple (t, r −1 , s) was observed in the train- ing set. Following <ref type="bibr" target="#b21">Socher et al. (2013)</ref>, we ac- count for this by excluding such "trivial" queries from the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Path query datasets</head><p>Given a base knowledge graph, we generate path queries by performing random walks on the graph. If we view compositional training as a form of reg- ularization, this approach allows us to generate ex- tremely large amounts of auxiliary training data. The procedure is given below.</p><p>Let G train be the training graph, which consists only of the edges in the training set of the base dataset. We then repeatedly generate training ex- amples with the following procedure:</p><p>1. Uniformly sample a path length L ∈ {1, . . . , L max }, and uniformly sample a start- ing entity s ∈ E.</p><p>2. Perform a random walk beginning at entity s and continuing L steps.</p><p>(a) At step i of the walk, choose a relation r i uniformly from the set of relations in- cident on the current entity e.</p><p>(b) Choose the next entity uniformly from the set of entities reachable via r i .</p><p>In practice, we do not sample paths of length 1 and instead directly add all of the edges from G train to the path query dataset.</p><p>To generate a path query test set, we repeat the above procedure except using the graph G full , which is G train plus all of the test edges from the base dataset. Then we remove any queries from the test set that also appeared in the training set. The statistics for the path query datasets are pre- sented in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Main results</head><p>We evaluate the models derived in Section 3 on two tasks: path query answering and knowledge base completion. On both tasks, we show that the compositional training strategy proposed in Sec- tion 3.3 leads to substantial performance gains over standard single-edge training. We also com- pare directly against the KBC results of <ref type="bibr" target="#b21">Socher et al. (2013)</ref>, demonstrating that previously inferior models now match or outperform state-of-the-art models after compositional training.</p><p>Evaluation metric. Numerous metrics have been used to evaluate knowledge base queries, in- cluding hits at 10 (percentage of correct answers ranked in the top 10) and mean rank. We evaluate on hits at 10, as well as a normalized version of mean rank, mean quantile, which better accounts for the total number of candidates. For a query q, the quantile of a correct answer t is the fraction of incorrect answers ranked after t: |{t ∈ N (q) : score(q, t ) &lt; score(q, t)}| |N (q)|</p><p>The quantile ranges from 0 to 1, with 1 being opti- mal. Mean quantile is then defined to be the aver- age quantile score over all examples in the dataset.</p><p>To illustrate why normalization is important, con- sider a set of queries on the relation gender. A model that predicts the incorrect gender on ev- ery query would receive a mean rank of 2 (since there are only 2 candidate answers), which is fairly good in absolute terms, whereas the mean quantile would be 0, rightfully penalizing the model. As a final note, several of the queries in the Freebase path dataset are "type-match trivial" in the sense that all of the type matching candidates C(q) are correct answers to the query. In this case, mean quantile is undefined and we exclude such queries from evaluation. <ref type="table" target="#tab_2">Table 2</ref> shows that compositional training improves path query- ing performance across all models and metrics on both datasets, reducing error by up to 76.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview. The upper half of</head><p>The lower half of <ref type="table" target="#tab_2">Table 2</ref> shows that surpris- ingly, compositional training also improves per- formance on knowledge base completion across almost all models, metrics and datasets. On Word- Net, TransE benefits the most, with a 43.3% re- duction in error. On Freebase, Bilinear benefits the most, with a 38.8% reduction in error.</p><p>In terms of mean quantile, the best overall model is TransE (COMP). In terms of hits at 10, the best model on WordNet is Bilinear (COMP), while the best model on Freebase is TransE (COMP).</p><p>Deduction and Induction. <ref type="table" target="#tab_5">Table 3</ref> takes a deeper look at performance on path query answer- ing. We divided path queries into two subsets: de- duction and induction. The deduction subset con- sists of queries q = s/p where the source and tar- get entities q are connected via relations p in the training graph G train , but the specific query q was never seen during training. Such queries can be answered by performing explicit traversal on the training graph, so this subset tests a model's abil- ity to approximate the underlying training graph and predict the existence of a path from a collec- tion of single edges. The induction subset consists of all other queries. This means that at least one edge was missing on all paths following p from source to target in the training graph. Hence, this subset tests a model's generalization ability and its robustness to missing edges.</p><p>Performance on the deduction subset of the dataset is disappointingly low for models trained with single-edge training: they struggle to answer path queries even when all edges in the path query have been seen at training time. Compositional training dramatically reduces these errors, some- times doubling mean quantile. In Section 6, we analyze how this might be possible. After com- positional training, performance on the harder in- duction subset is also much stronger. Even when edges are missing along a path, the models are able to infer them.</p><p>Interpretable queries. Although our path datasets consists of random queries, both datasets contain a large number of useful, interpretable queries. Results on a few illustrative examples are shown in <ref type="table" target="#tab_3">Table 4</ref>.    Meanings of each query (descending): "What professions are there at X's institution?"; "What is the religion of X's parents?"; "What are the ethnicities of people from the same country as X?"; "What types of parts does X have?"; and the transitive "What is X a type of?". (Note that a relation r and its inverse r −1 do not necessarily cancel out if r is not a one-to-one mapping. For example, X/institution/institution −1 denotes the set of all people who work at the institution X works at, which is not just X.)  Comparison with <ref type="bibr" target="#b21">Socher et al. (2013)</ref>. Here, we measure performance on the KBC task in terms of the accuracy metric of <ref type="bibr" target="#b21">Socher et al. (2013)</ref>. This evaluation involves sampled negatives, and is hence noisier than mean quantile, but makes our results directly comparable to <ref type="bibr" target="#b21">Socher et al. (2013)</ref>. Our results show that previously inferior models such as the bilinear model can outperform state- of-the-art models after compositional training. <ref type="bibr" target="#b21">Socher et al. (2013)</ref> proposed parametrizing each entity vector as the average of vectors of words in the entity (w tad lincoln = 1 2 (w tad + w lincoln ), and pretraining these word vectors us- ing the method of . <ref type="table">Table 5</ref> reports results when using this approach in con- junction with compositional training. We initial- ized all models with word vectors from <ref type="bibr" target="#b17">Pennington et al. (2014)</ref>. We found that composition- ally trained models outperform the neural tensor network (NTN) on WordNet, while being only slightly behind on Freebase. (We did not use word vectors in any of our other experiments.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path query task</head><p>When the strategy of averaging word vectors to form entity vectors is not applied, our composi- tional models are significantly better on WordNet and slightly better on Freebase. It is worth noting that in many domains, entity names are not lexi- cally meaningful, so word vector averaging is not</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet</head><p>Freebase EV WV EV WV NTN 70.6 86.2 87.2 90.0 Bilinear COMP 77.6 87.6 86.1 89.4 TransE COMP 80.3 84.9 87.6 89.6 <ref type="table">Table 5</ref>: Model performance in terms of accu- racy. EV: entity vectors are separate (initialized randomly); WV: entity vectors are average of word vectors (initialized with pretrained word vectors). always meaningful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section, we try to understand why com- positional training is effective. For concrete- ness, everything is described in terms of the bi- linear model. We will refer to the compositionally trained model as COMP, and the model trained with single-edge training as SINGLE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Why does compositional training improve path query answering?</head><p>It is tempting to think that if SINGLE has accurately modeled individual edges in a graph, it should ac- curately model the paths that result from those edges. This intuition turns out to be incorrect, as revealed by SINGLE's relatively weak performance on the path query dataset. We hypothesize that this is due to cascading errors along the path. For a given edge (s, r, t) on the path, single-edge train- ing encourages x t to be closer to x s W r than any other incorrect x t . However, once this is achieved by a margin of 1, it does not push x t any closer to x s W r . The remaining discrepancy is noise which gets added at each step of path traversal. This is illustrated schematically in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>To observe this phenomenon empirically, we examine how well a model handles each interme- diate step of a path query. We can do this by measuring the reconstruction quality (RQ) of the set vector produced after each traversal operation. Since each intermediate stage is itself a valid path query, we define RQ to be the average quantile over all entities that belong in q:</p><formula xml:id="formula_17">RQ (q) = 1 |q| t∈q quantile (q, t)<label>(14)</label></formula><p>When all entities in q are ranked above all in- correct entities, RQ is 1. In <ref type="figure" target="#fig_3">Figure 3</ref>, we illustrate how RQ changes over the course of a query. Given the nature of cascading errors, it might seem reasonable to address the problem by adding a term to our objective which explicitly encour- ages x s W r to be as close as possible to x t . With this motivation, we tried adding λx</p><formula xml:id="formula_18">s W r − x t 2 2</formula><p>term to the objective of the bilinear model and a λx s + w r − x t 2 2 term to the objective of TransE. We experimented with different settings of λ over the range <ref type="bibr">[0.001, 100]</ref>. In no case did this addi- tional 2 term improve SINGLE's performance on the path query or single edge dataset. These re- sults suggest that compositional training is a more effective way to combat cascading errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Why does compositional training</head><p>improve knowledge base completion? <ref type="table" target="#tab_2">Table 2</ref> reveals that COMP also performs better on the single-edge task of knowledge base comple- tion. This is somewhat surprising, since SINGLE is trained on a training set which distributionally matches the test set, whereas COMP is not. How- ever, COMP's better performance on path queries suggests that there must be another factor at play. At a high level, training on paths must be provid- ing some form of structural regularization which reduces cascading errors. Indeed, paths in a knowledge graph have proven to be important fea- tures for predicting the existence of single edges <ref type="bibr" target="#b11">(Lao et al., 2011;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2015)</ref>. For example, consider the following Horn clause:</p><p>parents (x, y) ∧ location (y, z) ⇒ place of birth (x, z) , Correspondingly, the set of 5 highest scoring entities computed at each step using COMP (green) is significiantly more accurate than the set given by SINGLE (blue). Correct entities are bolded.</p><p>which states that if x has a parent with location z, then x has place of birth z. The body of the Horn clause expresses a path from x to z. If COMP models the path better, then it should be better able to use that knowledge to infer the head of the Horn clause. More generally, consider Horn clauses of the form p ⇒ r, where p = r 1 / . . . /r k is a path type and r is the relation being predicted. Let us focus on Horn clauses with high precision as defined by:</p><formula xml:id="formula_19">prec(p) = |p ∩ r| |p| , (15)</formula><p>where p is the set of entity pairs connected by p, and similarly for r. Intuitively, one way for the model to implicitly learn and exploit such a Horn clause would be to satisfy the following two criteria:</p><p>1. The model should ensure a consistent spa- tial relationship between entity pairs that are related by the path type p; that is, keeping x s W r 1 . . . W r k close to x t for all valid (s, t) pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The model's representation of the path type p</head><p>and relation r should capture that spatial re- lationship; that is,</p><formula xml:id="formula_20">x s W r 1 . . . W r k ≈ x t im- plies x s W r ≈ x t , or simply W r 1 . . . W r k ≈ W r .</formula><p>We have already seen empirically that SINGLE does not meet criterion 1, because cascading errors cause it to put incorrect entity vectors x t closer to x s W r 1 . . . W r k than the correct entity. COMP mitigates these errors.</p><p>To empirically verify that COMP also does a bet- ter job of meeting criterion 2, we perform the following: for a path type p and relation r, de- fine dist(p, r) to be the angle between their corre- sponding matrices (treated as vectors in R d 2 ). This is a natural measure because x s W r x t computes the matrix inner product between W r and x s x t . Hence, any matrix with small distance from W r will produce nearly the same scores as W r for the same entity pairs.</p><p>If COMP is better at capturing the correlation be- tween p and r, then we would expect that when prec(p) is high, compositional training should shrink dist(p, r) more. To confirm this hypothe- sis, we enumerated over all 676 possible paths of length 2 (including inverted relations), and exam- ined the proportional reduction in dist(p, r) caused by compositional training, <ref type="figure" target="#fig_4">Figure 4</ref> shows that higher precision paths indeed correspond to larger reductions in dist(p, r).</p><formula xml:id="formula_21">∆dist(p, r) = dist COMP (p, r) − dist SINGLE (p, r) dist SINGLE (p, r) .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Knowledge base completion with vector space models. Many models have been proposed for knowledge base completion, including those re- viewed in Section 3.4 <ref type="bibr" target="#b14">(Nickel et al., 2011;</ref><ref type="bibr" target="#b3">Bordes et al., 2013;</ref><ref type="bibr" target="#b25">Yang et al., 2015;</ref><ref type="bibr" target="#b21">Socher et al., 2013)</ref>. <ref type="bibr" target="#b6">Dong et al. (2014)</ref> demonstrated that KBC models can improve the quality of relation extrac- tion by serving as graph-based priors. <ref type="bibr" target="#b19">Riedel et al. (2013)</ref> showed that such models can be also be directly used for open-domain relation extraction. Our compositional training technique is an orthog- onal improvement that could help any composable model. Here r = nationality. Each box plot shows the min, max, and first and third quartiles of ∆dist(p, r). As hypothesized, com- positional training results in large decreases in dist(p, r) for high precision paths p, modest de- creases for low precision paths, and little to no de- creases for irrelevant paths.</p><p>plored the ability of tensors to simulate logical cal- culi. <ref type="bibr" target="#b5">Bowman et al. (2014)</ref> showed that recursive neural networks can learn to distinguish impor- tant semantic relations.  found that compositional models were powerful enough to describe and retrieve images. We demonstrate that compositional representa- tions are also useful in the context of knowledge base querying and completion. In the aforemen- tioned work, compositional models produce vec- tors which represent truth values, sentiment or im- age features. In our approach, vectors represent sets of entities constituting the denotation of a knowledge base query.</p><p>Path modeling. Numerous methods have been proposed to leverage path information for knowl- edge base completion and question answering. <ref type="bibr" target="#b16">Nickel et al. (2014)</ref> proposed combining low-rank models with sparse path features. <ref type="bibr" target="#b10">Lao and Cohen (2010)</ref> used random walks as features and <ref type="bibr" target="#b8">Gardner et al. (2014)</ref> extended this approach by us- ing vector space similarity to govern random walk probabilities. <ref type="bibr" target="#b13">Neelakantan et al. (2015)</ref> addressed the problem of path sparsity by embedding paths using a recurrent neural network. <ref type="bibr" target="#b18">Perozzi et al. (2014)</ref> sampled random walks on social networks as training examples, with a different goal to clas- sify nodes in the network. <ref type="bibr" target="#b4">Bordes et al. (2014)</ref> em- bed paths as a sum of relation vectors for question answering. Our approach is unique in modeling the denotation of each intermediate step of a path query, and using this information to regularize the spatial arrangement of entity vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We introduced the task of answering path queries on an incomplete knowledge base, and presented a general technique for compositionalizing a broad class of vector space models. Our experiments show that compositional training leads to state-of- the-art performance on both path query answering and knowledge base completion.</p><p>There are several key ideas from this paper: reg- ularization by augmenting the dataset with paths, representing sets as low-dimensional vectors in a context-sensitive way, and performing function composition using vectors. We believe these three could all have greater applicability in the develop- ment of vector space models for knowledge repre- sentation and inference.</p><p>Reproducibility Our code, data, and exper- iments are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0xfcace41fdeec45f3bc6ddf31107b829f.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We propose performing path queries such as tad lincoln/parents/location ("Where are Tad Lincoln's parents located?") in a parallel low-dimensional vector space. Here, entity sets (boxed) are represented as real vectors, and edge traversal is driven by vector-to-vector transformations (e.g., matrix multiplication).</figDesc><graphic url="image-1.png" coords="1,340.96,204.54,150.90,140.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Bilinear</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cascading errors visualized for TransE. Each node represents the position of an entity in vector space. The relation parent is ideally a simple horizontal translation, but each traversal introduces noise. The red circle is where we expect Tad's parent to be. The red square is where we expect Tad's grandparent to be. Dotted red lines show that error grows larger as we traverse farther away from Tad. Compositional training pulls the entity vectors closer to the ideal arrangement.</figDesc><graphic url="image-2.png" coords="7,318.16,62.81,196.50,84.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reconstruction quality (RQ) at each step of the query tad lincoln/parents/place of birth/ place of birth −1 /profession. COMP experiences significantly less degradation in RQ as path length increases. Correspondingly, the set of 5 highest scoring entities computed at each step using COMP (green) is significiantly more accurate than the set given by SINGLE (blue). Correct entities are bolded.</figDesc><graphic url="image-3.png" coords="8,72.00,62.81,222.25,165.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Distributional compositional semantics.Figure 4 :</head><label>4</label><figDesc>Figure 4: We divide paths of length 2 into high precision (&gt; 0.3), low precision (≤ 0.3), and not co-occuring with r. Here r = nationality. Each box plot shows the min, max, and first and third quartiles of ∆dist(p, r). As hypothesized, compositional training results in large decreases in dist(p, r) for high precision paths p, modest decreases for low precision paths, and little to no decreases for irrelevant paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Bilinear-Diag TransE Path query task SINGLE COMP (%red) SINGLE COMP (%red) SINGLE COMP (%red)</head><label></label><figDesc></figDesc><table>WordNet 
MQ 
84.7 
89.4 
30.7 
59.7 
90.4 
76.2 
83.7 
93.3 
58.9 
H@10 
43.6 
54.3 
19.0 
7.9 
31.1 
25.4 
13.8 
43.5 
34.5 

Freebase 
MQ 
58.0 
83.5 
60.7 
57.9 
84.8 
63.9 
86.2 
88 
13.0 
H@10 
25.9 
42.1 
21.9 
23.1 
38.6 
20.2 
45.4 
50.5 
9.3 
KBC task 
SINGLE COMP (%red) SINGLE COMP (%red) SINGLE COMP (%red) 

WordNet 
MQ 
76.1 
82.0 
24.7 
76.5 
84.3 
33.2 
75.5 
86.1 
43.3 
H@10 
19.2 
27.3 
10.0 
12.9 
14.4 
1.72 
4.6 
16.5 
12.5 

Freebase 
MQ 
85.3 
91.0 
38.8 
84.6 
89.1 
29.2 
92.7 
92.8 
1.37 
H@10 
70.2 
76.4 
20.8 
63.2 
67.0 
10.3 
78.8 
78.6 
-0.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Path query answering and knowledge base completion. We compare the performance of 
single-edge training (SINGLE) vs compositional training (COMP). MQ: mean quantile, H@10: hits at 10, 
%red: percentage reduction in error. 

Interpretable Queries 
Bilinear SINGLE Bilinear COMP 

X/institution/institution −1 /profession 

50.0 
93.6 

X/parents/religion 

81.9 
97.1 

X/nationality/nationality −1 /ethnicity −1 

68.0 
87.0 

X/has part/has instance −1 

92.6 
95.1 

X/type of/type of/type of 

72.8 
79.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Path query performance (mean quantile) on a selection of interpretable queries. We compare Bilinear SINGLE and Bilinear COMP.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Deduction and induction. We compare 
mean quantile performance of single-edge training 
(SINGLE) vs compositional training (COMP). Length 
1 queries are excluded. 

</table></figure>

			<note place="foot" n="3">. Output a query-answer pair, (q, t), where q = s/r 1 / · · · /r L and t is the final entity of the random walk.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Can recursive neural tensor networks learn logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards a formal distributional semantics: Simulating logical calculi with tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5823</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Factorizing YAGO. In World Wide Web</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reducing the rank in relational factorization models by including observable patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1179" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Implementation of logical query languages for databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems (TODS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="289" to="321" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
