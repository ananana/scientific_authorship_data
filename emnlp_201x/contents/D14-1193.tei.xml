<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Muli-label Text Categorization with Hidden Components</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
							<email>li.l@pku.edu.cn, zhlongk@qq.com, wanghf@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Muli-label Text Categorization with Hidden Components</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1816" to="1821"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-label text categorization (MTC) is supervised learning, where a document may be assigned with multiple categories (labels) simultaneously. The labels in the MTC are correlated and the correlation results in some hidden components, which represent the &quot;share&quot; variance of correlated labels. In this paper, we propose a method with hidden components for MTC. The proposed method employs PCA to capture the hidden components, and incorporates them into a joint learning framework to improve the performance. Experiments with real-world data sets and evaluation metrics validate the effectiveness of the proposed method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many real-world text categorization applications are multi-label text categorization <ref type="bibr" target="#b6">(Srivastava and Zane-Ulman, 2005;</ref><ref type="bibr" target="#b1">Katakis et al., 2008;</ref><ref type="bibr">Rubin et al., 2012;</ref><ref type="bibr">Nam et al., 2013)</ref>, where a docu- ments is usually assigned with multiple labels si- multaneously. For example, as <ref type="figure" target="#fig_1">figure 1</ref> shows, a newspaper article concerning global warming can be classified into two categories, Environmen- t, and Science simultaneously. Let X = R d be the documents corpus, and Y = {0, 1} m be the label space with m labels. We denote by {(x <ref type="bibr">1</ref> x 1 x 1 , y 1 y 1 y 1 ), (x 2 x 2 x 2 , y 2 y 2 y 2 ), ..., (x n x n x n , y n y n y n )} the training set of n documents. Each document is denoted by a vec- tor x x x i = [x i,1 , x i,2 , ..., x i,d ] of d dimensions. The labeling of the i-th document is denoted by vector y y y i = [y i,1 , y i,2 , ..., y i,m ], where y il is 1 when the i-th document has the l-th label and 0 otherwise. The goal is to learn a function f f f : X → Y. Gener- ally, we can assume f f f consists of m functions, one for a label.  The labels in the MLC are correlated. For ex- ample, a "politics" document is likely to be an "e- conomic" document simultaneously, but likely not to be a "literature" document. According to the latent variable model ( <ref type="bibr" target="#b7">Tabachnick et al., 2001</ref>), the labels with correlation result in some hidden components, which represent the "share" variance of correlated labels. Intuitively, if we can capture and utilize these hidden components in MTC, the performance will be improved. To implement this idea, we propose a multi-label text categorization method with hidden components, which employ PCA to capture the hidden components, and then incorporates these hidden components into a joint learning framework. Experiments with various da- ta sets and evaluation metrics validate the values of our method. The research close to our work is ML-LOC (Multi-Label learning using LOcal Cor- relation) in <ref type="bibr" target="#b0">(Huang and Zhou, 2012</ref>). The differ-ences between ours and ML-LOC is that ML-LOC employs the cluster method to gain the local cor- relation, but we employ the PCA to obtain the hid- den code. Meanwhile, ML-LOC uses the linear programming in learning the local code, but we employ the gradient descent method since we add non-linear function to the hidden code.</p><p>The rest of this paper is organized as follows. Section 2 presents the proposed method. We con- duct experiments to demonstrate the effectiveness of the proposed method in section 3. Section 4 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Capturing Hidden Component via Principle Component Analysis</head><p>The first step of the proposed method is to capture hidden components of training instances. Here we employ Principal component analysis (PCA). This is because PCA is a well-known statistical tool that converts a set of observations of possibly correlat- ed variables into a set of values of linearly uncorre- lated variables called principle components. These principle components represent the inner structure of the correlated variables.</p><p>In this paper, we directly employ PCA to con- vert labels of training instances into their principle components, and take these principle components as hidden components of training instances. We denote by h h h i the hidden components of the i-th in- stance captured by PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Learning Framework</head><p>We expand the original feature representation of the instance x x x i by its hidden component code vec- tor c c c i . For simplicity, we use logistic regression as the motivating example. Let w w w l denote weights in the l-th function f l , consisting of two parts: 1) w w w x l is the part involving the instance features. 2) w w w c l is the part involving the hidden component codes. Hence f l is:</p><formula xml:id="formula_0">f l (x x x, c c c) = 1 1 + exp(−x x x T w w w x l − c c c T w w w c l ) (1)</formula><p>where C C C is the code vectors set of all training in- stances.</p><p>The natural choice of the code vector c c c is h h h. However, when testing an instance, the labeling is unknown (exactly what we try to predict), conse- quently we can not capture h h h with PCA to replace the code vector c c c in the prediction function Eq.(1).</p><p>Therefore, we assume a linear transformation M M M from the training instances to their independent components, and use M M Mx x x as the approximate in- dependent component. For numerical stability, we add a non-linear function (e.g., the tanh function) to M M Mx x x. This is formulated as follows.</p><formula xml:id="formula_1">c c c = tanh(M M Mx x x)<label>(2)</label></formula><p>Aiming to the discrimination fitting and the in- dependent components encoding, we optimize the following optimization problem.</p><formula xml:id="formula_2">min W W W ,C n i=1 m l=1 (x x x i , c c c i , y il , f l ) + λ 1 Ω(f f f ) +λ 2 Z(C C C)<label>(3)</label></formula><p>The first term of Eq. <ref type="formula" target="#formula_2">(3)</ref> is the loss function. is the loss function defined on the training data, and W W W denotes all weights in the our model, i.e., w w w 1 , ..., w w w l , ..., w w w m . Since we utilize the logistic re- gression in our model, the loss function is defined as follows.</p><formula xml:id="formula_3">(x x x, c c c, y, f ) = −ylnf (x x x, c c c) − (1 − y)ln(1 − f (x x x, c c c)) (4)</formula><p>The second term of Eq.(3) Ω is to punish the model complexity, which we use the 2 regular- ization term.</p><formula xml:id="formula_4">Ω(f f f ) = m l=1 ||w w w l || 2 .<label>(5)</label></formula><p>The third term of Eq. <ref type="formula" target="#formula_2">(3)</ref> Z is to enforce the code vector close to the independent component vector. To obtain the goal, we use the least square error between the code vector and the independent com- ponent vector as the third regularized term.</p><formula xml:id="formula_5">Z(C) = n i=1 ||c c c i − h h h i || 2 .<label>(6)</label></formula><p>By substituting the Eq. <ref type="formula" target="#formula_4">(5)</ref> and Eq. <ref type="formula" target="#formula_5">(6)</ref>   <ref type="formula" target="#formula_1">(2)</ref>), we obtain the following optimization problem.</p><formula xml:id="formula_6">min W W W ,M M M n i=1 m l=1 (x x x i , tanh(M M Mx x x i ), y il , f f f ) +λ 1 m l=1 ||w w w l || 2 + λ 2 n i=1 ||M M Mx x x i − h i h i h i || 2<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Alternative Optimization method</head><p>We solve the optimization problem in Eq. <ref type="formula" target="#formula_6">(7)</ref> by the alternative optimization method, which opti- mize one group of the two parameters with the other fixed. When the M M M fixed, the third term of Eq. <ref type="formula" target="#formula_6">(7)</ref> is a constant and thus can be ignored, then Eq. <ref type="formula" target="#formula_6">(7)</ref> can be rewritten as follows.</p><formula xml:id="formula_7">min W W W n i=1 m l=1 (x x x i , tanh(M M Mx x x i ), y il , f l ) +λ 1 m l=1 ||w w w l || 2<label>(8)</label></formula><p>By decomposing Eq. <ref type="formula" target="#formula_7">(8)</ref> based on the label, the e- quation Eq.(8) can be simplified to:</p><formula xml:id="formula_8">min w w w l n i=1 (x x x i , tanh(M M Mx x x i ), y il , f l ) + λ 1 ||w w w l || 2 (9)</formula><p>Eq. <ref type="formula">(9)</ref> is the standard logistic regression, which has many efficient optimization algorithms. When W W W fixed, the second term is constan- t and can be omitted, then Ep. <ref type="formula" target="#formula_6">(7)</ref> can rewritten to Eq.(10). We can apply the gradient descen- t method to optimize this problem.</p><formula xml:id="formula_9">min M M M n i=1 m l=1 (x x x i , tanh(M M Mx x x i ), y il , f l ) +λ 2 n i=1 ||M M Mx x x i − h i h i h i || 2<label>(10)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Metrics</head><p>Compared with the single-label classification, the multi-label setting introduces the additional de- grees of freedom, so that various multi-label eval- uation metrics are requisite. We use three differen- t multi-label evaluation metrics, include the ham- ming loss evaluation metric.</p><p>The hamming loss is defined as the percentage of the wrong labels to the total number of labels.</p><formula xml:id="formula_10">Hammingloss = 1 m |h(x x x)∆y y y|<label>(11)</label></formula><p>where ∆ denotes the symmetric difference of two sets, equivalent to XOR operator in Boolean logic. m denotes the label number.</p><p>The multi-label 0/1 loss, also known as subset accuracy, is the exact match measure as it requires any predicted set of labels h(x x x) to match the true set of labels S exactly. The 0/1 loss is defined as follows:</p><p>0/1loss = I(h(x x x) = y y y)</p><p>Let a j and r j denote the precision and recall for the j-th label. The macro-averaged F is a harmon- ic mean between precision and recall, defined as follows:</p><formula xml:id="formula_12">F = 1 m m i=j 2 * a j * r j a j + r j<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We perform experiments on three MTC data sets: 1) the first data set is slashdot <ref type="bibr" target="#b4">(Read et al., 2011</ref>). The slashdot data set is concerned about science and technology news categorization, which pre- dicts multiply labels given article titles and partial blurbs mined from Slashdot.org. 2) the second da- ta set is medical ( <ref type="bibr" target="#b3">Pestian et al., 2007)</ref>. This data set involves the assignment of ICD-9-CM codes to ra- diology reports. 3) the third data set is tmc2007 (S- rivastava and Zane- <ref type="bibr" target="#b6">Ulman, 2005</ref>). It is concerned about safety report categorization, which is to la- bel aviation safety reports with respect to what types of problems they describe. The characteris- tics of them are shown in <ref type="table">Table 1</ref>, where n denotes the size of the data set, d denotes the dimension of the document instance, and m denotes the number of labels. The measure label cardinality Lcard, which is one of the standard measures of "multi-label- ness", defined as follows, introduced in <ref type="bibr" target="#b8">(Tsoumakas and Katakis, 2007)</ref>.</p><formula xml:id="formula_13">Lcard(D) = n i=1 m j=1 y i j n</formula><p>where D denotes the data set, l i j denotes the j-th label of the i-th instance in the data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compared to Baselines</head><p>To examine the values of the joint learning frame- work, we compare our method to two baselines. The baseline 1 eliminates the PCA, which just adds an extra set of non-linear features. To im- plement this baseline, we only need to set λ 2 = 0. The baseline 2 eliminates the joint learning frame- work. This baseline captures the hidden compo- nent codes with PCA, trains a linear regression model to fit the hidden component codes, and u- tilizes the outputs of the linear regression model as features.</p><p>For the proposed method, we set λ 1 = 0.001 and λ 2 = 0.1. For the baseline 2, we employ l- ogistic regression with 0.001 2 regularization as the base classifier. Evaluations are done in ten- fold cross validation. Note that all of them pro- duce real-valued predictions. A threshold t needs to be used to determine the final multi-label set y y y such that l j ∈ y y y where p j ≥ t. We select threshold t, which makes the Lcard measure of predictions for the training set is closest to the Lcard mea- sure of the training set ( <ref type="bibr" target="#b4">Read et al., 2011</ref>). The threshold t is determined as follows, where D t is the training set and a multi-label model H t pre- dicts for the training set under threshold t.</p><formula xml:id="formula_14">t = argmin t∈[0,1] |Lcard(D t ) − Lcard(H t (D t ))| (14)</formula><p>Table 2 reports our method wins over the base- lines in terms of different evaluation metrics, which shows the values of PCA and our join- t learning framework. The hidden component code only fits the hidden component in the baseline method. The hidden component code obtains bal- ance of fitting hidden component and fitting the training data in the joint learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Compared to Other Methods</head><p>We compare the proposed method to BR, C- C (Read et al., 2011), RAKEL (Tsoumakas and Vlahavas, 2007) and ML-KNN ( <ref type="bibr" target="#b10">Zhang and Zhou, 2007)</ref>. entropy. ML-kNN is an adaption of kNN algorithm for multilabel classification. methods. Binary Revelance (BR) is a simple but effective method that trains binary classifiers for each label independently. BR has a low time complexity but makes an arbitrary assumption that the labels are independent from each other. CC organizes the classifiers along a chain and take predictions pro- duced by the former classifiers as features of the latter classifiers. ML-kNN uses kNN algorithms independently for each label with considering pri- or probabilities. The Label Powerset (LP) method models independencies among labels by treating each label combination as a new class. LP con- sumes too much time, since there are 2 m label combinations with m labels. RAndom K labEL (RAKEL) is an ensemble method of LP. RAKEL learns several LP models with random subsets of size k from all labels, and then uses a vote process to determine the final predictions.</p><p>For our proposed method, we employ the set- up in subsection 3.3. We utilize logistic regression with 0.001 2 regularization as the base classifier for BR, CC and RAKEL. For RAKEL, the num- ber of ensemble is set to the number of label and the size of the label subset is set to 3. For MLKN- N, the number of neighbors used in the k-nearest neighbor algorithm is set to 10 and the smooth pa- rameter is set to 1. Evaluations are done in ten- fold cross validation. We employ the threshold- selection strategy introduced in subsection 3.3 <ref type="table">Table 2</ref> also reports the detailed results in terms of different evaluation metrics. The mean metric value and the standard deviation of each method are listed for each data set. We see our proposed method shows majorities of wining over the other state-of-the-art methods nearly at all data sets un- der hamming loss, 0/1 loss and macro f score. E- specially, under the macro f score, the advantages of our proposed method over the other methods are very clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>Many real-world text categorization applications are multi-label text categorization (MTC), where a documents is usually assigned with multiple labels simultaneously. The key challenge of MTC is the label correlations among labels. In this paper, we propose a MTC method via hidden components to capture the label correlations. The proposed method obtains hidden components via PCA and incorporates them into a joint learning framework. Experiments with various data sets and evaluation metrics validate the effectiveness of the proposed method.</p><p>hamming↓. Lower is better. Dataset slashdot medical tmc2007 Proposed 0.044 ± 0.004 0.010 ± 0.002 0.056 ± 0.002 Baseline1 0.046 ± 0.003• 0.010 ± 0.002 0.056 ± 0.001 Baseline2 0.047 ± 0.003• 0.011 ± 0.001 0.059 ± 0.001• BR 0.058 ± 0.003• 0.010 ± 0.001 0.060 ± 0.001• CC 0.049 ± 0.003• 0.010 ± 0.001 0.058 ± 0.001• RAKEL 0.039 ± 0.002• 0.011 ± 0.002 0.057 ± 0.001 MLKNN 0.067 ± 0.003• 0.016 ± 0.003• 0.070 ± 0.002• 0/1 loss↓. Lower is better. Dataset slashdot medical tmc2007 Proposed 0.600 ± 0.042 0.316 ± 0.071 0.672 ± 0.010 Baseline1 0.615 ± 0.034• 0.324 ± 0.058• 0.672 ± 0.008 Baseline2 0.669 ± 0.039• 0.354 ± 0.062• 0.698 ± 0.007• BR 0.803 ± 0.018• 0.337 ± 0.063• 0.701 ± 0.008• CC 0.657 ± 0.025• 0.337 ± 0.064• 0.687 ± 0.010• RAKEL 0.686 ± 0.024• 0.363 ± 0.064• 0.682 ± 0.009• MLKNN 0.776 ± 0.020• 0.491 ± 0.083• 0.746 ± 0.003• F score↑. Larger is better. Dataset slashdot medical tmc2007 Proposed 0.429 ± 0.026 0.575 ± 0.067 0.587 ± 0.010 Baseline1 0.413 ± 0.032• 0.547 ± 0.056• 0.577 ± 0.011 Baseline2 0.398 ± 0.032• 0.561 ± 0.052• 0.506 ± 0.011• BR 0.204 ± 0.011• 0.501 ± 0.058• 0.453 ± 0.011• CC 0.303 ± 0.022• 0.510 ± 0.052• 0.505 ± 0.011• RAKEL 0.349 ± 0.023• 0.589 ± 0.063• 0.555 ± 0.011• MLKNN 0.297 ± 0.031• 0.410 ± 0.064• 0.431 ± 0.014• <ref type="table">Table 2</ref>: Performance (mean±std.) of our method and baseline in terms of different evaluation metrics.</p><p>•/• indicates whether the proposed method is statistically superior/inferior to baseline (pairwise t-test at 5% significance level). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>f f f</head><label></label><figDesc>= [f 1 , f 2 , ..., f m ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A newspaper article concerning global warming can be classified into two categories, Environment, and Science.</figDesc><graphic url="image-1.png" coords="1,308.41,204.54,216.00,223.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>into Eq.(3) and changing c c c to tanh(M M Mx x x) (Eq.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledge</head><p>We thank anonymous reviewers for their help-ful comments and suggestions. This research was partly supported by National High Tech-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilabel learning by exploiting label correlations locally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilabel text classification for automated tag suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grigorios Tsoumakas, and Ioannis Vlahavas</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Proceedings of the ECML/PKDD</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5419</idno>
		<title level="m">Iryna Gurevych, and Johannes Fürnkranz. 2013. Large-scale multi-label text classification-revisiting neural networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A shared task involving multi-label classification of clinical free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>John P Pestian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Dj Hovermale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Włodzisław</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Padhraic Smyth, and Mark Steyvers. 2012. Statistical topic models for multi-label document classification. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><surname>Timothy N Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chambers</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="157" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discovering recurring anomalies in text reports regarding complex space systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zane-Ulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aerospace Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3853" to="3862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using multivariate statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><forename type="middle">S</forename><surname>Barbara G Tabachnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining (IJDWM)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random k-labelsets: An ensemble method for multilabel classification</title>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2007</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="406" to="417" />
		</imprint>
	</monogr>
	<note>Grigorios Tsoumakas and Ioannis Vlahavas</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ml-knn: A lazy learning approach to multi-label learning. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2038" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
