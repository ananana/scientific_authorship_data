<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Relational Embeddings for Knowledge-based Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
							<email>mcyang@nlp.korea.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Chang</forename><surname>Rim</surname></persName>
							<email>rim@nlp.korea.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Dept. of Computer &amp; Radio Comms. Engineering</orgName>
								<orgName type="institution" key="instit2">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Relational Embeddings for Knowledge-based Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="645" to="650"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Transforming a natural language (NL) question into a corresponding logical form (LF) is central to the knowledge-based question answering (KB-QA) task. Unlike most previous methods that achieve this goal based on mappings between lex-icalized phrases and logical predicates, this paper goes one step further and proposes a novel embedding-based approach that maps NL-questions into LFs for KB-QA by leveraging semantic associations between lexical representations and KB-properties in the latent space. Experimental results demonstrate that our proposed method outperforms three KB-QA base-line methods on two publicly released QA data sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge-based question answering (KB-QA) involves answering questions posed in natural language (NL) using existing knowledge bases (KBs). As most KBs are structured databases, how to transform the input question into its corre- sponding structured query for KB (KB-query) as a logical form (LF), also known as semantic pars- ing, is the central task for KB-QA systems. Pre- vious works <ref type="bibr" target="#b12">(Mooney, 2007;</ref><ref type="bibr" target="#b11">Liang et al., 2011;</ref><ref type="bibr" target="#b4">Cai and Yates, 2013;</ref><ref type="bibr" target="#b6">Fader et al., 2013;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b0">Bao et al., 2014</ref>) usually leveraged map- pings between NL phrases and logical predicates as lexical triggers to perform transformation tasks in semantic parsing, but they had to deal with two limitations: (i) as the meaning of a logical pred- icate often has different natural language expres- sion (NLE) forms, the lexical triggers extracted for a predicate may at times are limited in size; (ii) entities detected by the named entity recognition (NER) component will be used to compose the logical forms together with the logical predicates, so their types should be consistent with the pred- icates as well. However, most NER components used in existing KB-QA systems are independent from the NLE-to-predicate mapping procedure.</p><p>We present a novel embedding-based KB-QA method that takes all the aforementioned lim- itations into account, and maps NLE-to-entity and NLE-to-predicate simultaneously using sim- ple vector operations for structured query con- struction. First, low-dimensional embeddings of n-grams, entity types, and predicates are jointly learned from an existing knowledge base and from entries &lt;entity subj , NL relation phrase, entity obj &gt; that are mined from NL texts labeled as KB- properties with weak supervision. Each such en- try corresponds to an NL expression of a triple &lt;entity subj , predicate, entity obj &gt; in the KB. These embeddings are used to measure the semantic as- sociations between lexical phrases and two prop- erties of the KB, entity type and logical predicate. Next, given an NL-question, all possible struc- tured queries as candidate LFs are generated and then they are ranked by the similarity between the embeddings of observed features (n-grams) in the NL-question and the embeddings of logical fea- tures in the structured queries. Last, answers are retrieved from the KB using the selected LFs.</p><p>The contributions of this work are two-fold: (1) as a smoothing technique, the low-dimensional embeddings can alleviate the coverage issues of lexical triggers; (2) our joint approach integrates entity span selection and predicate mapping tasks for KB-QA. For this we built independent entity embeddings as the additional component, solving the entity disambiguation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Supervised semantic parsers ( <ref type="bibr" target="#b17">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b18">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b12">Mooney, 2007)</ref> heavily rely on the &lt;sentence, semantic an-notation&gt; pairs for lexical trigger extraction and model training. Due to the data annotation re- quirement, such methods are usually restricted to specific domains, and struggle with the coverage issue caused by the limited size of lexical triggers.</p><p>Studies on weakly supervised semantic parsers have tried to reduce the amount of human supervi- sion by using question-answer pairs <ref type="bibr" target="#b11">(Liang et al., 2011</ref>) or distant supervision <ref type="bibr" target="#b9">(Krishnamurthy and Mitchell, 2012</ref>) instead of full semantic annota- tions. Still, for KB-QA, the question of how to leverage KB-properties and analyze the question structures remains. <ref type="bibr" target="#b3">Bordes et al. (2012)</ref> and <ref type="bibr" target="#b15">Weston et al. (2013)</ref> de- signed embedding models that connect free texts with KBs using the relational learning method ( <ref type="bibr" target="#b14">Weston et al., 2010)</ref>. Their inputs are often statement sentences which include subject and ob- ject entities for a given predicate, whereas NL- questions lack either a subject or object entity that is the potential answer. Hence, we can only use the information of a subject or object entity, which leads to a different training instance generation procedure and a different training criterion.</p><p>Recently, researchers have developed open do- main systems based on large scale KBs such as FREEBASE 1 <ref type="bibr" target="#b4">(Cai and Yates, 2013;</ref><ref type="bibr" target="#b6">Fader et al., 2013;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b10">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b0">Bao et al., 2014;</ref><ref type="bibr" target="#b1">Berant and Liang, 2014;</ref><ref type="bibr" target="#b16">Yao and Van Durme, 2014</ref>). Their semantic parsers for Open QA are unified formal and scal- able: they enable the NL-question to be mapped into the appropriate logical form. Our method ob- tains similar logical forms, but using only low- dimensional embeddings of n-grams, entity types, and predicates learned from texts and KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relational Components for KB-QA</head><p>Our method learns semantic mappings between NLEs and the KB 2 based on the paired relation- ships of the following three components: C de- notes a set of bag-of-words (or n-grams) as context features (c) for NLEs that are the lexical represen- tations of a logical predicate (p) in KB; T denotes a set of entity types (t) in KB and each type can be used as the abstract expression of a subject entity (s) that occurs in the input question; P denotes a set of logical predicates (p) in KB, each of which is the canonical form of different NLEs sharing an identical meaning (bag-of-words; c).</p><p>Based on the components defined above, the paired relationships are described as follows: T - P can investigate the relationship between sub- ject entity and logical predicate, as object entity is always missing in KB-QA; C-T can scruti- nize subject entity's attributes for the entity span selection such as its positional information and relevant entity types to the given context, which may solve the entity disambiguation problem in KB-QA; C-P can leverage the semantic overlap between question contexts (n-gram features) and logical predicates, which is important for mapping NL-questions to their corresponding predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NLE-KB Pair Extraction</head><p>This section describes how we extract the semantic associated pairs of NLE-entries and KB-triples to learn the relational embeddings (Section 4.1).</p><p>&lt;Relation Mention, Predicate&gt; Pair (MP) Each relation mention denotes a lexical phrase of an existing KB-predicate. Following informa- tion extraction methods, such as PATTY (Nakas- hole et al., 2012), we extracted the &lt;relation mention, logical predicate&gt; pairs from English WIKIPEDIA 3 , which is closely connected to our KB, as follows: Given a KB-triple &lt;entity subj , logical predicate, entity obj &gt;, we extracted NLE- entries &lt;entity subj , relation mention, entity obj &gt; where relation mention is the shortest path be- tween entity subj and entity obj in the dependency tree of sentences. The assumption is that any re- lation mention (m) in the NLE-entry containing such entity pairs that occurred in the KB-triple is likely to express the predicate (p) of that triple.</p><p>With obtaining high-quality MP pairs, we kept only relation mentions that were highly associated with a predicate measured by the scoring function:</p><formula xml:id="formula_0">S(m, p) = PMI(e m ; e p ) + PMI(u m ; u p ) (1)</formula><p>where e x is the set of total pairs of both-side entities of entry x (m or p) and u x is the set of unique (distinct) pairs of both-side entities of entry x. In this case, the both-side entities in- dicate entity subj and entity obj . For a frequency- based probability, PMI(x; y) = log P (x,y) <ref type="bibr" target="#b5">(Church and Hanks, 1990)</ref> can be re-written as PMI(x; y) = log |x y|·C |x|·|y| , where C denotes the total number of items shown in the corpus. The function is partially derived from the support score (Gerber and Ngonga Ngomo, 2011), but we fo- cus on the correlation of shared entity pairs be- tween relation mentions and predicates using the PMI computation.</p><p>&lt;Question Pattern, Predicate&gt; Pair (QP) Since WIKIPEDIA articles have no information to leverage interrogative features which highly de- pend on the object entity (answer), it is difficult to distinguish some questions that are composed of only different 5W1H words, e.g., {When|Where} was Barack Obama born? Hence, we used the method of collecting question patterns with human labeled predicates that are restricted by the set of predicates used in MP ( <ref type="bibr" target="#b0">Bao et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Embedding-based KB-QA</head><p>Our task is as follows. First, our model learns the semantic associations of C-T , C-P, and T -P (Sec- tion 3.1) based on NLE-KB pairs (Section 3.2), and then predicts the semantic-related KB-query which can directly find the answer to a given NL- question.</p><p>For our feature space, given an NLE-KB pair, the NLE (relation mention in MP or question pattern in QP) is decomposed into n-gram fea- tures: C = {c | c is a segment of NLE}, and the KB-properties are represented by entity type t of entity subj and predicate p. Then we can ob- tain a training triplet w = [C, t, p]. Each feature (c ∈ C, t ∈ T , p ∈ P) is encoded in the distributed representation which is n-dimensional embedding vectors (E n ): ∀x, x encode ⇒ E(x) ∈ E n . All n-gram features (C) for an NLE are merged into one embedding vector to help speed up the learning process: E(C) = c∈C E(c)/|C|. This feature representation is inspired by previous work in embedding-based relation extraction ( <ref type="bibr" target="#b15">Weston et al., 2013</ref>), but differs in the following ways: (1) entity information is represented on a separate em- bedding, but its positional information remains as symbol entity; (2) when the vectors are com- bined, we use the average of each index to normal- ize features.</p><p>For our joint relational approach, we focus on the set of paired relationships R = {C-t, C-p, t- p} that can be semantically leveraged. Formally, these features are embedded into the same latent space (E n ) and their semantic similarities can be computed by a dot product operation:</p><formula xml:id="formula_1">Sim(a, b) = Sim(r ab ) = E(a) E(b) (2)</formula><p>where r ab denotes a paired relationship a-b (or (a, b)) in the above set R. We believe that our joint re- lational learning can smooth the surface (lexical) features for semantic parsing using the aligned en- tity and predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Joint Relational Embedding Learning</head><p>Our ranking-based relational learning is based on a ranking loss ( <ref type="bibr" target="#b14">Weston et al., 2010</ref>) that supports the idea that the similarity scores of observed pairs in the training set (positive instances) should be larger than those of any other pairs (negative in- stances):</p><formula xml:id="formula_2">∀i, ∀y = y i , Sim(x i , y i ) &gt; 1+Sim(x i , y ) (3)</formula><p>More precisely, for each triplet w i = [C i , t i , p i ] obtained from an NLE-KB pair, the relationships R i = {C i -t i , C i -p i , t i -p i } are trained under the soft ranking criterion, which conducts Stochastic Gradient Descent (SGD). We thus aim to minimize the following:</p><formula xml:id="formula_3">∀i, ∀y = y i , max(0, 1−Sim(x i , y i )+Sim(x i , y ))<label>(4)</label></formula><p>Our learning strategy is as follows. First, we ini- tialize embedding space E n by randomly giving mean 0 and standard deviation 1/n to each vec- tor. Then for each training triplet w i , we select the negative pairs against positive pairs (C i -t i , C i -p i , and t i -p i ) in the triplet. Last, we make a stochastic gradient step to minimize Equation 4 and update E n at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KB-QA using Embedding Models</head><p>Our goal for KB-QA is to translate a given NL- question to a KB-query with the form &lt;subject entity, predicate, ?&gt;, where ? denotes the an- swer entity we are looking for. The decoding pro- cess consists of two stages. The first stage in- volves generating all possible KB-queries (K q ) for an NL-question q. We first extract n-gram fea- tures (C q ) from the NL-question q. Then for a KB-query k q , we find all available entity types (t q ) of the identified subject entities (s q ) using the dictionary-based entity detection on the NL- question q (all of spans can be candidate entities), and assign all items of predicate set (P) as the can- didate predicates (p q ). Like the training triplets, q where is the city of david? ˆ k(q) [The City of David, contained by, ?] C q n-grams of "where is entity ?" t q location p q contained by <ref type="table">Table 1</ref>: The corresponding KB-queryˆkqueryˆ queryˆk(q) for a NL-question q and its decoding triplet w q .</p><p>we also represent the above features as the triplet form w q i = [C q i , t q i , p q i ] which is directly linked to a KB-query k q i = [s q i , p q i , ?]. The second stage involves ranking candidate KB-queries based on the similarity scores between the following paired relationships from the triplet w q i :</p><formula xml:id="formula_4">R q i = {C q i -t q i , C q i -p q i , t q i -p q i }.</formula><p>Unlike in the training step, the sim- ilarities of C q i -t q i and C q i -p q i are computed by sum- mation of all pairwise elements (each context em- bedding E(c), not E(C), with each paired E(t) or E(p)) for a more precise measurement. Since sim- ilarites of R q are calculated on different scales, we normalize each value using Z-score (Z(x) = x−µ σ ) ( <ref type="bibr" target="#b8">Kreyszig, 1979)</ref>. The final score is measured by:</p><formula xml:id="formula_5">Sim q2k (q, k q ) = r∈R q Z(Sim(r))<label>(5)</label></formula><p>Then, given any NL-question q, we can predict the corresponding KB-queryˆkqueryˆ queryˆk(q):</p><formula xml:id="formula_6">ˆ k(q) = arg max k∈K q Sim q2k (q, k)<label>(6)</label></formula><p>Last, we can retrieve an answer from the KB using a structured queryˆkqueryˆ queryˆk(q). <ref type="table">Table 1</ref> shows an example of our decoding process.</p><p>Multi-related Question Some questions in- clude two-subject entities, both of which are cru- cial to understanding the question. For the ques- tion who plays gandalf in the lord of the rings? Gandalf (character) and The Lord Of The Rings (film) are explicit entities that should be joined to a pair of the two entities (implicit entity). More precisely, the two entities can be combined into one concatenated entity (character-in-film) using our manual rule, which compares the possi- ble pairs of entity types in the question with the list of pre-defined entity type pairs that can be merged into a concatenated entity. Our solution enables a multi-related question to be transformed to a single-related question which can be directly translated to a KB-query. Then, the two entity # Entries Accuracy MP pairs 291,585 89% QP pairs 4,764 98% <ref type="table">Table 2</ref>: Statistics of NLE-KB pairs mentions are replaced with the symbol entity (who play entity in entity ?). We re- gard the result of this transformation as one of the candidate KB-queries in the decoding step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Experimental Setting We first performed pre- processing, including lowercase transformation, lemmatization and tokenization, on NLE-KB pairs and evaluation data. We used 71,310 n-grams (uni-, bi-, tri-), 990 entity types, and 660 predi- cates as relational components shown in Section 3.1. The sum of these three numbers (72,960) equals the size of the embeddings we are going to learn. In <ref type="table">Table 2</ref>, we evaluated the quality of NLE-KB pairs (MP and QP) described in Sec- tion 3.2. We can see that the quality of QP pairs is good, mainly due to human efforts. Also, we ob- tained MP pairs that have an acceptable quality using threshold 3.0 for Equation 1, which lever- ages the redundancy information in the large-scale data (WIKIPEDIA). For our embedding learning, we set the embedding dimension n to 100, the learning rate (λ) for SGD to 0.0001, and the it- eration number to 30. To make the decoding procedure computable, we kept only the popular KB-entity in the dictionary to map different entity mentions into a KB-entity. We used two publicly released data sets for QA evaluations: <ref type="bibr">Free917 (Cai and Yates, 2013</ref>) in- cludes the annotated lambda calculus forms for each question, and covers 81 domains and 635 Freebase relations; WebQ. ( <ref type="bibr" target="#b2">Berant et al., 2013)</ref> provides 5,810 question-answer pairs that are built by collecting common questions from Web-query logs and by manually labeling answers. We used the previous three approaches <ref type="bibr" target="#b4">(Cai and Yates, 2013;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b0">Bao et al., 2014</ref>) as our baselines. <ref type="table" target="#tab_0">Table 3</ref> reports the over- all performances of our proposed KB-QA method on the two evaluation data sets and compares them with those of the three baselines. Note that we did not re-implement the baseline systems, but just borrowed the evaluation results reported in their</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Free917 <ref type="bibr">WebQ. Cai and Yates (2013)</ref> 59.00% N/A <ref type="bibr" target="#b2">Berant et al. (2013)</ref> 62.00% 31.40% <ref type="bibr" target="#b0">Bao et al. (2014)</ref> N/A 37.50% Our method 71.38% 41.34%  <ref type="table">Table 4</ref>: Ablation of the relationship types papers. Although the KB used by our system is much larger than FREEBASE, we still think that the experimental results are directly comparable because we disallow all the entities that are not in- cluded in FREEBASE. <ref type="table" target="#tab_0">Table 3</ref> shows that our method outperforms the baselines on both Free917 and WebQ. data sets. We think that using the low-dimensional embed- dings of n-grams rather than the lexical triggers greatly improves the coverage issue. Unlike the previous methods which perform entity disam- biguation and predicate prediction separately, our method jointly performs these two tasks. More precisely, we consider the relationships C-T and C-P simultaneously to rank candidate KB-queries. In <ref type="table">Table 1</ref>, the most independent NER in KB-QA systems may detect David as the subject entity, but our joint approach can predict the appropriate subject entity The City of David by leveraging not only the relationships with other components but also other relationships at once. The syntax- based (grammar formalism) approaches such as Combinatory Categorial Grammar (CCG) may ex- perience errors if a question has grammatical er- rors. However, our bag-of-words model-based ap- proach can handle any question as long as the question contains keywords that can help in un- derstanding it. <ref type="table">Table 4</ref> shows the contributions of the relation- ships (R) between relational components C, T , and P. For each row, we remove the similarity from each of the relationship types described in Section 3.1. We can see that the C-P relationship plays a crucial role in translating NL-questions to KB-queries, while the other two relationships are slightly helpful.</p><p>Result Analysis Since the majority of questions in WebQ. tend to be more natural and diverse, our method cannot find the correct answers to many questions. The errors can be caused by any of the following reasons. First, some NLEs cannot be easily linked to existing KB-predicates, mak- ing it difficult to find the answer entity. Second, some entities can be mentioned in several different ways, e.g., nickname (shaq→Shaquille O'neal) and family name (hitler→Adolf Hitler). Third, in terms of KB coverage issues, we cannot detect the entities that are unpopular. Last, feature represen- tation for a question can fail when the question consists of rare n-grams.</p><p>The two training sets shown in Section 3.2 are complementary: QP pairs provide more oppor- tunities for us to learn the semantic associations between interrogative words and predicates. Such resources are especially important for understand- ing NL-questions, as most of them start with such 5W1H words; on the other hand, MP pairs en- rich the semantic associations between context in- formation (n-gram features) and predicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel method that transforms NL-questions into their corresponding logical forms using joint relational embeddings. We also built a simple and robust KB-QA system based on only the learned embeddings. Such em- beddings learn the semantic associations between natural language statements and KB-properties from NLE-KB pairs that are automatically ex- tracted from English WIKIPEDIA using KB-triples with weak supervision. Then, we generate all pos- sible structured queries derived from latent logical features of the given NL-question, and rank them based on the similarity scores between those re- lational attributes. The experimental results show that our method outperforms the latest three KB- QA baseline systems. For our future work, we will build concept-level context embeddings by lever- aging latent meanings of NLEs rather than their surface n-grams with the aligned logical features on KB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy on the evaluation data 

Methods 
Free917 WebQ. 
Our method 71.38% 41.34% 
w/o T -P 
70.65% 40.55% 
w/o C-T 
67.03% 38.44% 
w/o C-P 
31.16% 19.24% 

</table></figure>

			<note place="foot" n="1"> http://www.freebase.com 2 For this paper, we used a large scale knowledge base that contains 2.3B entities, 5.5K predicates, and 18B assertions. A 16-machine cluster was used to host and serve the whole data.</note>

			<note place="foot">P (x)P (y) 3 http://en.wikipedia.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-based question answering as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="967" to="976" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 15th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>15th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bootstrapping the linked data web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Workshop on Web Scale Knowledge Extraction @ ISWC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Advanced Engineering Mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kreyszig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="754" to="765" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymondj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<editor>Alexander Gelbukh</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4394</biblScope>
			<biblScope unit="page" from="311" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Patty: A taxonomy of relational patterns with semantic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale image annotation: Learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
	<note>AAAI&apos;96</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
