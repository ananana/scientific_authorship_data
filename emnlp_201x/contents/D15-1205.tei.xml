<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Relation Extraction with Feature-Rich Compositional Embedding Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Machine Intelligence and Translation Lab Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Relation Extraction with Feature-Rich Compositional Embedding Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Compositional embedding models build a representation (or embedding) for a linguistic structure based on its component word embeddings. We propose a Feature-rich Compositional Embedding Model (FCM) for relation extraction that is expressive, generalizes to new domains, and is easy-to-implement. The key idea is to combine both (unlexicalized) hand-crafted features with learned word em-beddings. The model is able to directly tackle the difficulties met by traditional compositional embeddings models, such as handling arbitrary types of sentence annotations and utilizing global information for composition. We test the proposed model on two relation extraction tasks, and demonstrate that our model outper-forms both previous compositional models and traditional feature rich models on the ACE 2005 relation extraction task, and the SemEval 2010 relation classification task. The combination of our model and a log-linear classifier with hand-crafted features gives state-of-the-art results. We made our implementation available for general use 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Two common NLP feature types are lexical properties of words and unlexicalized linguis- tic/structural interactions between words. Prior work on relation extraction has extensively stud- ied how to design such features by combining dis- crete lexical properties (e.g. the identity of a word, ⇤ ⇤ Gormley and Yu contributed equally. 1 https://github.com/mgormley/pacaya its lemma, its morphological features) with as- pects of a word's linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical informa- tion relies on continuous word embeddings 2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency pars- ing, semantic role labeling, and relation extrac- tion ( <ref type="bibr" target="#b17">Miller et al., 2004;</ref><ref type="bibr" target="#b32">Turian et al., 2010;</ref><ref type="bibr" target="#b10">Koo et al., 2008;</ref><ref type="bibr" target="#b26">Roth and Woodsend, 2014;</ref><ref type="bibr" target="#b30">Sun et al., 2011;</ref><ref type="bibr" target="#b24">Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2014)</ref>. Embeddings can capture lexi- cal information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context.</p><p>In this paper, we introduce a compositional model that combines unlexicalized linguistic con- text and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embed- dings with arbitrary linguistic structure, as ex- pressed by hand crafted features. In the follow- ing sections, we begin with a precise construction of compositional embeddings using word embed- dings in conjunction with unlexicalized features. Various feature sets used in prior work ( <ref type="bibr" target="#b32">Turian et al., 2010;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2014;</ref><ref type="bibr" target="#b8">Hermann et al., 2014;</ref><ref type="bibr" target="#b26">Roth and Woodsend, 2014</ref>) are cap-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M1</head><p>M2 Sentence Snippet (1) ART(M1,M2) a man a taxicab A man driving what appeared to be a taxicab (2) PART-WHOLE(M1,M2) the southern suburbs Baghdad direction of the southern suburbs of Baghdad (3) PHYSICAL(M2,M1) the united states 284 people in the united states , 284 people died <ref type="table">Table 1</ref>: Examples from ACE 2005. In (1) the word "driving" is a strong indicator of the relation ART 3 between M1 and M2.</p><p>A feature that depends on the embedding for this context word could generalize to other lexical indicators of the same relation (e.g. "operating") that don't appear with ART during training. But lexical information alone is insufficient; relation extraction requires the identification of lexical roles: where a word appears structurally in the sentence. In (2), the word "of" between "suburbs" and "Baghdad" suggests that the first entity is part of the second, yet the earlier occurrence after "direction" is of no significance to the relation. Even finer information can be expressed by a word's role on the dependency path between entities.</p><p>In (3) we can distinguish the word "died" from other irrelevant words that don't appear between the entities. tured as special cases of this construction. Adding these compositional embeddings directly to a stan- dard log-linear model yields a special case of our full model. We then treat the word embeddings as parameters giving rise to our powerful, efficient, and easy-to-implement log-bilinear model. The model capitalizes on arbitrary types of linguistic annotations by better utilizing features associated with substructures of those annotations, including global information. We choose features to pro- mote different properties and to distinguish differ- ent functions of the input words. The full model involves three stages. First, it decomposes the annotated sentence into substruc- tures (i.e. a word and associated annotations). Second, it extracts features for each substructure (word), and combines them with the word's em- bedding to form a substructure embedding. Third, we sum over substructure embeddings to form a composed annotated sentence embedding, which is used by a final softmax layer to predict the out- put label (relation).</p><p>The result is a state-of-the-art relation extractor for unseen domains from ACE 2005 ( <ref type="bibr" target="#b33">Walker et al., 2006</ref>) and the relation classification dataset from <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2010</ref>.</p><p>Contributions This paper makes several contri- butions, including:</p><p>1. We introduce the FCM, a new compositional embedding model for relation extraction. 2. We obtain the best reported results on ACE- 2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as <ref type="bibr" target="#b5">Hashimoto et al. (2015)</ref>, , dos Santos <ref type="bibr">3</ref> In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufac- turer relationship <ref type="bibr">et al. (2015)</ref>,  and . Additionally, we have extended FCM to incorporate a low-rank embedding of the features ( , which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relation Extraction</head><p>In relation extraction we are given a sentence as in- put with the goal of identifying, for all pairs of en- tity mentions, what relation exists between them, if any. For each pair of entity mentions in a sen- tence S, we construct an instance <ref type="bibr">(y, x)</ref>, where x = (M 1 , M 2 , S, A). S = {w 1 , w 2 , ..., w n } is a sentence of length n that expresses a relation of type y between two entity mentions M 1 and M 2 , where M 1 and M 2 are sequences of words in S. A is the associated annotations of sentence S, such as part-of-speech tags, a dependency parse, and named entities. We consider directed rela- tions: for a relation type Rel, y=Rel(M 1 , M 2 ) and y 0 =Rel(M 2 , M 1 ) are different relations. <ref type="table">Ta- ble 1</ref> shows ACE 2005 relations, and has a strong label bias towards negative examples. We also consider the task of relation classification (Se- mEval), where the number of negative examples is artificially reduced.</p><p>Embedding Models Word embeddings and compositional embedding models have been suc- cessfully applied to a range of NLP tasks, however the applications of these embedding models to re- lation extraction are still limited. Prior work on relation classification (e.g. <ref type="bibr">SemEval 2010 Task 8)</ref> has focused on short sentences with at most one relation per sentence <ref type="bibr" target="#b27">(Socher et al., 2012;</ref><ref type="bibr" target="#b36">Zeng et al., 2014</ref>). For relation extraction, where neg- ative examples abound, prior work has assumed that only the named entity boundaries and not their types were available <ref type="bibr" target="#b24">(Plank and Moschitti, 2013;</ref>). Other work has as-sumed that the order of two entities in a relation are given while the relation type itself is unknown <ref type="bibr" target="#b20">(Nguyen and Grishman, 2014;</ref>). The standard relation extraction task, as adopted by <ref type="bibr">ACE 2005</ref><ref type="bibr" target="#b33">(Walker et al., 2006</ref>), uses long sentences containing multiple named en- tities with known types 4 and unknown relation di- rections. We are the first to apply neural language model embeddings to this task.</p><p>Motivation and Examples Whether a word is indicative of a relation depends on multiple prop- erties, which may relate to its context within the sentence. For example, whether the word is in- between the entities, on the dependency path be- tween them, or to their left or right may provide additional complementary information. Illustra- tive examples are given in <ref type="table">Table 1</ref> and provide the motivation for our model. In the next section, we will show how we develop informative repre- sentations capturing both the semantic information in word embeddings and the contextual informa- tion expressing a word's role relative to the entity mentions. We are the first to incorporate all of this information at once. The closest work is that of <ref type="bibr" target="#b20">Nguyen and Grishman (2014)</ref>, who use a log- linear model for relation extraction with embed- dings as features for only the entity heads. Such embedding features are insensitive to the broader contextual information and, as we show, are not sufficient to elicit the word's role in a relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Feature-rich Compositional Embedding Model for Relations</head><p>We propose a general framework to construct an embedding of a sentence with annotations on its component words. While we focus on the rela- tion extraction task, the framework applies to any task that benefits from both embeddings and typi- cal hand-engineered lexical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Combining Features with Embeddings</head><p>We begin by describing a precise method for con- structing substructure embeddings and annotated sentence embeddings from existing (usually un- lexicalized) features and embeddings. Note that these embeddings can be included directly in a log-linear model as features-doing so results in a special case of our full model presented in the next subsection. An annotated sentence is first decomposed into substructures. The type of substructures can vary by task; for relation extraction we consider one substructure per word <ref type="bibr">5</ref> . For each substructure in the sentence we have a hand-crafted feature vec- tor f w i and a dense embedding vector e w i . We represent each substructure as the outer product ⌦ between these two vectors to produce a matrix, herein called a substructure embedding: h w i = f w i ⌦ e w i . The features f w i are based on the local context in S and annotations in A, which can in- clude global information about the annotated sen- tence. These features allow the model to pro- mote different properties and to distinguish differ- ent functions of the words. Feature engineering can be task specific, as relevant annotations can change with regards to each task. In this work we utilize unlexicalized binary features common in relation extraction. <ref type="figure">Figure 1</ref> depicts the con- struction of a sentence's substructure embeddings.</p><p>We further sum over the substructure embed- dings to form an annotated sentence embedding:</p><formula xml:id="formula_0">e x = n X i=1 f w i ⌦ e w i (1)</formula><p>When both the hand-crafted features and word em- beddings are treated as inputs, as has previously been the case in relation extraction, this anno- tated sentence embedding can be used directly as the features of a log-linear model. In fact, we find that the feature sets used in prior work for many other NLP tasks are special cases of this simple construction ( <ref type="bibr" target="#b32">Turian et al., 2010;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2014;</ref><ref type="bibr" target="#b8">Hermann et al., 2014;</ref><ref type="bibr" target="#b26">Roth and Woodsend, 2014)</ref>. This highlights an im- portant connection: when the word embeddings are constant, our constructions of substructure and annotated sentence embeddings are just specific forms of polynomial (specifically quadratic) fea- ture combination-hence their commonality in the literature. Our experimental results suggest that such a construction is more powerful than directly including embeddings into the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Log-Bilinear Model</head><p>Our full log-bilinear model first forms the sub- structure and annotated sentence embeddings from</p><formula xml:id="formula_1">M i n X i=1 f i ⌦ e w i (17) (18) + (19) of an input sentence. (b)</formula><p>roduct between the feature se a tensor T = L⌦E ⌦F he set of labels, E refers to ture S, we have <ref type="formula" target="#formula_5">(2)</ref> e we decompose the struc- on the model parameters. is a matrix (y, ·, ·). Then <ref type="formula" target="#formula_6">(3)</ref> the equivalent form:</p><formula xml:id="formula_2">) T h f .</formula><p>tensor T can be written as:</p><formula xml:id="formula_3">f i .<label>(4)</label></formula><p>e tensor, making the model ross-entropy objective:</p><p>[9] to optimize above g; and for each in- g P (y|S; T, W ). Then</p><formula xml:id="formula_4">M i n X i=1 f i ⌦ e w i (17) (18) + (19) entation of the FCT model. (a) Representation of an input sentence. (b) rameter space.</formula><p>s, we can represent each factor as the outer product between the feature er of transformed embedding g f ⌦h f . The we use a tensor T = L⌦E ⌦F sform this input matrix to the labels. Here L is the set of labels, E refers to layer (|E| = 200) and F is the set of features.</p><p>nditional probability of a label y given the structure S, we have</p><formula xml:id="formula_5">P (y|S; T ) = exp{s(y, S; T )} P y 2L exp{s(y , S; T )} ,<label>(2)</label></formula><p>core of label y computed with our model. Since we decompose the struc- actor f i 2 S will contribute to the score based on the model parameters. corresponds to a slice of the tensor T y , which is a matrix (y, ·, ·). Then bute a score</p><formula xml:id="formula_6">s(y, f i ) = T y g f h f ,<label>(3)</label></formula><p>ensor product, while in the case of Eq. <ref type="formula" target="#formula_6">(3)</ref>, it has the equivalent form:</p><formula xml:id="formula_7">g f h f = T y (g f ⌦ h f ) = ((y, ·, ·) · g f ) T h f .</formula><p>re of label y given an instance S and parameter tensor T can be written as:</p><formula xml:id="formula_8">(y, S; T ) = n X i=1 s(y, f i ; T ) = n X i=1 T y g f i h f i .<label>(4)</label></formula><p>forms linear transformations on each view of the tensor, making the model lement.</p><p>train the parameters we optimize the following cross-entropy objective:</p><formula xml:id="formula_9">`(D; T, W ) = X (y,S)2D log P (y|S; T, W )</formula><p>of all training data. We used AdaGrad <ref type="bibr">[9]</ref> to optimize above re we are performing stochastic training; and for each in- s function`=`(function`function`=function`=`(y, S; T, W ) = log P (y|S; T, W ). epresent each factor as the outer product between the feature rmed embedding g f ⌦h f . The we use a tensor T = L⌦E ⌦F put matrix to the labels. Here L is the set of labels, E refers to 200) and F is the set of features.</p><p>obability of a label y given the structure S, we have ; T ) = exp{s(y, S; T )} P y 2L exp{s(y , S; T )} ,</p><p>l y computed with our model. Since we decompose the struc- will contribute to the score based on the model parameters. s to a slice of the tensor T y , which is a matrix (y, ·, ·). Then</p><formula xml:id="formula_11">s(y, f i ) = T y g f h f ,<label>(3)</label></formula><p>t, while in the case of Eq. <ref type="formula" target="#formula_6">(3)</ref>, it has the equivalent form:</p><formula xml:id="formula_12">T y (g f ⌦ h f ) = ((y, ·, ·) · g f ) T h f .</formula><p>given an instance S and parameter tensor T can be written as:</p><formula xml:id="formula_13">n X i=1 s(y, f i ; T ) = n X i=1 T y g f i h f i .<label>(4)</label></formula><p>transformations on each view of the tensor, making the model meters we optimize the following cross-entropy objective:</p><formula xml:id="formula_14">W ) = X (y,S)2D</formula><p>log P (y|S; T, W ) ning data. We used AdaGrad <ref type="bibr">[9]</ref> to optimize above performing stochastic training; and for each in- ` = `(y, S; T, W ) = log P (y|S; T, W ). Then</p><formula xml:id="formula_15">2 A A 0 of B 0 B (9) A B A 0 of B 0<label>(10)</label></formula><p>T f e Relations</p><formula xml:id="formula_16">(11) f ⌦ e [f : e] FCT CNN @` @R s(l, e 1 , -.5 .3 .8 .7 0 0 0 0 -.5 .3 .8 .7 0 0 0 0 0 0 0 0 -.5 .3 .8 .7 -.5 .3 .8 .7 -.5 .3 .8 .7 1 0 1 0 0 1 0 ( 2 0) -.5 .3 .8 .7 1 0 1 0 0 1 -.5 .3 .8 .7 1 0 1 0 0 1 0 ( 2 0) -.5 .3 .8 .7 1 0 1 0 0 1 A A 0 of B 0 B (9) A B A 0 of B 0<label>(10)</label></formula><p>T f e Relations Figure 1: Example construction of substructure embeddings. Each substructure is a word wi in S, augmented by the target entity information and related information from annotation A (e.g. a dependency tree). We show the factorization of the annotated sentence into substructures (left), the concatenation of the substructure embeddings for the sentence (middle), and a single substructure embedding from that concatenation (right). The annotated sentence embedding (not shown) would be the sum of the substructure embeddings, as opposed to their concatenation. the previous subsection. The model uses its pa- rameters to score the annotated sentence embed- ding and uses a softmax to produce an output la- bel. We call the entire model the Feature-rich Compositional Embedding Model (FCM).</p><formula xml:id="formula_17">(11) f ⌦ e [f : e] FCT CNN @`</formula><p>Our task is to determine the label y (relation) given the instance x = (M 1 , M 2 , S, A). We for- mulate this as a probability.</p><formula xml:id="formula_18">P (y|x; T, e) = exp ( P n i=1 T y (f w i ⌦ e w i )) Z(x)<label>(2)</label></formula><p>where is the 'matrix dot product' or Frobe- nious inner product of the two matrices. The normalizing constant which sums over all possi- ble output labels y 0 2 L is given by</p><formula xml:id="formula_19">Z(x) = P y 0 2L exp P n i=1 T y 0 (f w i ⌦ e w i )</formula><p>. The pa- rameters of the model are the word embeddings e for each word type and a list of weight matrix T = [T y ] y2L which is used to score each label y. The model is log-bilinear 6 (i.e. log-quadratic) since we recover a log-linear model by fixing ei- ther e or T . We study both the full log-bilinear and the log-linear model obtained by fixing the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion of the Model</head><p>Substructure Embeddings Similar words (i.e. those with similar embeddings) with similar func- tions in the sentence (i.e. those with similar fea- tures) will have similar matrix representations. To understand our selection of the outer product, con- sider the example in <ref type="figure">Fig. 1</ref>. The word "driving" can indicate the ART relation if it appears on the <ref type="bibr">6</ref> Other popular log-bilinear models are the log-bilinear language models <ref type="bibr" target="#b19">(Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013).</ref> dependency path between M 1 and M 2 . Suppose the third feature in f w i indicates this on-path feature. Our model can now learn parameters which give the third row a high weight for the ART label. Other words with embeddings similar to "driving" that appear on the dependency path between the mentions will similarly receive high weight for the ART label. On the other hand, if the embedding is similar but is not on the dependency path, it will have 0 weight. Thus, our model gen- eralizes its model parameters across words with similar embeddings only when they share similar functions in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smoothed Lexical Features</head><p>Another intuition about the selection of outer product is that it is actually a smoothed version of traditional lexical features used in classical NLP systems. Consider a lexical feature f = u ^ w, which is a conjunc- tion (logic-and) between non-lexical property u and lexical part (word) w. If we represent w as a one-hot vector, then the outer product exactly re- covers the original feature f . Then if we replace the one-hot representation with its word embed- ding, we get the current form of our FCM. There- fore, our model can be viewed as a smoothed ver- sion of lexical features, which keeps the expres- sive strength, and uses embeddings to generalize to low frequency features. </p><note type="other">have complexity O(C · nd 2 ), where C is a model dependent constant.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hybrid Model</head><p>We present a hybrid model which combines the FCM with an existing log-linear model. We do so by defining a new model:</p><formula xml:id="formula_20">p FCM+loglin (y|x) = 1 Z p FCM (y|x)p loglin (y|x)<label>(3)</label></formula><p>The log-linear model has the usual form:</p><formula xml:id="formula_21">p loglin (y|x) / exp(✓ · f (x, y)),</formula><p>where ✓ are the model parameters and f (x, y) is a vector of fea- tures. The integration treats each model as a pro- viding a score which we multiply together. The constant Z ensures a normalized distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>FCM training optimizes a cross-entropy objective:</p><formula xml:id="formula_22">`(D; T, e) = X (x,y)2D log P (y|x; T, e)</formula><p>where D is the set of all training data and e is the set of word embeddings. To optimize the objective, for each instance (y, x) we per- form stochastic training on the loss function`=`(function`function`= function`=`(y, x; T, e) = log P (y|x; T, e). The gradi- ents of the model parameters are obtained by backpropagation (i.e. repeated application of the chain rule). We define the vector s = [</p><formula xml:id="formula_23">P i T y (f w i ⌦ e w i )] 1yL , which yields @` @s = h I[y 0 = y] P (y 0 |x; T, e) 1yL i T ,</formula><p>where the indicator function I[x] equals 1 if x is true and 0 otherwise. We have the following gradi- ents: @` @T = @` @s ⌦ P n i=1 f w i ⌦ e w i , which is equiv- alent to:</p><formula xml:id="formula_24">@` @T y 0 = I[y = y 0 ] P (y 0 |x; T, e) · n X i=1 f w i ⌦ e w i .</formula><p>When we treat the word embeddings as parameters (i.e. the log-bilinear model), we also fine-tune the word embeddings with the FCM model:</p><formula xml:id="formula_25">@` @e w = n X i=1 X y @` @s y T y ! · f i ! · I[w i = w].</formula><p>As is common in deep learning, we initialize these embeddings from an neural language model and then fine-tune them for our supervised task. The training process for the hybrid model ( § 4) is also easily done by backpropagation since each sub-model has separate parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set</head><p>Template</p><formula xml:id="formula_26">HeadEmb {I[i = h1], I[i = h2]} (wi is head of M1/M2) ⇥{, t h 1 , t h 2 , t h 1 t h 2 } Context I[i = h1 ± 1] (left/right token of w h 1 ) I[i = h2 ± 1] (left/right token of w h 2 ) In-between I[i &gt; h1]&amp;I[i &lt; h2] (in between )</formula><p>⇥{, t h 1 , t h 2 , t h 1 t h 2 } On-path I[wi 2 P ] (on path) ⇥{, t h 1 , t h 2 , t h 1 t h 2 } <ref type="table">Table 2</ref>: Feature sets used in FCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Settings</head><p>Features Our FCM features <ref type="table">(Table 2</ref>) use a fea- ture vector f w i over the word w i , the two tar- get entities M 1 , M 2 , and their dependency path. Here h 1 , h 2 are the indices of the two head words of M 1 , M 2 , ⇥ refers to the Cartesian product be- tween two sets, t h 1 and t h 2 are entity types (named entity tags for ACE 2005 or WordNet supertags for SemEval 2010) of the head words of two entities, and stands for the empty feature. refers to the conjunction of two elements. The In-between features indicate whether a word w i is in between two target entities, and the On-path features in- dicate whether the word is on the dependency path, on which there is a set of words P , between the two entities.</p><p>We also use the target entity type as a feature. Combining this with the basic features results in more powerful compound features, which can help us better distinguish the functions of word embed- dings for predicting certain relations. For exam- ple, if we have a person and a vehicle, we know it will be more likely that they have an ART rela- tion. For the ART relation, we introduce a corre- sponding weight vector, which is closer to lexical embeddings similar to the embedding of "drive".</p><p>All linguistic annotations needed for fea- tures (POS, chunks 7 , parses) are from Stanford CoreNLP ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>). Since SemEval does not have gold entity types we obtained Word- Net and named entity tags using <ref type="bibr" target="#b2">Ciaramita and Altun (2006)</ref>. For all experiments we use 200- d word embeddings trained on the NYT portion of the Gigaword 5.0 corpus (Parker et al., 2011), with word2vec ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>). We use the CBOW model with negative sampling (15 nega- tive words). We set a window size c=5, and re- move types occurring less than 5 times.</p><p>Models We consider several methods. (1) FCM in isolation without fine-tuning. (2) FCM in isola- tion with fine-tuning (i.e. trained as a log-bilinear model). <ref type="formula" target="#formula_6">(3)</ref> A log-linear model with a rich binary feature set from Sun et al. (2011) (Baseline)- this consists of all the baseline features of <ref type="bibr" target="#b37">Zhou et al. (2005)</ref> plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research. We ex- clude the Country gazetteer and WordNet features from <ref type="bibr" target="#b37">Zhou et al. (2005)</ref>. The two remaining meth- ods are hybrid models that integrate FCM as a sub- model within the log-linear model ( § 4). We con- sider two combinations. <ref type="formula" target="#formula_3">(4)</ref> The feature set of <ref type="bibr" target="#b20">Nguyen and Grishman (2014)</ref> obtained by using the embeddings of heads of two entity mentions (+HeadOnly). (5) Our full FCM model (+FCM). All models use L2 regularization tuned on dev data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACE 2005</head><p>We evaluate our relation extraction system on the English portion of the ACE 2005 corpus ( <ref type="bibr" target="#b33">Walker et al., 2006</ref>). 8 There are 6 do- mains: Newswire (nw), Broadcast Conversation (bc), Broadcast News (bn), Telephone Speech (cts), Usenet Newsgroups (un), and Weblogs (wl). Following prior work we focus on the do- main adaptation setting, where we train on one set (the union of the news domains (bn+nw), tune hyperparameters on a dev domain (half of bc) and evaluate on the remainder (cts, wl, and the remainder of bc) <ref type="bibr" target="#b24">(Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b20">Nguyen and Grishman, 2014</ref>). We assume that gold entity spans and types are available for train and test. We use all pairs of entity mentions to yield 43,518 total relations in the training set. We report precision, recall, and F1 for relation extrac- tion. While it is not our focus, for completeness we include results with unknown entity types fol- lowing <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref>  <ref type="figure">(Appendix 1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SemEval 2010 Task 8</head><p>We evaluate on the Se- mEval 2010 Task 8 dataset <ref type="bibr">9 (Hendrickx et al., 2010)</ref> to compare with other compositional mod- els and highlight the advantages of FCM. This task is to determine the relation type (or no relation) between two entities in a sentence. We adopt the setting of <ref type="bibr" target="#b27">Socher et al. (2012)</ref>. We use 10-fold <ref type="bibr">8</ref> Many relation extraction systems evaluate on the ACE 2004 corpus ( <ref type="bibr" target="#b18">Mitchell et al., 2005</ref>). Unfortunately, the most common convention is to use 5-fold cross validation, treating the entirety of the dataset as both train and evaluation data. Rather than continuing to overfit this data by perpetuating the cross-validation convention, we instead focus on ACE 2005. cross validation on the training data to select hy- perparameters and do regularization by early stop- ping. The learning rates for FCM with/without fine-tuning are 5e-3 and 5e-2 respectively. We report macro-F1 and compare to previously pub- lished results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>ACE 2005 Despite FCM's (1) simple feature set, it is competitive with the log-linear baseline <ref type="formula" target="#formula_6">(3)</ref> on out-of-domain test sets <ref type="table" target="#tab_2">(Table 3</ref>). In the typi- cal gold entity spans and types setting, both <ref type="bibr" target="#b24">Plank and Moschitti (2013)</ref> and <ref type="bibr" target="#b20">Nguyen and Grishman (2014)</ref> found that they were unable to obtain im- provements by adding embeddings to baseline fea- ture sets. By contrast, we find that on all do- mains the combination baseline + FCM (5) obtains the highest F1 and significantly outperforms the other baselines, yielding the best reported results for this task. We found that fine-tuning of em- beddings (2) did not yield improvements on our out-of-domain development set, in contrast to our results below for SemEval. We suspect this is be- cause fine-tuning allows the model to overfit the training domain, which then hurts performance on the unseen ACE test domains. Accordingly, Ta- ble 3 shows only the log-linear model. Finally, we highlight an important contrast be- tween FCM (1) and the log-linear model (3): the latter uses over 50 feature templates based on a POS tagger, dependency parser, chunker, and con- stituency parser. FCM uses only a dependency parse but still obtains better results (Avg. F1). <ref type="table" target="#tab_3">Table 4</ref> shows FCM compared to the best reported results from the SemEval-2010 Task 8 shared task and several other compositional models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SemEval 2010 Task 8</head><p>For the FCM we considered two feature sets. We found that using NE tags instead of WordNet tags helps with fine-tuning but hurts without. This may be because the set of WordNet tags is larger mak- ing the model more expressive, but also introduces more parameters. When the embeddings are fixed, they can help to better distinguish different func- tions of embeddings. But when fine-tuning, it be- comes easier to over-fit. Alleviating over-fitting is a subject for future work ( § 9).</p><p>With either WordNet or NER features, FCM achieves better performance than the RNN and MVRNN. With NER features and fine-tuning, it outperforms a CNN ( <ref type="bibr" target="#b36">Zeng et al., 2014</ref>) and also   the combination of an embedding model and a tra- ditional log-linear model (RNN/MVRNN + lin- ear) <ref type="bibr" target="#b27">(Socher et al., 2012)</ref>. As with ACE, FCM uses less linguistic resources than many close competi- tors ( <ref type="bibr" target="#b25">Rink and Harabagiu, 2010)</ref>. We also compared to concurrent work on en- hancing the compositional models with task- specific information for relation classification, in- cluding <ref type="bibr">Hashimoto et al. (2015) (RelEmb)</ref>, which trained task-specific word embeddings, and dos <ref type="bibr" target="#b4">Santos et al. (2015)</ref>  <ref type="figure">(CR-CNN)</ref>, which proposed a task-specific ranking-based loss function. Our Hybrid methods (FCM + linear) get comparable re- sults to theirs. Note that their base compositional model results without any task-specific enhance- ments, i.e. RelEmb with word2vec embeddings and CR-CNN with log-loss, are still lower than the best FCM result. We believe that FCM can be also improved with these task-specific enhancements, e.g. replacing the word embeddings to the task- specific ones from ( <ref type="bibr" target="#b5">Hashimoto et al., 2015</ref>) in- creases the result to 83.7% (see §7.2 for details). We leave the application of ranking-based loss to future work.</p><p>Finally, a concurrent work ( <ref type="bibr" target="#b12">Liu et al., 2015)</ref> proposes DepNN, which builds representations for the dependency path (and its attached subtrees) between two entities by applying recursive and convolutional neural networks successively. Com- pared to their model, our FCM achieves compa- rable results. Of note, our FCM and the RelEmb are also the most efficient models among all above compositional models since they have linear time complexity with respect to the dimension of em- beddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Effects of the embedding sub-models</head><p>We next investigate the effects of different types of features on FCM using ablation tests on ACE 2005 (  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Effects of the word embeddings</head><p>Good word embeddings are critical for both FCM and other compositional models. In this section, we show the results of FCM with embeddings used to initialize other recent state-of-the-art mod- els. Those embeddings include the 300-d baseline embeddings trained on English Wikipedia (w2v- enwiki-d300) and the 100-d task-specific embed- dings (task-specific-d100) <ref type="bibr">10</ref> from the RelEmb pa- per ( <ref type="bibr" target="#b5">Hashimoto et al., 2015)</ref>, the 400-d embed- dings from the CR-CNN paper (dos <ref type="bibr" target="#b4">Santos et al., 2015)</ref>. Moreover, we list the best result (DepNN) in <ref type="bibr" target="#b12">Liu et al. (2015)</ref>, which uses the same embed- dings as ours. <ref type="table" target="#tab_6">Table 6</ref> shows the effects of word embeddings on FCM and provides relative compar- isons between FCM and the other state-of-the-art models. We use the same hyperparameters and number of iterations in <ref type="table" target="#tab_3">Table 4</ref>. The results show that using different embed- dings to initialize FCM can improve F1 beyond our previous results. We also find that increas- ing the dimension of the word embeddings does not necessarily lead to better results due to the problem of over-fitting (e.g.w2v-enwiki-d400 vs. w2v-enwiki-d300). With the same initial embed- dings, FCM usually gets better results without any changes to the hyperparameters than the compet- ing model, further confirming the advantage of FCM at the model-level as discussed under Ta- ble 4. The only exception is the DepNN model, which gets better result than FCM on the same embeddings. The task-specific embeddings from ( <ref type="bibr" target="#b5">Hashimoto et al., 2015</ref>) leads to the best perfor- mance (an improvement of 0.7%). This observa- <ref type="bibr">10</ref> In the task-specific setting, FCM will represent entity words and context words with separate sets of embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embeddings</head><p>Model F1</p><p>w2v-enwiki-d300 RelEmb 81.8 (2) FCM (log-bilinear) 83.4</p><p>task-specific-d100</p><p>RelEmb 82.8 RelEmb+linear 83.5 (2) FCM (log-bilinear) 83.7 w2v-enwiki-d400 CR-CNN 82.7 (2) FCM (log-bilinear) 83.0 w2v-nyt-d200 DepNN 83.6 (2) FCM (log-bilinear) 83.0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Compositional Models for Sentences In order to build a representation (embedding) for a sen- tence based on its component word embeddings and structural information, recent work on compo- sitional models (stemming from the deep learning community) has designed model structures that mimic the structure of the input. For example, these models could take into account the order of the words (as in Convolutional Neural Networks (CNNs)) <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) or build off of an input tree (as in Recursive Neural Networks (RNNs) or the Semantic Matching Energy Func- tion) <ref type="bibr" target="#b29">(Socher et al., 2013b;</ref><ref type="bibr" target="#b1">Bordes et al., 2012)</ref>.</p><p>While these models work well on sentence-level representations, the nature of their designs also limits them to fixed types of substructures from the annotated sentence, such as chains for CNNs and trees for RNNs. Such models cannot capture arbi- trary combinations of linguistic annotations avail- able for a given task, such as word order, depen- dency tree, and named entities used for relation extraction. Moreover, these approaches ignore the differences in functions between words appearing in different roles. This does not suit more general substructure labeling tasks in NLP, e.g. these mod- els cannot be directly applied to relation extraction since they will output the same result for any pair of entities in a same sentence.</p><p>Compositional Models with Annotation Fea- tures To tackle the problem of traditional com- positional models, <ref type="bibr" target="#b27">Socher et al. (2012)</ref> made the RNN model specific to relation extraction tasks by working on the minimal sub-tree which spans the two target entities. However, these specializations to relation extraction does not generalize easily to other tasks in NLP. There are two ways to achieve such specialization in a more general fashion:</p><p>1. Enhancing Compositional Models with Fea- tures. A recent trend enhances compositional models with annotation features. Such an ap- proach has been shown to significantly improve over pure compositional models. For example, <ref type="bibr" target="#b8">Hermann et al. (2014)</ref> and <ref type="bibr" target="#b20">Nguyen and Grishman (2014)</ref> gave different weights to words with dif- ferent syntactic context types or to entity head words with different argument IDs. <ref type="bibr" target="#b36">Zeng et al. (2014)</ref> use concatenations of embeddings as fea- tures in a CNN model, according to their posi- tions relative to the target entity mentions. <ref type="bibr" target="#b0">Belinkov et al. (2014)</ref> enrich embeddings with lin- guistic features before feeding them forward to a RNN model. <ref type="bibr" target="#b28">Socher et al. (2013a)</ref> and <ref type="bibr">Hermann and Blunsom (2013)</ref> enhanced RNN models by refining the transformation matrices with phrase types and CCG super tags.</p><p>2. Engineering of Embedding Features. A dif- ferent approach to combining traditional linguistic features and embeddings is hand-engineering fea- tures with word embeddings and adding them to log-linear models. Such approaches have achieved state-of-the-art results in many tasks including NER, chunking, dependency parsing, semantic role labeling, and relation extraction <ref type="bibr" target="#b17">(Miller et al., 2004;</ref><ref type="bibr" target="#b32">Turian et al., 2010;</ref><ref type="bibr" target="#b10">Koo et al., 2008;</ref><ref type="bibr" target="#b26">Roth and Woodsend, 2014;</ref><ref type="bibr" target="#b30">Sun et al., 2011;</ref><ref type="bibr" target="#b24">Plank and Moschitti, 2013)</ref>. <ref type="bibr" target="#b26">Roth and Woodsend (2014)</ref> con- sidered features similar to ours for semantic role labeling.</p><p>However, in prior work both of above ap- proaches are only able to utilize limited informa- tion, usually one property for each word. Yet there may be different useful properties of a word which can contribute to the performances of the task. By contrast, our FCM can easily utilize these features without changing the model structures.</p><p>In order to better utilize the dependency anno- tations, recently work built their models according to the dependency paths ( <ref type="bibr" target="#b13">Ma et al., 2015;</ref><ref type="bibr" target="#b12">Liu et al., 2015)</ref>, which share similar motivations to the usage of On-path features in our work.</p><p>Task-Specific Enhancements for Relation Clas- sification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific informa- tion. For example, <ref type="bibr" target="#b5">Hashimoto et al. (2015)</ref> trained task-specific word embeddings, and dos <ref type="bibr" target="#b4">Santos et al. (2015)</ref> proposed a ranking-based loss function for relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have presented FCM, a new compositional model for deriving sentence-level and substruc- ture embeddings from word embeddings. Com- pared to existing compositional models, FCM can easily handle arbitrary types of input and handle global information for composition, while remain- ing easy to implement. We have demonstrated that FCM alone attains near state-of-the-art perfor- mances on several relation extraction tasks, and in combination with traditional feature based log- linear models it obtains state-of-the-art results.</p><p>Our next steps in improving FCM focus on en- hancements based on task-specific embeddings or loss functions as in <ref type="bibr" target="#b5">Hashimoto et al. (2015;</ref><ref type="bibr" target="#b4">dos Santos et al. (2015)</ref>. Moreover, as the model pro- vides a general idea for representing both sen- tences and sub-structures in language, it has the potential to contribute useful components to vari- ous tasks, such as dependency parsing, SRL and paraphrasing. Also as kindly pointed out by one anonymous reviewer, our FCM can be applied to the TAC-KBP ( <ref type="bibr" target="#b9">Ji et al., 2010</ref>) tasks, by replac- ing the training objective to a multi-instance multi- label one (e.g. <ref type="bibr" target="#b31">Surdeanu et al. (2012)</ref>). We plan to explore the above applications of FCM in the fu- ture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Time Complexity Inference in FCM is much faster than both CNNs (Collobert et al., 2011) and RNNs (Socher et al., 2013b; Bordes et al., 2012). FCM requires O(snd) products on average with sparse features, where s is the average number of per-word non-zero feature values, n is the length of the sentence, and d is the dimension of word embedding. In contrast, CNNs and RNNs usually</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9</head><label></label><figDesc>http://docs.google.com/View?docid=dfvxd49s_36c28v9pmw</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>[A man] M1 driving what appeared to be [a taxicab] M2</head><label></label><figDesc></figDesc><table>@R 

w 

1 


w 

2 

, … 

, w 

n 


e w 

f w 

f wi e wi 

e wi 

f wi 

(w i ="driving") 

(w i is on path?) 

y 

M 1 man 
M =taxicab 

w 1 ="A" w i ="driving" 

A 

f wi 
f w1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Comparison of models on ACE 2005 out-of-domain test sets. Baseline + HeadOnly is</head><label>3</label><figDesc></figDesc><table>our 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Comparison of FCM with previously published results for SemEval 2010 Task 8.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>) We focus on FCM alone with the fea-
ture templates of Table 2. Additionally, we show 
results of using only the head embedding features 
from Nguyen and Grishman (2014) (HeadOnly). 
Not surprisingly, the HeadOnly model performs 
poorly (F1 score = 14.30%), showing the impor-
tance of our rich binary feature set. Among all the 
features templates, removing HeadEmb results in 
the largest degradation. The second most im-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ablation test of FCM on development set. 

portant feature template is In-between, while 
Context features have little impact. Remov-
ing all entity type features (t h i ) does significantly 
worse than the full model, showing the value of 
our entity type features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Evaluation of FCMs with different word 
embeddings on SemEval 2010 Task 8. 

tion suggests that the other compositional models 
may also benefit from the work of Hashimoto et 
al. (2015). 

</table></figure>

			<note place="foot" n="2"> Such embeddings have a long history in NLP, including term-document frequency matrices and their lowdimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF), Brown clusters, random projections and vector space models. Recently, neural networks / deep learning have provided several popular methods for obtaining such embeddings.</note>

			<note place="foot" n="4"> Since the focus of this paper is relation extraction, we adopt the evaluation setting of prior work which uses gold named entities to better facilitate comparison.</note>

			<note place="foot" n="5"> We use words as substructures for relation extraction, but use the general terminology to maintain model generality.</note>

			<note place="foot" n="7"> Obtained from the constituency parse using the CONLL 2000 chunking converter (Perl script).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their com-ments, and Nicholas Andrews, Francis Ferraro, and Benjamin Van Durme for their input. We thank Kazuma Hashimoto, Cícero Nogueira dos Santos, Bing Xiang and Bowen Zhou for sharing their word embeddings and many helpful discus-sions. Mo Yu is supported by the China Scholar-ship Council and by NSFC 61173073.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring compositional architectures and word vector representations for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="561" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP2006</title>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Task-oriented learning of word embeddings for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00095</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval-2 Workshop</title>
		<meeting>SemEval-2 Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic frame identification with distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the tac 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><forename type="middle">Ellis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Text Analysis Conference</title>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-08: HLT</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dependency-based neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Houfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dependency-based convolutional neural networks for sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.4546</idno>
		<title level="m">Distributed representations of words and phrases and their compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Name tagging with word clusters and discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jethran</forename><surname>Guinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zamanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004: Main Proceedings. Association for Computational Linguistics</title>
		<editor>Susan Dumais, Daniel Marcu, and Salim Roukos</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ace 2004 multilingual training corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramez</forename><surname>Zakhary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Employing word representations and regularization for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="68" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL Workshop on Vector Space Modeling for NLP</title>
		<meeting>NAACL Workshop on Vector Space Modeling for NLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic representations for domain adaptation: A case study on the tree kernelbased method for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">English gigaword fifth edition, june. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Composition of word representations improves semantic role labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">ACE 2005 multilingual training corpus. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning composition models for phrase embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="227" to="242" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining word embeddings and feature embeddings for fine-grained relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
		<respStmt>
			<orgName>August. Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
