<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Embedding with Hierarchical Relation Structure</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Rutgers Business School</orgName>
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<postCode>07102</postCode>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Search Product Center</orgName>
								<orgName type="department" key="dep2">WeChat Search Application Department</orgName>
								<address>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Embedding with Hierarchical Relation Structure</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3198" to="3207"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3198</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless , those models simply embed entities and relations into latent vectors without lever-aging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and Dist-Mult, to learn knowledge representations by leveraging the information from the HRS. Particularly , our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Graphs (KGs) are extremely useful resources for many AI-related applications, such as question answering, information retrieval and query expansion. Indeed, KGs are multi-relational directed graphs composed of entities as nodes and relations as edges. They represent information about real-world entities and relations in the form of knowledge triples, which is denoted as (h, r, t), where h and t correspond to the head and tail enti- ties and r denotes the relation between them, e.g., (Donald T rump, presidentOf, U SA). Large scale, collaboratively created KGs , such as Free- base ( <ref type="bibr" target="#b0">Bollacker et al., 2008)</ref>, WordNet <ref type="bibr" target="#b10">(Miller, 1994)</ref>, <ref type="bibr">Yago (Suchanek et al., 2007)</ref>, Gene On- tology <ref type="bibr" target="#b13">(Sherlock, 2009)</ref>, NELL <ref type="bibr" target="#b5">(Carlson et al., 2010</ref>) and Google's KG 1 , have recently become available. However, despite the impressively large sizes, the coverage of most existing KGs are far from complete. This has motivated research in knowledge base completion task, which includes KGE methods aiming to embed entities and rela- tions in KGs into low-dimensional embeddings.</p><p>In the literature, there are a number of studies about KGE models. These models embed enti- ties and relations into latent vectors and complete KGs based on these vectors, such as TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>), TransH ( <ref type="bibr" target="#b18">Wang et al., 2014</ref>) and TransR ( <ref type="bibr" target="#b9">Lin et al., 2015b</ref>). However, most of the existing works simply embed relations into vec- tors. Less efforts have been made for investigating the rich information from the relation structure. Indeed, in this research, we define a three-layer hi- erarchical relation structure (HRS), which can be conformed by relation clusters, relations and sub- relations in KGs.</p><p>• Relation clusters: Semantically similar rela- tions are often observed in Large-scales KGs. For example, the relation 'producerOf' and 'di- rectorOf' may be semantically related if both of them describe a relation between a person and a film. These semantically similar relations can make up relation clusters. We believe the in- formation from semantically similar relations is of great value, and relations in the same group can be trained in a collective way to facilitate the knowledge sharing when learning the em- beddings of related relations.</p><p>• Relations: A relation connects the head and tail entities in a knowledge triple, denoted as (h, r, t), where h and t correspond to the head and tail entities and r denotes the relation be- tween them.</p><p>• Sub-relations: There are relations that have multiple semantic meanings and can be split into several sub-relations. For example, the relation partOf has at least two seman- tics: location-related as (N ew Y ork, partOf , U SA) and composition-related as (monitor, partOf , television). We believe the sub- relations can give fine-grained descriptions for each relation.</p><p>The relation clusters, relations and sub-relations correspond to the top, middle and bottom layer of the three-layer HRS.</p><p>In this paper, we extend state-of-the-art models TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>), TransH ( <ref type="bibr" target="#b18">Wang et al., 2014</ref>) and DistMult ( <ref type="bibr" target="#b23">Yang et al., 2015</ref>) to learn knowledge representations by leveraging the rich information from the HRS. Moreover, the same technique can easily be used to extend other state- of-the-art models and utilize the HRS information. In the proposed models, for each knowledge triple (h, r, t), the embedding of r is the sum of three embedding vectors, which correspond to the three layers of the HRS respectively and therefore, the information from the HRS is leveraged. Particu- larly, instead of using additional information like text or paths, our model simply use the knowledge triples in KGs and the rich information from the HRS. Extensive experiments on popular bench- mark data sets demonstrate the effectiveness of our models.</p><p>In summary, we highlight our key contributions as follows,</p><p>1. We propose a technique by making use of the HRS information to conduct the KGE task, and extend three state-of-the-art models to utilize this technique. The technique can be easily ap- plied to other KGE models.</p><p>2. Our proposed models don't use additional in- formation like text or paths, instead, we only use the knowledge triples in KGs and take ad- vantage of the rich information from the HRS.</p><p>3. We evaluate our models on popular bench- mark data sets, and the results show that our extended models achieve substantial improve- ments against the original models as well as other state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head><p>We extend three popular KGE models by lever- aging the HRS information in this study. There- fore, in this section, we first introduce the three existing models TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>), TransH ( <ref type="bibr" target="#b18">Wang et al., 2014</ref>) and DistMult ( <ref type="bibr" target="#b23">Yang et al., 2015</ref>) in detail. Then, we further summarize other state-of-the-art models on the topic of KGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TransE, TransH and DistMult</head><p>Recently, a number of KGE models have been pro- posed. These methods learn low-dimensional vec- tor representations for entities and relations <ref type="bibr" target="#b3">(Bordes et al., 2013;</ref><ref type="bibr" target="#b18">Wang et al., 2014;</ref><ref type="bibr" target="#b9">Lin et al., 2015b</ref>). <ref type="bibr">TransE (Bordes et al., 2013</ref>) is one of the most widely used model, which views relations as trans- lations from a head entity to a tail entity on the same low-dimensional hyperplane, i.e, h + r ≈ t when (h, r, t) holds. This indicates that t should be the nearest neighbor of h + r. In this case, the score function of TransE is defined as</p><formula xml:id="formula_0">f r (h, t) = h + r − t Ln ,<label>(1)</label></formula><p>which can be measured by L 1 or L 2 norm. Posi- tive triples are supposed to have lower scores than negative ones. TransH ( <ref type="bibr" target="#b18">Wang et al., 2014</ref>) introduces a mech- anism of projecting entities into relation-specific hyperplanes that enables different roles of an en- tity in different relations. TransH models the rela- tion as a vector r on a hyperplane w r and assumes that h ⊥ + r ≈ t ⊥ when (h, r, t) holds, where h ⊥ and t ⊥ are the projection of h and t in the relation- specific hyperplane. The score function of TransH is defined as</p><formula xml:id="formula_1">f r (h, t) = h ⊥ + r − t ⊥ 2 2 ,<label>(2)</label></formula><p>where h ⊥ = h − w r hw r , t ⊥ = t − w r tw r and w r 2 = 1. Like triples in TransE, positive triples in TransH should have lower scores than negative ones.</p><p>DistMult ( <ref type="bibr" target="#b23">Yang et al., 2015</ref>) adopts a bilinear score function to compute the scores given (h, r, t) triples. The score function is defined as</p><formula xml:id="formula_2">f r (h, t) = hM r t,<label>(3)</label></formula><p>where M r is a relation-specific diagonal matrix, which represents the characteristics of a relation. Different from TransE and TransH, positive triples should have larger scores than negative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other KGE Models</head><p>Besides TransE, TransH and DistMult, there are also many models on the topic of KGE.  <ref type="bibr" target="#b12">Schlichtkrull et al., 2017</ref>) use paths as additional information, while DKRL ( <ref type="bibr" target="#b22">Xie et al., 2016</ref>) and SSP ( <ref type="bibr" target="#b21">Xiao et al., 2017</ref>) adopt text to as- sist the embedding task.</p><p>Some KGE works focus on making use of the information from relations. CTransR ( <ref type="bibr" target="#b9">Lin et al., 2015b</ref>), <ref type="bibr">TransD (Ji et al., 2015)</ref> and <ref type="bibr">TransG (Xiao et al., 2016</ref>) try to find fine-grained representations for each relation. However, these works didn't uti- lize the information from semantically similar re- lations and the HRS is also not exploited. Dif- ferent from the above studies, we believe seman- tically similar relations can make up relation clus- ters, and some relations may have multiple seman- tic meanings and can be split into fine-grained sub- relations. In this paper, we take advantage of the three-layer HRS and conduct the KGE task by ex- tending three widely used models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we provide the technical details of how to extend existing KGE models by leveraging the HRS information. We first formally define the HRS and its integration with existing models.Then we introduce the new loss functions of extended models TransE-HRS, TransH-HRS and DistMult- HRS. Finally, two variants of the HRS models and implementation details are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Relation Structure</head><p>Given a KG G = {(h, r, t)} ⊆ E × R × E, where E and R are the entity (node) set and relation (edge) set respectively. We believe the relations in KGs can make up relation clusters as well as be split into fine-grained sub-relations. On the one hand, large scale KGs always have semantically related relations. The information from semanti- cally similar relations is of great value and these relations should be trained in a collective way. In this way, meaningful associations among related relations can be utilized and less frequent relations can be enriched with more training data. On the other hand, some relations may have multiple se- mantic meanings and can be split into several sub- relations, which can provide fine-grained descrip- tions for each relation. In general, relations in KGs conform to a three-layer HRS, as shown in Fig- ure 1. The HRS include a relation cluster layer, a relation layer and a sub-relation layer, which are denoted in yellow, green and blue in <ref type="figure" target="#fig_0">Figure 1</ref> re- spectively.</p><p>For a triple (h, r, t) in the HRS model, the em- bedding of r is comprised of three parts: the re- lation cluster embedding r c , relation-specific em- bedding r and sub-relation embedding r s , which is denotes as</p><formula xml:id="formula_3">r = r c + r + r s .<label>(4)</label></formula><p>According to the above equation, the embedding of each relation can leverage the information from the three-layer HRS. The relation clusters and sub- relations are determined by k-means algorithm based on the results of TransE:</p><p>• Relation clusters. We first run TransE on a given data set and obtain the embeddings of re- lations r 1 , r 2 , r 3 , ..., r |R| , where |R| is the num- ber of relations. Then, the k-means algorithm is applied on these embeddings. In this way, we get relation clusters C 1 , C 2 , C 3 , ..., C |C| , where C is the set of relation clusters. Previous stud- ies have shown that the embeddings of semanti- cally similar relations locate near each other in the latent space ( <ref type="bibr" target="#b23">Yang et al., 2015)</ref>. In this way, we are able to find relation clusters composed of semantically related relations. • Sub-relations. TransE assumes that t − h ≈ r when (h, r, t) holds. For each triple (h, r, t), we define that r = t−h, where h and t are obtained from the results of TransE. For each relation, we collect all the r and adopt the k-means algorithm to cluster these vectors into several groups S r 1 , S r 2 , S r 3 , ..., S r nr , where n r is the number of sub- relations for relation r. Each group corresponds to a fine-grained sub-relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Function</head><p>The loss of the extended HRS model is comprised of two parts, as is shown in Equation <ref type="formula" target="#formula_4">(5)</ref>,</p><formula xml:id="formula_4">L T otal = L Orig + L HRS ,<label>(5)</label></formula><p>where L Orig is the loss function of the original model, while L HRS is the loss function for the HRS information. We know that TransE, TransH and DistMult all adopt a margin-based ranking loss. Taking TransE as an example, the loss function of TransE for the first part L Orig is shown as Equation <ref type="formula" target="#formula_5">(6)</ref>,</p><formula xml:id="formula_5">LOrig = |C| c=1 r∈Cc (h,r,t)∈∈r (h ,r,t )∈∈ r [γ + fr(h, t) − fr(h , t )]+,<label>(6)</label></formula><p>where [x] + = max(0, x), r denotes the set of positive triples for relation r and r = {(h , r, t)|h ∈ E} ∪ {(h, r, t )|t ∈ E} is the set of negative ones for relation r. γ is the margin sep- arating the positive triples from the negative ones. f r (h, t) is the score function as shown in Equation (7),</p><formula xml:id="formula_6">f r (h, t) = h + r c + r + r s − t Ln ,<label>(7)</label></formula><p>which can be measured by L 1 or L 2 norm. Posi- tive triples are supposed to have lower scores than negative ones.</p><p>The second part, L HRS , is composed of three regularized terms, which is shown in Equation <ref type="formula" target="#formula_7">(8)</ref>,</p><formula xml:id="formula_7">L HRS = λ 1 rc∈C r c 2 2 + λ 2 r ∈R r 2 2 + λ 3 rs∈S r s 2 2 ,<label>(8)</label></formula><p>where C = {C 1 , C 2 , ..., C |C| } is the set of relation clusters, S = {S r 1 , S r 2 , S r 3 , ..., S r nr |r ∈ R} is the set of fine-grained sub-relations, n r is the number of sub-relations for relation r. λ 1 , λ 2 and λ 3 are trade-off parameters. Large value of λ 1 will re- sult in the separate training of each relation, while large value of λ 2 will lead to all relations in the same relation cluster sharing the same embedding vector. λ 3 should be larger than λ 1 and λ 2 to re- strict r s to be a small value, i.e., the sub-relations from the same relation should be close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variants of the HRS Model and Implementation details</head><p>Additionally, we introduce two variants of the HRS model: the top-middle model and the middle-bottom model. The top-middle model only uses the HRS by leveraging the information from the top to the middle layer. For this model, the re- lation embedding and the loss for HRS is defined as Equation <ref type="formula" target="#formula_8">(9)</ref> and <ref type="formula" target="#formula_0">(10)</ref>.</p><formula xml:id="formula_8">r = r c + r ,<label>(9)</label></formula><formula xml:id="formula_9">L HRS = λ 1 rc∈C r c 2 2 + λ 2 r ∈R r 2 2 .<label>(10)</label></formula><p>While the middle-bottom model only utilizes the information from the middle to the bottom layer. The relation embedding and HRS loss are defined as Equation <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>.</p><formula xml:id="formula_10">r = r + r s ,<label>(11)</label></formula><formula xml:id="formula_11">L HRS = λ 2 r ∈R r 2 2 + λ 3 rs∈S r s 2 2 . (12)</formula><p>The learning process of the extended models is carried out by using the Adam ( <ref type="bibr" target="#b7">Kingma and Ba, 2014)</ref> </p><note type="other">optimizer. For the extended models of TransE, all the entity and relation embedding parameters are initialized with a uniform distri- bution U − 6 √ k , 6 √ k following TransE, where k is the dimension of the embedding space. For the extended models of TransH and DistMult, we initialize these parameters with the results of TransE. For the relation cluster embeddings and sub-relation embeddings, we initialize all the pa- rameters with the value of zero.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>In this research, we evaluate the performances of our extended models on popular bench- marks  <ref type="bibr" target="#b10">(Miller, 1994)</ref>, which provides seman- tic knowledge of words. FB15k-237 and WN18 are used for the task of link prediction, FB13 and WN11 are used for the triple classification task, while FB15k is used for both tasks. The statistics of the five data sets are summarized in <ref type="table" target="#tab_1">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>To demonstrate the effectiveness of our models, we compare results with the following baselines.</p><p>• TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>): one of the most widely used KGE models.</p><p>• TransH ( <ref type="bibr" target="#b18">Wang et al., 2014</ref>): a KGE model which adopts relation-specific hyperplanes to lay entities and relations.</p><p>• DistMult ( <ref type="bibr" target="#b23">Yang et al., 2015)</ref>: a state of the art model which uses a bilinear score function to compute scores of knowledge triples.</p><p>• CTransR (Lin et al., 2015b): a pioneering KGE model which exploits fine-grained sub-relations for each relation.</p><p>• TransD ( <ref type="bibr" target="#b6">Ji et al., 2015)</ref>: an improvement of CTransR, which embeds KGs using dynamic mapping matrices.</p><p>• TransG ( <ref type="bibr" target="#b20">Xiao et al., 2016)</ref>: the first generative KGE model that uses a non-parametric bayesian model to embed KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Link Prediction</head><p>Link prediction, a.k.a. knowledge graph comple- tion, aims to fill the missing values into incom- plete knowledge triples. More formally, the goal of link prediction is to predict either the head en- tity in a given query (?, r, t) or the tail entity in a given query (h, r, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Experimental Settings</head><p>All the parameters are set by some preliminary test. For TransE-HRS, TransE-top-middle and TransE-middle-bottom, λ 1 , λ 2 , λ 3 and the mar- gin γ are set as λ 1 = 1e − 5, λ 2 = 1e − 4, λ 3 = 1e−3, γ = 2. For the extended models of TransH, we set the parameters as λ 1 = 1e − 5, λ 2 = 1e − 5, λ 3 = 1e − 3, γ = 1. For the extended models of DistMult, the parameters are set as λ 1 = 1e − 5, λ 2 = 1e − 4, λ 3 = 1e − 3, γ = 1. For all the above models, the learning rate ς, batch size b and embedding size k are set as ς = 1e − 3, b = 4096, k = 100. The L 1 norm is adopted by the score function of TransE and its extended models. The number of relation clusters are set as 300, 120 and 10 for FB15k, FB15k-237 and WN18 respec- tively. For all the data sets, we generate 3 sub- relations for relations that have more than 500 oc- currences in the training set. For all the extended models and baselines, we produce negative triples following the "bern" sampling strategy which was introduced in TransH ( <ref type="bibr" target="#b18">Wang et al., 2014</ref>). For baselines TransE, TransH and DistMult, the em- bedding parameters of entities and relations are initialized the same way as the extended models for a fair comparison.</p><p>In the test phase, we replace the head and tail entities with all the entities in KG in turn for each triple in the test set. Then we compute a score for each corrupted triple. Note that for each corrupted triple (h , r, t ), the sub-relation is determined by t − h , i.e., the k-means model is adopted to as- sign t − h to a specific sub-relation of r. We rank all the candidate entities according to the scores. Specifically, positive candidates are supposed to precede negative ones. Finally, the rank of the cor- rect entity is stored. We compare our models with baselines using the following metrics: (1) Mean Rank (MR, the mean of all the predicted ranks); (2) Mean Reciprocal Rank (MRR, the mean of all the reciprocals of predicted ranks); (3) Hits@n (Hn, the proportion of ranks not larger than n). Lower values of MR and larger values of MRR and Hn indicate better performance. All the re- sults are reported in the "filtered" setting ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experimental Results</head><p>Evaluation results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We di- vide all the results into 4 groups. The second, third and forth group are results of TransE, TransH, DistMult and their extended models respectively, while the first group are results of other state-of- the-art competitors. Results in bold font are the best results in the group and the underlined results denote the best results in the column. From Ta- ble 1, we have the following findings: (1) Our extended models outperform the original mod- els, which indicates that the information learned from the HRS is valuable; (2) For WN18, the re- sults from 'top-middle' models of TransE, TransH and DistMult are worse than the original models, and HRS models can't outperform middle-bottom ones. We conjecture the reason lies as follows: WN18 has only 18 relations and the semantic cor- relation among relations is small. In this case, the information learned from the top to the mid- dle layer of the HRS may lead to worse results since for each relation, even though the informa- tion learned from semantically similar relations are useful, the information learned from unrelated relations may damage the results. The results in- dicate that HRS models are especially useful for KGs with dense semantic distributions over rela- tions; (3) For WN18, TransE-middle-bottom and DistMult-middle-bottom achieve the best results on MRR, Hits@10, Hits@3 and Hits@1 while failing to get the best results on MR in the same group. Further analysis shows that in the results of TransE-middle-bottom, 56 test triples get ranks more than 10000, leading to more than 110 MR loss. While in the results of DistMult-middle- bottom, there exist 37 test triples whose ranks are more than 7000, which would lead to about 50 MR loss. Indeed, MR is sensitive to these high ranks, which lead to worse results on the metric of MR; (4) From all the results, based on the good basic model DistMult, the extended models of DistMult can achieve the best performance compared with other state-of-the-art baselines CTransR, TransD and TransG.</p><p>We also provide some case studies on rela- tion clusters and sub-relations. <ref type="table" target="#tab_3">Table 3</ref> shows some relation clusters of FB15k. Cluster 1 to 3 are Olympics-related, basketball-related and software-related relations respectively. From Ta- ble 3 we can see that semantically related relations can join the same cluster. <ref type="table" target="#tab_4">Table 4</ref> shows some (head, tail) pairs for the sub-relations of '/educa- tional institution/education/degree'. Sub-relation 1 to 3 are about the degree of Doctor, Master and Bachelor respectively. <ref type="table" target="#tab_5">Table 5</ref> gives some (head, tail) pairs for the sub-relations of '/mu- sic/artist/genre'. Sub-relation 1 and 2 are about rock music and pop music respectively while sub- cluster 3 is about other kinds of music. From Ta- ble 4 and 5, we can see that different sub-relations give fine-grained descriptions for each relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Parameter Study</head><p>In this section, we study the performance affected by the number of relation clusters N 1 as well as the number of sub-relations for each relation N 2 . The results in <ref type="figure">Figure 2</ref> and 3 clearly show that there exists an optimal value of N 1 and N 2 for each dataset. All three models keep achiev- ing better results as we increase the number of clusters from 0 to the optimal value. Then, after N 1 and N 2 exceed the optimal point, the perfor- mance starts falling down. The reason lies as: <ref type="formula" target="#formula_0">(1)</ref> Smaller value of N 1 leads to large-sized relation clusters. Some unrelated relations may join in the same large-sized cluster and degrade the perfor- mance of our models. Larger value of N 1 leads to small-sized relation clusters, thus less informa- tion can be leveraged by each relation, leading to the unsatisfying performance; (2) Smaller value of N 2 can't provide sufficient representations for each relation and degrade the performance of our models. Larger value of N 2 may lead to lacking of training data for each sub-relation and also result in the unsatisfying performance. Link prediction results on FB15k, FB15k-237 and WN18. We implement TransE, TransH, DistMult and their extended models by ourselves.</p><p>The code of CTransR, TransD and TransG are taken from https://github.com/thunlp/TensorFlow-TransX, https://github.com/thunlp/KB2E and https://github.com/BookmanHan/Embedding respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FB15k</head><p>FB15k <ref type="table" target="#tab_1">-237  WN18   MR MRR H10  H3  H1  MR MRR H10  H3  H1  MR MRR H10  H3</ref>     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Triple Classification</head><p>In order to testify the discriminative capability of our models, we conduct a triple classification task aiming to predict the label (True or False) of a given triple (h, r, t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Experimental Settings</head><p>In this paper, we use three datasets WN11, FB13 and FB15k to evaluate our models. The data sets WN11 and FB13 released by <ref type="bibr">NTN (Socher et al., 2013</ref>) already have negative triples. The test set of FB15k only contains correct triples, which re- quires us to construct negative triples. In this study, we construct negative triples following the same setting used for FB13 <ref type="bibr" target="#b15">(Socher et al., 2013)</ref>. relations that have more than 500 occurrences in the training set. Other parameters are set as in- troduced in Section 4.3.1. We follow the same decision process as <ref type="bibr">NTN (Socher et al., 2013)</ref>: for TransE and TransH, a triple is predicted to be positive if f r (h, t) is below a threshold, while for DistMult, a triple is regarded as a positive one if f r (h, t) is above a threshold; otherwise negative. The thresholds are determined on the validation set. We adopt accuracy as our evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Experimental Results</head><p>Finally, the evaluation results in <ref type="table" target="#tab_6">Table 6</ref> lead to the following findings: (1) Our models outperform other baselines on WN11 and FB15k, and obtain comparable results with baselines on FB13, which validate the effectiveness of our models; (2) The extended models TransE-HRS, TransH-HRS and DistMult-HRS achieve substantial improvements against the original models. On WN11, TransE-  <ref type="figure" target="#fig_4">Figure 4</ref> shows the classification accuracy of different rela- tions on WN11. We can see that extended models significantly improve the original models in each relation classification task, which again validate the effectiveness of our models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we found that relations in KGs con- form to a three-layer HRS. This HRS model pro- vides a critical capacity for embedding entities and relations, and along this line we extended three state-of-the-art models to leverage the HRS infor- mation. The technique we used can be easily ap- plied to extend other KGE models. Moreover, our proposed models don't need additional informa- tion like text or paths, instead, we made full use of the knowledge triples in KGs and the rich in- formation from the HRS. We evaluate our model on the link prediction task and triple classification task. The results show that our extended models achieve substantial improvements against the orig- inal models as well as other baseline competitors.</p><p>In the future, we will utilize more sophisticated models to leverage the HRS information, e.g, (1) utilize the embeddings of the three layers in a more sophisticated way instead of sum them together; (2) determine the number of relation clusters and sub-relations automatically instead of manually.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hierarchical Relation Structure</figDesc><graphic url="image-1.png" coords="4,86.17,62.81,425.20,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>FB15k (Bordes et al., 2013), FB15k- 237 (Toutanova and Chen, 2015), FB13(Socher et al., 2013), WN18 (Bordes et al., 2013) and WN11 (Socher et al., 2013). FB15k, FB15k- 237 and FB13 are extracted from Freebase (Bol- lacker et al., 2008), which provides general facts of the world. WN18 and WN11 are obtained from WordNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The Change of Hits@10 with The Value of N 1 Increasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification Accuracies of Different Relations on WN11</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Statistics of the Five Datasets.</head><label>1</label><figDesc></figDesc><table>Dataset 
|E| 
|R| 
#triples in Train/Valid/Test 
FB15k 
14,951 1,345 483,142 / 50,000 / 59,071 
FB15k-237 14,541 237 
272,115 / 17,535 / 20,466 
FB13 
75,043 13 
316,232 / 5,908 / 23,733 
WN18 
40,943 18 
141,442 / 5,000 / 5,000 
WN11 
38,696 11 
112,581 / 2,609 / 10,544 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of Relation Clusters in FB15k 

relations 

1 
/olympics/olympic athlete/medals won./olympics/olympic medal honor/country 
/olympics/olympic athlete/country./olympics/olympic athlete affiliation/country 

2 

/sports/sports team/roster./basketball/basketball roster position/player, 
/basketball/basketball team/roster./sports/sports team roster/player 
/basketball/basketball team/roster./basketball/basketball roster position/player 

3 
/computer/software/developer, /computer/operating system/developer, 
/cvg/computer videogame/developer 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Examples of Sub-relations for Relation '/educational institution/education/degree' in FB15k 

(head, tail) 
1 (Munich Institute of Technology, Doctors of Medicine), (California Institute of Technology, Higher Doctorate), ... 
2 (Central Michigan College of Education, M.Sc.), (The University of Pittsburgh, M.Sc.), ... 
3 (University of Massaschusetts, Amherst, Bachelor's Degree), (New Mexico State College, Bachelor's Degree), ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Examples of Sub-relations for Relation '/music/artist/genre' in FB15k 

(head, tail) 
1 (Steve Stills, Rock Music), (Velvet Underground, Rock Music), (Benjamin Chase Harper, Rock Music), ... 
2 
(Justin Beiber, Pop Music), (Natalie Maria Cole, Pop Music), (Peter Thorkelson, Pop Music), ... 
3 
(Billy Preston, R &amp; B), (Earth Wind Fire, Funk Rap), (Alvin Joiner, Hip-hop), ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Triple Classification Results. The results of 
baselines on WN11 and FB13 are directly taken from 
the original paper except DistMult. We obtain other 
results by ourselves. 

Model 
WN11 FB13 FB15k Avg 
CTransR 
85.7 
-
84.4 
-
TransD 
86.4 
89.1 
88.2 
87.9 
TransG 
87.4 
87.3 
88.5 
87.7 
TransE 
75.9 
81.5 
78.7 
78.7 
TransH 
78.8 
83.3 
81.1 
81.1 
DistMult 
87.1 
86.2 
86.3 
86.5 
TransE-HRS 
86.8 
88.4 
87.6 
87.6 
TransH-HRS 
87.6 
88.9 
88.7 
88.4 
DistMult-HRS 
88.9 
89.0 
89.1 
89.0 

HRS outperforms TransE with a margin as large 
as 10.9%. These improvements indicates the tech-
nique of utilizing the HRS information is capable 
to be extended to different KGE models. </table></figure>

			<note place="foot" n="1"> https://www.google.com/intl/es419/insidesearch/features/ search/knowledge.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The research <ref type="bibr">work</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A semantic matching energy function for learning with multi-relational data. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Human Language Technology</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Sherlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Institute of Food Science and Technology Journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">415</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proje: Embedding projection for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Weninger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Yago:a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Continuous Vector Space MODELS and Their Compositionality</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transg : A generative model for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssp: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
