<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Interaction Network for Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Information Engineering CAS</orgName>
								<orgName type="laboratory">State Key Laboratory of Information Security</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><surname>Shanghai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Interaction Network for Natural Language Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1576" to="1585"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1576</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Attention-based neural models have achieved great success in natural language inference (NLI). In this paper, we propose the Con-volutional Interaction Network (CIN), a general model to capture the interaction between two sentences, which can be an alternative to the attention mechanism for NLI. Specifically , CIN encodes one sentence with the filters dynamically generated based on another sentence. Since the filters may be designed to have various numbers and sizes, CIN can capture more complicated interaction patterns. Experiments on three very large datasets demonstrate CIN&apos;s efficacy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language inference (NLI) is a pivotal and challenging natural language processing (NLP) task. The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothe- sis. Generally, NLI is also related to many other NLP tasks under the paradigm of semantic match- ing of two texts, such as question answering <ref type="bibr" target="#b12">Hu et al. (2014)</ref>; <ref type="bibr" target="#b25">Wan et al. (2016)</ref> and information retrieval <ref type="bibr" target="#b17">Liu et al. (2015)</ref>, and so on. An essential challenge is to capture the semantic relevance of two sentences. Due to the semantic gap (or lexical chasm) problem, natural language inference is still a challenging problem.</p><p>Recently, deep learning is raising a substan- tial interest in natural language inference and has achieved some great progresses <ref type="bibr" target="#b12">Hu et al. (2014)</ref>; <ref type="bibr" target="#b19">Parikh et al. (2016)</ref>; <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>. To model the complicated semantic relationship be- tween two sentences, previous models heavily uti- lize various attention mechanism <ref type="bibr">Bahdanau</ref>   , and so on. While attention is very successful in natural language inference, its mechanism is quite simple and can be regarded as a weighted sum of the target vectors. This paradigm results in a lack of flexibility in more complicated interaction model. In this paper, we propose a new interaction module, called Convolutional Interaction Network (CIN), which can serve as an alternative module of attention mechanism. Specifically, CIN utilizes convolutional neural network to extract the valued features (or representations) from sentences. In the case of NLI, whether a feature of one sentence being important depends on another sentence. In- spired by the idea of using one network to gen- erate the parameters of another network <ref type="bibr" target="#b9">Ha et al. (2016a);</ref>, we intro- duce a filter generation network to dynamically generate convolutional filters. Each sentence is convolved by a dynamically generated filter by another sentence. Thus, the convolved features of one sentence can be regarded as context-aware representations under the influence of another sen- tence.</p><p>The contributions of this paper can be summa- rized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">CIN is a new interaction model, invented as</head><p>an alternative module to the attention model. CIN can also capture both the intra-or inter- interactions of two sentences.</p><p>2. Compared to attention model, CIN is more general and flexible to capture the compli- cated interaction. As discussed in Section 3.3, the attention model is approximately equivalent to a special case of CIN.</p><p>3. We perform extensive empirical studies on three very large datasets. Experiment results demonstrate that our proposed architecture is effective for natural language inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attentive Interaction for Natural Language Inference</head><p>Currently, the dominative method for natural lan- guage inference is to use attention mechanism to model the interaction between two sentence. Given two input sentences</p><formula xml:id="formula_0">x = [x 1 , x 2 , · · · , x m ] and y = [y 1 , y 2 , · · · , y n ]</formula><p>with length m and n respectively, we first encode them into two vectorial sequences</p><formula xml:id="formula_1">X = [x 1 , x 2 , · · · , x m ] ∈ R d×m ,<label>(1)</label></formula><formula xml:id="formula_2">Y = [y 1 , y 2 , · · · , y n ] ∈ R d×n .<label>(2)</label></formula><p>The encoder usually consists of one or several CNN/RNN layers to get the context-aware token representations.</p><p>To capture the interaction between two sen- tence, various neural attentions can be used, such as sentence2word attention <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref>, word2word attention <ref type="bibr" target="#b19">Parikh et al. (2016)</ref>; <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word2word</head><p>Attentive Interaction The word2word attention captures the dependency between two words x i and y j from the concerned two sentences respectively.</p><p>The word2word attention computes a similarity matrix M , in which each element m i,j is the alignment score between x i and y j .</p><formula xml:id="formula_3">m i,j = f (x i , y j ), 1 ≤ i ≤ m, 1 ≤ j ≤ n, (3)</formula><p>where f is a score function.</p><p>There are two most prevalent attention func- tions: multiplicative attention and additive atten- tion. Multiplicative attention is:</p><formula xml:id="formula_4">f (x i , y j ) = x T i y j .<label>(4)</label></formula><p>Additive attention computes a compatibility func- tion by a feed-forward network with a single hid- den layer.</p><formula xml:id="formula_5">f (x i , y j ) = w σ(W 1 x i + W 2 y j + b),<label>(5)</label></formula><p>where w, W 1 , W 2 and b are learnable parame- ters.</p><p>While these two kinds of attentions have similar performance, the multiplicative attention is more popular in practice since it requires less computa- tion power and have less memory demand with op- timized matrix multiplication. With multiplicative attention, we can compute the mimic representa- tions for both X and Y .</p><formula xml:id="formula_6">¯ X = Y softmax(Y T X) ∈ R d×m , (6) ¯ Y = Xsoftmax(X T Y ) ∈ R d×n ,<label>(7)</label></formula><p>where softmax(·) is column-wise normalization function. Each vector ¯ x i ∈ ¯ X is called as mimic vector, which is a weighted summation of {y j } n j=1 . Intuitively, the mimic vector ¯ x i provides the related information of token x i extracted from sentence Y .</p><p>Prediction After interaction, a prediction mod- ule is used to aggregate the interaction informa- tion and extract the fix-length representation of two sentences. Finally, the final sentence repre- sentations are fed into a feed-forward network to predict the relationship between two sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional Interaction Network</head><p>In this section, we propose a new interaction method by utilizing dynamic convolutional filters, called Convolutional Interaction Network (CIN). CIN can serve as an alternative module of atten- tion mechanism.</p><p>We first briefly introduce how the convolution works over text sequence, then describe the pro- posed model and its connection to attention model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolution over Sequence</head><p>Convolution is an effective operation in deep neu- ral networks, which convolves the input with a set of filters to extract non-linear compositional fea- tures. Although originally designed for computer vision, convolutional models have subsequently shown to be effective for NLP and have achieved excellent performance in sentence modeling <ref type="bibr" target="#b15">Kim (2014)</ref>; <ref type="bibr" target="#b14">Kalchbrenner et al. (2014)</ref>, and other tra- ditional NLP tasks <ref type="bibr" target="#b12">Hu et al. (2014)</ref>; <ref type="bibr" target="#b29">Zeng et al. (2014)</ref>; <ref type="bibr" target="#b6">Gehring et al. (2017)</ref>.</p><p>Given a sentence representation X = [x 1 , x 2 , · · · , x m ] ∈ R d×m , a convolutional filter W (f ) ∈ R d×kd , the convolution process is defined as where f (·) is a non-linear activation function, such as ReLU, k indicates the size of convolution win- dow, and</p><formula xml:id="formula_7">x i = f W (f ) [x i−[k/2] , · · · , x i+[k/2] ] + b (f ) ,<label>(8)</label></formula><formula xml:id="formula_8">b (f ) ∈ R d is a bias vector.</formula><p>The convolution can be abbreviated as</p><formula xml:id="formula_9">X = f (W (f ) ⊗ X) ∈ R d×m ,<label>(9)</label></formula><p>where ⊗ denotes the convolutional operation. To ensure the output of convolution has equal length as to the input, we pad [ k 2 ] zero vectors on both sides of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolutional Interaction Network</head><p>Convolution is very effective when it comes to extracting useful features from a sentence. But for NLI, whether a word (or feature) being im- portant in one sentence depends on another sen- tence. Therefore, a better convolution operation should have the ability to extract substantial fea- tures from one sentence according to another sen- tence. Thus, the convolutional filter should be dynamically changeable. Inspired by ; <ref type="bibr" target="#b10">Ha et al. (2016b)</ref>, we propose a filter gen- eration network (FGN) to generate a dynamical filter, which is used to extract the context-aware information.</p><p>Given two sentences x, y, and their representa-</p><formula xml:id="formula_10">tions X = [x 1 , x 2 , · · · , x m ] ∈ R d×m and Y = [y 1 , y 2 , · · · , y n ] ∈ R d×n ,</formula><p>the filter for each sen- tence is generated according to the other sentence by</p><formula xml:id="formula_11">W (f ) x = FGN(X) ∈ R d×τ d ,<label>(10)</label></formula><formula xml:id="formula_12">W (f ) y = FGN(Y ) ∈ R d×τ d ,<label>(11)</label></formula><p>where τ is the width of filter, FGN(·) is the filter generation network. A detailed implementation of FGN is presented in Section 3.4. Now we can convolve the two sentences with the generated filters.</p><formula xml:id="formula_13">¯ X = f (W (f ) y ⊗ X) ∈ R d×m ,<label>(12)</label></formula><formula xml:id="formula_14">¯ Y = f (W (f ) x ⊗ Y ) ∈ R d×n ,<label>(13)</label></formula><p>where the attained matrix ¯ X and ¯ Y can be re- garded as the context-aware representation of sen- tences x and y, depending on each other. <ref type="figure" target="#fig_1">Figure 1</ref> gives an illustration of CIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Connection to Attentive Interaction</head><p>CIN is more general than attention model. Assum- ing that we set k = 1 and FGN to be a function of FGN(X) = XX T , Eq. <ref type="formula" target="#formula_1">(12)</ref> and <ref type="formula" target="#formula_1">(13)</ref> of CIN can be written as</p><formula xml:id="formula_15">¯ X = (Y Y T )X = Y (Y T X),<label>(14)</label></formula><formula xml:id="formula_16">¯ Y = (XX T )Y = X(X T Y ).<label>(15)</label></formula><p>Compared to Eq. (6) and <ref type="formula" target="#formula_6">(7)</ref>, under the above assumption, CIN is equivalent to the word2word multiplicative attention model without softmax normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">An Implementation of Filter Generation</head><p>Network (FGN)</p><p>To generate the dynamic filters, the key factor is how to choose the filter generation network FGN(·) in Eq. <ref type="formula" target="#formula_1">(10)</ref> and <ref type="formula" target="#formula_1">(11)</ref>. Although many so- phisticated networks can be employed, we give an simple implementation in this paper.</p><p>For ease of presentation, we only describe how we generate dynamical filter according to sentence x. The same procedure is utilized for sentence y.</p><p>Firstly, we summarize the information of sen- tence x with an over-time k-max pooling on X,</p><formula xml:id="formula_17">U x = ReLU(W u ⊗ X) ∈ R d×m ,<label>(16)</label></formula><formula xml:id="formula_18">z 1:k x = k-max(U x ) ∈ R d×k ,<label>(17)</label></formula><p>where U x is a non-linear transformation of X by convolution filter W u ∈ R d×d . The idea of k-max pooling is to capture the most important features (the k highest values) from sentence X.</p><p>Then we generate k filters W j x for j = 1, · · · , k by</p><formula xml:id="formula_19">W j x = ReLU P diag(z j x )Q + B ,<label>(18)</label></formula><p>where</p><formula xml:id="formula_20">P ∈ R d k ×d , Q ∈ R d×τ d and B ∈ R d k ×τ d</formula><p>are learnable parameters. The final filter is obtained by concatenating the k generated filters,</p><formula xml:id="formula_21">W (f ) x = [W 1 x ; W 2 x ; · · · ; W k x ] ∈ R d×τ d . (19)</formula><p>Similar to x, we can also obtain the dynamic filters W (f ) y according to the sentence y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Incorporating CIN into a Deep</head><p>Network Architecture for NLI Our overall network architecture for NLI is based on a successful model proposed by <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>. The major difference is that we use CIN to capture the interaction, instead of bi-directional attention.</p><p>The network architecture consists of three com- ponents: (1) an encoding layer; (2) convolutional interaction layers; (3) a prediction layer. <ref type="figure" target="#fig_3">Figure 2</ref> gives an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding Layer</head><p>The input of natural language inference task is a pair of sentences x and y. Since each word in a sentence is a symbol that can not be directly pro- cessed by neural networks, we need first map each word to a d dimensional embedding vector.</p><p>Thus, the two sentences are mapped to two ma- trix E x ∈ R de×m and E y ∈ R de×n respectively. We also use syntactical and lexical information such as part of speech tagging information, ex- act match feature and character representation. In this paper, exact match value of each word is set to 1(default to be 0) if the word concerned share the same stem or lemma with any word in coun- terpart sentence. Character representation of the word is obtained using a convolution neural net- work followed by a max pooling along sequence length dimension as same as <ref type="bibr" target="#b15">Kim (2014)</ref>. The fi- nal representation of word is a concatenation of word embedding, character encoded vector, POS tagging embedding and exact match feature. Both character embedding and POS tagging embedding are randomly initialized. All embeddings are up- dated during training.</p><p>We use bi-directional LSTM (BiLSTM) <ref type="bibr" target="#b11">Hochreiter and Schmidhuber (1997)</ref> to in- corporate the forward and backward context information of sequence. Thus, we can get the  phrase-level encoding of two input sentences,</p><formula xml:id="formula_22">X = [Bi-LSTM(E x ); E x ],<label>(20)</label></formula><formula xml:id="formula_23">Y = [Bi-LSTM(E y ); E y ],<label>(21)</label></formula><p>where X ∈ R d×m and Y ∈ R d×n are the phrase- level encoding representation of sentence x and y respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convolutional Interaction Layers</head><p>In the interaction layers, we use our proposed CIN to model the interaction between two sentences. We first dynamically generate context-aware fil- ters W (f )</p><p>x and W (f ) y based on the sentence encod- ings X and Y respectively, which are further used for both intra-sentence and inter-sentence interac- tion.</p><p>Intra-Sentence Interaction The intra-sentence convolutional interaction is to convolve one sen- tence by the filter generated by itself.</p><formula xml:id="formula_24">X intra = f (W (f ) x ⊗ X),<label>(22)</label></formula><formula xml:id="formula_25">Y intra = f (W (f ) y ⊗ Y ),<label>(23)</label></formula><p>The role of the intra-sentence convolutional in- teraction is the same as self-attention <ref type="bibr" target="#b23">Shen et al. (2017)</ref>, which is also very useful in NLI.</p><p>Inter-Sentence Interaction The inter-sentence interaction takes filters generated from the coun-terpart sentence to convolve the inputs.</p><formula xml:id="formula_26">X inter = f (W (f ) x ⊗ Y ),<label>(24)</label></formula><formula xml:id="formula_27">Y inter = f (W (f ) y ⊗ X),<label>(25)</label></formula><p>The inter-sentence convolutional interaction plays a role similar to the cross-attention between two sentences.</p><p>Fusion Layer After CIN, we can fuse two kinds of context-aware representations of each sentence. For sentence x, the X intra and X inter represent the extracted features under consideration of in- formation of itself and sentence y respectively.</p><p>To efficiently utilize X intra and X inter , a fusion layer is used. We use the comparing operation pro- posed in <ref type="bibr" target="#b3">Chen et al. (2017a)</ref> to fuse the two kinds of representation. Let u i and v i be intra and in- ter attentive vector of the i-th word in sentence x, a heuristic and effective composition operator is used to combine two vectors.</p><formula xml:id="formula_28">¯ x (c) i = [u i ; v i ; u i − v i ; u i v i ; |u i − v i |], (26) x (c) i = ReLU(W c ¯ x c i + W x x i + b c ),<label>(27)</label></formula><p>Thus, we can obtain two fused representations X (c) and Y (c) for two sentences, which are fur- ther fed into the prediction layer or another stacked interaction layer. The interaction layers can be stacked for N x times to capture the complicated matching information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prediction Layer</head><p>After interaction layers, an aggregation layer is employed to aggregate the two sequences of fus- tion vectors X (c) and Y (c) into a fixed-length matching vectors. The aggregation component usually consists of another BiLSTM layer and a following pooling layer. We then perform max pooling over time for both X (c) and Y (c) to get two fix representation vector for two sentences, p and h:</p><formula xml:id="formula_29">p = max(X (c) ),<label>(28)</label></formula><formula xml:id="formula_30">h = max(Y (c) ),<label>(29)</label></formula><p>where the functions max is the max pooling oper- ations over time steps. Finally, the pooled vectors are composed as one relation vector and fed into a feed-forward net- work to predict the relationship between two sen- tences. Specially, the two-layer feed-forward net- work has one hidden layers with tanh activation  and sof tmax output layer in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train Dev Test Len(P) Len(H) Vocab</head><formula xml:id="formula_31">m = [p; h; p − h; p * h; |p − h|],<label>(30)</label></formula><formula xml:id="formula_32">p(·|x, y) = FNN(m).<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>Given a trainset</p><formula xml:id="formula_33">{x (i) , y (i) , t (i) } N i=1</formula><p>, the objective is to minimize a cross entropy loss J (θ):</p><formula xml:id="formula_34">J (θ) = − 1 N i log p(t (i) |x (i) , y (i) )+λ||θ|| 2 2 ,<label>(32)</label></formula><p>where θ represents all the connection weights. We use the Adam optimizer <ref type="bibr" target="#b16">Kingma and Ba (2014)</ref> with an initial learning rate of 0.0004. De- fault L2 regularization λ is set to 10 −6 . To avoid overfitting, dropout is applied after each fully con- nected, recurrent or convolutional layer.</p><p>Initialization We take advantage of pre-trained word embeddings such as Glove <ref type="bibr" target="#b20">Pennington et al. (2014)</ref> to transfer more knowledge from vast un- labeled data. For the words that don't appear in Glove, we randomly initialize their embeddings from a normal distribution with mean 0.0 and stan- dard deviation 0.1.</p><p>The network weights are initialized with Xavier normalization <ref type="bibr" target="#b7">Glorot and Bengio (2010)</ref> to main- tain the variance of activations throughout the for- ward and backward passes. Biases are uniformly set to zero when the network is constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>To make quantitative evaluation, our model was evaluated on three well known datasets: Stan- ford Natural Language Inference dataset (SNLI), MultiNLI dataset and Quora Question pair dataset (Quora). Detailed statistical information of these datasets is shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Train Test   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Results</head><p>We use the accuracy to evaluate the performance of our convolutional interaction network (CIN) and other models on SNLI, MultiNLI and Quora.</p><p>SNLI <ref type="table" target="#tab_3">Table 2</ref> shows the results of different models on the train set and test set of SNLI. The first row gives a baseline model with handcrafted features presented by <ref type="bibr" target="#b2">Bowman et al. (2015)</ref>. All the other models are attention-based neural net- works. <ref type="bibr" target="#b26">Wang and Jiang (2016)</ref> exploits the long short-term memory (LSTM) for NLI. <ref type="bibr" target="#b19">Parikh et al. (2016)</ref> uses attention to decompose the problem into subproblems that can be solved separately. <ref type="bibr" target="#b3">Chen et al. (2017a)</ref> incorporates the chain LSTM and tree LSTM jointly. Zhiguo Wang (2017) pro- poses a bilateral multi-perspective matching for NLI.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, the second block gives the single models. As we can see, our proposed model CIN achieves 88.0% in accuracy on SNLI test set. Compared to the previous work, CIN obtains com- petitive performance.</p><p>To further improve the performance of NLI sys- tems, researchers have built ensemble models. En- semble systems obtained the best performance on SNLI. Our ensemble model obtains 89.1% in ac- curacy and outperforms the current state-of-the-art model. Overall, single model of CIN performs compet- itively well and outperforms the previous models on ensemble scenarios for the natural language in- ference task.</p><p>MultiNLI <ref type="table" target="#tab_5">Table 3</ref> shows the performance of different models on MultiNLI. The original aim of this dataset is to evaluate the quality of sentence representations. Recently this dataset is also used to evaluate the interaction model involving atten- tion mechanism.</p><p>The first line of <ref type="table" target="#tab_5">Table 3</ref> gives a baseline model without interaction. The second block of <ref type="table" target="#tab_5">Table 3</ref> gives the attention-based models. The proposed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Match Mismatch   Quora <ref type="table" target="#tab_6">Table 4</ref> shows the performance of differ- ent models on the Quora test set. The baselines on <ref type="table" target="#tab_6">Table 4</ref> are all implemented in Zhiguo <ref type="bibr">Wang (2017)</ref>. The Siamese-CNN model and Siamese- LSTM model encode sentences with CNN and LSTM respectively, and then predict the re- lationship between them based on the cosine similarity. Multi-Perspective-CNN and Multi- Perspective-LSTM are transformed from Siamese- CNN and Siamese-LSTM respectively by replac- ing the cosine similarity calculation layer with their multi-perspective cosine matching function. The L.D.C is a general compare-aggregate frame- work that performs word-level matching followed by a aggregation of convolution neural networks.</p><p>As we can see, our model outperforms the base-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Dev Test CIN 88.6 88.0 Remove whole interaction 85.6 85.1 Remove intra-attention 88.1 87.7  lines and achieve 88.62% in the test sets of Quora corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Ablation</head><p>To better understand the performance of our model, we analyze the effect of each key compo- nent of the proposed model. As illustrated in <ref type="table" target="#tab_7">Table  5</ref>, the first row is the full CIN model. By dropping convolutional interaction layers, the performance decreases to 85.1% on the test set, which indi- cate the interaction information is crucial for NLI. By just dropping intra-attention layer, the perfor- mance decreases to 87.7% on the test set. Accord- ing to the results, all of the components positively contribute to the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>To give an intuitive understanding of how our model works, we give an analysis on the follow- ing case from the test set.</p><p>Premise: A girl playing a violin along with a group of people. Hypothesis: A girl is playing an instrument. Label: Entailment.</p><p>The visualization results are produced from model with two stacked CINs. X, Y is the hid- den states at encoding layer, and   || 2 to show its contribution to final prediction. <ref type="table" target="#tab_8">Table 6</ref> shows the gradient scales of hidden states of each word in the encoding layer and the first CIN layer. As we can see, some phrases (like playing a violin and playing an instrument) in- stead of isolated words (like violin and instrument) become more focused after a CIN layer. It implies CIN could capture some higher level patterns. <ref type="figure" target="#fig_6">Figure 3</ref> gives a visualization of correlations of hidden states of two sentences. (a) shows the correlations after the encoding layer, the same words are most correlated. This is because em- bedding layer and encoding layer are shared be- tween premise and hypothesis. (b) shows the cor- relations after the first CIN layer, the correlation exists between phrases {playing a violin vs. play- ing an instrument}, instead of the same words. The interaction layer connects playing in Premise to Hypothesis instrument, and connects playing in Hypothesis to Premise violin. Thus, the correla- tion between instrument in Hypothesis and violin in Premise are boosted, as we know these are im- portant to reasoning.</p><formula xml:id="formula_35">X (c) , Y (c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There are mainly two threads of work related to ours.</p><p>One thread of work is using attention-based model for natural language inference (NLI). NLI has been widely investigated for many years. Ben- efiting from the development of deep learning and the availability of large-scale annotated datasets, deep neural models have achieved great success. <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> firstly use LSTM with attention for text matching task. <ref type="bibr" target="#b26">Wang and Jiang (2016)</ref> use word-by-word attention to exploit the word-level match. <ref type="bibr" target="#b19">Parikh et al. (2016)</ref> propose a new framework to model the relationship between two sentences using interact-compare-aggregate architecture. <ref type="bibr" target="#b3">Chen et al. (2017a)</ref> incorporates the chain LSTM and tree LSTM jointly. Zhiguo Wang (2017) use self-attention mechanism to capture contextual information from the whole sentence.</p><p>Unlike the above models, we use an alternative model to capture the complicate interaction infor- mation of two sentences.</p><p>Another thread of work is the idea of using one network to generate the parameters of another net- work. De  proposed the dynamic filter network to implicitly learn a variety of filtering operations. <ref type="bibr" target="#b9">Ha et al. (2016a)</ref> proposed the model hypernetwork, which uses a small net- work to generate the weights for a larger network.</p><p>Unlike these models, our dynamical filter is em- ployed for interaction. Therefore, a filter genera- tion function is proposed to capture the related in- tra and inter dependent information of a sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose an alternative interaction model, Convolutional Interaction Network (CIN), for natural language inference. CIN utilizes the dynamic convolutional filters to model the inter- action between two sentences. Specifically, each sentence is convolved by dynamical filters gener- ated based on another sentence. CIN is more gen- eral and flexible since the filters may have various numbers and sizes, thereby capturing more com- plicated interaction patterns. Experiments on three very large datasets demonstrate the efficacy of our proposed model.</p><p>In future work, we hope to improve the extensi- bility of CIN and apply it to other NLP tasks, such as machine comprehension. supported by Shanghai Municipal Science and Technology Commission (No. 17JC1404100 and 16JC1420401), and National Natural Sci- ence Foundation of China (No. 61672162 and 61751201), Fundamental Theory and Cutting- Edge Technology Research Program of Institute of Information Engineering, CAS (Grant No. Y7Z0281102).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>2014); Vaswani et al. (2017) to build the interac- tion at different granularity (word, phrase and sen- tence level), such as ABCNN Yin et al. (2016), Attention LSTM Rocktäschel et al. (2015), bi- directional attention LSTM Chen et al. (2017a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Convolutional Interaction Network. ⊗ denotes the convolution operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall network architecture for natural language inference. The N x means the number of the stacking interaction layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>SNLI</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Handcrafted</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A visualization of word to word correlation. Darker color correspond to a higher correlation. (a) Correlation of X T Y at encoder layer. (b) Correlation of X (c) T Y (c) at first CIN layer.</figDesc><graphic url="image-7.png" coords="8,123.31,124.30,62.81,114.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Performance on SNLI dataset.</head><label>2</label><figDesc></figDesc><table>SNLI The SNLI corpus Bowman et al. (2015) 
consists of 570,152 sentence pairs. Each sentence 
pair is labeled as one of entailment, contradiction 
and neutral relation. 

MultiNLI Orgnized the same as SNLI, 
MultiNLI corpus Williams et al. (2017) is another 
dataset for NLI, it contains 433,000 sentence 
pairs. Like SNLI, each pair is labeled with one 
of entailment, contradiction and neutral label. 
Difference between MultiNLI and SNLI is that, 
MultiNLI have in-domain test set and develop-
ment set as well as an out-of-domain test and 
development set. 

Quora The Quora Question pair dataset have 
over 400k question pairs, each question pair is as-
signed with a binary label to indicate if the pair are 
paraphrase to each other. We evaluate our model 
on the data which was previously partitioned by 
Zhiguo Wang (2017) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 3 : Performance on MultiNLI test set.</head><label>3</label><figDesc></figDesc><table>Models 
Test 

Siamese-CNN 
79.60 
Multi-Perspective CNN 
81.38 
Siamese-LSTM 
82.58 
Multi-Perspective-LSTM 
83.21 
L.D.C 
85.55 
BiMPM (Zhiguo Wang, 2017) 88.17 

CIN 
88.62 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance on Quora question pair 
dataset. 

model, CIN, achieves the accuracies of 77.0% and 
77.6% on the match and mismatch test sets respec-
tively. The results show that our model outper-
forms the other models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ablation experiment on SNLI dataset. 

Premise 

(1) A girl playing a violin along with a group of people 
(2) A girl playing a violin along with a group of people 

Hypothesis 

(1) A girl is playing an instrument . 
(2) A girl is playing an instrument . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Gradient visualization of premise and hy-
pothesis. (1) Gradient scale of X, Y on encoding 
layer. (2) Gradient scale of X (c) , Y (c) on first CIN 
layer. Darker color corresponds to a higher scale 
of gradient, and implies a higher contribution to 
the final prediction. 

</table></figure>

			<note place="foot">Radu Florian Zhiguo Wang, Wael Hamza. 2017. Bilateral multi-perspective matching for natural language sentences. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pages 4144-4150.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. We would like to thank the anonymous reviewers for their valuable comments.</p><p>The research work is</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Refining raw sentence representations for textual entailment recognition via attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Jorge A Balazs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03103</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Recurrent neural network-based sentence encoder with gated attention for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01353</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04348</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural tree indexers for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference. Association for Computational Linguistics. Meeting</title>
		<meeting>the conference. Association for Computational Linguistics. Meeting</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">A decomposable attention model for natural language inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reading and thinking: Re-read lstm unit for textual entailment recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2870" to="2879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter</title>
		<meeting>the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ABCNN: attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
