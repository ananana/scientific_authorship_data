<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Inter-modal Attention for Multi-modal Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Md</roleName><forename type="first">Shad</forename><surname>Akhtar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Chauhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<email>sporia@ntu.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Patna</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Inter-modal Attention for Multi-modal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3454" to="3466"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3454</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-modal sentiment analysis offers various challenges, one being the effective combination of different input modalities, namely text, visual and acoustic. In this paper, we propose a recurrent neural network based multi-modal attention framework that leverages the con-textual information for utterance-level sentiment prediction. The proposed approach applies attention on multi-modal multi-utterance representations and tries to learn the contributing features amongst them. We evaluate our proposed approach on two multi-modal sentiment analysis benchmark datasets, viz. CMU Multi-modal Opinion-level Sentiment Intensity (CMU-MOSI) corpus and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Evaluation results show the effectiveness of our proposed approach with the accuracies of 82.31% and 79.80% for the MOSI and MO-SEI datasets, respectively. These are approximately 2 and 1 points performance improvement over the state-of-the-art models for the datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Traditionally, sentiment analysis ( <ref type="bibr">Lee, 2005, 2008</ref>) has been applied to a wide variety of texts ( <ref type="bibr" target="#b8">Hu and Liu, 2004;</ref><ref type="bibr" target="#b11">Liu, 2012;</ref><ref type="bibr" target="#b22">Turney, 2002;</ref><ref type="bibr" target="#b1">Akhtar et al., 2016</ref><ref type="bibr" target="#b0">Akhtar et al., , 2017</ref><ref type="bibr" target="#b13">Mohammad et al., 2013)</ref>. In contrast, multi-modal sentiment analysis has recently gained attention due to the tremen- dous growth of many social media platforms such as <ref type="bibr">YouTube, Instagram, Twitter, Facebook (Chen et al., 2017;</ref><ref type="bibr" target="#b20">Poria et al., 2016</ref><ref type="bibr" target="#b21">Poria et al., , 2017d</ref><ref type="bibr" target="#b24">Zadeh et al., , 2016</ref> etc. It depends on the infor- mation that can be obtained from more than one modality (e.g. text, visual and acoustic) for the analysis. The motivation is to leverage the vari- eties of (often distinct) information from multiple sources for building an efficient system. For ex- ample, it is a non-trivial task to detect the senti- ment of a sarcastic sentence "My neighbours are home!! it is good to wake up at 3am in the morn- ing." as negative considering only the textual in- formation. However, if the system has access to some other sources of information, e.g. visual, it can easily detect the unpleasant gestures of the speaker and would classify it with the negative sentiment polarity. Similarly, for some instances acoustic features such as intensity, pitch, pause etc. have important roles to play in the correctness of the system. However, combining these informa- tion in an effective manner is a non-trivial task that researchers often have to face .</p><p>A video provides a good source for extracting multi-modal information. In addition to the visual frames, it also provides information such as acous- tic and textual representation of spoken language. Additionally, a speaker can utter multiple utter- ances in a single video and these utterances can have different sentiments. The sentiment informa- tion of an utterance often has inter-dependence on other contextual utterances. Classifying such an utterance in an independent manner poses many challenges to the underlying algorithm.</p><p>In this paper, we propose a novel method that employs a recurrent neural network based multi- modal multi-utterance attention framework for sentiment prediction.We hypothesize that apply- ing attention to contributing neighboring utter- ances and/or multi-modal representations may as- sist the network to learn in a better way. The main challenge in multi-modal sentiment analysis lies in the proper utilization of the information ex- tracted from multiple modalities. Although it is of- ten argued that incorporation of all the available modalities is always beneficial for enhanced per- formance, it must be noted that not all the modal- ities play equal role. Another concern in multi-modal framework is that the presence of noise in one modality can affect the overall performance. To better address these concerns we propose a novel fusion method by focusing on inter-modality relations computed between the target utterance and its context. We argue that in multi-modal sen- timent classification, not only the relation among two modalities of the same utterance is important, but also relatedness with the modalities across its context are important.</p><p>Think of an utterance U t that constitutes of three modalities, say A t (i.e. audio), V t (i.e. vi- sual) and T t (i.e. text). Let us also assume U k being a member of the contextual utterances consisting of the modalities -A k , V k and T k . In this case, our model computes the relatedness among the modalities (for e.g., V t and T k ) of U t and U k in or- der to produce a richer multi-modal representation for final classification. The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity (computed using inter-modality correlations) with the target utterance.</p><p>Unlike previous approaches that simply apply attentions over the contextual utterance for classi- fication, we attend over the contextual utterances by computing correlations among the modali- ties of the target utterance and the context ut- terances. This explicitly helps us to distinguish which modalities of the relevant contextual utter- ances are more important for sentiment predic- tion of the target utterance. The model facilitates this modality selection by attending over the con- textual utterances and thus generates better multi- modal feature representation when these modali- ties from the context are combined with the modal- ities of the target utterance. We evaluate our pro- posed approach on two recent benchmark datasets, i.e. CMU-MOSI ( <ref type="bibr" target="#b24">Zadeh et al., 2016)</ref> and CMU- MOSEI ( <ref type="bibr" target="#b27">Zadeh et al., 2018c)</ref>, with one being the largest (CMU-MOSEI) available dataset for multi- modal sentiment analysis (c.f. Section 4.1). Evalu- ation shows that the proposed attention framework attains better performance than the state-of-the-art systems for various combinations of input modal- ities (i.e. text, visual &amp; acoustic).</p><p>The main contributions of our proposed work are three-fold: a) we propose a novel technique for multi-modal sentiment analysis; b) we propose an effective attention framework that leverages con- tributing features across multiple modalities and neighboring utterances for sentiment analysis; and c) we present the state-of-the-art systems for senti- ment analysis in two different benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A survey of the literature suggests that multi- modal sentiment prediction is relatively a new area as compared to textual based sentiment prediction <ref type="bibr" target="#b14">(Morency et al., 2011;</ref><ref type="bibr" target="#b12">Mihalcea, 2012;</ref><ref type="bibr" target="#b20">Poria et al., 2016</ref><ref type="bibr" target="#b18">Poria et al., , 2017b</ref><ref type="bibr" target="#b23">Zadeh et al., 2018a)</ref>. A good re- view covering the literature from uni-modal anal- ysis to multi-modal analysis is presented in <ref type="bibr" target="#b17">(Poria et al., 2017a</ref>). An application of multi-kernel learning based fusion technique was proposed in ( <ref type="bibr" target="#b20">Poria et al., 2016)</ref>, where they employed deep convolutional neural networks for extracting the textual features and fused it with other <ref type="bibr">(visual &amp; acoustic)</ref> modalities for prediction. <ref type="bibr" target="#b24">Zadeh et al. (2016)</ref> introduced the multi-modal dictionary to better understand the interaction be- tween facial gestures and spoken words when ex- pressing the sentiment. Authors introduced the MOSI dataset, the first of its kind to enable the studies of multi-modal sentiment intensity analy- sis.  proposed a Tensor Fusion Network (TFN) model to learn the intra-modality and inter-modality dynamics of the three modali- ties (i.e. text, visual and acoustic). They reported the improved accuracy using multi-modality on the CMU-MOSI dataset. An application to lever- age on the gated multi-modal embedded Long Short Term Memory (LSTM) with temporal at- tention (GME-LSTM(A)) for the word-level fu- sion of multi-modality inputs is proposed in ). The Gated Multi-modal Embedding (GME) alleviates the difficulties of fusion while the LSTM with Temporal Attention (LSTM(A)) performs word-level fusion.</p><p>The works mentioned above did not take con- textual information into account. <ref type="bibr" target="#b18">Poria et al. (2017b)</ref> proposed a LSTM based framework that leverages the contextual information to capture the inter-dependencies between the utterances. In another work, <ref type="bibr" target="#b21">Poria et al. (2017d)</ref> proposed an user opinion based framework to combine the three modality inputs (i.e. text, visual &amp; acous- tic) by applying a multi-kernel learning based method. <ref type="bibr" target="#b23">Zadeh et al. (2018a)</ref> proposed multi- attention blocks (MAB) to capture information across three modalities <ref type="bibr">(text, visual &amp; acoustic)</ref>. They reported improved accuracies in the range of 2-3% over the state-of-the-art models for the dif- ferent datasets.</p><p>The fundamental difference between our pro- posed method and the existing works is that our framework applies focus on the neighboring ut- terances to leverage contextual information for utterance-level sentiment prediction. To the best of our knowledge, our current work is the very first of its kind that attempts to employ multi-modal at- tention block (exploiting neighboring utterances) for sentiment prediction. We use multi-modal at- tention framework that leverages contributing fea- tures across multiple modalities and the neighbor- ing utterances for sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>In our proposed framework, we aim to leverage the multi-modal and contextual information for predicting the sentiment of an utterance. Utter- ances of a particular speaker in a video represent the time series information and it is logical that the sentiment of a particular utterance would af- fect the sentiments of the other neighboring utter- ances. To model the relationship with the neigh- boring utterances and multi-modality, we propose a recurrent neural network based multi-modal at- tention framework. The proposed framework takes multi-modal information (i.e. text, visual &amp; acous- tic) for a sequence of utterances and feeds it into three separate bi-directional Gated Recurrent Unit (GRU) ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>). This is followed by a dense (fully-connected) operation which is shared across the time-steps or utterances (one each for text, visual &amp; acoustic). We then apply multi- modal attention on the outputs of the dense layers. The objective is to learn the joint-association be- tween the multiple modalities &amp; utterances, and to emphasize on the contributing features by putting more attention to these. In particular, we employ bi-modal attention framework, where an atten- tion function is applied to the representations of pairwise modalities i.e. visual-text, text-acoustic and acoustic-visual. Finally, the outputs of pair- wise attentions along with the representations are concatenated and passed to the softmax layer for classification. We call our proposed architecture Multi-Modal Multi-Utterance -Bi-Modal Atten- tion (MMMU-BA) framework. An overall archi- tecture of the proposed MMMU-BA framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Please refer to <ref type="figure" target="#fig_6">Figure 3</ref> in appendix for illustration of attention computation.</p><p>For comparison, we also experiment with two other variants of the proposed MMMU-BA framework i.e. a). Multi-Modal Uni-Utterance- Self Attention (MMUU-SA) framework and b). Multi-Utterance-Self Attention (MU-SA) frame- work. The architecture of these variants differ with respect to the attention computation module and the naming conventions "MMMU", "MMUU" or "MU" signify the information that partici- pates in the attention computation. For example, in MMMU-BA, we compute attention over the multi-modal and multi-utterance inputs, whereas in MMUU-SA, the attention is computed over the mutli-modal but uni-utterance inputs. In contrast, we compute attention over only multi-utterance in- puts in MU-SA. Rest of the components for all the three variants remain same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-modal Multi-utterance -Bi-modal Attention (MMMU-BA) Framework</head><p>Assuming a particular video has 'u' utterances, the raw utterance level multi-modal features are rep- resented as</p><formula xml:id="formula_0">T R ∈ R u×300 (raw text), V R ∈ R u×35 (raw visual) and A R ∈ R u×74 (raw acoustic).</formula><p>Three separate Bi-GRU layers with forward &amp; backward state concatenation are first applied on the raw data followed by the fully-connected dense layers, resulting in T ∈ R u×d (text), V ∈ R u×d (visual) and A ∈ R u×d (acoustic), where 'd' is the number of neurons in the dense layer. Finally, pairwise-attentions are computed on various com- binations of three modalities-(V, T), (T, A) &amp; (A, V). In particular the attention between V and T is computed as follows:</p><p>• Bi-modal Attention: Modality representations of V &amp; T are obtained from the Bi-GRU network, and hence contain the contextual information of the utterances for each modality. At first, we com- pute a pair of matching matrices M 1 , M 2 ∈ R u×u over two representations that account for the cross- modality information.</p><formula xml:id="formula_1">M 1 = V.T T &amp; M 2 = T.V T</formula><p>• Multi-Utterance Attention: As mentioned ear- lier, in the proposed model we aim to leverage the contextual information of each utterance for the prediction. We compute the probability distribu- tion scores (  utterances. Finally, soft attention is applied over the multi-modal multi-utterance attention matrices to compute the modality-wise attentive representa-</p><formula xml:id="formula_2">N 1 ∈ R u×u &amp; N 2 ∈ R u×u ) over</formula><formula xml:id="formula_3">tions (i.e. O 1 &amp; O 2 ). N 1 (i, j) = e M 1 (i,j) u k=1 e M 1 (i,k) for i, j = 1, .., u N 2 (i, j) = e M 2 (i,j) u k=1 e M 2 (i,k) for i, j = 1, .., u. O 1 = N 1 .T &amp; O 2 = N 2 .V</formula><p>• Multiplicative Gating &amp; Concatenation: Fi- nally, a multiplicative gating function following <ref type="bibr" target="#b5">(Dhingra et al., 2016)</ref> is computed between the multi-modal utterance specific representations of each individual modality and the other modalities. This element-wise matrix multiplication assists in attending to the important components of multiple modalities and utterances. </p><formula xml:id="formula_4">A 1 = O 1 V &amp; A 2 = O 2 T Attention matrices A 1 &amp; A 2 are then concatenated to obtain the MMMU-BA VT ∈ R u×2d between V and T. MMMU-BA V T = concat[A 1 , A 2 ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Modal Uni-Utterance -Self</head><p>Attention (MMUU-SA) Framework MMUU-SA framework does not account for infor- mation from the other utterances at the attention level, rather it utilizes multi-modal information of single utterance for predicting the sentiment. For a video having 'q' utterances, 'q' separate attention blocks are needed, where each block computes the self-attention over multi-modal information of a single utterance. Let X up ∈ R 3×d is the informa- tion matrix of the p th utterance where the three 'd' dimensional rows are the outputs of the dense lay- ers for the three modalities. The attention matrix A up ∈ R 3×d is computed separately for, p = 1 st , 2 nd , ... q th utterances. Fi- nally, for each utterance p, A up and X up are con- catenated and passed to the output layer for clas- sification. Please refer to the appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-Utterance -Self Attention (MU-SA) Framework</head><p>In MU-SA framework, we apply self attention on the utterances of each modality separately, and use these for classification. In contrast to MMUU-SA framework, MU-SA utilizes the contextual infor- mation of the utterances at the attention level. Let, T ∈ R u×d (text), V ∈ R u×d (visual) and A ∈ R u×d (acoustic) are the outputs of the dense lay- ers. For the three modalities, three separate atten- tion blocks are required, where each block takes multi-utterance information of a single modality and computes the self attention matrix. Attention matrices A t , A v and A a are computed for text, vi- sual and acoustic, respectively. Finally A v , A t , A a , V , T &amp; A are concatenated and passed to the out- put layer for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets, Experiments and Analysis</head><p>In this section we describe the datasets used for our experiments and report the results along with the necessary analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our proposed approach on two benchmark datasets, namely CMU Multi-modal Opinion- Each utterance in CMU-MOSI dataset has been annotated as either positive or negative, whereas in CMU-MOSEI dataset labels are in the continuous range of -3 to +3. However, in this work we project the instances of CMU-MOSEI in a two-class clas- sification setup with values ≥ 0 signify positive sentiments and values &lt; 0 signify negative sen- timents. We adopt such a strategy to be consistent with the previous published works on CMU-MOSI datasets ( <ref type="bibr" target="#b18">Poria et al., 2017b;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature extraction</head><p>We use the CMU-Multi-modal Data SDK <ref type="bibr">1</ref>  compute the average of word-level features in an utterance to obtain the utterance-level features. For each word, the dimension of the feature vector is set to 300 (text), 35 (visual) &amp; 74 (acoustic). In contrast, for MOSI dataset we use utterance- level features 3 provided in ( <ref type="bibr" target="#b18">Poria et al., 2017b</ref></p><note type="other">). These utterance-level features represent the out- puts of a convolutional neural network (Karpathy et al., 2014), 3D convolutional neural network (Ji et al., 2013) &amp; openSMILE (Eyben et al., 2010) for text, visual &amp; acoustic modalities, respectively. Dimensions of utterance-level features are 100, 100 &amp; 73 for text, visual &amp; acoustic, respectively.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments</head><p>We evaluate our proposed approach for CMU- MOSI (test data) &amp; CMU-MOSEI (dev data) <ref type="bibr">4</ref> . Accuracy score is used as the evaluation metric.</p><p>We use Bi-directional GRUs having 300 neu- rons, each followed by a dense layer consisting of 100 neurons. Utilizing the dense layer, we project the input features of all the three modalities to the same dimensions. We set dropout=0.5 (MOSI) &amp; 0.3 (MOSEI) as a measure of regularization. In addition, we also use dropout=0.4 (MOSI) &amp; 0.3 (MOSEI) for the Bi-GRU layers. We employ ReLu activation function in the dense layers, and soft- max activation in the final classification layer. For training the network we set the batch size=32, use Adam optimizer with cross-entropy loss function and train for 50 epochs. We report the average re- sult of 5 runs for all our experiments.</p><p>We experiment with all the valid combinations of uni-modal (where only one modality is taken at a time), bi-modal (any two modalities are taken at a time) and tri-modal (all three modalities are taken at a time) inputs for text, visual and acoustic. In multi-modal attention frameworks i.e. MMMU- BA &amp; MMUU-SA, the attention is computed over at least two modalities, hence, these two frame- works are not-applicable (NA) for uni-modal ex- periments in <ref type="table">Table 1</ref>).</p><p>For MOSEI dataset, we obtain better per- formance with text. Subsequently, we take two modalities at a time for constructing bi-modal in- puts and feed it to the network. For text-acoustic input pairs, we obtain the highest accuracies with 79.74%, 79.60% and 79.32% for MMMU-BA,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality</head><p>T V A MMUU-SA and MU-SA frameworks, respectively. The results that we obtain from the bi-modal com- binations suggest that the text-acoustic combina- tion is a better choice than the others as it improves the overall performance. Finally, we experiment with tri-modal inputs and observe an improved performance of 79.80%, 79.76% and 79.63% for MMMU-BA, MMUU-SA and MU-SA frameworks, respectively. This improvement entails that combi- nation of all the three modalities is a better choice. The performance improvement was also found to be statistically significant (T-test) than the bi- modality and uni-modality inputs. Further, we ob- serve that the MMMU-BA framework reports the best accuracy of 79.80% for the MOSEI dataset, thus supporting our claim that multi-modal atten- tion framework (i.e. MMMU-BA) captures more information than the self-attention frameworks (i.e. MMUU-SA &amp; MU-SA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CMU-MOSEI CMU-MOSI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMMU-BA MMUU-SA MU-SA MMMU-BA MMUU-SA MU-SA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Attention Mechanism</head><p>We analyze the attention values to understand the learning behavior of the proposed architecture. To illustrate, we take an example video from the CMU-MOSI test dataset. The transcript of the ut- terances for this particular video are presented in <ref type="table">Table 2</ref>. The gold sentiments are positive for all the utterances except u 3 &amp; u 4 . We found that the proposed tri-modal MMMU-BA model predicts the labels of all the nine instances correctly, whereas other models make at least one misclassification. We further analyze our proposed architecture (i.e. MMMU-BA) with and without attention. In MOSI for tri-modal inputs, the MMMU-BA archi- tecture reports a reduced accuracy of 80.89% with- out attention framework as compared to 82.31% with attention. We observe similar performance in the MOSEI dataset, where we obtain 79.02%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transcript</head><p>Gold Label Predicted MMMU-BA MMUU-SA MU-SA u1 well he plays that well so he's good villain Positive Positive Positive Positive u2 he also has some really cool guns so that its like a desert eagle but it has to barrels to it  <ref type="table">Table 2</ref>: Transcript, gold labels and predicted labels of a video in CMU-MOSI dataset having nine utter- ances. 7 utterances are labeled positive whereas 2 utterances are labeled negative. Predicted labels are for our different tri-modal models. Bolded labels are misclassified by at least one model. u1 u2 u3 u4 u5 u6 u7 u8 u9 u1 u2 u3 u4 u5 u6 u7 u8 u9 u9 u8 u7 u6 u5 u4 u3 u2 u1</p><p>(a) Softmax attention weights N1 &amp; N2 for MMMU-BAVT.</p><p>u1 u2 u3 u4 u5 u6 u7 u8 u9 u1 u2 u3 u4 u5 u6 u7 u8 u9 u9 u8 u7 u6 u5 u4 u3 u2 u1</p><p>(b) Softmax attention weights N1 &amp; N2 for MMMU-BAAV.</p><p>u1 u2 u3 u4 u5 u6 u7 u8 u9 u1 u2 u3 u4 u5 u6 u7 u8 u9 u9 u8 u7 u6 u5 u4 u3 u2 u1</p><p>(c) Softmax attention weights N1 &amp; N2 for MMMU-BATA.</p><p>u1 u2 u3 u4 u5 u6 u7 u8 u9 u9 u8 u7 u6 u5 u4 u3 u2 u1</p><formula xml:id="formula_5">(d) MU-SAtext matrix.</formula><p>u1 u2 u3 u4 u5 u6 u7 u8 u9 u9 u8 u7 u6 u5 u4 u3 u2 u1</p><p>(e) MU-SA visual matrix.</p><p>u1 u2 u3 u4 u5 u6 u7 u8 u9 u9 u8 u7 u6 u5 u4 u3 u2 u1</p><formula xml:id="formula_6">(f) MU-SAacoustic matrix. u1(visual) u1(acoustic) u1(text) u1(text) u1(acoustic) u1(visual) u2(visual) u2(acoustic) u2(text) u2(text) u2(acoustic) u2(visual) u3(visual) u3(acoustic) u3(text) u3(text) u3(acoustic) u3(visual) u4(visual) u4(acoustic) u4(text) u4(text) u4(acoustic) u4(visual) u5(visual) u5(acoustic) u5(text) u5(text) u5(acoustic) u5(visual) u6(visual) u6(acoustic) u6(text) u6(text) u6(acoustic) u6(visual) u7(visual) u7(acoustic) u7(text) u7(text) u7(acoustic) u7(visual) u8(visual) u8(acoustic) u8(text) u8(text) u8(acoustic) u8(visual) u9(visual) u9(acoustic) u9(text) u9(text) u9(acoustic) u9(visual)</formula><p>(g) Softmax attention weights (Nu 1 , Nu 2 , .., Nu 9 ) for MMUU-SA model. The Tri-modal MMMU-BA model predicts all 9 instances correctly, whereas, the other two models makes at least one misclassification. Heatmap signifies that the model is able to predict labels of all the utterances correctly by incorporating multi-modal &amp; multi-utterance information.  <ref type="table">Table 3</ref>: Analysis of attention mechanism in MMMU-BA architecture. w/ attention → with multi-modal multi utterance attention mechanism and w/o attention → without attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T V A CMU-MOSEI CMU-MOSI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>w/ attention w/o attention w/ attention w/o attention</head><p>accuracy without attention framework against 79.80% accuracy with attention framework. Sta- tistical T-test shows these improvements to be sig- nificant. We also observed the similar trends for bi-modal inputs in both the datasets. All these ex- periments (c.f. <ref type="table">Table 3</ref>) suggest that the attention framework is an important component in our pro- posed architecture, and in absence of this the net- work finds it more difficult for learning in all the cases (i.e. bi-modal &amp; tri-modal input setups). We successfully show that attention computa- tion on pairwise combination of modalities (i.e. bi- modal attention framework) is more effective than the combination of self-attention on single modal- ity. Further for the completeness of the proposed approach, we also experiment with tri-modal at- tention framework (attention is computed on three modalities at a time). Though the results that we obtain are convincing, it does not improve the per- formance over the bi-modal attention framework. We obtain the accuracies of 79.58% &amp; 81.25% on MOSEI and MOSI, respectively, for the tri-modal attention framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparative Analysis</head><p>For MOSI datasets we compare the performance of our proposed approach with the the following state-of-the-art systems: i). In <ref type="table" target="#tab_7">Table 4</ref> we present the comparative perfor- mance between our proposed model and other state-of-the-art systems. In MOSI dataset, <ref type="bibr" target="#b18">Poria et al. (2017b;</ref><ref type="bibr" target="#b19">2017c)</ref> reported the accuracies of 80.3% &amp; 81.3 %, respectively, utilizing tri-modal inputs. <ref type="bibr" target="#b23">Zadeh et al. (2018a)</ref> obtained an accuracy of &amp; 77.4%.  reported accuracies of 75.7% (LSTM(A)) &amp; 76.5% (GME-LSTM(A)) for two variants of their model. In contrast to the state-of-the-art systems, our proposed model at- tains an improved accuracy of 82.31% when we utilize all the three modalities, i.e. text, visual &amp; acoustic. Our proposed system also obtains better performance as compared to the state-of-the-arts for bi-modal inputs.</p><p>For MOSEI dataset, we evaluate against the fol- lowing systems: i) <ref type="bibr" target="#b18">Poria et al. (2017b)</ref>, ii) <ref type="bibr" target="#b23">Zadeh et al. (2018a)</ref>, and iii) <ref type="bibr">Zadeh et al. (2018b)</ref>, where authors proposed a memory fusion network for multi-view sequential learning. We evaluate the system of <ref type="bibr" target="#b18">Poria et al. (2017b)</ref> on MOSEI dataset and obtain 77.64% accuracy with the tri-modal in- puts. Authors in ( <ref type="bibr" target="#b23">Zadeh et al., 2018a</ref><ref type="bibr">) &amp; (Zadeh et al., 2018b</ref>) reported the accuracy 76.0% and 76.4%, respectively, with the tri-modal inputs. In comparison, our proposed approach yields an ac- curacy of 79.80%. As reported in <ref type="table" target="#tab_7">Table 4</ref> the pro- posed approach also attains better performance for all the bi-modal and uni-modal input combinations when compared to <ref type="bibr" target="#b18">Poria et al. (2017b)</ref>.</p><p>As reported in <ref type="table" target="#tab_7">Table 4</ref>, we observe that the per- formance achieved in our proposed approach is significantly better in comparison to the state-of- the-art systems with p-value&lt; 0.05 (obtained us- ing T-test). For further analysis, we also report re- sults for three-class classification (positive, neu- tral &amp; negative classes) problem setup for MOSEI dataset in <ref type="table" target="#tab_10">Table 7</ref>. Note that this setup is not feasi- ble in MOSI as labels are only positive or negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Error Analysis</head><p>We perform error analysis on the predictions of our proposed MMMU-BA model with all the three input sources. Confusion matrices for both the datasets are demonstrated in <ref type="table" target="#tab_8">Table 5</ref>. For MO- SEI dataset we observe that the precision and re- call for positive class (84% precision &amp; 88% re- call; are quite encouraging. However, the same are comparatively on the lower side for the negative class (68% precision &amp; 58% recall. In contrast, for the MOSI dataset -which is relatively balanced - we obtain quite similar performance for both the classes i.e. positive (86% precision &amp; 85% recall) and negative <ref type="bibr">(77% precision &amp; 75% recall</ref>   </p><formula xml:id="formula_7">T-test (p-values) - - - 0.0025 - - - - - 0.0006</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MOSEI</head><p>And when I was going to school it was really difficult for me to find avenues and resources to be able to reach higher education.</p><p>negative positive Implicit sentiment. We could have a decision from the court on the stay any day now.</p><p>positive negative</p><p>Holidays never really happen in online courses I guess. negative positive Negation &amp; strong word. Young people dropping out of the labour market are actually not counted anymore as unemployed as they are inactive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>positive negative</head><p>Thank you for your efforts and consideration. negative positive Sarcastic sentence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a recurrent neu- ral network based multi-modal attention frame- work that leverages the contextual information for utterance-level sentiment prediction. The network learns on top of three modalities, viz. text, vi- sual and acoustic, considering sequence of utter- ances in a video. Through evaluation results on two benchmark datasets (one being the popular &amp; commonly used (MOSI) and other being the most recent &amp; largest (MOSEI) dataset for multi-modal sentiment analysis), we successfully showed that the proposed attention based framework performs better than various state-of-the-art systems.</p><p>In future, we would like to investigate new tech- niques, and explore the ways to handle implicit sentiment and sarcasm. Future direction of work also include adding more dimensions, e.g. emo- tion analysis &amp; intensity prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgment</head><p>Let X p ∈ R 3×d is the information matrix of the p th utterance where the three 'd' dimensional rows are the outputs of the time-distributed dense layer for the three modalities. Computation in the p th attention block proceeds as follows:</p><formula xml:id="formula_8">M up = X up .X T up N up (i, j) =</formula><p>e Mu p (i,j) 3 k=1 e Mu p (i,k) for i, j = 1, 2, 3;</p><formula xml:id="formula_9">O up = N up .X up A up = O up X up</formula><p>The attention matrix A up ∈ R 3×d is computed separately for, p = 1 st , 2 nd , ... q th utterances. Fi- nally, for each utterance p, A up and X up are con- catenated and passed to the output layer for classi- fication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multi-Utterance -Self Attention (MU-SA) Framework</head><p>In MU-SA framework, we apply self attention on the utterances of each modality separately, and use these for classification. In contrast to MMUU-SA framework, MU-SA utilizes the contextual infor- mation of the utterances at the attention level. Let, T ∈ R u×d (text), V ∈ R u×d (visual) and A ∈ R u×d (acoustic) are the outputs of the dense lay- ers. For the three modalities, three separate atten- tion blocks are required, where each block takes multi-utterance information of a single modality and computes the self attention matrix. Specifi- cally, the MU-SA attention (A v ) on V (visual) will be computed as follows,</p><formula xml:id="formula_10">M v = V.V T N v (i, j) = e Mv(i,j) u k=1 e Mv(i,k) for i, j = 1, .., u O v = N v .V A v = O v V</formula><p>The attention matrix A p ∈ R 3×d is computed for p = 1 st , 2 nd , ...u th utterances. Finally, for each ut- terance u, A p and X p are concatenated and passed to the output layer with softmax activation for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Dataset Statistics</head><p>Dataset statistics are presented in <ref type="table" target="#tab_11">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Attention Computation</head><p>MMMU-BA VT attention computation is illustrated in <ref type="figure" target="#fig_6">Figure 3</ref>.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall architecture of the proposed MMMU-BA framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>MMMU-BA AV &amp; MMMU-BA TA computations: Similar to MMMU-BA VT , we follow the same procedure to compute MMMU-BA AV &amp; MMMU- BA TA . For a data source comprising of raw vi- sual (V R ), acoustic (A R ) &amp; text (T R ) modalities, at first, we compute the bi-modal attention pairs for each combination i.e. MMMU-BA VT , MMMU- BA AV &amp; MMMU-BA TA . Finally, motivated by the residual skip connection network (He et al., 2016), we concatenate the bi-modal attention pairs with individual modalities (i.e. V, A &amp; T) to boost the gradient flow to the lower layers. This concate- nated feature is then used for final classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>For the proposed tri-modal MMMU-BA model, the heatmaps of the pair-wise MMMU-BA softmax at- tention weights N 1 &amp; N 2 of visual-text, acoustic- visual &amp; text-acoustic are illustrated in Figure 2a, Figure 2b &amp; Figure 2c, respectively. N 1 &amp; N 2 are the softmax attention weights obtained from the pairwise matching matrices M 1 &amp; M 2 . Elements of the rows of N 1 &amp; N 2 matrices signify differ- ent weights across multiple utterances. From the attention heatmaps, it is evident that by applying different weights across contextual utterances and modalities the model is able to predict labels of all the utterances correctly. All the heatmaps justify that the model learns to incorporate multi-modal &amp; multi-utterance information and thus is able to cor- rectly predict the labels of all the utterances. For example, heatmap of MMMU-BA VT (Figure 2a) signifies that elements of N 1 are weighted higher than N 2 , and thus the model puts more attention on the textual part and relatively lesser on the visual part (as N 1 is multiplied with T &amp; N 2 is multiplied with V). Also it can be concluded that textual fea- tures of the first few utterances are the most help- ful compared to the rest of the textual features and visual features. The softmax attention weights of text (N t ), vi- sual (N v ) &amp; acoustic (N a ) in tri-modal MU-SA model are illustrated in Figure 2d, Figure 2e &amp; Figure 2f, respectively. The attention matrices are 9*9 dimensional. This model wrongly predicts the label of the utterance u 5 . On the other hand, softmax attention weights in tri-modal MMUU-SA model are illustrated in Figure 2g. Nine separate attention weights (N u 1 , N u 2 , .., N u 9 ) are com- puted for the nine utterances. This model wrongly predicts the labels of the utterances u 4 &amp; u 5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a), (b) &amp; (c): Pair-wise softmax attention weights N 1 &amp; N 2 of visual-text, acoustic-visual &amp; text-acoustic in Tri-modal MMMU-BA model. Solid line at the center represents boundary of N 1 &amp; N 2. The heatmaps represent attention weights of a particular utterance with respect to other utterances in N 1 &amp; N 2. (d), (e) &amp; (f) Softmax attention weights of text (N t ), visual (N v ) and acoustic (N a ) in Tri-modal MU-SA model. This model wrongly predicts the label of utterance u 5. (g) Softmax attention weights of the 9 utterances (N U 1 , N U 2 , .., N U 9 ) in Tri-modal MMUU-SA model. This model wrongly predicts the label of utterance u 4 &amp; u 5. The Tri-modal MMMU-BA model predicts all 9 instances correctly, whereas, the other two models makes at least one misclassification. Heatmap signifies that the model is able to predict labels of all the utterances correctly by incorporating multi-modal &amp; multi-utterance information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Poria et al. (2017b)- LSTM-based sequence model to capture the con- textual information of the utterances; ii). Poria et al. (2017c)-Tensor level fusion technique for combining all the three modalities; iii). Chen et al. (2017)-A gated multi-modal embedded LSTM with temporal attention (GME-LSTM(A)) for word-level fusion of multi-modality inputs. and iv). Zadeh et al. (2018a)-Multiple attention blocks for capturing the information across the three modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>a) Data Statistics. Tr→Train set; Dv→Development set; Ts→Test set;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MMMU-BA VT attention computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision, Recall &amp; F-measure for different input combinations in MMMU-BA architecture of MOSEI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparative analysis of the proposed approach with recent state-of-the-art systems. Significance 
T-test p-values &lt; 0.05 

MOSEI 

102 
234 

1230 
269 

Positive 
Negative 

63 
215 

404 
70 

Positive 
Negative 

MOSI 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Con-
fusion matrix 
for tri-modal 
MMMU-BA. 

Text 
Actual Predicted 
Possible Reason 

MOSI 

At first I thought the movie would appeal more to younger audience. 
negative 
positive 
Implicit sentiment. 
Its really non-stop from beginning to end. 
negative 
positive 

But its action isn't particularly memorable. 
negative 
positive 
Negation &amp; strong word. 
I mean I don't regret seeing it. 
positive 
negative 

Um I was really looking forward to it. 
negative 
positive 
Sarcastic sentence. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Error Analysis: Frequent error cases and their possible reasons of failure 
for the tri-modal MMMU-BA framework. 

Metric 

CMU-MOSEI 

Poria et al. 
(2017b) 

MMMU-BA 
(Tri-modal) 

Accuracy 
61.89 
63.30 

F1 Score 
61.60 
63.07 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Three class (positive, negative, neutral) 
classification results in MOSEI dataset. 

refer to the appendix for PR curves of different in-
put combinations. 
We further analyze our outputs qualitatively and 
list a few frequently occurring error categories 
with examples in Table 6. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Dataset statistics for MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018c). 

</table></figure>

			<note place="foot" n="3"> https://github.com/SenticNet/ contextual-sentiment-analysis 4 Gold annotation of CMU-MOSEI test data wasn&apos;t released at the time of paper submission.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head><p>CMU-MOSI CMU-MOSEI </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Feature selection and ensemble construction: A two-step method for aspect based sentiment analysis. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="116" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Hybrid Deep Learning Architecture for Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shad</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING 2016): Technical Papers</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING 2016): Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-11" />
			<biblScope unit="page" from="482" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Covarep-a collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th KDD</title>
		<meeting>the 10th KDD<address><addrLine>Seattle, WAs</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2013. 3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis</title>
		<meeting>the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis<address><addrLine>Jeju Island, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">NRC-Canada: Building the state-ofthe-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Multimodal Interaction</title>
		<meeting>the 13th International Conference on Multimodal Interaction<address><addrLine>Alicante, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11-14" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Fusion</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-level multiple attentions for contextual multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional mkl based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newton</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="217" to="230" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Association for Computational Linguistics (ACL)<address><addrLine>Philadelphia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-2018)</title>
		<meeting><address><addrLine>New Orleans, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5642" to="5649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00927</idno>
		<title level="m">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018b. Memory Fusion Network for Multi-view Sequential Learning</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal Language Analysis in the Wild: CMUMOSEI Dataset and Interpretable Dynamic Fusion Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-Modal Uni-Utterance-Self Attention (MMUU-SA) Framework</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
