<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale Expected BLEU Training of Phrase-based Reordering Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-scale Expected BLEU Training of Phrase-based Reordering Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1250" to="1260"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work by Cherry (2013) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains. Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features. We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammar- based translation models <ref type="bibr" target="#b29">(Wu, 1997;</ref><ref type="bibr" target="#b8">Galley et al., 2004;</ref><ref type="bibr" target="#b9">Galley et al., 2006;</ref><ref type="bibr" target="#b5">Chiang, 2007)</ref>, phrase- based models ( <ref type="bibr" target="#b15">Koehn et al., 2003;</ref><ref type="bibr" target="#b22">Och and Ney, 2004</ref>) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored inter- phrase reorderings using a restricted linear dis- tortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory com- pletely unrestricted reordering patterns, move- ments are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering mod- els trained with maximum likelihood-based ap- proaches <ref type="bibr" target="#b27">(Tillmann, 2003;</ref><ref type="bibr" target="#b30">Xiong et al., 2006</ref>; <ref type="bibr" target="#b7">Galley and Manning, 2008;</ref><ref type="bibr" target="#b21">Nguyen et al.,2009;</ref><ref type="bibr">§2)</ref>.</p><p>More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU ( <ref type="bibr" target="#b24">Papineni et al., 2002</ref>). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; <ref type="bibr" target="#b4">Chiang et al., 2009</ref>) on a small development set of a few thousand sentences. While simple, the approach outper- forms the state-of-the-art hierarchical reordering model of <ref type="bibr" target="#b7">Galley and Manning (2008)</ref>, a maximum likelihood-based model trained on millions of sen- tences to fit millions of parameters.</p><p>Ideally, we would like to scale sparse reorder- ing models to similar dimensions but recent at- tempts to increase the amount of training data for MIRA was met with little success <ref type="bibr" target="#b6">(Eidelman et al., 2013)</ref>. In this paper we propose much larger sparse ordering models that combine the scalabil- ity of likelihood-based approaches with the higher accuracy of maximum BLEU training ( §3). We train on the output of a hierarchical reordering model-based system and scale to millions of fea- tures learned on hundreds of thousands of sen- tences ( §4). Specifically, we use the expected BLEU objective function ( <ref type="bibr" target="#b25">Rosti et al., 2010;</ref><ref type="bibr" target="#b26">Rosti et al., 2011;</ref><ref type="bibr" target="#b13">He and Deng, 2012;</ref><ref type="bibr" target="#b10">Gao and He, 2013;</ref><ref type="bibr" target="#b11">Gao et al., 2014;</ref><ref type="bibr" target="#b12">Green et al., 2014</ref>) which allows us to train models that use training data and feature sets that are two to three orders of magni- tudes larger than in previous work ( §5).</p><p>Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to <ref type="bibr" target="#b3">Cherry (2013)</ref>. We also demonstrate that our approach greatly benefits from more training data than is typically used for maximum BLEU training. Previous work concluded that sparse reordering models perform better than maximum entropy models, however, the two approaches do not only differ in the objective function but also the type of training data <ref type="bibr" target="#b3">(Cherry, 2013)</ref>. Our analysis isolates the objective function and shows that expected BLEU optimization is the most important factor to train accurate ordering models. Finally, we compare expected BLEU training to pair-wise ranked optimization (PRO) on a feature set similar to Cherry (2013; §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Reordering Models</head><p>Reordering models for phrase-based translation are typically part of the log-linear framework which forms the basis of many statistical machine translation systems ( <ref type="bibr" target="#b22">Och and Ney, 2004</ref>).</p><p>Formally, we are given K training pairs</p><formula xml:id="formula_0">D = (f (1) , e (1) )...(f (K) , e (K) ), where each f (i) ∈ F</formula><p>is drawn from a set of possible foreign sentences, and each English sentence e (i) ∈ E(f (i) ) is drawn from a set of possible English translations of f (i) . The log-linear model is parameterized by m pa- rameters θ where each θ k ∈ θ is the weight of an associated feature h k (f, e) such as a language model or a reordering model. Function h(f, e) maps foreign and English sentences to the vector h 1 (f, e)...h m (f, e), and we usually choose trans- lationsêlationsˆlationsê according to the following decision rule:</p><formula xml:id="formula_1">ˆ e = arg max e ∈ E(f ) θ T h(f, e)<label>(1)</label></formula><p>In practice, computingêcomputingˆcomputingê exactly is intractable and we resort to an approximate but more efficient beam search ( <ref type="bibr" target="#b22">Och and Ney, 2004</ref>). Early phrase-based models simply relied on a linear distortion feature, which measures the dis- tance between the first word of the current source phrase and the last word of the previous source phrase ( <ref type="bibr" target="#b15">Koehn et al., 2003;</ref><ref type="bibr" target="#b22">Och and Ney, 2004</ref>). Unfortunately, this approach is agnostic to the ac- tual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases <ref type="bibr" target="#b27">(Tillmann, 2003;</ref><ref type="bibr" target="#b17">Koehn et al., 2007)</ref>.</p><p>Reordering models generally assume a se- quence of English phrases e = {¯ e 1 , . . . , ¯ e n } cur- rently hypothesized by the decoder, a phrase align- ment a = {a 1 , . . . , a n } that defines a foreign phrase ¯ f a i for each English phrase ¯ e i , and an ori- entation o i which describes how a phrase pair should be reordered with respect to the previous phrases. There are typically three orientation types and the exact definition depends on the specific models which we describe below. Orientations can be determined during decoding and from word- aligned training corpora. Most models estimate a probability distribution p(o i |pp i , a 1 , . . . , a i ) for the i-th phrase pair pp i = ¯ e i , ¯ f a i and the align- ments a 1 , . . . , a i of the previous target phrases. Lexicalized Reordering. This model defines the three orientation types based only on the posi- tion of the current and previously translated source phrase a i and a i−1 , respectively <ref type="bibr" target="#b27">(Tillmann, 2003;</ref><ref type="bibr" target="#b17">Koehn et al., 2007)</ref>. The orientation types gen- erally are: monotone (M), indicating that a i−1 is directly followed by a i . swap (S) assumes that a i precedes a i−1 , i.e., the two phrases swap places. Finally, discontinuous (D) indicates that a i is not adjacent to a i−1 . The probability distribution over these reordering events is based on a maximum likelihood estimate:</p><formula xml:id="formula_2">p(o|pp, a i−1 , a i ) = cnt(o, pp) cnt(pp)<label>(2)</label></formula><p>where o ∈ {M, S, D} and cnt returns smoothed frequency counts over a word-aligned corpus. Hierarchical Reordering. An extension of the lexicalized reordering model better handles long- distance reordering by conditioning the orientation of the current phrase on a context larger than just the previous phrase ( <ref type="bibr" target="#b7">Galley and Manning, 2008)</ref>. In particular, the hierarchical reordering model does so by building a compact representations of the preceding context using an efficient shift- reduce parser. During translation new phrases get moved on a stack and are then combined with any previous phrase if they are adjacent. <ref type="figure">Figure 1</ref> shows an illustrative example: when the decoder shifts phrase pp 8 onto the stack, this phrase is then merged with pp 7 (reduce operation), which then can be merged with previous phrases to finally form a hierarchical block h 1 . These merge opera- tions stop once we reach a phrase (here, pp 3 ) that is not contiguous with the current block. Then, as another phrase (pp 9 ) is hypothesized, the decoder uses the hierarchical block at the top of the stack (h 1 ) to determine the orientation of the current   <ref type="figure">Figure 1</ref>: The hierarchical reordering model (HRM) analyzes a non-local context to determine the orientation of the current phrase. For exam- ple, the phrase pair pp 9 has a swap orientation (o 9 = S) with respect to a hierarchical block (h 1 ) that comprises the five preceding phrase pairs. phrase pp 9 , which in this case is a swap (S) orien- tation. <ref type="bibr">1</ref> The model has the advantage that the ori- entations computed are more robust to derivational ambiguity of the underlying translation model. A given surface translation may be derived through different phrases but the shift-reduce parser com- bines them into a single representation which is more consistent with the orientations observed in the word-aligned training data. Maximum Entropy-based models. The statis- tics used to estimate the lexicalized and the hierar- chical reordering models are based on very sparse estimates, simply because certain phrases are not very frequent. Maximum entropy models address this problem by estimating Eq. 2 through sparse indicator features over phrase pairs instead, but prior work with such models still relies on word aligned corpora for estimation ( <ref type="bibr" target="#b30">Xiong et al., 2006;</ref><ref type="bibr" target="#b21">Nguyen et al., 2009</ref>). However, recent evalua- tions of the approach show little gain over the sim- pler frequency-based estimation method <ref type="bibr" target="#b3">(Cherry, 2013)</ref>. Sparse Hierarchical Reordering model. All of the models so far are trained to maximize the like- lihood of reordering decisions observed in word aligned corpora. <ref type="bibr" target="#b3">Cherry (2013)</ref> argues that it is probably too difficult to learn human reorder- ing patterns through noisy word alignments that were generated by unsupervised methods. Instead, he proposes to learn a discriminative reordering model based on the outputs of the actual machine translation system, adjusting the feature weights to maximize a task-specific objective, which is BLEU in their case. Their model is based on a set of sparse features derived from the hierarchi- cal reordering model which we scale to millions of features ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Simple Linear Reordering Model</head><p>Our reordering model is defined as a simple linear model over the basic orientation types, similar to Cherry (2013). In particular, our model defines score s φ (o, e, f ) over orientations o = {M, S, D}, and a sentence pair {e, f, a} with alignment a as a linear combination of weighted indicator features:</p><formula xml:id="formula_3">s φ (o, e, f, a) = φ T u(o, e, f, a) = I i=1 φ T u(o, pp i , c i ) = I i=1 s φ (o, pp i , c i ) (3)</formula><p>where φ is a vector of weights,</p><formula xml:id="formula_4">{pp i } I i=1</formula><p>is a set of phrases that decompose the sentence pair {e, f, a}, and u(o, pp i , c i ) is a function that maps orientation o, phrase pair pp i and local context c i to a sparse vector of indicator features. The lo- cal context c i represents information used by the model that is in addition to the phrase pair. For example, the features of Cherry (2013) condition on the top-stack of the hierarchical shift reduce parser, information that is non-local with respect to the phrase pair. In our experiments, we use fea- tures that go beyond the top-stack, in order to con- dition on various parts of the source and target side contexts ( §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Training</head><p>Optimization of our model is based on standard stochastic gradient descent (SGD; <ref type="bibr">Bottou, 2004</ref>) with an expected BLEU loss l(φ) which we detail next ( §5). The update is:</p><formula xml:id="formula_5">φ t = φ t−1 − µ ∂l(φ t−1 ) ∂φ t−1<label>(4)</label></formula><p>where φ t and φ t−1 are model weights at time t and t − 1 respectively, and µ is a learning rate. We add the model as a small number of dense features to the log-linear framework of translation (Eq. 1). Specifically, we extend the m baseline features by a set of new features h m+1 , . . . , h m+j , where each represents a linear combination of sparse indicator features corresponding to one of the orientation types. Exposing each orientation as a separate dense feature within the log-linear model is common practice for lexicalized reorder- ing models ( <ref type="bibr" target="#b16">Koehn et al., 2005</ref>):</p><formula xml:id="formula_6">h m+j = s φ (o j , e, f, a)</formula><p>where o j ∈ {M, S, D}.</p><p>The translation model is then parameterized by both θ, the log-linear weights of the baseline fea- tures, as well as φ, the weights of the reordering model. The reordering model is learned as follows ( <ref type="bibr" target="#b10">Gao and He, 2013;</ref><ref type="bibr" target="#b11">Gao et al., 2014</ref>):</p><p>1. We first train a baseline translation system to learn θ, without the discriminative reordering model, i.e., we set θ m+1 = 0, . . . , θ m+j = 0.</p><p>2. Using these weights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section ( §7). The n-best lists serve as an ap- proximation to E(f ), the set of possible trans- lations of f , used in the next step for expected BLEU training of the reordering model ( §5).</p><p>3. Next, we fix θ, set θ m+1 = 1, . . . θ m+j = 1 and optimize φ with respect to the loss func- tion on the training data using stochastic gra- dient descent. <ref type="bibr">2</ref> 4. Finally, we fix φ and re-optimize θ in the presence of the discriminative reordering model using Minimum Error Rate Training (MERT; Och 2003; §7).</p><p>We found that re-optimizing θ after a few iter- ations of stochastic gradient descent in step 3 did not improve accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expected BLEU Objective Function</head><p>The expected BLEU objective ( <ref type="bibr" target="#b10">Gao and He, 2013;</ref><ref type="bibr" target="#b11">Gao et al., 2014</ref>) allows us to efficiently optimize a large scale discriminative reordering model to- wards the desired task-specific metric, which in our setting is BLEU.</p><p>Formally, we define our loss function l(φ) as the negative expected BLEU score, denoted as xBLEU(φ), for a given foreign sentence f and a log-linear parameter set θ:</p><formula xml:id="formula_7">l(φ) = − xBLEU(φ) = − e∈E(f ) p θ,φ (e|f ) sBLEU(e, e (i) ) (5)</formula><p>where sBLEU(e, e (i) ) is a smoothed sentence- level BLEU score with respect to the reference translation e (i) , and E(f ) is the generation set ap- proximated by an n-best list. In our experiments we use n-best lists with unique entries and there- fore our definitions do not take into account mul- tiple derivations of the same translation. Specif- ically, our n-best lists are generated by choosing the highest scoring derivationêderivationˆderivationê amongst string identical translations e for f . We use a sentence- level BLEU approximation similar to <ref type="bibr" target="#b11">Gao et al. (2014)</ref>. <ref type="bibr">3</ref> Finally, p θ,φ (e|f ) is the normalized prob- ability of translation e given f , defined as:</p><formula xml:id="formula_8">p θ,φ (e|f ) = exp{γθ T h(f, e)} e ∈E(f ) exp{γθ T h(f, e )}<label>(6)</label></formula><p>where θ T h(f, e) includes the discriminative re- ordering model h m+1 (e, f ), . . . , h m+j (e, f ) pa- rameterized by φ, and γ ∈ [0, inf) is a tuned scal- ing factor that flattens the distribution for γ &lt; 1 and sharpens it for γ &gt; 1 ( <ref type="bibr" target="#b28">Tromble et al., 2008</ref>). <ref type="bibr">4</ref> Next, we define the gradient of the expected BLEU loss function l(φ). To simplify our notation we omit the local context c in s φ (o, pp, c) (Eq. 3) from now on and assume it to be part of pp. Us- ing the observation that the loss does not explicitly depend on φ, we get:</p><formula xml:id="formula_9">∂l(φ) ∂φ = o,pp ∂l(φ) ∂s φ (o, pp) ∂s φ (o, pp) ∂φ = o,pp −δ o,pp u(o, pp)</formula><p>where δ o,pp is the error term for orientation o of phrase pair pp:</p><formula xml:id="formula_10">δ o,pp = − ∂l(φ) ∂s φ (o, pp)</formula><p>The error term indicates how the expected BLEU loss changes with the reordering score which we derive in the next section.</p><p>Finally, the gradient of the reordering score s φ (o, pp) with respect to φ is simply given by this:</p><formula xml:id="formula_11">∂s φ (o, pp) ∂φ = ∂φ T u(o, pp) ∂φ = u(o, pp)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Derivation of the Error Term δ o,pp</head><p>We rewrite the loss function (Eq. 5) using Eq. 6 and separate it into two terms G(φ) and Z(φ):</p><formula xml:id="formula_12">l(φ) = −xBLEU(φ) = − G(φ) Z(φ)<label>(7)</label></formula><formula xml:id="formula_13">= − e∈E(f ) exp{γθ T h(f, e)} sBLEU(e, e (i) ) e ∈E(f ) exp{γθ T h(f, e )} Next,</formula><p>we apply the quotient rule of differentiation:</p><formula xml:id="formula_14">δ o,pp = ∂xBLEU(φ) ∂s φ (o, pp) = ∂(G(φ)/Z(φ)) ∂s φ (o, pp) = 1 Z(φ) ∂G(φ) ∂s φ (o, pp) − ∂Z(φ) ∂s φ (o, pp) xBLEU(φ)</formula><p>The gradients for G(φ) and Z(φ) with respect to s φ (o, pp) are:</p><formula xml:id="formula_15">∂G(φ) ∂s φ (o, pp) = e∈E(f ) sBLEU(e, e (i) ) ∂ exp{γθ T h(f, e)} ∂s φ (o, pp) ∂Z(φ) ∂s φ (o, pp) = e∈E(f ) ∂ exp{γθ T h(f, e)} ∂s φ (o, pp)</formula><p>By using the following definition:</p><formula xml:id="formula_16">U (φ, e) = sBLEU(e, e (i) ) − xBLEU(φ)</formula><p>together with the chain rule, Eq. 6 and Eq. 7, we can rewrite δ o,pp as follows:</p><formula xml:id="formula_17">δ o,pp = 1 Z(φ) e∈E(f ) ∂ exp{γθ T h(f, e)} ∂s φ (o, pp) U (φ, e) = e∈E(f ) p θ,φ (e|f ) ∂γθ T h(f, e) ∂s φ (o, pp) U (φ, e)</formula><p>Because φ is only relevant to the reordering model, represented by h m+1 , . . . , h m+j , we have:</p><formula xml:id="formula_18">∂γθ T h(f, e) ∂s φ (o, pp) = γλ k ∂h k (e, f ) ∂s φ (o, pp) = γλ k N (o, pp, e, f ) 1: function TRAINSGD(D, µ) 2:</formula><p>t ← 0 3:</p><formula xml:id="formula_19">for all (f (i) , e (i) ) in D do 4: xBLEU = 0 Compute xBLEU 5:</formula><p>for all e in E(f (i) ) do  end for 19: end function This simplifies the error term to:</p><formula xml:id="formula_20">δ o,pp = p θ,φt (e|f (i) )γλ k N D 14: φ t+1 = φ t − µδ o,pp u(o,</formula><formula xml:id="formula_21">δ o,pp = e∈E(f ) p θ,φ (e|f )γλ k N (o, pp, e, f )U (φ, e)<label>(8)</label></formula><p>where λ k is the weight of the dense feature sum- marizing orientation o in the log-linear model. We use Eq. 8 in a simple algorithm to train our model ( <ref type="figure" target="#fig_2">Figure 2</ref>). Our SGD trainer uses a mini-batch size of a single sentence ( §7) which entails all hypoth- esis in the n-best list for this sentence and the pa- rameters are updated after each mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Feature Sets</head><p>Our features are inspired by Cherry (2013) who bases his features on the local phrase-pair pp = ¯ e, ¯ f as well as the top stack of the shift re- duce parser of the baseline hierarchical ordering model. We experiment with these variants and ex- tensions:</p><p>• SparseHRMLocal: This feature set is exclu- sively based on the local phrase-pair and consists of features over the first and last word of both the source and target phrase. <ref type="bibr">5</ref> We use four different word representations:</p><p>The word identity itself, but only for the 80 most common source and target language words. The three other word representations are based on Brown clustering with either 20, 50 or 80 classes ( <ref type="bibr" target="#b1">Brown et al., 1992</ref>). There is one feature for every orientation type.</p><p>• SparseHRM: The main feature set of <ref type="bibr" target="#b3">Cherry (2013)</ref>. This is an extension of SparseHRM- Local adding features based on the first and last word of both the source and the target of the hierarchical block at the top of the stack. There are also features based on the source words in-between the current phrase and the hierarchical block at the top of the stack.</p><p>• SparseHRM+UncommonWords: This set is identical to SparseHRM, except that word- identity features are not restricted to the 80 most frequent words, but can be instantiated for all words, regardless of frequency.</p><p>• SparseHRM+BiPhrases: This augments SparseHRM by phrase-identity features re- sulting in millions of instances compared to only a few thousand for SparseHRM. We add three features for each possible phrase pair: the source phrase, the target phrase, and the whole phrase pair.</p><p>The baseline hierarchical lexicalized reorder- ing model is most similar to SparseHRM+BiPhrases feature set since both have parameters for phrase, orientation pairs. <ref type="bibr">6</ref> The feature set closest to Cherry (2013) is SparseHRM. However, while Cherry had to severely restrict his features for batch lattice MIRA-based training, our maximum expected BLEU approach can handle millions of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>Baseline. We experiment with a phrase-based system similar to Moses ( <ref type="bibr" target="#b17">Koehn et al., 2007)</ref>, scoring translations by a set of common fea- tures including maximum likelihood estimates of source given target phrases p M LE (e|f ) and vice versa, p M LE (f |e), lexically weighted esti- mates p LW (e|f ) and p LW (f |e), word and phrase- penalties, as well as a linear distortion feature. The baseline uses a hierarchical reordering model with five orientation types, including monotone and swap, described in §2, as well as two discon- tinuous orientations, distinguishing if the previous phrase is to the left or right of the current phrase. Finally, monotone global indicates that all previ- ous phrases can be combined into a single hier- archical block. The baseline includes a modified Kneser-Ney word-based language model trained on the target-side of the parallel data, which is de- scribed below. Log-linear weights are estimated with MERT <ref type="bibr" target="#b23">(Och, 2003)</ref>. We regard the 1-best output of the phrase-based decoder with the hierar- chical reordering model as the baseline accuracy. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English translation <ref type="bibr" target="#b2">(Callison-Burch et al., 2012</ref>). Translation mod- els are estimated on 102M words of parallel data for French-English and 91M words for German- English; between 7.5-8.2M words are newswire, depending on the language pair, and the remainder are parliamentary proceedings. All discrimina- tive reordering models are trained on the newswire subset since we found this portion of the data to be most useful in initial experiments. We evaluate on six newswire domain test sets from 2008, 2010 to 2013 as well as the 2010 system combination test set containing between 2034 to 3003 sentences. Log-linear weights are estimated on the 2009 data set comprising 2525 sentences. We evaluate using BLEU with a single reference. Discriminative Reordering Model. We use 100- best lists generated by the phrase-based decoder to train the discriminative reordering model. The n-best lists are generated by ten systems, each trained on 90% of the available data in order to de- code the remaining 10%. The purpose of this pro- cedure is to avoid a bias introduced by generating n-best lists for sentences on which the translation model was previously trained. <ref type="bibr">7</ref>   mentioned, we train our reordering model on the news portion of the parallel data, corresponding to 136K-150K sentences, depending on the language pair. We tuned the various hyper-parameters on a held-out set, including the learning rate, for which we found a simple setting of 0.1 to be useful. To prevent overfitting, we experimented with 2 regu- larization, but found that it did not improve test ac- curacy. We also tuned the probability scaling pa- rameter γ (Eq. 6) but found γ = 1 to be very good among other settings. We evaluate the perfor- mance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling 2 reg- ularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO's tendency to produce too short translations (Nakov et al., 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Scaling the Feature Set</head><p>We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; <ref type="bibr" target="#b7">Galley &amp; Manning, 2008)</ref>, to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger.</p><p>Our results on French-English translation (Ta- ble 1) and German-English translation <ref type="table" target="#tab_3">(Table 2)</ref> show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1 BLEU on new- stest2011 for German-English. <ref type="bibr">8</ref> Increasing the size of the feature set improves accuracy across the board: The average accuracy over all test sets improves from 1.0 BLEU for the most basic fea- ture set to 1.5 BLEU for the largest feature set on French-English and from 0.3 BLEU to 0.8 BLEU on German-English. <ref type="bibr">9</ref> The most compa- rable setting to Cherry (2013) is the feature set SparseHRM, which we outperform by up to 0.5 BLEU on French-English and by 0.3 BLEU on av- erage on both language pairs, demonstrating the benefit of being able to effectively train large fea- ture sets. Furthermore, the increase in the num- ber of features does not affect runtime, since most features can be pre-computed and stored in the phrase <ref type="table">-table, only requiring a constant time table-</ref> lookup, similar to traditional reordering models.</p><p>Another appeal of our approach is that train- ing is very fast given a set of n-best lists for the training data. The SparseHRM model with 4,407 features is trained in only 26 minutes, while the SparseHRM+BiPhrases model with over three mil- lion parameters can be trained in just over two hours (136K sentences and 100 epochs in both cases). We attribute this to the training regime ( §4), which does not iteratively re-decode the training data for expected BLEU training. <ref type="bibr">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Varying Training Set Size</head><p>Previous work on sparse reordering models was restricted to small data sets <ref type="bibr" target="#b3">(Cherry, 2013)</ref> due to the limited ability of standard machine trans- lation optimizers to handle more than a few thou- sand sentences. In particular, recent attempts to scale the margin-infused relaxation algorithm, a variation which was also used by <ref type="bibr" target="#b3">Cherry (2013)</ref>, to larger data sets showed that more data does not necessarily help to improve test set accuracy for large feature sets <ref type="bibr" target="#b6">(Eidelman et al., 2013</ref>).</p><p>In the next set of experiments, we shed light on the advantage of training discriminative reordering models with expected BLEU on large training sets. Specifically, we start off by estimating a reorder- ing model on only 2,000 sentences, similar to the size of the development set used by <ref type="bibr" target="#b3">Cherry (2013)</ref>, and incrementally increase the amount of training data to nearly three hundred thousand sentences. To avoid overfitting to small data sets we experi- ment with our most basic feature set SparseHRM- Local, comprising of just over 4,400 types.</p><p>For this experiment only, we measure accuracy in a re-ranking framework for faster experimen- tation where we use the 100-best output of the baseline system relying on a likelihood-based hi- erarchical reordering model. We re-estimate the log-linear weights by running a further iteration of MERT on the n-best list of the development set which is augmented by scores corresponding to the discriminative reordering model. The weights of those features are initially set to one and we use 20 random restarts for MERT. At test time we rescore the 100-best list of the test set using the new set of log-linear weights learned previously. <ref type="bibr">10</ref> We would expect better accuracy when iteratively decod- ing the training data but did not do so in this study for effi- ciency reasons. Figure 3: Effect of increasing the training set size from 2,000 to 272,000 sentences measured on the dev set (top) and news2011 (bottom) in an n-best list rescoring setting. <ref type="figure">Figure 3</ref> confirms that more training data in- creases accuracy and that the best model requires a substantially larger amount of training data than what is typically used for maximum BLEU train- ing. We expect an even steeper curve for larger feature sets where more parameters need to be es- timated and where the amount of training data is likely to have an even larger effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Likelihood versus BLEU Optimization</head><p>Previous research has shown that directly training a reordering model for BLEU can vastly outper- form a likelihood trained maximum entropy re- ordering model <ref type="bibr" target="#b3">(Cherry, 2013)</ref>. However, the two approaches do not only differ in the objectives used, but also in the type of training data. The maximum entropy reordering model is trained on a word-aligned corpus, trying to learn human re- ordering patterns, whereas the sparse reordering model is trained on machine translation output, trying to learn from the mistakes made by the ac- tual system. It is therefore not clear how much either one contributes to good accuracy.</p><p>Our next experiment teases those two aspects apart and clearly shows the effect of the objec- tive function. Specifically, we compare the tra- ditionally used conditional log-likelihood (CLL) objective to expected BLEU on the French- English translation task in a small feature con- dition (SparseHRM) of about 9K features and dev <ref type="bibr">2008 2010 sc2010 2011 2012</ref>    <ref type="table">Table 4</ref>: French-English results on the SparseHRMLocal feature set when when trained with pair-wise ranked optimization (PRO) and expected BLEU (xBLEU).</p><p>a large feature setting of over 3M features (SparseHRM+BiPhrases). In the CLL setting, we maximize the likelihood of the hypothesis with the highest BLEU score in the n-best list of each train- ing sentence.</p><p>Our results <ref type="table" target="#tab_6">(Table 3)</ref> show that CLL training achieves only a fraction of the gains yielded by the expected BLEU objective. For SparseHRM, CLL improves the baseline by less than 0.2 BLEU on average across all test sets, whereas expected BLEU achieves 1.2 BLEU. Increasing the number of features to 3M (SparseHRM+BiPhrases) results in a slightly better average gain of 0.3 BLEU for CLL but but expected BLEU still achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accu- racy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Comparison to PRO</head><p>In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for ma- chine translation models with large feature sets ( <ref type="bibr" target="#b14">Hopkins and May, 2011</ref>). 11 Previous work has shown that PRO does not scale to truly large fea- ture sets with millions of types ( <ref type="bibr" target="#b31">Yu et al., 2013)</ref> and we therefore restrict ourselves to our smallest <ref type="bibr">11</ref> MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets <ref type="bibr" target="#b6">(Eidelman et al., 2013).</ref> set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set compris- ing of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baseline features ( §7) as well as the 4.4K indica- tor features corresponding to the sparse reordering model. For expected BLEU training we use the full 136K sentences from the training data. The results <ref type="table">(Table 4)</ref> demonstrate that expected BLEU outperforms a typical setup commonly used to train large feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>The expected BLEU objective is a simple and ef- fective approach to train large-scale discriminative reordering models. We have demonstrated that it scales to millions of features, which is orders of magnitudes larger than other modern machine translation optimizers can currently handle.</p><p>Empirically, our sparse reordering model im- proves machine translation accuracy across the board, outperforming a strong hierarchical lexi- calized reordering model by up to 2.0 BLEU on a French to English WMT2012 setup, where the baseline was trained on over two million sentence pairs. We have shown that scaling to large train- ing sets is crucial to good performance and that the best performance is reached when hundreds of thousands of training sentences are used. Fur- thermore, we demonstrate that task-specific train- ing towards expected BLEU is much more effec- tive than optimizing conditional log-likelihood as is usually done. We attribute this to the fact that likelihood is a strict zero-one loss that does not as- sign credit to partially correct solutions, whereas expected BLEU does.</p><p>In future work we plan to extend expected BLEU training to lattices and to evaluate the ef- fect of estimating weights for the dense baseline features as well. Our current training procedure ( <ref type="bibr" target="#b10">Gao and He, 2013;</ref><ref type="bibr" target="#b11">Gao et al., 2014</ref>) decodes the training data only once. In future work, we would like to compare this to repeated decoding as done by conventional optimization methods as well as other large-scale discriminative training approaches ( <ref type="bibr" target="#b31">Yu et al., 2013)</ref>. We expect this to yield additional accuracy gains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>wBLEU</head><label></label><figDesc>← p θ,φt (e|f ) sBLEU(e, e (i) ) 7: xBLEU ← xBLEU + wBLEU 8: end for 9: for all e in E(f (i) ) do 10: D = sBLEU(e, e (i) ) − xBLEU 11: for all o, pp in e, f (i) do 12: N = N (o, pp, e, f ) 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Algorithm for computing the expected BLEU loss with SGD updates (Eq. 4) based on training data D and learning rate µ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>German-English results of expected BLEU trained sparse reordering models (cf. Table 1). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>French-English results comparing the baseline hierarchical reordering model (HRM) to sparse 
reordering model trained towards conditional log-likelihood (CLL) and expected BLEU (xBLEU). 

dev 2008 2010 sc2010 2011 2012 2013 AllTest 

PRO 

24.05 20.90 25.42 
25.28 25.79 25.09 26.07 
24.94 
xBLEU 25.24 21.26 25.99 
25.93 26.98 26.34 26.77 
25.77 

</table></figure>

			<note place="foot" n="1"> Galley and Manning (2008) provide a more formal explanation.</note>

			<note place="foot" n="2"> We tuned θm+1,. .. θm+j on the development set but found that setting them uniformly to one resulted in faster training and equal accuracy.</note>

			<note place="foot" n="3"> We found in early experiments that the BLEU+1 approximation used by Liang et al. (2006) and Nakov et. al (2012) worked equally well in our setting. 4 γ is only used during expected BLEU training.</note>

			<note place="foot" n="5"> Phrase-local features allow pre-computation which results in significant speed-ups at run-time. Cherry (2013) shows that local features are responsible for most of his gains. 6 Although, our model is likely to learn significantly fewer parameters since many phrase, orientation pairs will only be seen in the word-aligned data but not in actual machine translation output.</note>

			<note place="foot" n="7"> Later, we found that the bias has only a negligible effect on end-to-end accuracy since we obtained very similar results when decoding with a system trained on all data. This setting increased the training data BLEU score from 27.5 to 37.8. We used a maximum source and target phrase length of 7 words.</note>

			<note place="foot" n="8"> Different to the setups of Galley &amp; Manning (2008) and Cherry (2013) our WMT evaluation framework uses only one instead of four references, which makes our BLEU score improvements not directly comparable. 9 We attribute smaller improvements on German-English to the low distortion limit of only six words of our system and the more difficult reordering patterns when translating from German which may require more elaborate features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Arul Menezes and Xi-aodong He for helpful discussion related to this work and the three anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ulrike Von Luxburg</surname></persName>
		</author>
		<title level="m">Advanced Lectures in Machine Learning, Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="146" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2012 Workshop on Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="10" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved Reordering for PhraseBased Translation using Sparse Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">001 New Features for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="218" to="226" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Simple and Effective Hierarchical Phrase Reordering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-05" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable Inference and Training of Context-Rich Syntactic Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Deneefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Thayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training MRFBased Phrase Translation Models using Gradient Ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Continuous Phrase Representations for Translation Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An Empirical Comparison of Features and Tuning for Phrase-based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT. Association for Computational Linguistics</title>
		<meeting>of WMT. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximum Expected BLEU Training of Phrase and Lexicon Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP. Association for Computational Linguistics</title>
		<meeting>of EMNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><forename type="middle">Birch</forename><surname>Mayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWSLT</title>
		<meeting>of IWSLT</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL Demo and Poster Sessions</title>
		<meeting>of ACL Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An end-to-end discriminative approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACLCOLING</title>
		<meeting>of ACLCOLING</meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="761" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Optimizing for Sentence-Level BLEU+1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Yields Short Translations</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING. Association for Computational Linguistics</title>
		<meeting>of COLING. Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving A Lexicalized Hierarchical Reordering Model Using Maximum Entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Vinh Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Le</forename><surname>Shimazu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit XII</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The alignment template approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="449" />
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BBN System Description for WMT10 System Combination Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Antti-Veikko I Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Expected BLEU Training for Graphs: BBN System Description for WMT11 System Combination Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Antti-Veikko I Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="159" to="165" />
		</imprint>
	</monogr>
	<note>Proc. of WMT</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Unigram Orientation Model for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003-06" />
			<biblScope unit="page" from="106" to="108" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">W</forename><surname>Tromble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACLCOLING</title>
		<meeting>of ACLCOLING<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Max-Violation Perceptron and Forced Decoding for Scalable MT Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1112" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
