<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pathologies of Neural Models Make Interpretations Difficult</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><forename type="middle">Grissom</forename><surname>Ii</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ursinus College</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
							<email>miyyer@cs.umass.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">UMass Amherst</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Rodriguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pathologies of Neural Models Make Interpretations Difficult</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3719" to="3728"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3719</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One way to interpret neural model predictions is to highlight the most important input features-for example, a heatmap visu-alization over the words in an input sentence. In existing interpretation methods for NLP, a word&apos;s importance is determined by either input perturbation-measuring the decrease in model confidence when that word is removed-or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments , the reduced examples lack information to support the prediction of any label , but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies , we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many interpretation methods for neural networks explain the model's prediction as a counterfactual: how does the prediction change when the input is modified? Adversarial examples ( <ref type="bibr" target="#b31">Szegedy et al., 2014;</ref><ref type="bibr" target="#b9">Goodfellow et al., 2015)</ref> highlight the in- stability of neural network predictions by showing how small perturbations to the input dramatically change the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQUAD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>In 1899, John Jacob Astor IV invested $100,000 for Tesla to further develop and produce a new lighting system. In- stead, Tesla used the money to fund his Colorado Springs experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>What did Tesla spend Astor's money on ? Reduced did Confidence 0.78 â†’ 0.91</p><p>Figure 1: SQUAD example from the validation set. Given the original Context, the model makes the same correct prediction ("Colorado Springs experiments") on the Reduced question as the Original, with even higher confidence. For humans, the reduced question, "did", is nonsensical.</p><p>A common, non-adversarial form of model in- terpretation is feature attribution: features that are crucial for predictions are highlighted in a heatmap. One can measure a feature's importance by input perturbation. Given an input for text clas- sification, a word's importance can be measured by the difference in model confidence before and after that word is removed from the input-the word is important if confidence decreases signifi- cantly. This is the leave-one-out method ( <ref type="bibr" target="#b18">Li et al., 2016b</ref>). Gradients can also measure feature im- portance; for example, a feature is influential to the prediction if its gradient is a large positive value. Both perturbation and gradient-based methods can generate heatmaps, implying that the model's pre- diction is highly influenced by the highlighted, im- portant words.</p><p>Instead, we study how the model's prediction is influenced by the unimportant words. We use in- put reduction, a process that iteratively removes the unimportant words from the input while main- taining the model's prediction. Intuitively, the words remaining after input reduction should be important for prediction. Moreover, the words should match the leave-one-out method's selec- tions, which closely align with human percep- tion ( <ref type="bibr" target="#b18">Li et al., 2016b;</ref><ref type="bibr" target="#b20">Murdoch et al., 2018)</ref>. How- ever, rather than providing explanations of the original prediction, our reduced examples more closely resemble adversarial examples. In <ref type="figure">Fig- ure 1</ref>, the reduced input is meaningless to a hu- man but retains the same model prediction with high confidence. Gradient-based input reduction exposes pathological model behaviors that contra- dict what one expects based on existing interpreta- tion methods.</p><p>In Section 2, we construct more of these coun- terintuitive examples by augmenting input reduc- tion with beam search and experiment with three tasks: SQUAD ( <ref type="bibr" target="#b26">Rajpurkar et al., 2016</ref>) for read- ing comprehension, SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> for textual entailment, and VQA ( <ref type="bibr" target="#b0">Antol et al., 2015</ref>) for visual question answering. Input re- duction with beam search consistently reduces the input sentence to very short lengths-often only one or two words-without lowering model confi- dence on its original prediction. The reduced ex- amples appear nonsensical to humans, which we verify with crowdsourced experiments. In Sec- tion 3, we draw connections to adversarial exam- ples and confidence calibration; we explain why the observed pathologies are a consequence of the overconfidence of neural models. This elucidates limitations of interpretation methods that rely on model confidence. In Section 4, we encourage high model uncertainty on reduced examples with entropy regularization. The pathological model behavior under input reduction is mitigated, lead- ing to more reasonable reduced examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Input Reduction</head><p>To explain model predictions using a set of impor- tant words, we must first define importance. Af- ter defining input perturbation and gradient-based approximation, we describe input reduction with these importance metrics. Input reduction dras- tically shortens inputs without causing the model to change its prediction or significantly decrease its confidence. Crowdsourced experiments con- firm that reduced examples appear nonsensical to humans: input reduction uncovers pathological model behaviors. <ref type="bibr" target="#b27">Ribeiro et al. (2016)</ref> and <ref type="bibr" target="#b18">Li et al. (2016b)</ref> de- fine importance by seeing how confidence changes when a feature is removed; a natural approxima- tion is to use the gradient ( <ref type="bibr" target="#b2">Baehrens et al., 2010;</ref><ref type="bibr" target="#b29">Simonyan et al., 2014</ref>). We formally define these importance metrics in natural language contexts and introduce the efficient gradient-based approx- imation. For each word in an input sentence, we measure its importance by the change in the con- fidence of the original prediction when we remove that word from the sentence. We switch the sign so that when the confidence decreases, the impor- tance value is positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Importance from Input Gradient</head><p>Formally, let x = x 1 , x 2 , . . . x n denote the in- put sentence, f (y | x) the predicted probability of label y, and y = argmax y f (y | x) the original predicted label. The importance is then</p><formula xml:id="formula_0">g(x i | x) = f (y | x) âˆ’ f (y | x âˆ’i ).<label>(1)</label></formula><p>To calculate the importance of each word in a sen- tence with n words, we need n forward passes of the model, each time with one of the words left out. This is highly inefficient, especially for longer sentences. Instead, we approximate the impor- tance value with the input gradient. For each word in the sentence, we calculate the dot product of its word embedding and the gradient of the output with respect to the embedding. The importance of n words can thus be computed with a single forward-backward pass. This gradient approxima- tion has been used for various interpretation meth- ods for natural language classification models ( <ref type="bibr" target="#b17">Li et al., 2016a;</ref><ref type="bibr" target="#b1">Arras et al., 2016)</ref>; see <ref type="bibr" target="#b7">Ebrahimi et al. (2017)</ref> for further details on the derivation. We use this approximation in all our experiments as it selects the same words for removal as an ex- haustive search (no approximation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Removing Unimportant Words</head><p>Instead of looking at the words with high impor- tance values-what interpretation methods com- monly do-we take a complementary approach and study how the model behaves when the sup- posedly unimportant words are removed. Intu- itively, the important words should remain after the unimportant ones are removed.</p><p>Our input reduction process iteratively removes the unimportant words. At each step, we remove the word with the lowest importance value un- til the model changes its prediction. We experi-ment with three popular datasets: SQUAD (Ra- jpurkar et al., 2016) for reading comprehension, SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015</ref>) for textual entail- ment, and VQA ( <ref type="bibr" target="#b0">Antol et al., 2015</ref>) for visual question answering. We describe each of these tasks and the model we use below, providing full details in the Supplement.</p><p>In SQUAD, each example is a context para- graph and a question. The task is to predict a span in the paragraph as the answer. We reduce only the question while keeping the context paragraph unchanged. The model we use is the DRQA Doc- ument Reader ( <ref type="bibr" target="#b6">Chen et al., 2017)</ref>.</p><p>In SNLI, each example consists of two sen- tences: a premise and a hypothesis. The task is to predict one of three relationships: entailment, neutral, or contradiction. We reduce only the hy- pothesis while keeping the premise unchanged. The model we use is Bilateral Multi-Perspective Matching (BIMPM) ( <ref type="bibr" target="#b32">Wang et al., 2017)</ref>.</p><p>In VQA, each example consists of an image and a natural language question. We reduce only the question while keeping the image unchanged. The model we use is Show, Ask, Attend, and An- swer ( <ref type="bibr" target="#b15">Kazemi and Elqursh, 2017)</ref>.</p><p>During the iterative reduction process, we en- sure that the prediction does not change (exact same span for SQUAD); consequently, the model accuracy on the reduced examples is identical to the original. The predicted label is used for input reduction and the ground-truth is never revealed. We use the validation set for all three tasks.</p><p>Most reduced inputs are nonsensical to humans ( <ref type="figure">Figure 2</ref>) as they lack information for any reason- able human prediction. However, models make confident predictions, at times even more confi- dent than the original.</p><p>To find the shortest possible reduced inputs (potentially the most meaningless), we relax the requirement of removing only the least impor- tant word and augment input reduction with beam search. We limit the removal to the k least impor- tant words, where k is the beam size, and decrease the beam size as the remaining input is shortened. <ref type="bibr">1</ref> We empirically select beam size five as it pro- duces comparable results to larger beam sizes with reasonable computation cost. The requirement of maintaining model prediction is unchanged.  Figure 2: Examples of original and reduced inputs where the models predict the same Answer. Reduced shows the input after reduction. We remove words from the hypothesis for SNLI, questions for SQUAD and VQA. Given the nonsensical reduced inputs, humans would not be able to provide the answer with high con- fidence, yet, the neural models do.</p><p>With beam search, input reduction finds ex- tremely short reduced examples with little to no decrease in the model's confidence on its orig- inal predictions. <ref type="figure">Figure 3</ref> compares the length of input sentences before and after the reduction. For all three tasks, we can often reduce the sen- tence to only one word. <ref type="figure" target="#fig_1">Figure 4</ref> compares the model's confidence on original and reduced in- puts. On SQUAD and SNLI the confidence de- creases slightly, and on VQA the confidence even increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Humans Confused by Reduced Inputs</head><p>On the reduced examples, the models retain their original predictions despite short input lengths.</p><p>The following experiments examine whether these predictions are justified or pathological, based on how humans react to the reduced inputs.</p><p>For each task, we sample 200 examples that are correctly classified by the model and generate their reduced examples. In the first setting, we com- pare the human accuracy on original and reduced examples. We recruit two groups of crowd work- ers and task them with textual entailment, reading comprehension, or visual question answering. We show one group the original inputs and the other the reduced. Humans are no longer able to give  <ref type="figure">Figure 3</ref>: Distribution of input sentence length before and after reduction. For all three tasks, the input is often reduced to one or two words without changing the model's prediction. the correct answer, showing a significant accuracy loss on all three tasks (compare Original and Re- duced in <ref type="table">Table 1</ref>). The second setting examines how random the reduced examples appear to humans. For each of the original examples, we generate a version where words are randomly removed until the length matches the one generated by input reduc- tion. We present the original example along with the two reduced examples and ask crowd work- ers their preference between the two reduced ones. The workers' choice is almost fifty-fifty (the vs. Random in <ref type="table">Table 1</ref>): the reduced examples appear almost random to humans.</p><p>These results leave us with two puzzles: why are the models highly confident on the nonsensical reduced examples? And why, when the leave-one- out method selects important words that appear reasonable to humans, the input reduction process selects ones that are nonsensical?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Original  <ref type="table">Table 1</ref>: Human accuracy on Reduced examples drops significantly compared to the Original examples, how- ever, model predictions are identical. The reduced ex- amples also appear random to humans-they do not prefer them over random inputs (vs. Random). For SQUAD, accuracy is reported using F1 scores, other numbers are percentages. For SNLI, we report results on the three classes separately: entailment (-E), neutral (-N), and contradiction (-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Making Sense of Reduced Inputs</head><p>Having established the incongruity of our defini- tion of importance vis-` a-vis human judgements, we now investigate possible explanations for these results. We explain why model confidence can empower methods such as leave-one-out to gen- erate reasonable interpretations but also lead to pathologies under input reduction. We attribute these results to two issues of neural models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overconfidence</head><p>Neural models are overconfident in their predic- tions ( <ref type="bibr" target="#b10">Guo et al., 2017)</ref>. One explanation for overconfidence is overfitting: the model overfits the negative log-likelihood loss during training by learning to output low-entropy distributions over classes. Neural models are also overconfident on examples outside the training data distribution. As <ref type="bibr" target="#b9">Goodfellow et al. (2015)</ref> observe for image classi- fication, samples from pure noise can sometimes trigger highly confident predictions. These so- called rubbish examples are degenerate inputs that a human would trivially classify as not belonging to any class but for which the model predicts with high confidence. <ref type="bibr" target="#b9">Goodfellow et al. (2015)</ref> argue that the rubbish examples exist for the same rea- son that adversarial examples do: the surprising linear nature of neural models. In short, the confi- dence of a neural model is not a robust estimate of its prediction uncertainty. Our reduced inputs satisfy the definition of rub- bish examples: humans have a hard time making predictions based on the reduced inputs <ref type="table">(Table 1)</ref>, but models make predictions with high confidence <ref type="figure" target="#fig_1">(Figure 4)</ref>. Starting from a valid example, input reduction transforms it into a rubbish example.</p><p>The nonsensical, almost random results are best explained by looking at a complete reduction path ( <ref type="figure" target="#fig_2">Figure 5</ref>). In this example, the transition from valid to rubbish happens immediately after the first step: following the removal of "Broncos", humans can no longer determine which team the ques- tion is asking about, but model confidence remains high. Not being able to lower its confidence on rubbish examples-as it is not trained to do so- the model neglects "Broncos" and eventually the process generates nonsensical results.</p><p>In this example, the leave-one-out method will not highlight "Broncos". However, this is not a failure of the interpretation method but of the model itself. The model assigns a low impor- tance to "Broncos" in the first step, causing it to be removed-leave-one-out would be able to expose this particular issue by not highlighting "Bron- cos". However, in cases where a similar issue only appear after a few unimportant words are removed, the leave-one-out method would fail to expose the unreasonable model behavior.</p><p>Input reduction can expose deeper issues of model overconfidence and stress test a model's un- certainty estimation and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Second-order Sensitivity</head><p>So far, we have seen that the output of a neural model is sensitive to small changes in its input. We call this first-order sensitivity, because interpreta- tion based on input gradient is a first-order Taylor expansion of the model near the input ( <ref type="bibr" target="#b29">Simonyan et al., 2014</ref>). However, the interpretation also shifts drastically with small input changes <ref type="figure">(Fig- ure 6</ref>). We call this second-order sensitivity.</p><p>The shifting heatmap suggests a mismatch be- tween the model's first-and second-order sensi- SQUAD Context: The Panthers used the San Jose State practice facility and stayed at the San Jose Marriott. The Broncos practiced at Stanford University and stayed at the Santa Clara Marriott. tivities. The heatmap shifts when, with respect to the removed word, the model has low first-order sensitivity but high second-order sensitivity.</p><p>Similar issues complicate comparable interpre- tation methods for image classification models. For example, <ref type="bibr" target="#b8">Ghorbani et al. (2017)</ref> modify im- age inputs so the highlighted features in the in- terpretation change while maintaining the same prediction. To achieve this, they iteratively mod- ify the input to maximize changes in the distribu- tion of feature importance. In contrast, the shift- ing heatmap we observe occurs by only remov- ing the least impactful features without a targeted optimization. They also speculate that the steep- est gradient direction for the first-and second- order sensitivity values are generally orthogonal. Loosely speaking, the shifting heatmap suggests that the direction of the smallest gradient value can sometimes align with very steep changes in second-order sensitivity.</p><p>When explaining individual model predictions, the heatmap suggests that the prediction is made based on a weighted combination of words, as in a linear model, which is not true unless the model is indeed taking a weighted sum such as in a <ref type="bibr">DAN (Iyyer et al., 2015</ref>). When the model composes representations by a non-linear combi- nation of words, a linear interpretation oblivious to second-order sensitivity can be misleading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQUAD</head><p>Context: QuickBooks sponsored a "Small Business Big Game" contest, in which Death Wish Coffee had a 30-second commercial aired free of charge courtesy of QuickBooks. Death Wish Coffee beat out nine other contenders from across the United States for the free advertisement.</p><p>Question: What company won free advertisement due to QuickBooks contest ? What company won free advertisement due to QuickBooks ? What company won free advertisement due to ? What company won free due to ? What won free due to ? What won due to ? What won due to What won due What won What <ref type="figure">Figure 6</ref>: Heatmap generated with leave-one-out shifts drastically despite only removing the least important word (underlined) at each step. For instance, "adver- tisement", is the most important word in step two but becomes the least important in step three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mitigating Model Pathologies</head><p>The previous section explains the observed pathologies from the perspective of overconfi- dence: models are too certain on rubbish exam- ples when they should not make any prediction. Human experiments in Section 2.3 confirm that the reduced examples fit the definition of rubbish examples. Hence, a natural way to mitigate the pathologies is to maximize model uncertainty on the reduced examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regularization on Reduced Inputs</head><p>To maximize model uncertainty on reduced ex- amples, we use the entropy of the output distri- bution as an objective. Given a model f trained on a dataset (X , Y), we generate reduced exam- ples using input reduction for all training examples X . Beam search often yields multiple reduced ver- sions with the same minimum length for each in- put x, and we collect all of these versions together to formËœXformËœ formËœX as the "negative" example set.</p><p>Let H (Â·) denote the entropy and f (y | x) denote the probability of the model predicting y given x. We fine-tune the existing model to simultaneously maximize the log-likelihood on regular examples and the entropy on reduced examples:</p><formula xml:id="formula_1">(x,y)âˆˆ(X ,Y) log(f (y | x)) + Î» Ëœ xâˆˆËœXxâˆˆËœ xâˆˆËœX H (f (y | Ëœ x)) ,<label>(2)</label></formula><p>where hyperparameter Î» controls the trade-off be- tween the two terms. Similar entropy regulariza- tion is used by <ref type="bibr" target="#b24">Pereyra et al. (2017)</ref>  combination with input reduction; their entropy term is calculated on regular examples rather than reduced examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Regularization Mitigates Pathologies</head><p>On regular examples, entropy regularization does no harm to model accuracy, with a slight increase for SQUAD (Accuracy in <ref type="table" target="#tab_4">Table 2</ref>). After entropy regularization, input reduction produces more reasonable reduced inputs <ref type="figure" target="#fig_3">(Fig- ure 7)</ref>. In the SQUAD example from <ref type="figure">Figure 1</ref>, the reduced question changed from "did" to "spend Astor money on ?" after fine-tuning. The average length of reduced examples also increases across all tasks (Reduced length in <ref type="table" target="#tab_4">Table 2</ref>). To verify that model overconfidence is indeed mitigated- that the reduced examples are less "rubbish" com- pared to before fine-tuning-we repeat the human experiments from Section 2.3.</p><p>Human accuracy increases across all three tasks <ref type="table">(Table 3)</ref>. We also repeat the vs. Random exper- iment: we re-generate the random examples to match the lengths of the new reduced examples from input reduction, and find humans now pre- fer the reduced examples to random ones. The in- crease in both human performance and preference suggests that the reduced examples are more rea- sonable; model pathologies have been mitigated.</p><p>While these results are promising, it is not clear whether our input reduction method is necessary to achieve them. To provide a baseline, we fine- tune models using inputs randomly reduced to the same lengths as the ones generated by input reduc- tion. This baseline improves neither the model ac- curacy on regular examples nor interpretability un- der input reduction (judged by lengths of reduced examples). Input reduction is effective in generat- ing negative examples to counter model overcon- fidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQUAD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>In 1899, John Jacob Astor IV invested $100,000 for Tesla to further develop and produce a new lighting system. In- stead, Tesla used the money to fund his Colorado Springs experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original What did Tesla spend Astor's money on ? Answer</head><p>Colorado  <ref type="figure">Figure 2</ref>. We ap- ply input reduction to models both Before and After en- tropy regularization. The models still predict the same Answer, but the reduced examples after fine-tuning ap- pear more reasonable to humans.</p><note type="other">Springs experiments Before did After spend Astor money on ? Confidence 0.78 â†’ 0.91 â†’ 0.52 SNLI Premise Well dressed man and woman dancing in the street Original Two man is dancing on the street Answer Contradiction Before dancing After two man dancing Confidence 0.977 â†’ 0.706 â†’ 0.717 VQA Original What color is the flower ? Answer yellow Before flower ? After What color is flower ? Confidence 0.847 â†’ 0.918 â†’ 0.745</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Rubbish examples have been studied in the image domain ( <ref type="bibr" target="#b9">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b21">Nguyen et al., 2015</ref>), but to our knowledge not for NLP. Our in- put reduction process gradually transforms a valid input into a rubbish example. We can often deter- mine which word's removal causes the transition to occur-for example, removing "Broncos" in <ref type="figure" target="#fig_2">Figure 5</ref>. These rubbish examples are particularly interesting, as they are also adversarial: the dif- ference from a valid example is small, unlike im- age rubbish examples generated from pure noise which are far outside the training data distribution.</p><p>The robustness of NLP models has been studied extensively <ref type="bibr" target="#b22">(Papernot et al., 2016;</ref><ref type="bibr" target="#b14">Jia and Liang, 2017;</ref><ref type="bibr" target="#b13">Iyyer et al., 2018;</ref><ref type="bibr" target="#b28">Ribeiro et al., 2018)</ref>, and most studies define adversarial examples similar to the image domain: small perturbations to the input lead to large changes in the output. Hot- Flip ( <ref type="bibr" target="#b7">Ebrahimi et al., 2017</ref>) uses a gradient-based approach, similar to image adversarial examples, to flip the model prediction by perturbing a few characters or words. Our <ref type="bibr">work and Belinkov and Bisk (2018)</ref>   <ref type="table">Table 3</ref>: Human Accuracy increases after fine-tuning the models. Humans also prefer gradient-based re- duced examples over randomly reduced ones, indicat- ing that the reduced examples are more meaningful to humans after regularization.</p><p>user inputs become adversarial by accident: com- mon misspellings break neural machine transla- tion models; we show that incomplete user input can lead to unreasonably high model confidence.</p><p>Other failures of interpretation methods have been explored in the image domain. The sensi- tivity issue of gradient-based interpretation meth- ods, similar to our shifting heatmaps, are observed by <ref type="bibr" target="#b8">Ghorbani et al. (2017)</ref> and <ref type="bibr" target="#b16">Kindermans et al. (2017)</ref>. They show that various forms of input perturbation-from adversarial changes to simple constant shifts in the image input-cause signifi- cant changes in the interpretation. <ref type="bibr" target="#b8">Ghorbani et al. (2017)</ref> make a similar observation about second- order sensitivity, that "the fragility of interpreta- tion is orthogonal to fragility of the prediction".</p><p>Previous work studies biases in the annotation process that lead to datasets easier than desired or expected which eventually induce pathological models. We attribute our observed pathologies primarily to the lack of accurate uncertainty es- timates in neural models trained with maximum likelihood. SNLI hypotheses contain artifacts that allow training a model without the premises (Gu- rurangan et al., 2018); we apply input reduction at test time to the hypothesis. Similarly, VQA images are surprisingly unimportant for training a model; we reduce the question. The recent SQUAD 2.0 ( <ref type="bibr" target="#b25">Rajpurkar et al., 2018</ref>) augments the original reading comprehension task with an un- certainty modeling requirement, the goal being to make the task more realistic and challenging. Section 3.1 explains the pathologies from the overconfidence perspective. One explanation for overconfidence is overfitting: <ref type="bibr" target="#b10">Guo et al. (2017)</ref> show that, late in maximum likelihood training, the model learns to minimize loss by outputting low-entropy distributions without improving vali- dation accuracy. To examine if overfitting can ex- plain the input reduction results, we run input re- duction using DRQA model checkpoints from ev- ery training epoch. Input reduction still achieves similar results on earlier checkpoints, suggesting that better convergence in maximum likelihood training cannot fix the issues by itself-we need new training objectives with uncertainty estima- tion in mind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods for Mitigating Pathologies</head><p>We use the reduced examples generated by input reduction to regularize the model and improve its interpretability. This resembles adversarial train- ing ( <ref type="bibr" target="#b9">Goodfellow et al., 2015)</ref>, where adversar- ial examples are added to the training set to im- prove model robustness. The objectives are dif- ferent: entropy regularization encourages high un- certainty on rubbish examples, while adversarial training makes the model less sensitive to adver- sarial perturbations. <ref type="bibr" target="#b24">Pereyra et al. (2017)</ref> apply entropy regulariza- tion on regular examples from the start of train- ing to improve model generalization. A similar method is label smoothing ( <ref type="bibr" target="#b30">Szegedy et al., 2016)</ref>. In comparison, we fine-tune a model with entropy regularization on the reduced examples for better uncertainty estimates and interpretations.</p><p>To mitigate overconfidence, <ref type="bibr" target="#b10">Guo et al. (2017)</ref> propose post-hoc fine-tuning a model's confidence with Platt scaling. This method adjusts the soft- max function's temperature parameter using a small held-out dataset to align confidence with ac- curacy. However, because the output is calibrated using the entire confidence distribution, not indi- vidual values, this does not reduce overconfidence on specific inputs, such as the reduced examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generalizability of Findings</head><p>To highlight the erratic model predictions on short examples and provide a more intuitive demonstra- tion, we present paired-input tasks. On these tasks, the short lengths of reduced questions and hy- potheses obviously contradict the necessary num- ber of words for a human prediction (further sup- ported by our human studies). We also apply input reduction to single-input tasks including sentiment analysis <ref type="bibr" target="#b19">(Maas et al., 2011</ref>) and Quizbowl <ref type="bibr">(BoydGraber et al., 2012)</ref>, achieving similar results. Interestingly, the reduced examples transfer to other architectures.</p><p>In particular, when we feed fifty reduced SNLI inputs from each class-generated with the BIMPM model ( <ref type="bibr" target="#b32">Wang et al., 2017)</ref>-through the Decomposable Atten- tion Model ( <ref type="bibr" target="#b23">Parikh et al., 2016</ref>), 2 the same predic- tion is triggered 81.3% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce input reduction, a process that it- eratively removes unimportant words from an in- put while maintaining a model's prediction. Com- bined with gradient-based importance estimates often used for interpretations, we expose patholog- ical behaviors of neural models. Without lowering model confidence on its original prediction, an in- put sentence can be reduced to the point where it appears nonsensical, often consisting of one or two words. Human accuracy degrades when shown the reduced examples instead of the orig- inal, in contrast to neural models which maintain their original predictions.</p><p>We explain these pathologies with known is- sues of neural models: overconfidence and sen- sitivity to small input changes. The nonsensical reduced examples are caused by inaccurate uncer- tainty estimates-the model is not able to lower its confidence on inputs that do not belong to any label. The second-order sensitivity is another issue why gradient-based interpretation methods may fail to align with human perception: a small change in the input can cause, at the same time, a minor change in the prediction but a large change in the interpretation. Input reduction perturbs the input multiple times and can expose deeper issues of model overconfidence and oversensitivity that other methods cannot. Therefore, it can be used to stress test the interpretability of a model. Finally, we fine-tune the models by maximizing entropy on reduced examples to mitigate the de- ficiencies. This improves interpretability without sacrificing model accuracy on regular examples.</p><p>To properly interpret neural models, it is impor- tant to understand their fundamental characteris- tics: the nature of their decision surfaces, robust- ness against adversaries, and limitations of their training objectives. We explain fundamental diffi- culties of interpretation due to pathologies in neu- ral models trained with maximum likelihood. Our work suggests several future directions to improve interpretability: more thorough evaluation of in- terpretation methods, better uncertainty and con- fidence estimates, and interpretation beyond bag- of-word heatmap.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SNLI</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Density distribution of model confidence on reduced inputs is similar to the original confidence. In SQUAD, we predict the beginning and the end of the answer span, so we show the confidence for both.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A reduction path for a SQUAD validation example. The model prediction is always correct and its confidence stays high (shown on the left in parentheses) throughout the reduction. Each line shows the input at that step with an underline indicating the word to remove next. The question becomes unanswerable immediately after "Broncos" is removed in the first step. However, in the context of the original question, "Broncos" is the least important word according to the input gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: SQUAD example from Figure 1, SNLI and VQA (image omitted) examples from Figure 2. We apply input reduction to models both Before and After entropy regularization. The models still predict the same Answer, but the reduced examples after fine-tuning appear more reasonable to humans.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>, but not in</head><label></label><figDesc></figDesc><table>Accuracy 
Reduced length 

Before After Before After 

SQUAD 77.41 78.03 
2.27 
4.97 
SNLI 
85.71 85.72 
1.50 
2.20 
VQA 
61.61 61.54 
2.30 
2.87 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model Accuracy on regular validation ex-
amples remains largely unchanged after fine-tuning. 
However, the length of the reduced examples (Reduced 
length) increases on all three tasks, making them less 
likely to appear nonsensical to humans. 

</table></figure>

			<note place="foot" n="1"> We set beam size to max(1, min(k, L âˆ’ 3)) where k is maximum beam size and L is the current length of the input sentence.</note>

			<note place="foot" n="2"> http://demo.allennlp.org/ textual-entailment</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Feng was supported under subcontract to Raytheon BBN Technologies by DARPA award HR0011-15-C-0113. JBG is supported by NSF Grant IIS1652666.</p><p>Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor. The authors would like to thank Hal DaumÃ© III, Alexander M. Rush, Nicolas Papernot, members of the CLIP lab at the University of Maryland, and the anonymous reviewers for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explaining predictions of non-linear classifiers in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GrÃ©goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klausrobert</forename><surname>MÃ¼ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Besting the quiz master: Crowdsourcing incremental classification games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>He He, and Hal DaumÃ© III</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HotFlip: White-box adversarial examples for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirata</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10547</idno>
		<title level="m">Interpretation of neural networks is fragile</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>DaumÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>SchÃ¼tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>DÃ¤hne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Been</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00867</idno>
		<title level="m">The (un)reliability of saliency methods</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding neural networks through representation erasure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond word importance: Contextual decomposition to extract interactions from lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Mai Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Crafting adversarial input sequences for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Harang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE Military Communications Conference</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SQuAD: 100, 000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Why should i trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco TÃºlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantically equivalent adversarial rules for debugging nlp models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
