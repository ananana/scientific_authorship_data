<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Residual Learning for Sequence Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Residual Learning for Sequence Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="938" to="943"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely deep neural network for image understanding, to improve recurrent neural networks (RNN) for modeling sequential data. We show that for sequence classification tasks, incorporating residual connections into recurrent structures yields similar accuracy to Long Short Term Memory (LSTM) RNN with much fewer model parameters. In addition, we propose two novel models which combine the best of both residual learning and LSTM. Experiments show that the new models significantly outperform LSTM.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent Neural Networks (RNNs) are powerful tools to model sequential data. Among various RNN models, Long Short Term Memory (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) is one of the most effective structures. In LSTM, gating mech- anism is used to control the information flow such that gradient vanishing problem in vanilla RNN is better handled, and long range dependency is bet- ter captured. However, as empirically verified by previous works and our own experiments, to obtain fairly good results, training LSTM RNN needs care- fully designed optimization procedure <ref type="bibr" target="#b6">(Hochreiter et al., 2001</ref>; <ref type="bibr" target="#b15">Pascanu et al., 2013;</ref><ref type="bibr" target="#b2">Dai and Le, 2015;</ref><ref type="bibr" target="#b8">Laurent et al., 2015;</ref><ref type="bibr" target="#b4">He et al., 2016</ref>; Arjovsky et * This work was done when the author was visiting <ref type="bibr">Microsoft Research Asia.</ref> al., 2015), especially when faced with unfolded very deep architectures for fairly long sequences <ref type="bibr" target="#b2">(Dai and Le, 2015)</ref>.</p><p>From another perspective, for constructing very deep neural networks, recently Residual Networks (ResNet) ( <ref type="bibr" target="#b3">He et al., 2015</ref>) have shown their ef- fectiveness in quite a few computer vision tasks. By learning a residual mapping between layers with identity skip connections ( <ref type="bibr" target="#b7">Jaeger et al., 2007)</ref>, ResNet ensures a fluent information flow, leading to efficient optimization for very deep structures (e.g., with hundreds of layers). In this paper, we explore the possibilities of leveraging residual learning to improve the performances of recurrent structures, in particular, LSTM RNN, in modeling fairly long se- quences (i.e., whose lengths exceed 100). To sum- marize, our main contributions include:</p><p>1. We introduce residual connecting mechanism into the recurrent structure and propose recur- rent residual networks for sequence learning. Our model achieves similar performances to LSTM in text classification tasks, whereas the number of model parameters is greatly reduced.</p><p>2. We present in-depth analysis of the strengths and limitations of LSTM and ResNet in respect of sequence learning.</p><p>3. Based on such analysis, we further propose two novel models that incorporate the strengths of the mechanisms behind LSTM and ResNet. We demonstrate that our models outperform LSTM in many sequence classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>938</head><p>RNN models sequences by taking sequential input x = {x 1 , · · · , x T } and generating T step hidden states h = {h 1 , · · · , h T }. At each time step t, RNN takes the input vector x t ∈ R n and the previous hid- den state vector h t−1 ∈ R m to produce the next hid- den state h t . Based on this basic structure, LSTM avoids gradi- ent vanishing in RNN training and thus copes better with long range dependencies, by further augment- ing vanilla RNN with a memory cell vector c t ∈ R m and multiplicative gate units that regulate the infor- mation flow. To be more specific, at each time step t, an LSTM unit takes x t , c t−1 , h t−1 as input, gen- erates the input, output and forget gate signals (de- noted as i t , o t and f t respectively), and produces the next cell state c t and hidden state h t :</p><formula xml:id="formula_0">˜ c t = tanh(W c [h t−1 , x t ] + b c ) i t = σ(W i [h t−1 , x t ] + b i ) f t = σ(W f [h t−1 , x t ] + b f ) o t = σ(W o [h t−1 , x t ] + b o ) c t = f t ⊗ C t−1 + i t ⊗ ˜ c t h t = o t ⊗ tanh(c t )<label>(1)</label></formula><p>where ⊗ refers to element-wise product. σ(x) is the sigmoid function σ(x) = 1/(1+exp(−x)). W j (j ∈ {i, o, f, c}) are LSTM parameters. In the following part, such functions generating h t and c t are denoted as h t , c t = LST M (x t , h t−1 , c t−1 ). Residual Networks (ResNet) are among the pio- neering works ( <ref type="bibr" target="#b18">Szegedy et al., 2015;</ref><ref type="bibr" target="#b16">Srivastava et al., 2015</ref>) that utilize extra identity connections to enhance information flow such that very deep neural networks can be effectively optimized. <ref type="bibr">ResNet (He et al., 2015</ref>) is composed of several stacked resid- ual units, in which the l th unit takes the following transformation:</p><formula xml:id="formula_1">h l+1 = f (g(h l ) + F(h l ; W l ))<label>(2)</label></formula><p>where h l and h l+1 are the input and output for the l th unit respectively. F is the residual function with weight parameters W l . f is typically the ReLU func- tion ( <ref type="bibr" target="#b14">Nair and Hinton, 2010)</ref>. g is set as identity function, i.e., g(h l ) = h l . Such an identity con- nection guarantees the direct propagation of signals among different layers, thereby avoids gradient van- ishing. The recent paper ( <ref type="bibr" target="#b11">Liao and Poggio, 2016)</ref> talks about the possibility of using shared weights in ResNet, similar to what RNN does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Residual Learning</head><p>The basic idea of recurrent residual learning is to force a direct information flow in different time steps of RNNs by identity (skip) connections. In this sec- tion, we introduce how to leverage residual learning to 1) directly construct recurrent neural network in subsection 3.1; 2) improve LSTM in subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Residual Networks (RRN)</head><p>The basic architecture of Recurrent Residual Net- work (RRN for short) is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, in which orange arrows indicate the identity connec- tions from each h t−1 to h t , and blue arrows rep- resent the recurrent transformations taking both h t and x t as input. Similar to equation <ref type="formula" target="#formula_1">(2)</ref>, the recur- rent transformation in RRN takes the following form (denoted as h t = RRN (x t , h t−1 ) in the following sections):</p><formula xml:id="formula_2">h t = f (g(h t−1 ) + F(h t−1 , x t ; W )),<label>(3)</label></formula><p>where g is still the identity function s.t. g(h t−1 ) = h t−1 , corresponding to the orange arrows in <ref type="figure" target="#fig_0">Figure  1</ref>. f is typically set as tanh. For function F with weight parameters W (corresponding to the blue ar- rows in <ref type="figure" target="#fig_0">Figure 1</ref>), inspired by the observation that higher recurrent depth tends to lead to better perfor- mances ( , we impose K deep transformations in F:</p><formula xml:id="formula_3">y t 1 = σ(x t W 1 + h t−1 U 1 + b 1 ) y t 2 = σ(x t W 2 + y t 1 U 2 + b 2 ) · · · y t K = σ(x t W K + y t K−1 U K + b K ) F(h t−1 , x t ) = y t K (4)</formula><p>where x t is taken at every layer such that the input information is better captured, which works simi- larly to the mechanism of highway network <ref type="bibr" target="#b16">(Srivastava et al., 2015)</ref>. K is the recurrent depth de- fined in ( . The weights W m (m ∈ {1, · · · , K}) are shared across different time steps t. RRN forces the direct propagation of hidden state signals between every two consecutive time steps with identity connections g. In addition, the mul- tiple non-linear transformations in F guarantees its capability in modelling complicated recurrent rela- tionship. In practice, we found that K = 2 yields fairly good performances, meanwhile leads to half of LSTM parameter size when model dimensions are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gated Residual RNN</head><p>Identity connections in ResNet are important for propagating the single input image information to higher layers of CNN. However, when it comes to sequence classification, the scenario is quite differ- ent in that there is a new input at every time step. Therefore, a forgetting mechanism to "forget" less critical historical information, as is employed in LSTM (controlled by the forget gate f t ), becomes necessary. On the other hand, while LSTM benefits from the flexible gating mechanism, its parametric nature brings optimization difficulties to cope with fairly long sequences, whose long range informa- tion dependencies could be better captured by iden- tity connections.</p><p>Inspired by the success of the gating mechanism of LSTM and the residual connecting mechanism with enhanced information flow of ResNet, we fur- ther propose two Gated Residual Recurrent models leveraging the strengths of the two mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Model 1: Skip-Connected LSTM (SC-LSTM)</head><p>Skip-Connected LSTM (SC-LSTM for short) in- troduces identity connections into standard LSTM to enhance the information flow. Note that in <ref type="figure" target="#fig_0">Figure 1</ref>, a straightforward approach is to replace F with an LSTM unit. However, our preliminary experiments do not achieve satisfactory results. Our conjecture is that identity connections between consecutive mem- ory cells, which are already sufficient to maintain short-range memory, make the unregulated informa- tion flow overly strong, and thus compromise the merit of gating mechanism.</p><p>To reasonably enhance the information flow for LSTM network while keeping the advantage of gat- ing mechanism, starting from equation <ref type="formula" target="#formula_0">(1)</ref>, we pro- pose to add skip connections between two LSTM hidden states with a wide range of distance L (e.g.,</p><formula xml:id="formula_4">L = 20), such that ∀t = {1, 1 + L, 1 + 2L, · · · , 1 + T −L−1 L L}: h t+L = tanh(c t+L ) ⊗ o t+L + αh t<label>(5)</label></formula><p>Here α is a scalar that can either be fixed as 1 (i.e., identity mapping) or be optimized during train- ing process as a model parameter (i.e., parametric skip connection). We refer to these two variants as SC-LSTM-I and SC-LSTM-P respectively. Note that in SC-LSTM, the skip connections only exist in time steps 1,</p><formula xml:id="formula_5">1 + L, 1 + 2L, · · · , 1 + T −L−1 L L.</formula><p>The basic structure is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Model 2: Hybrid Residual LSTM (HRL)</head><p>Since LSTM generates sequence representations out of flexible gating mechanism, and RRN gener- ates representations with enhanced residual histori- cal information, it is a natural extension to combine the two representations to form a signal that bene- fits from both mechanisms. We denote this model as Hybrid Residual LSTM (HRL for short).</p><p>In HRL, two independent signals, h LST M t gen- erated by LSTM (equation <ref type="formula" target="#formula_0">(1)</ref>) and h RRN t gener- ated by RRN (equation <ref type="formula" target="#formula_2">(3)</ref>), are propagated through LSTM and RRN respectively:</p><formula xml:id="formula_6">h LST M t , c t = LST M (x t , h LST M t−1 , c t−1 ) h RRN t = RRN (x t , h RRN t−1 )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>940</head><p>The final representation h HRL T is obtained by the mean pooling of the two "sub" hidden states:</p><formula xml:id="formula_7">h HRL T = 1 2 (h LST M T + h RRN T )<label>(7)</label></formula><p>h HRL T is then used for higher level tasks such as pre- dicting the sequence label. Acting in this way, h HRL T contains both the statically forced and dynamically adjusted historical signals, which are respectively conveyed by h RRN t and h LST M t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct comprehensive empirical analysis on sequence classification tasks. Listed in the ascend- ing order of average sequence lengths, several public datasets we use include:</p><p>1. AG's news corpus 1 ,a news article corpus with categorized articles from more than 2, 000 news sources. We use the dataset with 4 largest classes constructed in ( ).</p><p>2. IMDB movie review dataset 2 , a binary senti- ment classification dataset consisting of movie review comments with positive/negative senti- ment labels <ref type="bibr" target="#b12">(Maas et al., 2011</ref>).</p><p>3. 20 Newsgroups (20NG for short), an email collection dataset categorized into 20 news groups. Simiar to <ref type="bibr" target="#b2">(Dai and Le, 2015)</ref>, we use the post-processed version 3 , in which attach- ments, PGP keys and some duplicates are re- moved.</p><p>4. Permuted-MNIST (P-MNIST for short). Fol- lowing ( <ref type="bibr" target="#b0">Arjovsky et al., 2015)</ref>, we shuffle pixels of each MNIST image (Le- Cun et al., 1998) with a fixed random per- mutation, and feed all pixels sequentially into recurrent network to predict the image label. Permuted-MNIST is assumed to be a good testbed for measuring the ability of modeling very long range dependencies ( <ref type="bibr" target="#b0">Arjovsky et al., 2015</ref>).</p><p>Detailed statistics of each dataset are listed in <ref type="table" target="#tab_0">Table 1</ref>. For all the text datasets, we take every word as input and feed word embedding vectors pre-trained by <ref type="bibr">Word2Vec (Mikolov et al., 2013</ref>) on Wikipedia into the recurrent neural network. The top most frequent words with 95% total frequency coverage are kept, while others are replaced by the token "UNK". We use the standard training/test split along with all these datasets and randomly pick 15% of training set as dev set, based on which we perform early stopping and for all models tune hyper-parameters such as dropout ratio (on non-recurrent layers) ( <ref type="bibr">Zaremba et al., 2014</ref>), gradient clipping value ( <ref type="bibr" target="#b15">Pascanu et al., 2013</ref>) and the skip connection length L for SC-LSTM (cf. equation <ref type="formula" target="#formula_4">(5)</ref>). The last hidden states of recurrent networks are put into logistic regression classifiers for label predictions. We use Adadelta <ref type="bibr" target="#b20">(Zeiler, 2012)</ref> to perform parameter optimization. All our implementations are based on <ref type="bibr">Theano (Theano Development Team, 2016</ref>) and run on one K40 GPU. All the source codes and datasets can be down- loaded at https://publish.illinois. edu/yirenwang/emnlp16source/.</p><p>We compare our proposed models mainly with the state-of-art standard LSTM RNN. In addition, to fully demonstrate the effects of residual learning in our HRL model, we employ another hybrid model as baseline, which combines LSTM and GRU ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>), another state-of-art RNN variant, in a similar way as HRL. We use LSTM+GRU to de- note such a baseline. The model sizes (word embed- ding size × hidden state size) configurations used for each dataset are listed in <ref type="table" target="#tab_1">Table 2. In Table 2</ref>, "Non-Hybrid" refers to LSTM, RRN and SC-LSTM models, while "Hybrid" refers to two methods that combines two basic models: HRL and LSTM+GRU. The model sizes of all hybrid models are smaller than the standard LSTM. All models have only one recurrent layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Results</head><p>All the classification accuracy numbers are listed in <ref type="table" target="#tab_2">Table 3</ref>. From this table, we have the following ob- servations and analysis:</p><p>1. RRN achieves similar performances to stan- dard LSTM in all classification tasks with only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Ave. <ref type="table" target="#tab_0">Len Max Len #Classes  #Train : #Test  AG's News  34  211  4  120, 000 : 7, 600  IMDB  281  2, 956  2  25, 000 : 25, 000  20NG  267  11, 924  20  11, 293 : 7, 528  P-MNIST  784  784  10</ref> 60, 000 : 10, 000   half of the model parameters, indicating that residual network structure, with connecting mechanism to enhance the information flow, is also an effective approach for sequence learn- ing. However, the fact that it fails to sig- nificantly outperform other models (as it does in image classification) implies that forgetting mechanism is desired in recurrent structures to handle multiple inputs.</p><p>2. Skip-Connected LSTM performs much better than standard LSTM. For tasks with shorter se- quences such as AG's News, the improvement is limited. However, the improvements get more significant with the growth of sequence lengths among different datasets 4 , and the per- formance is particularly good in P-MNIST with very long sequences. This reveals the impor- tance of skip connections in carrying on histor- ical information through a long range of time steps, and demonstrates the effectiveness of our approach that adopts the residual connecting mechanism to improve LSTM's capability of handling long-term dependency. Furthermore, SC-LSTM is robust with different hyperparam- 4 t-test on SC-LSTM-P and SC-LSTM-I with p value &lt; 0.001.</p><p>eter values: we test L = 10, 20, 50, 75 in P- MNIST and find the performance is not sensi- tive w.r.t. these L values.</p><p>3. HRL also outperforms standard LSTM with fewer model parameters <ref type="bibr">5</ref> . In comparison, the hybrid model of LSTM+GRU cannot achieve such accuracy as HRL. As we expected, the ad- ditional long range historical information prop- agated by RRN is proved to be good assistance to standard LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explore the possibility of lever- aging residual network to improve the performance of LSTM RNN. We show that direct adaptation of ResNet performs well in sequence classification. In addition, when combined with the gating mecha- nism in LSTM, residual learning significantly im- prove LSTM's performance. As to future work, we plan to apply residual learning to other se- quence tasks such as language modeling, and RNN based neural machine translation ( <ref type="bibr" target="#b17">Sutskever et al., 2014</ref>) ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The basic structure of Recurrent Residual Networks.</figDesc><graphic url="image-1.png" coords="3,72.00,57.83,228.36,102.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The basic structure of Skip-Connected LSTM.</figDesc><graphic url="image-2.png" coords="3,313.20,384.24,232.32,86.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Classification Datasets.</head><label>1</label><figDesc></figDesc><table>AG's News 
IMDB 
20NG 
P-MNIST 
Non-Hybird 256 × 512 256 × 512 500 × 768 1 × 100 
Hybrid 
256 × 384 256 × 384 256 × 512 
1 × 80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model Sizes on Different Dataset. 

Model/Task AG's News 
IMDB 
20NG 
P-MNIST 
LSTM 
91.76% 
88.88% 79.21% 
90.64% 
RRN 
91.19% 
89.13% 79.76% 
88.63% 
SC-LSTM-P 
92.01% 
90.74% 82.98% 94.46% 
SC-LSTM-I 
92.05% 
90.67% 81.85% 94.80% 
LSTM+GRU 
91.05% 
89.23% 80.12% 
90.28% 
HRL 
91.90% 
90.92% 81.73% 
90.33% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Classification Results (Test Accuracy).</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.di.unipi.it/~gulli/AG corpus of news articles.html 2 http://ai.stanford.edu/~amaas/data/sentiment/ 3 http://ana.cachopo.org/datasets-for-single-label-textcategorization</note>

			<note place="foot" n="5"> t-test on HRL with p value &lt; 0.001.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06464</idno>
		<title level="m">Unitary evolution recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimization and applications of echo state networks with leaky-integrator neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Lukoševičius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Siewert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="352" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philémon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01378</idno>
		<title level="m">Batch normalized recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bridging the gaps between residual learning, recurrent neural networks and visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03640</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2368" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08210</idno>
		<title level="m">Architectural complexity measures of recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
