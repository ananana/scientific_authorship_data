<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2246</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba-Zhejiang University Frontier Technology Research Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2246" to="2255"/>
							<date type="published">October 31-November 4, 2018. 2018. 2246</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation clas-sifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distant Supervision was first proposed by <ref type="bibr" target="#b11">Mintz (2009)</ref>, which used seed triples in Freebase instead of manual annotation to supervise text. It marked text as relation r if (h, r, t) can be found in a known KG, where (h, t) is the pair of entities contained in the text. This method can generate large amounts of training data, therefore widely used in recent research. But it can also produce much noise when there are multiple relations between the entities. For instance in <ref type="figure" target="#fig_1">Figure 1</ref>, we may wrongly mark the sentence "Donald Trump is the president of America" as relation born-in,  with the seed triple (Donald Trump, born-in, America).</p><p>Previous works have tried different ways to ad- dress this issue. One way named Multi-Instance Learning(MIL) divided the sentences into differ- ent bags by <ref type="bibr">(h, t)</ref>, and tried to select well-labeled sentences from each bag ( <ref type="bibr" target="#b23">Zeng et al., 2015)</ref> or re- duced the weight of mislabeled data ( <ref type="bibr" target="#b9">Lin et al., 2016)</ref>. Another way tended to capture the reg- ular pattern of the translation from true label to noise label, and learned the true distribution by modeling the noisy data ( <ref type="bibr" target="#b13">Riedel et al., 2010;</ref><ref type="bibr" target="#b10">Luo et al., 2017)</ref>. Some novel methods like  used reinforcement learning to train an instance-selector, which will choose true labeled sentences from the whole sentence set. These methods focus on adding an extra model to reduce the noisy label. However, stacking extra model does not fundamentally solve the problem of inad- equate supervision signals of distant supervision, and will introduce expensive training costs.</p><p>Another solution is to exploit extra supervision signal contained in a KG.  added the confidence of (h, r, t) in the KG as extra super- vision signal. <ref type="bibr" target="#b5">Han (2018)</ref> used mutual attention of KG and text to calculate a weight distribution of train data. Both of them got a better perfor- mance by introducing more information from KG. However, they still used the hard relation label de- rived from distant supervision, which also brought in much noise.</p><p>In this paper, we tend to avoid supervision by hard relation labels, and make full use of prior knowledge from a KG as soft supervision signal. We consider the TransE model proposed by , which encodes entities and relations of a KG into a continuous low-dimensional space with the translation law h + r â‡¡ t, where h, r, t describe the head entity, the relation and the tail entity respectively. Inspired by TransE model, we use t h, instead of a concrete relation label r, as the supervision signal and make the sentence em- bedding close to t h. Concrete relation labels may introduce mislabeled sentences, while t h is label-free, which is only determined by the two aligned entities and the the translation law.</p><p>Our assumption is that each relation r in a KG has one or more sentence patterns that can describe the meaning of r. For the example in <ref type="figure" target="#fig_2">Figure 2</ref>, we first replace the entity mentions in a sentence with the types of the aligned enti- ties in the KG to form a sentence pattern. For example, "in Guadalajara, Mexico" will be re- placed by "in PLACE, PLACE" to form a sen- tence pattern "in A, B" which conveys the mean- ing of "B contains A" and indicates the relation contains. For this sentence pattern, there may be a group of sentences sharing the same pat- tern but with different aligned entity pairs. In the first sentence "The talks, in Ankara, Turkey, continued late into the evening", (T urkey Ankara) implies both "/location/country/capital" and "/location/location/contains" as there are mul- tiple relations between Ankara and Turkey in the KG. But in the similar sentence "She raised the family comfortably in Guadalajara, Mexico.", (Mexico Guadalajara) only implies "/loca- tion/location/contains" as there is no relation of "/location/country/capital" between Mexico and Guadalajara in the KG. As both (T urkey Ankara) and (Mexico Guadalajara) will be used to supervise the learning of the encoder for the pattern "in A, B", it makes the embedding of the sentence pattern closer to the correct relation "/location/location/contains" instead of the wrong relation "/location/country/capital". In this way, we do not need to label the sentences with the hard relation labels anymore.</p><p>The main contributions of this paper can be summarized as follows:</p><p>â€¢ As compared to existing distant supervision for relation extraction, our method makes better use of the prior knowledge derived from KG to address the wrong labeling prob- lem.</p><p>â€¢ The proposed approach tends to supervise the learning process directly and softly by the type information and translation law, both de- rived from KG. Neither hard labels nor ex- tra noise-reduction model for the bag of sen- tences is needed in this way.</p><p>â€¢ In the experiments, we show that the label- free approach performs well in current distant supervision dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Relation extraction is intended to find the relation- ship between two entities given an unstructured text. Traditional methods use artificial character- istics or tree kernels to train a classification model ( <ref type="bibr" target="#b1">Culotta and Sorensen, 2004;</ref><ref type="bibr" target="#b4">Guodong et al., 2002</ref>). Recent works concentrate on deep neural networks to avoid error propagation during gen- erating features <ref type="bibr" target="#b2">(Ebrahimi and Dou, 2010;</ref><ref type="bibr" target="#b22">Zeng et al., 2014;</ref><ref type="bibr" target="#b25">Zhou et al., 2016;</ref>. More complicated models were proposed to learn deeper semantic features, like PCNN ( <ref type="bibr" target="#b23">Zeng et al., 2015)</ref> and attention pooling CNN ( <ref type="bibr" target="#b15">Wang et al., 2016)</ref>, graph LSTMs ( <ref type="bibr" target="#b12">Peng et al., 2017</ref>).</p><p>Most of the early works were trained on the standard dataset by manual annotation, such as SemEval-2010 Task 8. In actual scenarios, it will cost a lot of manual resources to generate labeled data. Distant supervision ( <ref type="bibr" target="#b11">Mintz et al., 2009)</ref> aimed to obtain large-scale training data automat- ically, which becomes the most versatile supervi- sion method. However, it suffers from the noisy label problem. Many works concentrate on deal- ing with the noise of distant supervision. Multi- instance learning ( <ref type="bibr" target="#b13">Riedel et al., 2010;</ref><ref type="bibr" target="#b14">Surdeanu et al., 2012</ref>) addresses the problem in bag-level, which divides sentences into different bags by (h, t). Zeng (2015) selects the most correct sen- tence from each bag. <ref type="bibr" target="#b9">Lin (2016)</ref> introduces atten- tion mechanism by distributing different weight to each sentence in the same bag, which reduces the effect of noisy labels and increases utilization of train data. <ref type="bibr" target="#b10">Luo (2017)</ref> uses a transition matrix to characterize the inherent noise, convert true dis- tribution to noise distribution. The model is en- hanced by curriculum learning.  trains an instance selector to select correct labeled sen- tences by reinforcement learning.</p><p>Most of the above methods introduce a com- plicated extra model to deal with the noisy label problem. Our work tends to avoid the noisy label from distant supervision, by using entity informa- tion and translation law in KG to introduce more supervision signal.</p><p>KG is composed of many triples like (head, re- lation, tail), which describe relationships between head entities and tail entities. TransE is first pro- posed by  to encode triples into a continuous low-dimensional space, which based on the translation h+r â‡¡ t. Many follow-up works like TransH ( <ref type="bibr" target="#b16">Wang et al., 2014</ref>), DistMult ( <ref type="bibr" target="#b21">Yang et al., 2014</ref>), and TransR ( <ref type="bibr" target="#b8">Lin et al., 2015)</ref>, proposed advanced method of translation by intro- ducing different embedding spaces. Some recent works attempt to jointly learn text and KG triples, including (Xie et al., 2016) and ( <ref type="bibr" target="#b18">Xiao et al., 2016</ref>). These models tend to strengthen the representation of entities and relationships for KG tasks, but not for text representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Here we present LFDS (Label-Free Distant Su- pervision) that essentially avoids noisy labels in- troduced by traditional distant supervision. <ref type="figure" target="#fig_2">Fig- ure 2</ref> shows an instance of our method. First, we pre-train representations for entities and relations based on the translation law h + r â‡¡ t defined by typical KG embedding models such as TransE. Second, for each sentence in the train sets, we re- place the entity mentions with the types of the en- tities in the KG. An attention mechanism is then applied to calculate the importance of words with regard to the sentence pattern. Third, we train the sentence encoder by the margin loss between t h and sentence embedding. Note we do not use the noisy relation labels to train the model. Finally, for prediction, we calculate the embedding of test sentences, then compare the sentence embedding with all relation embeddings learned by TransE, and choose the closest relation as our predicted re- sult. We describe these four parts in details as be- low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KG Embedding</head><p>We use typical KG embedding models such as TransE to pre-train the embedding of entities and relations. We intend to supervise the learning by t h instead of hard relation label r. Concretely speaking, given two entities, h and t, we regard the translation based upon TransE between h and t as the target relation representation. TransE in- terprets relationships as translations operating on low-dimensional embeddings of entities, with the formula h + r â‡¡ t, where h, r, t represent head en- tity, relation, and tail entity separately. The model is proved to perform well in predicting the tail en- tity when given head entity and relation.</p><p>The problem is that there may be multi- ple relations between t and h. As the ex- ample in <ref type="figure" target="#fig_2">Figure 2</ref>, the vector calculated by T urkey Ankara contains information for both relations: "/location/country/capital" and "/loca- tion/location/contains". While supervising the learning of the sentence pattern "in PLACE, PLACE", it is difficult to distinguish the two re- lations by supervision signal from only one sen- tence. However, other sentences with the simi- lar pattern but different aligned entity pairs can push the embedding of the pattern close to another vector, such as Mexico Guadalajara, which only represents "/location/location/contains" rela- tion. As a result, the pattern will be closer to its correct relation "/location/location/contains".</p><p>Our work chooses TransE instead of other KG embedding models such as TransH or TransR, be- cause TransE builds representations for h and t in- dependent from fixed relation type r as the model assumes we do not know the specific relation r when training the encoder with supervision from t h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Embedding</head><p>In order to get a better representation of sentences, we had tried a variety of NRE models, such as Bi- LSTM( <ref type="bibr" target="#b25">Zhou et al., 2016)</ref>, SDP-LSTM ( <ref type="bibr" target="#b20">Yan et al., 2015)</ref>, and typical CNN models. We chose PCNN ( <ref type="bibr" target="#b23">Zeng et al., 2015)</ref> to encode the sentence finally, which performs the best in our experiments. The encoder contains three parts as below.</p><p>Word Embeddings and Attentions. Instead of encoding sentences directly, we first replace the entity mentions e in the sentences with cor- responding entity types type e in the KG, such as PERSON, PLACE, ORGANIZATION, etc. We then pre-train the word embedding by word2vec.</p><p>Attention mechanism is further applied to cap- ture the importance of words with regards to the types information of entities as we assume the words close to the types information are more im- portant.</p><p>First, we calculate the similarity between each word w j and two entity types respectively:</p><formula xml:id="formula_0">A j 1 = f (type e 1 , w j )<label>(1)</label></formula><formula xml:id="formula_1">A j 2 = f (type e 2 , w j )<label>(2)</label></formula><p>f (type e , w j ) is the similarity function, which is defined as cosine similarity in this paper. type e 1 and type e 2 are the embeddings of the two entity types. Then the weight distribution for each word can be derived by exponential function:</p><formula xml:id="formula_2">â†µ j 1 = exp(A j 1 ) P n i=1 exp(A i 1 )<label>(3)</label></formula><formula xml:id="formula_3">â†µ j 2 = exp(A j 2 ) P n i=1 exp(A i 2 )<label>(4)</label></formula><p>We use the average weights of two entities as the attention of word w j . Finally, the word embedding W F j is derived as follows: Position embedding. Zeng (2014) first pro- posed PFs to specify entity pairs. PF is a series of relative distances from current word to the two entities. For instance, for the sentence "Damas- cus, the capital of Syria", the distances from "cap- ital" to the two entities are 3 and -2 respectively. The initial embedding matrix is randomly gener- ated. Then we look up vector in the matrix by the two relative distances. The final position embed- ding will be the concatenation of [P F 1 , P F 2 ]. As a result, we get a representation for each word:</p><formula xml:id="formula_4">W F j = â†µ j 1 + â†µ j 2 2 â‡¤ w j<label>(5)</label></formula><formula xml:id="formula_5">w j = [W F j , P F j 1 , P F j 2 ]</formula><p>Then the input sentence representation will be:</p><formula xml:id="formula_6">x = w 1 , w 2 , ..., w n</formula><p>Piecewise-CNN. It was proved by <ref type="bibr" target="#b23">(Zeng et al., 2015</ref>) that piecewise max pooling layer performs well in relation extraction, which tends to capture structural information between two entities. For each sentence, we use CNN to obtain a represen- tation, then divide it into three parts by the two entities index. For each part, we perform a max pooling layer, thus we get 3-dimensional vector:</p><formula xml:id="formula_7">p i = [p i 1 , p i 2 , p i 3 ]</formula><p>The shape of final vector will be (bz, dc â‡¤ 3), where bz represents batch size and dc is the num- ber of channels.</p><p>The structure of whole model is shown in <ref type="figure" target="#fig_3">Fig- ure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Margin loss</head><p>In order to make the sentence embedding encoded by the PCNN model and relation embedding spec- ified by t h based on the translation law as close as possible, we use margin loss with linear layer instead of cross-entropy loss with softmax layer. For the sentence embedding via PCNN layer, we perform a linear transformation to make its dimen- sion equal to the relation representation.</p><formula xml:id="formula_8">s e = W â‡¤ P CNN(x) + b<label>(6)</label></formula><p>Where W is the transformation matrix with shape (dc â‡¤ 3, embedding dim). Then we define margin loss between t h and s e as follows:</p><formula xml:id="formula_9">L = X se2S [(ths e +(rand(t 0 h, th 0 )s e ))] +<label>(7)</label></formula><p>Where rand(a, b) means choosing a or b. t 0 h is a negative instance of t h, which is generated by randomly replacing t with other entities in KG, so does t h 0 . For each sentence, we decrease the distance between t h and s e , while increase the distance between the negative instance and s e . is the reasonable margin between positive triple and negative triple. If the margin is already larger than , the loss of the sentence will be zero.</p><p>Another point to note is the special label NA in the dataset, which means there is no relationship between the two entities in the KG. In this case, t h is pointless and will confuse our encoder. To deal with this issue, we generate a fixed rela- tion for NA, used as the negative relation for those sentences having some relationships. The mini- mum distance from NA to other relations is forced to be greater than 2 â‡¤ , where is the margin in loss function. When the model is used for predic- tion, the NA is also included.</p><p>The training target of our model is shown as <ref type="figure" target="#fig_4">Figure 4</ref>, including the sentence encoder we in- troduced above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction</head><p>We build a sentence encoder which can output a sentence embedding with the same dimension as relation embedding from the KG. For a new test sentence, we first encode it with the model, then calculate the similarity between the sentence em- bedding and the embeddings of all candidate re- lations. The most similar relation to the sentence embedding is the predicted category.</p><formula xml:id="formula_10">r = arg max i (f (S e , r i ))<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments aim to provide positive evidence for the two main questions: (1) Whether or not the sentence pattern can express the essential part of the sentence? (2) Whether the abundant supervi- sion signal in a KG is helpful to predict the true label for those mislabeled sentences? To this end, we first introduce the widely used dataset for distant supervision, and evaluate our performance on the dataset. To further investigate the effectiveness of our model with noisy data, We divide the sentences in dataset into different cat- egories, and show the study about some specific cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The most widely used dataset was generated by <ref type="bibr" target="#b13">Riedel (2010)</ref> ation calculates the precision-recall curves on the whole test set. For the false positives produced by the noisy labels in the test data, the precision will drop rapidly as the recall increases. In order to measure the precision, we need manual evaluation to check misclassified samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Word Embeddings</head><p>In this paper, we use word2vec to train word em- beddings on the NYT corpus. The window size of word2vec model is set as 5, and the embedding size is 50. We preserve those words appearing more than 10 times as vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">KG embeddings</head><p>We train the entities and relationships on FB40k 1 ( <ref type="bibr" target="#b8">Lin et al., 2015)</ref>, which is generated for knowl- edge graph completion, with about 40,000 entities and 1318 relations. We set the embedding size as 100 instead of 50, which performs better in our experiment. Besides, we set the margin as 1 and train with learning rate 0.01. In order to test the performance of the vectors, we evaluate our model in KG completion tasks. The hit@10 of our final TransE model is 0.67, which is evaluated by pre- dicting the closest 10 tail entities with specified head entities and relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Parameter Settings</head><p>We use three-fold validation to determine the hyper-parameters. In the network layer, we try {3, 4, 5} for the kernel size, {100, 150, 200, 250, 300} for the number of channels, {5, 10, 15} for the position embedding size. In the update proce- dure, we use adaptive gradient descent with try- ing {0.1, 0.05, 0.01, 0.001} for the initial learn- ing rate, and {64, 128, 256} for the mini-batch size. In the dropout operation, we set the proba- bility as 0.5 referring to most of the classical ex-  periments. <ref type="table">Table 1</ref> shows our final setting for all hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Traditional Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Held-out Evaluation</head><p>The held-out evaluation is performed directly on the test data. For the labels produced by distant su- pervision may not be precise, held-out evaluation is an approximate measure of our model, which is usually depicted by the precision-recall curve. We select six representative models for com- parison. <ref type="bibr" target="#b11">Mintz (Mintz et al., 2009</ref>) proposed a feature-based model that first used distant su- pervision. <ref type="bibr">MultiR (Hoffmann et al., 2011</ref>) is a multi-instance learning model under the at- least-one assumption. PCNN+MIL ( <ref type="bibr" target="#b23">Zeng et al., 2015)</ref> proposed the piece-wise pooling method, which is used as the encoder of our works. PCNN+ATT ( <ref type="bibr" target="#b9">Lin et al., 2016</ref>) performed selec- tive attention over instances and got better results in the datasets. SEE <ref type="figure" target="#fig_1">(He et al., 2018)</ref> is a novel work that learned syntax-aware entity embedding for relation extraction and achieved state-of-the- art. The precision-recall curves are shown in <ref type="figure" target="#fig_6">Fig- ure 5</ref>, where LFDS denotes our label-free distant supervision method.</p><p>We can observe from the figure that our LFDS method has an overall good performance com- pared to current works, especially with the growth of recall. It demonstrates that our model has a good classification ability in general, because the sentence pattern can capture the meaning of rela- tions better than a sentence. The result can answer the first question we proposed at section 4.  <ref type="table">Table 2</ref>: Precision values for the top 100, 200 and 500 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Manual Evaluation</head><p>For the wrong labels produced by distant super- vision, there will be many false positives in our evaluation inevitably, thus causing a sharp decline in the held-out precision-recall curves. Manual evaluation is necessary to evaluate the model more precisely. Following the previous works, we se- lected the top 100, top 200, and top 500 sentences, which is ranked by the predicted confidence, then evaluated the precision artificially. The result is shown in table 2.</p><p>We can see that the precision is higher than held-out evaluation, because manual evaluation avoid the effect of wrong labels. Our LFDS method achieved a consistently higher precision compared with current works, especially when re- call increases. Compared to held-out evaluation, manual evaluation can show our model's ability in differentiating noisy sentence. Detail analysis will be shown in Section 4.4.</p><p>In the manual procedure, we found some wrong cases caused by entity types. The entity types in Freebase can be ambiguous, where "ORGANIZA- TION" may be confused with "PLACE". It causes error propagation in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>To further prove the effectiveness of our model, especially in distinguishing noisy labels, we se- lect some specific relationships for detail analysis. The noisy labels are produced by the entity pairs which have multiple relationships between them. In this case, different relationships will share the same entity pairs in knowledge graph. We de- fined this kind of relationships as "overlapping" relationships. The more entity pairs it shares with other relation, the overlapping degree of the rela- tion is higher, which means the relation is harder to distinguish.</p><p>Case 1: Non-overlapping Relations. The first case is the non-overlapping relation. For triples of the non-overlapping relation r 1 as (h, r 1 , t), there are few triples like (h, r 2 , t) in KG, where r 2 is another relation in our candidate relations set. That means for this kind of relation, almost no noisy label will be produced. One of these rela- tion is /business/person/company. There are near 200 sentences in the test set, with our evaluation of precision achieving 0.98. It proves that our en- coder with sentence pattern and label-free super- vision is effective in basic classification, which is a convincing answer of the first question we pro- posed at section 4.</p><p>Case 2: Partly-overlapping Relations. The second case is the partly-overlapping relation, in which two relations may share a certain number of entity pairs in Freebase. For instance, the relation /location/country/capital shares many entity pairs with /location/location/contains but not all entity pairs in Freebase have both capital and contain relations.</p><p>For those entity pairs having both relations, tra- ditional distant supervision would produce two la- bels for sentences such as:</p><p>"The talks, in Ankara, Turkey, contin- ued late into the evening."</p><p>The noisy labels in the train set are hard to dif- ferentiate. Recent noise reduction methods com- mit to improving the distinguishing ability of the model by adding extra models. Our experiment proves that our label-free supervision method not only achieves better differentiation performance but also does not need to train extra noise reduc- tion models. Cases are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>The prediction results indicate that the model is capable of learning the embedding of the sen- tence pattern we want. For instance, the model captures the pattern like "in PLACE, PLACE", and tends to predict the sentence with this pat- tern for /location/location/contains, while the pat- tern "PLACE, the capital of PLACE" for /loca- tion/country/capital respectively. When both two relations are labeled for the same sentence in the test set, our model can predict the correct label with the corresponding patterns.</p><p>Another similar but more interesting ex- ample is /people/person/nationality and /peo- ple/person/place lived. In this case, the two rela- tions share a certain number of entity pairs in Free- base like the previous example. But because of the incompleteness of Freebase, many sentences with only one label are actually wrongly labeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>Label with normal distant supervision Prediction with LFDS Pattern  The comparison between labels from normal distant supervision and our label-free relation prediction For example,the sentence "Farah has lived in In- dia, ..." is labeled with only one relation /peo- ple/person/nationality because there is only one nationality relation in Freebase. But the ac- tual meaning of the sentence is to say F arah's place lived is India. This type of wrongly label- ing problem is caused by incompleteness of Free- base which is very common for many other knowl- edge graphs. However, our label-free method can correct this problem because it essentially learns the sentence patterns that are determined only by the sentence itself and the aligned entity pairs. As shown by the last two examples in Ta- ble 3, our model successfully learned the pat- terns "PERSON lived in PLACE" for /peo- ple/person/place lived and "PERSON of PLACE" for /people/person/nationality respectively.</p><p>These instances show that our model is capa- ble of learning some sentence patterns and map- ping them to the corresponding relations in Free- base, which can distinguish noise sentences effec- tively. It indicates that our label-free supervision with prior knowledge introduced by the translation laws and entity types in KG is effective in avoid- ing noise, which can answer the second question we proposed at section 4 credibly.</p><p>Case 3: Mostly-overlapping Relations. The final case is mostly-overlapping relations, in which the two relations share most entity pairs in Freebase.</p><p>One example is /peo- ple/person/place of birth, which shares most of its entity pairs with /people/person/place lived in FB40k, because a person's birthplace and resi- dence are likely to be the same. That means in the process of training with TransE, the two rela- tions are updated by similar gradients, which will produce similar representations for t h. In this case, the relations are really hard to differentiate, because there are not enough distinct supervision signals in the KG. We tend to resolve this situa- tion in future work by utilizing prior knowledge derived from relation paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we argue that the noise label prob- lem in distant supervision is mainly caused by the incomplete use of KG information. Thus we propose a label-free distant supervision method, which supervises the learning of the embedding of sentence patterns by t h and entity types, in- stead of hard relation labels. We conducted ex- periments on the widely used relation extraction dataset and showed that with the recall increasing, our model performs better than state-of-the-art re- sults. This demonstrates that our approach can ef- fectively deal with the noise problem and encod- ing sentence pattern for relation extraction.</p><p>In the future, we plan to utilize more informa- tion in knowledge graphs to improve the distant supervision signal. For instance, the reasoning path can introduce new prior knowledge, which is a key direction in current works of KG. The path may produce new supervision signals for two en- tities even there is no direct connection between them. We also plan to apply this method to other</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>â‡¤</head><label></label><figDesc>Corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The mislabeled sentences produced by Distant Supervision.</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,226.75,78.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An instance of our label-free distant supervision method.</figDesc><graphic url="image-2.png" coords="2,72.00,62.81,453.56,154.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The sentence encoder with word attention and PCNN.</figDesc><graphic url="image-3.png" coords="4,307.28,62.81,226.77,252.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The training target.</figDesc><graphic url="image-4.png" coords="5,307.28,62.81,226.78,184.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label></label><figDesc>https://github.com/thunlp/KB2E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance comparison with Traditional methods.</figDesc><graphic url="image-5.png" coords="6,307.28,62.81,226.75,170.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. It aligns the entities in Freebase with the New York Times (NYT) corpus, which contains all the news during 2005-2007. The sen- tences derived from news in 2005-2006 were used as the training data, while those from year 2007 were used as test data. After the alignment, there are 522,611 training sentences and 172,448 test sentences, labeled by 53 candidate relations in Freebase, and an extra label NA, which means there is no relation between the two entities in Freebase. According to previous work (Mintz et al., 2009), we evaluate our model in the held-out eval- uation and manual evaluation. The held-out evalu-</figDesc><table>Parameter 
Settings 
Kernel size k 
3 
Sentence embedding size 100 
Word embedding size 
50 
Position embedding size 
5 
Number of Channels 
250 
Margin 
2 
Learning rate 
0.001 
Dropout 
0.5 
Batch size 
128 

Table 1: Parameter settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is funded by NSFC 61673338/61473260, and supported by Alibaba-Zhejiang University Joint Institute of Frontier Technologies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chain based rnn for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1244" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2005, Meeting of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Usa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="419" to="444" />
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural knowledge acquisition via mutual attention between knowledge graph and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">See: Syntax-aware entity embedding for neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqiu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning with noise: Enhance distantly supervised relation extraction with dynamic transition matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="430" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference of the Meeting of the ACL and the International Joint Conference on Natural Language Processing of the Afnlp: Volume</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1134" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ssp: Semantic space projection for knowledge graph embedding with text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Xu Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="61" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction based on a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="issue">000</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
