<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deeper Attention to Abusive User Content Moderation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Pavlopoulos</forename><surname>Straintek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Androutsopoulos</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greece</forename><surname>Athens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Androutsopoulos</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malakasiotis</forename><surname>Prodromos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Androutsopoulos</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Straintek</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Androutsopoulos</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greece</forename><surname>Athens</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Androutsopoulos</orgName>
								<orgName type="institution">Athens University of Economics and Business</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deeper Attention to Abusive User Content Moderation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1125" to="1135"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Experimenting with a new dataset of 1.6M user comments from a news portal and an existing dataset of 115K Wikipedia talk page comments, we show that an RNN operating on word embeddings outpeforms the previous state of the art in moderation, which used logistic regression or an MLP classifier with character or word n-grams. We also compare against a CNN operating on word embeddings, and a word-list baseline. A novel, deep, classification-specific attention mechanism improves the performance of the RNN further, and can also highlight suspicious words for free, without including highlighted words in the training data. We consider both fully automatic and semi-automatic moderation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>User comments play a central role in social me- dia and online discussion fora. News portals and blogs often also allow their readers to com- ment to get feedback, engage their readers, and build customer loyalty. <ref type="bibr">1</ref> User comments, how- ever, and more generally user content can also be abusive (e.g., bullying, profanity, hate speech) ( <ref type="bibr" target="#b1">Cheng et al., 2015)</ref>. Social media are under pres- sure to combat abusive content, but so far rely mostly on user reports and tools that detect fre- quent words and phrases of reported posts. <ref type="bibr">2 Wulczyn et al. (2017)</ref> estimated that only 17.9% of personal attacks in Wikipedia discussions were followed by moderator actions. News portals also suffer from abusive user comments, which dam- age their reputations and make them liable to fines, e.g., when hosting comments encouraging illegal actions. They often employ moderators, who are frequently overwhelmed, however, by the volume and abusiveness of comments. <ref type="bibr">3</ref> Readers are dis- appointed when non-abusive comments do not ap- pear quickly online because of moderation delays. Smaller news portals may be unable to employ moderators, and some are forced to shut down their comments sections entirely.</p><p>We examine how deep learning ( <ref type="bibr" target="#b9">Goodfellow et al., 2016;</ref><ref type="bibr" target="#b7">Goldberg, 2016</ref><ref type="bibr" target="#b8">Goldberg, , 2017</ref> can be em- ployed to moderate user comments. We experi- ment with a new dataset of approx. 1.6M manually moderated (accepted or rejected) user comments from a Greek sports news portal (called Gazzetta), which we make publicly available. <ref type="bibr">4</ref> This is one of the largest publicly available datasets of mod- erated user comments. We also provide word em- beddings pre-trained on 5.2M comments from the same portal. Furthermore, we experiment on the 'attacks' dataset of <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref>, approx. 115K English Wikipedia talk page comments la- beled as containing personal attacks or not.</p><p>In a fully automatic scenario, there is no moder- ator and a system accepts or rejects comments. Al- though this scenario may be the only available one, e.g., when news portals cannot afford moderators, it is unrealistic to expect that fully automatic mod- eration will be perfect, because abusive comments may involve irony, sarcasm, harassment without profane phrases etc., which are particularly diffi- cult for a machine to detect. When moderators are available, it is more realistic to develop semi- automatic systems aiming to assist, rather than re- place the moderators, a scenario that has not been considered in previous work. In this case, com- ments for which the system is uncertain <ref type="figure" target="#fig_0">(Fig. 1</ref>) are shown to a moderator to decide; all other com- ments are accepted or rejected by the system. We discuss how moderation systems can be tuned, de- pending on the availability and workload of the moderators. We also introduce additional evalu- ation measures for the semi-automatic scenario.</p><p>On both datasets (Gazzetta and Wikipedia com- ments) and for both scenarios (automatic, semi- automatic), we show that a recurrent neural net- work (RNN) outperforms the system of <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref>, the previous state of the art for com- ment moderation, which employed logistic regres- sion or a multi-layer Perceptron (MLP), and rep- resented each comment as a bag of (character or word) n-grams. We also propose an attention mechanism that improves the overall performance of the RNN. Our attention mechanism differs from most previous ones ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015</ref>) in that it is used in a classifi- cation setting, where there is no previously gen- erated output subsequence to drive the attention, unlike sequence-to-sequence models <ref type="bibr" target="#b29">(Sutskever et al., 2014</ref>). In that sense, our attention is similar to that of of <ref type="bibr" target="#b34">Yang et al. (2016)</ref>, but our attention mechanism is a deeper MLP and it is only applied to words, whereas Yang et al. also have a second attention mechanism that assigns attention scores to entire sentences. In effect, our attention detects the words of a comment that affect most the clas- sification decision (accept, reject), by examining them in the context of the particular comment.</p><p>Although our attention mechanism does not al- ways improve the performance of the RNN, it has the additional advantage of allowing the RNN to highlight suspicious words that a moderator could consider to decide more quickly if a comment should be accepted or rejected. The highlighting  <ref type="table" target="#tab_3">Accepted  Rejected  Total   G-TRAIN-L   960,378 (66%) 489,222 (34%) 1.45M   G-TRAIN-S   67,828 (68%)  32,172 (32%) 100,000   G-DEV   20,236 (68%)  9,464 (32%)  29,700   G-TEST-</ref> comes for free, i.e., the training data do not con- tain highlighted words. We also show that words highlighted by the attention mechanism correlate well with words that moderators would highlight. Our main contributions are: (i) We release a dataset of 1.6M moderated user comments. (ii) We introduce a novel, deep, classification-specific at- tention mechanism and we show that an RNN with our attention mechanism outperforms the previous state of the art in user comment moderation. (iii) Unlike previous work, we also consider a semi- automatic scenario, along with threshold tuning and evaluation measures for it. (iv) We show that the attention mechanism can automatically high- light suspicious words for free, without manually highlighting words in the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Datasets</head><p>We first discuss the datasets we used, to help ac- quaint the reader with the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gazzetta comments</head><p>There are approx. 1.45M training comments <ref type="bibr">(covering Jan. 1, 2015</ref><ref type="bibr">to Oct. 6, 2016</ref>) in the Gazzetta dataset; we call them G-TRAIN-L <ref type="table">(Table 1)</ref>. Some experiments use only the first 100K comments of G-TRAIN-L, called G-TRAIN-S. An additional set of 60,900 comments (Oct. 7 to Nov. 11, 2016) was split to development (G-DEV, 29,700 com- ments), large test (G-TEST-L, 29,700), and small test set (G-TEST-S, 1,500). Gazzetta's moderators (2 full-time, plus journalists occasionally helping) are occasionally instructed to be stricter (e.g., dur- ing violent events). To get a more accurate view of performance in normal situtations, we manu- ally re-moderated (labeled as 'accept' or 'reject') the comments of G-TEST-S, producing G-TEST-S- R. The reject ratio is approx. 30% in all subsets, except for G-TEST-S-R where it drops to 22%, be- cause there are no occasions where the moderators were instructed to be stricter in G-TEST-S-R. Each G-TEST-S-R comment was re-moderated by five annotators. <ref type="bibr" target="#b13">Krippendorff's (2004)</ref> alpha was 0.4762, close to the value (0.45) reported by <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref> for the Wikipedia 'attacks' dataset. Using Cohen's Kappa <ref type="bibr" target="#b3">(Cohen, 1960)</ref>, the mean pairwise agreement was 0.4749. The mean pairwise percentage of agreement (% of comments each pair of annotators agreed on) was 81.33%. Cohen's Kappa and Krippendorff's alpha lead to lower scores, because they account for agreement by chance, which is high when there is class im- balance (22% reject, 78% accept in G-TEST-S-R).</p><p>During the re-moderation of G-TEST-S-R, the annotators were also asked to highlight snippets they considered suspicious, i.e., words or phrases that could lead a moderator to consider reject- ing each comment. <ref type="bibr">5</ref> We also asked the annota- tors to classify each snippet into one of the fol- lowing categories: calumniation (e.g., false accu- sations), discrimination (e.g., racism), disrespect (e.g., looking down at a profession), hooliganism (e.g., calling for violence), insult (e.g., making fun of appearance), irony, swearing, threat, other. <ref type="figure" target="#fig_2">Fig- ure 2</ref> shows how many comments of G-TEST-S-R contained at least one snippet of each category, ac- cording to the majority of annotators; e.g., a com- ment counts as containing irony if at least 3 anno- tators annotated it with an irony snippet (not nec- essarily the same). The gold class of each com- ment (accept or reject) is determined by the ma- jority of the annotators. Irony and disrespect are particularly frequent in both classes, followed by calumniation, swearing, hooliganism, insults. No- tice that comments that contain irony, disrespect etc. are not necessarily rejected. They are, how- ever, more likely in the rejected class, consider- ing that the accepted comments are 2.5 times more than the rejected ones (78% vs. 22%).</p><p>We also provide 300-dimensional word em- beddings, pre-trained on approx. 5.2M comments (268M tokens) from Gazzetta using WORD2VEC <ref type="bibr">(Mikolov et al., 2013a,b)</ref>. <ref type="bibr">6</ref> This larger dataset can- not be used to directly train classifiers, because most of its comments are from a period (before 2015) when Gazzetta did not employ moderators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wikipedia comments</head><p>The Wikipedia 'attacks' dataset ( <ref type="bibr" target="#b32">Wulczyn et al., 2017</ref>) contains approx. 115K English Wikipedia talk page comments, which were labeled as con- taining personal attacks or not. Each comment was labeled by at least 10 annotators. Inter-annotator agreement, measured on a random sample of 1K comments using <ref type="bibr" target="#b13">Krippendorff's (2004)</ref> alpha, was 0.45. The gold label of each comment is deter- mined by the majority of annotators, leading to bi- nary labels (accept, reject). Alternatively, the gold label is the percentage of annotators that labeled the comment as 'accept' (or 'reject'), leading to probabilistic labels. <ref type="bibr">7</ref> The dataset is split in three parts <ref type="table">(Table 1)</ref>: training (W-ATT-TRAIN, 69,526 comments), development (W-ATT-DEV, 23,160), and test (W-ATT-TEST, 23,178). In all three parts, the rejected comments are 12%, but this is an arti- ficial ratio (Wulczyn et al. oversampled comments posted by banned users). By contrast, the ratio of rejected comments in all the Gazzetta subsets is the truly observed one. The Wikipedia comments are also longer (median length 38 tokens) com- pared to Gazzetta's (median length 25 tokens). <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref> also provide two ad- ditional datasets of English Wikipedia talk page comments, which are not used in this paper. The first one, called 'aggression' dataset, contains the same comments as the 'attacks' dataset, now la- beled as 'aggressive' or not. The (probabilistic) labels of the 'attacks' and 'aggression' datasets are very highly correlated (0.8992 Spearman, 0.9718 Pearson) and we did not consider the aggression dataset any further. The second additional dataset, called 'toxicity' dataset, contains approx. 160K comments labeled as being toxic or not. Experi- ments we reported elsewhere ( <ref type="bibr" target="#b24">Pavlopoulos et al., 2017)</ref> show that results on the 'attacks' and 'tox- icity' datasets are very similar; we do not include results on the latter in this paper to save space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We experimented with an RNN operating on word embeddings, the same RNN enhanced with our attention mechanism (a-RNN), a vanilla convo- lutional neural network (CNN) also operating on word embeddings, the DETOX system of <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref>, and a baseline that uses word lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DETOX</head><p>DETOX ( <ref type="bibr" target="#b32">Wulczyn et al., 2017</ref>) was the previous state of the art in comment moderation, in the sense that it had the best reported results on the Wikipedia datasets (Section 2.2), which were in turn the largest previous publicly available dataset of moderated user comments. 8 DETOX represents each comment as a bag of word n-grams (n ≤ 2, each comment becomes a bag containing its 1- grams and 2-grams) or a bag of character n-grams (n ≤ 5, each comment becomes a bag containing character 1-grams, . . . , 5-grams). DETOX can rely on a logistic regression (LR) or MLP classifier, and it can use binary or probabilistic gold labels (Sec- tion 2.2) during training.</p><p>We used the DETOX implementation provided by Wulczyn et al. and the same grid search (and code) to tune the hyper-parameters of DETOX that select word or character n-grams, classifier (LR or MLP), and gold labels (binary or probabilis- tic). For Gazzetta, only binary gold labels were possible, since G-TRAIN-L and G-TRAIN-S have a single gold label per comment. Unlike Wulczyn et al., we tuned the hyper-parameters by evalu- ating (computing AUC and Spearman, Section 4) on a random 2% of held-out comments of W-ATT- TRAIN or G-TRAIN-S, instead of the development subsets, to be able to obtain more realistic results from the development sets while developing the methods. For both Wikipedia and Gazzetta, the tuning selected character n-grams, as in the work of Wulczyn et al. Also, for both Wikipedia and Gazzetta, it preferred LR to MLP, whereas Wul- czyn et al. reported slightly higher performance <ref type="bibr">8</ref> Two of the co-authors of <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref> are with Jigsaw, who recently announced Perspective, a system to detect 'toxic' comments. Perspective is not the same as DETOX (personal communication), but we were unable to obtain scientific articles describing it. An API for Perspec- tive is available at https://www.perspectiveapi. com/, but we did not have access to the API at the time the experiments of this paper were carried out.</p><p>for the MLP on W-ATT-DEV. <ref type="bibr">9</ref> The tuning also se- lected probabilistic labels for Wikipedia, as in the work of Wulczyn et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN-based methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN:</head><p>The RNN method is a chain of GRU cells <ref type="bibr" target="#b2">(Cho et al., 2014</ref>) that transforms the tokens w 1 . . . , w k of each comment to the hidden states h 1 . . . , h k , followed by an LR layer that uses h k to classify the comment (accept, reject). Formally, given the vocabulary V , a matrix E ∈ R d×|V | con- taining d-dimensional word embeddings, an initial h 0 , and a comment c = w 1 , . . . , w k , the RNN computes h 1 , . . . , h k as follows (h t ∈ R m ):</p><formula xml:id="formula_0">˜ h t = tanh(W h x t + U h (r t h t−1 ) + b h ) h t = (1 − z t ) h t−1 + z t ˜ h t z t = σ(W z x t + U z h t−1 + b z ) r t = σ(W r x t + U r h t−1 + b r )</formula><p>where˜hwhere˜ where˜h t ∈ R m is the proposed hidden state at po- sition t, obtained by considering the word embed- ding x t of token w t and the previous hidden state h t−1 ; denotes element-wise multiplication; r t ∈ R m is the reset gate (for r t all zeros, it allows the RNN to forget the previous state h t−1 ); z t ∈ R m is the update gate (for z t all zeros, it allows the RNN to ignore the new proposed˜hproposed˜ proposed˜h t , hence also x t , and copy h t−1 as h t ); σ is the sigmoid func-</p><formula xml:id="formula_1">tion; W h , W z , W r ∈ R m×d ; U h , U z , U r ∈ R m×m ; b h , b z , b r ∈ R m .</formula><p>Once h k has been computed, the LR layer estimates the probability that comment c should be rejected, with W p ∈ R 1×m , b p ∈ R:</p><formula xml:id="formula_2">P RNN (reject|c) = σ(W p h k + b p )</formula><p>a-RNN: When the attention mechanism is added, the LR layer considers the weighted sum h sum of all the hidden states, instead of just h k <ref type="figure" target="#fig_3">(Fig. 3</ref>): 10</p><formula xml:id="formula_3">h sum = k t=1 a t h t<label>(1)</label></formula><formula xml:id="formula_4">P a−RNN (reject|c) = σ(W p h sum + b p )</formula><p>The weights a t are produced by an attention mech-anism, which is an MLP with l layers:</p><formula xml:id="formula_5">a (1) t = RELU(W (1) h t + b (1) )<label>(2)</label></formula><p>. . .</p><formula xml:id="formula_6">a (l−1) t = RELU(W (l−1) a (l−2) t + b (l−1) ) a (l) t = W (l) a (l−1) t + b (l) a t = softmax(a (l) t ; a (l) 1 , . . . , a (l) k ) (3)</formula><p>where a</p><p>t , . . . , a</p><formula xml:id="formula_8">(l−1) t ∈ R r , a (l) t , a t ∈ R, W (1) ∈ R r×m , W (2) , . . . , W (l−1) ∈ R r×r , W (l) ∈ R 1×r , b (1) , . . . , b (l−1) ∈ R r , b (l) ∈ R.</formula><p>The softmax operates across the a (l) t (t = 1, . . . , k), making the weights a t sum to 1. Our attention mecha- nism differs from most previous ones ( <ref type="bibr" target="#b21">Mnih et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b33">Xu et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015</ref>) in that it is used in a classifi- cation setting, where there is no previously gen- erated output subsequence (e.g., partly generated translation) to drive the attention (e.g., assign more weight to source words to translate next), unlike seq2seq models ( <ref type="bibr" target="#b29">Sutskever et al., 2014</ref>). It assigns larger weights a t to hidden states h t correspond- ing to positions where there is more evidence that the comment should be accepted or rejected. <ref type="bibr" target="#b34">Yang et al. (2016)</ref> use a similar attention mech- anism, but ours is deeper. In effect they always set l = 2, whereas we allow l to be larger (tuning selects l = 4). 11 On the other hand, the attention mechanism of Yang et al. is part of a classification method for longer texts (e.g., product reviews). Their method uses two GRU RNNs, both bidirec- tional <ref type="bibr" target="#b26">(Schuster and Paliwal, 1997)</ref>, one turning the word embeddings of each sentence to a sen- tence embedding, and one turning the sentence embeddings to a document embedding, which is then fed to an LR layer. Yang et al. use their at- tention mechanism in both RNNs, to assign atten- tion scores to words and sentences. We consider shorter texts (comments), we have a single RNN, and we assign attention scores to words only. 12 da-CENT: We also experiment with a variant of a-RNN, called da-CENT, which does not use the hidden states of the RNN. The input to the first layer of the attention mechanism is now directly the embedding x t instead of h t (cf. Eq. 2), and <ref type="bibr">11</ref> Yang et al. use tanh instead of RELU in Eq. 2, which works worse in our case, and no bias b (l) in the l-th layer. <ref type="bibr">12</ref> We tried a bidirectional instead of unidirectional GRU chain in our methods, also replacing the LR layer by a deeper classification MLP, but there were no improvements.  h sum is now the weighted sum (centroid) of word embeddings h sum = k t=1 a t x t (cf. Eq. 1). 13 We set l = 4, d = 300, r = m = 128, hav- ing tuned all hyper-parameters on the same 2% held-out comments of W-ATT-TRAIN or G-TRAIN- S that were used to tune DETOX. We use Glorot initialization <ref type="bibr" target="#b6">(Glorot and Bengio, 2010)</ref>, categor- ical cross-entropy loss, and Adam ( <ref type="bibr" target="#b12">Kingma and Ba, 2015)</ref>. <ref type="bibr">14</ref> Early stopping evaluates on the same held-out subsets. For Gazzetta, word embeddings are initialized to the WORD2VEC embeddings we provide (Section 2.1). For Wikipedia, they are ini- tialized to GLOVE embeddings ( <ref type="bibr" target="#b25">Pennington et al., 2014)</ref>. <ref type="bibr">15</ref> In both cases, the embeddings are up- dated during backpropagation. Out of vocabulary (OOV) words, meaning words for which we have no initial embeddings, are mapped to a single ran- domly initialized embedding, also updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CNN</head><p>We also compare against a vanilla CNN operating on word embeddings. We describe the CNN only briefly, because it is very similar to that of of <ref type="bibr" target="#b11">Kim (2014)</ref>; see also <ref type="bibr" target="#b7">Goldberg (2016)</ref> for an introduc- tion to <ref type="bibr">CNNs, and Zhang and Wallace (2015)</ref>.</p><p>For Wikipedia comments, we use a 'narrow' convolution layer, with kernels sliding (stride 1) over (entire) embeddings of word n-grams of sizes n = 1, . . . , 4. We use 300 kernels for each n value, a total of 1,200 kernels. The outputs of each kernel, obtained by applying the kernel to the different n-grams of a comment c, are then   , and then to an LR layer, which provides P CNN (reject|c). For Gazzetta, the CNN is the same, except that n = 1, . . . , 5, lead- ing to 1,500 features per comment. All hyper- parameters were tuned on the 2% held-out com- ments of W-ATT-TRAIN or G-TRAIN-S that were used to tune the other methods. Again, we use 300-dimensional embeddings, which are now ran- domly initialized, since tuning indicated this was better than initializing to pre-trained embeddings. OOV words are treated as in the RNN-based meth- ods. All embeddings are updated during back- propagation. Early stopping evaluates on the held- out subsets. Again, we use Glorot initialization, categorical cross-entropy loss, and Adam. <ref type="bibr">16</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LIST baseline</head><p>A baseline, called LIST, collects every word w that occurs in more than 10 (for W-ATT-TRAIN, G-TRAIN-S) or 100 comments (for G-TRAIN-L) in the training set, along with the precision of w, i.e., the ratio of rejected training comments con- taining w divided by the total number of training comments containing w. The resulting lists con- tain <ref type="bibr">10,423, 16,864, and 21,940</ref> word types, when using W-ATT-TRAIN, G-TRAIN-S, G-TRAIN-L, re- spectively. For a comment c, LIST returns as P LIST (reject|c) the maximum precision of all the words in c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Tuning thresholds</head><p>All methods produce a p = P (reject|c) per com- ment c. In semi-automatic moderation <ref type="figure" target="#fig_0">(Fig. 1)</ref>, a comment is directly rejected if its p is above a re- jection theshold t r , it is directly accepted if p is below an acceptance threshold t a , and it is shown to a moderator if t a ≤ p ≤ t r (gray zone of <ref type="figure" target="#fig_6">Fig. 4)</ref>.</p><p>In our experience, moderators (or their employ- ers) can easily specify the approximate percent- age of comments they can afford to check manu- ally (e.g., 20% daily) or, equivalently, the approx- imate percentage of comments the system should <ref type="bibr">16</ref> We implemented the CNN directly in <ref type="bibr">TensorFlow.</ref> handle automatically. We call coverage the latter percentage; hence, 1 − coverage is the approxi- mate percentage of comments to be checked man- ually. By contrast, moderators are baffled when asked to tune t r and t a directly. Consequently, we ask them to specify the approximate desired coverage. We then sort the comments of the de- velopment set (G-DEV or W-ATT-DEV) by p, and slide t a from 0.0 to 1.0 <ref type="figure" target="#fig_6">(Fig. 4)</ref>. For each t a value, we set t r to the value that leaves a 1 − coverage percentage of development comments in the gray zone (t a ≤ p ≤ t r ). We then select the t a (and t r ) that maximizes the weighted harmonic mean F β (P reject , P accept ) on the development set:</p><formula xml:id="formula_9">F β (P reject , P accept ) = (1 + β 2 ) · P reject · P accept β 2 · P reject + P accept</formula><p>where P reject is the rejection precision (correctly rejected comments divided by rejected comments) and P accept is the acceptance precision (correctly accepted divided by accepted). Intuitively, cover- age sets the width of the gray zone, whereas P reject and P accept show how certain we can be that the red (reject) and green (accept) zones are free of misclassified comments. We set β = 2, emphasiz- ing P accept , because moderators are more worried about wrongly accepting abusive comments than wrongly rejecting non-abusive ones. <ref type="bibr">17</ref> The se- lected t a , t r (tuned on development data) are then used in experiments on test data. In fully auto- matic moderation, coverage = 100 and t a = t r ; otherwise, threshold tuning is identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comment classification evaluation</head><p>Following <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref>, we report in Ta- ble 2 AUC scores (area under ROC curve), along with Spearman correlations between system- generated probabilities P (accept|c) and human probabilistic gold labels (Section 2.2) when prob- abilistic gold labels are available. <ref type="bibr">18</ref>    always better than CNN and DETOX; there is no clear winner between CNN and DETOX. Fur- thermore, a-RNN is always better than RNN on Gazzetta comments, but not on Wikipedia com- ments, where RNN is overall slightly better accord- ing to <ref type="table" target="#tab_3">Table 2</ref>. Also, da-CENT is always worse than a-RNN and RNN, confirming that the hid- den states (intuitively, context-aware word embed- dings) of the RNN chain are important, even with the attention mechanism. Increasing the size of the Gazzetta training set (G-TRAIN-S to G-TRAIN- L) significantly improves the performance of all methods. The implementation of DETOX could not handle the size of G-TRAIN-L, which is why we do not report DETOX results for G-TRAIN-L. No- tice, also, that the Wikipedia dataset is easier than the Gazzetta one (all methods perform better on Wikipedia comments, compared to Gazzetta). <ref type="figure" target="#fig_7">Figure 5</ref> shows F 2 (P reject , P accept ) on G-TEST- L and W-ATT-TEST, when t a , t r are tuned on G- DEV, W-ATT-DEV for varying coverage. For G- TEST-L, we show results training on G-TRAIN-S (solid lines) and G-TRAIN-L (dotted). The differ- ences between RNN and a-RNN are again small, but it is now easier to see that a-RNN is overall better. Again, a-RNN and RNN are better than CNN and DETOX. All three deep learning meth- ods benefit from the larger training set (dotted). In Wikipedia, a-RNN obtains P accept , P reject ≥ 0.94 for all coverages <ref type="figure" target="#fig_7">(Fig. 5, call-outs)</ref>. On the more difficult Gazzetta dataset, a-RNN still ob- tains P accept , P reject ≥ 0.85 when tuned for 50% coverage. When tuned for 100% coverage, com- ments for which the system is uncertain (gray zone) cannot be avoided and there are inevitably more misclassifications; the use of F 2 during threshold tuning places more emphasis on avoid- ing wrongly accepted comments, leading to high P accept (0.82), at the expense of wrongly rejected comments, i.e., sacrificing P reject (0.59). On the re-moderated G-TEST-S-R (similar diagrams, not shown), P accept , P reject become 0.96, 0.88 for cov- erage 50%, and 0.92, 0.48 for coverage 100%.</p><p>We also repeated the annotator ensemble exper- iment of <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref> on 8K randomly chosen comments of W-ATT-TEST (4K comments from random users, 4K comments from banned users). <ref type="bibr">19</ref> The decisions of 10 randomly chosen annotators (possibly different per comment) were used to construct the gold label of each comment. The gold labels were then compared to the deci- sions of the systems and the decisions of an en- semble of k other annotators, k ranging from 1 to 10. <ref type="table">Table 3</ref> shows the mean AUC and Spearman scores, averaged over 25 runs of the experiment, along with standard errrors (in brackets). We con- clude that RNN and a-RNN are as good as an en- semble of 7 human annotators; CNN is as good as 4 annotators; DETOX is as good as 4 in AUC and 3 annotators in Spearman correlation, which is con- sistent with the results of <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref>.  <ref type="table">Table 3</ref>: Comparing to an ensemble of k humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Snippet highlighting evaluation</head><p>To investigate if the attention scores of a-RNN can highlight suspicious words, we focused on G- TEST-S-R, the only dataset with suspicious snip- pets annotated by humans. We removed comments with no human-annotated snippets, leaving 841 comments (515 accepted, 326 rejected), a total of 40,572 tokens, of which 13,146 were inside a sus- picious snippet of at least one annotator. In each remaining comment, each token was assigned a gold suspiciousness score, defined as the percent- age of annotators that included it in their snippets.</p><p>We evaluated three methods that score each to- ken w t of a comment c for suspiciousness. The first one assigns to each w t the attention score a t <ref type="bibr">19</ref> We used the protocol, code, and data of Wulczyn et al. (Eq. 3) of a-RNN (trained on G-TRAIN-L). The second method assigns to each w t its precision, as computed by LIST <ref type="figure" target="#fig_3">(Section 3.4)</ref>. The third method (RAND) assigns to each w t a random (uniform dis- tribution) score between 0 and 1. In the latter two methods, a softmax is applied to the scores of all the tokens per comment, as in a-RNN. <ref type="figure" target="#fig_8">Figure 6</ref> shows three comments (from W-ATT-TEST) high- lighted by a-RNN; heat corresponds to attention. <ref type="bibr">20</ref> We computed Pearson and Spearman correla- tions between the gold suspiciousness scores and the scores of the three methods on the 40,572 to- kens. <ref type="figure" target="#fig_9">Figure 7</ref> shows the correlations on com- ments that were accepted (left) and rejected (right) by the majority of moderators. In both cases, a-RNN performs better than LIST and RAND by both Pearson and Spearman correlations. The high Pearson correlations of a-RNN also show that its attention scores are to a large extent linearly re- lated to the gold ones. By contrast, LIST performs reasonably well in terms of Spearman correlation, but much worse in terms of Pearson, indicating that its precision scores rank reasonably well the tokens from most to least suspicious ones, but are not linearly related to the gold scores. <ref type="bibr" target="#b4">Djuric et al. (2015)</ref> experimented with 952K man- ually moderated comments from Yahoo Finance, but their dataset is not publicly available. They convert each comment to a comment embedding using DOC2VEC <ref type="bibr" target="#b14">(Le and Mikolov, 2014)</ref>, which is then fed to an LR classifier. <ref type="bibr" target="#b23">Nobata et al. (2016)</ref> experimented with approx. 3.3M manually mod- erated comments from Yahoo Finance and News; their data are also not available. <ref type="bibr">21</ref> They used Vowpal Wabbit <ref type="bibr">22</ref> with character n-grams (n = 3, . . . , 5) and word n-grams (n = 1, 2), hand- crafted features (e.g., number of capitalized or black-listed words), features based on dependency <ref type="bibr">20</ref> In innocent comments, a-RNN spreads its attention to all tokens, leading to quasi-uniform low color intensity. <ref type="bibr">21</ref> According to Nobata et al., their clean test dataset (2K comments) would be made available, but it is currently not. <ref type="bibr">22</ref> See http://hunch.net/ ˜ vw/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>trees, averages of WORD2VEC embeddings, and DOC2VEC-like embeddings. Character n-grams were the best, on their own outperforming <ref type="bibr" target="#b4">Djuric et al. (2015)</ref>. The best results, however, were ob- tained using all features. We use no hand-crafted features and parsers, making our methods more easily portable to other domains and languages.  train a (token or character- based) RNN language model per class (accept, re- ject), and use the probability ratio of the two mod- els to accept or reject user comments. Experi- ments on the dataset of <ref type="bibr" target="#b4">Djuric et al. (2015)</ref>, how- ever, showed that their method (RNNLMs) per- formed worse than a combination of SVM and Naive Bayes classifiers (NBSVM) that used char- acter and token n-grams. An LR classifier operat- ing on DOC2VEC-like comment embeddings ( <ref type="bibr" target="#b14">Le and Mikolov, 2014</ref>) also performed worse than NBSVM. To surpass NBSVM, <ref type="bibr">Mehdad et al. used</ref> an SVM to combine features from their three other methods (RNNLMs, LR with DOC2VEC, NBSVM). <ref type="bibr" target="#b32">Wulczyn et al. (2017)</ref> experimented with char- acter and word n-grams. We included their dataset and moderation system (DETOX) in our experi- ments. <ref type="bibr" target="#b31">Waseem et al. (2016)</ref> used approx. 17K tweets annotated for hate speech. Their best re- sults were obtained using an LR classifier with character n-grams (n = 1, . . . , 4), plus gender. <ref type="bibr" target="#b30">Warner and Hirschberg (2012)</ref> aimed to detect anti-semitic speech, experimenting with 9K para- graphs and a linear SVM. Their features consider windows of at most 5 tokens, examining the to- kens of each window, their order, POS tags, Brown clusters etc., following <ref type="bibr" target="#b35">Yarowsky (1994)</ref>. <ref type="bibr" target="#b1">Cheng et al. (2015)</ref> aimed to predict which users would be banned from on-line communities. Their best system used a random forest or LR classifier, with features examining readability, activity (e.g., number of posts daily), community and moderator reactions (e.g., up-votes, number of deleted posts). <ref type="bibr" target="#b27">Sood et al. (2012a;</ref><ref type="bibr" target="#b28">2012b</ref>) experimented with 6.5K comments from Yahoo Buzz, moderated via crowdsourcing. They showed that a linear SVM, representing each comment as a bag of word bi- grams and stems, performs better than word lists. Their best results were obtained by combining the SVM with a word list and edit distance. <ref type="bibr" target="#b36">Yin et al. (2009)</ref> used posts from chat rooms and discussion fora (&lt;15K posts in total) to train an SVM to detect online harassment. They used TF-IDF, sentiment, and context features (e.g., sim- ilarity to other posts in a thread). Our methods might also benefit by considering threads, rather than individual comments. Yin at al. point out that unlike other abusive content, spam in comments or dicsussion fora ( <ref type="bibr" target="#b20">Mishne et al., 2005;</ref><ref type="bibr" target="#b22">Niu et al., 2007</ref>) is off-topic and serves a commercial pur- pose. Spam is unlikely in Wikipedia discussions and not an issue in the Gazzetta dataset <ref type="figure" target="#fig_2">(Fig. 2)</ref>.</p><p>For a more extensive discussion of related work, consult <ref type="bibr" target="#b24">Pavlopoulos et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We experimented with a new publicly available dataset of 1.6M moderated user comments from a Greek sports news portal and an existing dataset of 115K English Wikipedia talk page comments. We showed that a GRU RNN operating on word embeddings outpeforms the previous state of the art, which used an LR or MLP classifier with char- acter or word n-gram features, also outperform- ing a vanilla CNN operating on word embeddings, and a baseline that uses an automatically con- structed word list with precision scores. A novel, deep, classification-specific attention mechanism improves further the overall results of the RNN, and can also highlight suspicious words for free, without including highlighted words in the train- ing data. We considered both fully automatic and semi-automatic moderation, along with threshold tuning and evaluation measures for both.</p><p>We plan to consider user-specific information (e.g., ratio of comments rejected in the past) <ref type="bibr" target="#b1">(Cheng et al., 2015;</ref><ref type="bibr" target="#b31">Waseem and Hovy, 2016)</ref> and explore character-level RNNs or <ref type="bibr">CNNs (Zhang et al., 2015</ref>), e.g., as a first layer to produce em- beddings of unknown words from characters (dos <ref type="bibr" target="#b5">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b15">Ling et al., 2015)</ref>, which would then be passed on to our current methods that operate on word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Semi-automatic moderation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Re-moderated comments with at least one snippet of the corresponding category.</figDesc><graphic url="image-4.png" coords="3,88.37,62.81,185.50,127.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of a-RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of threshold tuning. max-pooled, leading to a single output per kernel. The resulting feature vector (1,200 maxpooled outputs) goes through a dropout layer (Hinton et al., 2012) (p = 0.5), and then to an LR layer, which provides P CNN (reject|c). For Gazzetta, the CNN is the same, except that n = 1,. .. , 5, leading to 1,500 features per comment. All hyperparameters were tuned on the 2% held-out comments of W-ATT-TRAIN or G-TRAIN-S that were used to tune the other methods. Again, we use 300-dimensional embeddings, which are now randomly initialized, since tuning indicated this was better than initializing to pre-trained embeddings. OOV words are treated as in the RNN-based methods. All embeddings are updated during backpropagation. Early stopping evaluates on the heldout subsets. Again, we use Glorot initialization, categorical cross-entropy loss, and Adam. 16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: F 2 scores for varying coverage. Dotted lines were obtained using a larger training set.</figDesc><graphic url="image-5.png" coords="7,94.68,256.87,408.19,163.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Word highlighting by a-RNN.</figDesc><graphic url="image-7.png" coords="8,318.19,62.81,98.22,63.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Suspicious snippet highlighting results.</figDesc><graphic url="image-8.png" coords="8,416.41,62.81,98.22,63.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comment classification results. Scores reported by Wulczyn et al. (2017) are shown in brackets. 

</table></figure>

			<note place="foot" n="1"> See, for example, http://niemanreports.org/ articles/the-future-of-comments/. 2 Consult, for example, https://www.facebook. com/help/131671940241729 and https://www. theguardian.com/technology/2017/feb/07/ twitter-abuse-harassment-crackdown.</note>

			<note place="foot" n="3"> See, e.g., https://www.wired.com/2017/04/ zerochaos-google-ads-quality-raters and https://goo.gl/89M2bI. 4 The portal is http://www.gazzetta.gr/. Instructions to download the dataset will become available at http://nlp.cs.aueb.gr/software.html.</note>

			<note place="foot" n="5"> Treating snippet overlaps as agreements, the mean pairwise Dice coefficient for snippet highlighting was 50.03%.</note>

			<note place="foot" n="6"> We used CBOW, window size 5, min. term freq. 5, negative sampling, obtaining a vocabulary size of approx. 478K. 7 We also construct probabilistic labels for G-TEST-S-R, where there are five annotators.</note>

			<note place="foot" n="9"> We repeated the tuning by evaluating on W-ATT-DEV, and again character n-grams with LR were selected. 10 We tried replacing the LR layer by a deeper classification MLP, and the RNN chain by a bidirectional RNN (Schuster and Paliwal, 1997), but there were no improvements.</note>

			<note place="foot" n="13"> For experiments with additional variants of a-RNN, consult Pavlopoulos et al. (2017). 14 We implemented the methods of this sub-section using Keras (keras.io) and TensorFlow (tensorflow.org). 15 See https://nlp.stanford.edu/projects/ glove/. We use &apos;Common Crawl&apos; (840B tokens).</note>

			<note place="foot" n="17"> More precisely, when computing F β , we reorder the development comments by time posted, and split them into batches of 100. For each ta (and tr) value, we compute F β per batch and macro-average across batches. The resulting thresholds lead to F β scores that are more stable over time. 18 When computing AUC, the gold label is the majority label of the annotators. When computing Spearman, the gold label is probabilistic (% of annotators that accepted the comment). The decisions of the systems are always probabilistic.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by Google's Digital News Initiative (project ML2P, contract 362826). <ref type="bibr">23</ref> We are grateful to Gazzetta for the data they pro-vided. We also thank Gazzetta's moderators for their feedback, insights, and advice. <ref type="bibr">23</ref> See https://digitalnewsinitiative.com/.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Antisocial behavior in online discussion communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International AAAI Conference on Web and Social Media</title>
		<meeting>the 9th International AAAI Conference on Web and Social Media<address><addrLine>England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
		<respStmt>
			<orgName>Oxford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hate speech detection with comment embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="29" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics. Sardinia</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics. Sardinia<address><addrLine>Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Neural Network Methods in Natural Language Processing</title>
		<imprint>
			<publisher>Morgan and Claypool Publishers</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>CoRR abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<title level="m">Content Analysis: An Introduction to Its Methodology</title>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do characters abuse more than words?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="299" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at International Conference on Learning Representations</title>
		<meeting>Workshop at International Conference on Learning Representations<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blocking blog spam with language model disagreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronny</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web</title>
		<meeting>the 1st International Workshop on Adversarial Information Retrieval on the Web<address><addrLine>Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A quantitative study of forum spamming using context-based analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Network and Distributed System Security Symposium</title>
		<meeting>the 14th Annual Network and Distributed System Security Symposium<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="79" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abusive language detection in online user content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikashi</forename><surname>Nobata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achint</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for user comment moderation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACL Workshop on Abusive Language Online</title>
		<meeting>the 1st ACL Workshop on Abusive Language Online<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Prodromos Malakasiotis, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transacions of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Profanity use in online communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judd</forename><surname>Antin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">F</forename><surname>Churchill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1481" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using crowdsourcing to improve profanity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judd</forename><surname>Antin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">F</forename><surname>Churchill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Wisdom of the Crowd</title>
		<meeting><address><addrLine>Stanford, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting hate speech on the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Language in Social Media</title>
		<meeting>the 2nd Workshop on Language in Social Media<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Student Research Workshop</title>
		<meeting>the NAACL Student Research Workshop<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ex machina: Personal attacks seen at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web<address><addrLine>Perth, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1391" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Las Cruces, NM, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detection of harassment on Web 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Dawei Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">April</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynne</forename><surname>Kontostathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the WWW workshop on Content Analysis in the Web 2.0</title>
		<meeting>the WWW workshop on Content Analysis in the Web 2.0<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno>CoRR abs/1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
