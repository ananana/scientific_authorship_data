<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Similarity-Based Reconstruction Loss for Meaning Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Similarity-Based Reconstruction Loss for Meaning Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4875" to="4880"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4875</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper addresses the problem of representation learning. Using an autoencoder framework , we propose and evaluate several loss functions that can be used as an alternative to the commonly used cross-entropy reconstruction loss. The proposed loss functions use similarities between words in the embedding space, and can be used to train any neural model for text generation. We show that the introduced loss functions amplify semantic diversity of reconstructed sentences, while preserving the original meaning of the input. We test the derived autoencoder-generated representations on paraphrase detection and language inference tasks and demonstrate performance improvement compared to the traditional cross-entropy loss.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language processing (NLP) tasks that use an encoder-decoder architecture tend to rely on the cross-entropy reconstruction loss to generate the target output. A great majority of deep learn- ing models used at present for state-of-the-art ma- chine translation, question answering, summariza- tion, and dialogue generation employ this type of architecture.</p><p>The standard cross-entropy loss penalizes the model whenever it fails to produce the exact word from the ground truth data used for training. How- ever, in many NLP tasks that deal with generating text from semantic representation, recovering the exact word is not necessarily optimal, and often generating a near-synonym or just a semantically close word is nearly as good or even better from the point of view of model performance. Consider a situation when a decoder model generates a word by sampling from a softmax over the vocabulary- sized final layer to produce an output. Since cross- entropy loss forces a model to generate the exact words corresponding to those in the input text, the model will be penalized when semantically close but distinct outputs are generated. This is clearly undesirable in many cases when the exact output is not required.</p><p>In this paper, we introduce and experiment with a series of distance-based reconstruction losses. Using an auto-encoder derived representation of sentence meaning, we test their impact on model performance in several tasks that require building a semantic representation, including paraphrase detection and entailment / inference. We show that the loss functions that take into account distribu- tional similarity between the word embeddings of the generated output and the ground truth tokens lead to a substantial improvement in performance on such tasks in an unsupervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The encoder-decoder setting was first used in deep learning by <ref type="bibr" target="#b16">Sutskever et al. (2014)</ref> and has been successfully adapted to a problem of representa- tion learning since then. To date, numerous ap- proaches based on the encoder-decoder idea have been suggested for unsupervised feature extraction from textual data. <ref type="bibr" target="#b3">Cer et al. (2018)</ref> modify the Transformer ar- chitecture ( <ref type="bibr" target="#b17">Vaswani et al., 2017</ref>) originally sug- gested for machine translation to produce sentence embeddings that target transfer learning to other NLP tasks. <ref type="bibr" target="#b0">Arora et al. (2016)</ref> claim that sen- tence representation as simple weighted averaging of word vectors beats more sophisticated recur- rent network-based models. <ref type="bibr" target="#b10">McCann et al. (2017)</ref> show that adding machine translation-learned vec- tors to models designed for other NLP tasks im- proves their performance. <ref type="bibr" target="#b11">Nangia et al. (2017)</ref> in <ref type="bibr">RepEval-2017</ref> report that in-sentence atten- tion and biLSTM-based models extract represen-tation of meaning from text reasonably well. <ref type="bibr" target="#b8">Logeswaran and Lee (2018)</ref> and <ref type="bibr" target="#b7">Kiros et al. (2015)</ref> change the problem of learning sentence repre- sentations to a classification task for predicting context sentences. <ref type="bibr" target="#b15">Subramanian et al. (2018)</ref> demonstrate that sharing the same sentence en- coder across different tasks leads to performance improvements.</p><p>All the listed works, however, propose methods that either develop task-specific architectures, or use large corpora of labeled data to learn embed- dings at a sentence level. Unlike the mentioned papers, the simple modification we propose does not require data annotation and can be used with any state-of-the-art neural models for text genera- tion.</p><p>Surprisingly, we have not found other work that uses the proposed idea despite its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The objective of a classic autoencoder is to mini- mize the difference between the given input X and the reconstructed outputˆXoutputˆ outputˆX.</p><formula xml:id="formula_0">L(X, g(f (X)) (1)</formula><p>where f is the encoder function and g is the de- coder function. We propose and compare several modifications of distance-based losses, that apply different penalties to the model depending on the similarity of the produced words to the targets in the embedding space.</p><p>• Weighted similarity loss</p><formula xml:id="formula_1">L = − V i=1 sim(y t , y i )p i (2)</formula><p>where p i is the softmax probability over vo- cabulary size, −1 ≤ sim ≤ 1 is the similarity between the tokens embeddings vectors, y t and y i are the ground-truth token and the pre- dicted token, respectively, and V is the total vocabulary size. Intuitively, this loss encour- ages the model to produce high probabilities for words that are close to the target word. In the present experiments, we use cosine as the similarity measure.</p><p>• Weighted cross-entropy loss</p><formula xml:id="formula_2">L = − V i=1 sim(y t , y i ) log p i (3)</formula><p>Here the optimization function can be seen as the "weighted" cross-entropy, meaning that every ground-truth token is represented with similarities to other words in the vocabu- lary rather than with a traditional one-hot- encoding scheme. The schematic illustration of the true label encoding for the weighted similarity and weighted cross-entropy loss functions is shown in <ref type="figure">Figure 1</ref> (right).</p><p>• Soft label loss</p><formula xml:id="formula_3">L = − V i=1 y * i logp i (4)</formula><p>This cost function is similar to the previous one in terms of true label y i representation: we encode ground-truth tokens as their simi- larities across the vocabulary, but we consider only the top N closest words in the vocab- ulary and normalize the similarities so that they add up to one V i=1 y * i = 1. Essentially, the loss function can be interpreted as cross- entropy with soft targets. We vary N from 3 to 10 in our experiments. We also exclude common English stop-words from soft target encoding, i.e. we apply a regular cross en- tropy loss for reconstructing of these words. The schematic illustration is given in <ref type="figure">Figure  1</ref> (center).</p><formula xml:id="formula_4">y * i =    sim(yt,y i ) N j=1 sim(yt,y j ) , y i ∈ top N 0, y i ∈ top N<label>(5)</label></formula><p>We use pre-trained fastText ( <ref type="bibr" target="#b1">Bojanowski et al., 2016</ref>) word vectors to compute similarities be- tween words. Note that the more recently pro- posed ELMo embeddings ( <ref type="bibr" target="#b13">Peters et al., 2018)</ref>, for example, can not be used in our case, since they are context-dependent, which means that similar- ities between individual words can not be pre- computed.</p><p>To find out how the proposed loss functions af- fect the quality of the derived representations, we trained several autoencoder models using the reg- ular cross-entropy, as well as the three variants of the similarity-based reconstruction loss described above. In these experiments, we use the Yelp restaurant reviews dataset <ref type="bibr" target="#b14">(Shen et al., 2017)</ref>. This dataset was originally introduced for a sentiment classification task and consists of 600K sentences.   <ref type="figure">Figure 1</ref>: Schematic illustration of true-label encoding using the standard cross-entropy loss (left), soft label loss for N = 3 (center) and weighted similarity/weighted cross-entropy loss (right). All the three examples "good", "great" and "bad" are close in the embedding space, since they appear in similar contexts. Note that all the soft labels add up to 1, while weighted similarity labels for the third loss can vary in the range from -1 to 1. Our autoencoder model is implemented using the PyTorch deep learning framework ( <ref type="bibr" target="#b12">Paszke et al., 2017</ref>). In our architecture, both the encoder and the decoder are implemented as single layer LSTMs, each with the hidden size of 256 units. We divide our dataset into train/dev/test splits in 70/10/20 ratio. The resulting vocabulary size of the training dataset is 9.5K tokens. For our train- ing, we use the Adam optimizer ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) with the learning rate that varies depending on the tested loss between 0.001 and 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We test our learned representations using the SentEval toolkit ( <ref type="bibr" target="#b4">Conneau et al., 2017)</ref>. SentE- val is an open-source Python library for evaluating sentence embeddings on a diverse set of language tasks. This toolkit provides a cluster of down- stream tasks taken from various competitions such as SemEval as well as a set of probing tasks. In current paper, we focus on the paraphrase detec- tion task using the Microsoft Research Paraphrase Corpus (MSRP) ( <ref type="bibr" target="#b5">Dolan et al., 2004</ref>), as well as the inference/entailment tasks using the Stanford Nat- ural Language Inference corpus (SNLI) <ref type="bibr" target="#b2">(Bowman et al., 2015</ref>) and the SICK-Entailment dataset from <ref type="bibr">SemEval-2014</ref><ref type="bibr" target="#b9">(Marelli et al., 2014</ref>). We selected these tasks because they seem to be likely to ben- efit from capturing word-level semantic similarity. <ref type="table">Table 1</ref> shows the scores averaged over three (3) runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We find that almost all of the proposed loss func- tions outperform the vanilla autoencoder trained with cross-entropy on all three tasks (see <ref type="table">Table 1</ref>). The only exception is the weighted similarity loss function. Compared to the logarithm-based losses, this loss applies softer penalties when the ground- truth tokens are predicted to have lower probabili- ties. We conclude that the non-linearity introduced by a logarithm function contributes to more effi- cient training.</p><p>Among the models we tested, the best scores were achieved by the weighted cross-entropy loss for MSRP (68.2%), the weighted similarity loss for SNLI (69.1%) and by the soft label loss for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Configuration</head><p>Autoencoder outputs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input sentence Reconstructed sentence</head><p>Cross-entropy you can trust this business you can trust this business Soft label, N = 3 the taste was so good the flavor was so good Soft label, N = 5 her tone was incredibly rude her attitude was incredibly unprofessional</p><p>Soft label, N = 10 a very nice spot for a quiet lunch a very nice slot for a tranquil lunchtime Weighted similarity once again the staff were wonderful that so that service and great Weighted cross-entropy great breakfast option great food place SICK-E (72.4%). We observe that for the para- phrase task, all the soft label losses behaved simi- larly, while for the inference/entailment, increas- ing the number of neighbors improved perfor- mance.</p><p>In order to better understand how different mod- ifications of the soft label loss affected model per- formance on transfer tasks, we conducted some additional experiments. Specifically, we investi- gated the effects of (a) varying the number N of word neighbors used to compute the loss function, and (b) removing the normalization factor by get- ting rid of the denominator in Eq. 5 (i.e. soft la- bel similarities no longer sum up to 1). Note that when N = 1, the soft label loss becomes identical to cross-entropy. When the normalization factor is removed, having N = V makes the soft label loss identical to the weighted similarity loss.</p><p>We found that the normalization factor slightly reduced the accuracy for all of the three tasks (see <ref type="figure" target="#fig_1">Figure 2</ref>). Interestingly, we have not established a universal tendency for the optimal choice of N : for the language inference tasks, the best accuracy was achieved at N close to 10, while for the para- phrasing task the suitable choice for N was in the range of 3-5.</p><p>The performance figures obtained for each loss are well illustrated by the quality of the recon- structed examples in <ref type="table" target="#tab_2">Table 2</ref>. The standard cross- entropy, as expected, aims at the accurate word- by-word reconstruction of the input sentence. The autoencoder with our least successful weigthed similarity loss function manages to learn most fre- quent corpus-specific words (e.g. "great service"), but the overall meaning is not conveyed well. he rest of the models succeed in reconstructing syn- onyms at the word-level. This results in a slightly different expression style (e.g. "her tone was in- credibly rude" becomes "her attitude was incred- ibly unprofessional"), but the overall meaning is reconstructed correctly.</p><p>Obviously, the quality of the generated repre- sentations depends to a large extent on the selec- tion of pre-trained word embeddings. The related drawback that we observe in our choice of the fast- Text vectors is that the target ground-truth tokens can be replaced with word inflections as well as with antonyms, which in certain cases can change the meaning of the sentence to the opposite.</p><p>For a subset of configurations, we conducted exploratory testing on additional tasks, including different subsets of STS and SICK-Relatedness data. For nearly all tasks tested, we recorded better performance compared to cross-entropy, with the minimum relative gain being 1%. The only per- formance reduction was in plagiarism detection, which may be expected to favor exact replication.</p><p>Although the scores we obtained are below the state-of-the-art for the considered tasks, our goal was to demonstrate that in a traditional encoder- decoder setting, which is extensively used for a number of NLP problems, the proposed loss func- tions beat the conventional cross-entropy. The ma- jor advantage of our proposal is that it is very sim- ple and highly generalizable, i.e. without a so- phisticated model architecture, our model is able to produce diversified outputs and can be easily integrated in any existing encoder-decoder archi- tectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced the loss functions that leverage word-level distributional similarity between the generated output and the ground truth. Compared to the representations learned by a vanilla autoencoder, the proposed reconstruction loss variants show substantially improved perfor- mance on several semantic representation tasks. Further, relative to the conventional cross-entropy, the tested loss variants produce more diverse out- put while preserving the underlying semantics.</p><p>We focused on the autoencoder architecture which requires no pre-annotated data to generate the representations. The major benefit of our pro- posal is that the proposed loss functions can be plugged directly into any NLP model that gener- ates text word-by-word and that may benefit from more diverse output. The potential applications of our proposal therefore include any of the common NLP problems where language diversity is desir- able, including conversational agents, paraphrase generation and text summarization.</p><p>The next step for this work is to evaluate the per- formance of the proposed loss functions in state- of-the-art models for the NLP tasks that lever- age sentence-level semantic representation, such as the ones we explored in the present study. Fur- ther experiments with the proposed loss functions are also needed to evaluate the effects of different word embedding models on the quality of derived representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of the soft label loss modifications on task performance. N is the number of closest words neighbors used to compute the loss function.</figDesc><graphic url="image-1.png" coords="5,76.54,62.81,444.47,185.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Sample autoencoder reconstruction outputs for the tested loss configurations.</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Universal sentence encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 350. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08172</idno>
		<title level="m">The repeval 2017 shared task: Multi-genre natural language inference with sentence representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<title level="m">Deep contextualized word representations. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6833" to="6844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00079</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
