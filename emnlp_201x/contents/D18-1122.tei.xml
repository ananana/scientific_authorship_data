<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event Detection with Neural Networks: A Rigorous Empirical Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Walker</forename><surname>Orr</surname></persName>
							<email>jorr@georgefox.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
							<email>tadepall@eecs.oregonstate.edu,</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
							<email>xfern@eecs.oregonstate.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">George Fox University</orgName>
								<address>
									<addrLine>414 N Meridian St. Newberg</addrLine>
									<postCode>97132</postCode>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<postCode>97331</postCode>
									<settlement>Corvallis</settlement>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Event Detection with Neural Networks: A Rigorous Empirical Evaluation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="999" to="1004"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>999</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Detecting events and classifying them into pre-defined types is an important step in knowledge extraction from natural language texts. While the neural network models have generally led the state-of-the-art, the differences in performance between different architec-tures have not been rigorously studied. In this paper we present a novel GRU-based model that combines syntactic information along with temporal structure through an attention mechanism. We show that it is competitive with other neural network architectures through empirical evaluations under different random initializations and training-validation-test splits of ACE2005 dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Events are the lingua franca of news stories and narratives and describe important changes of state in the world. Identifying events and classifying them into different types is a challenging aspect of understanding text. This paper focuses on the task of event detection, which includes identifying the "trigger" words that indicate events and classify- ing the events into refined types. Event detection is the necessary first step in inferring more semantic information about the events including extracting the arguments of events and recognizing temporal and causal relationships between different events.</p><p>Neural network models have been the most suc- cessful methods for event detection. However, most current models ignore the syntactic relation- ships in the text. One of the main contributions of our work is a new DAG-GRU architecture <ref type="bibr" target="#b2">(Chung et al., 2014</ref>) that captures the context and syntac- tic information through a bidirectional reading of the text with dependency parse relationships. This generalizes the GRU model to operate on a graph by novel use of an attention mechanism. <ref type="bibr">1</ref> Also associated with Oregon State University.</p><p>Following the long history of prior work on event detection, ACE2005 is used for the precise definition of the task and the data for the purposes of evaluation. One of the challenges of the task is the size and sparsity of this dataset. It con- sists of 599 documents, which are broken into a training, development, testing split of 529, 30, and 40 respectively. This split has become a de-facto evaluation standard since ( <ref type="bibr" target="#b8">Li et al., 2013)</ref>. Fur- thermore, the test set is small and consists only of newswire documents, when there are multiple domains within ACE2005. These two factors lead to a significant difference between the training and testing event type distribution. Though some work had been done comparing method across domains <ref type="bibr" target="#b14">(Nguyen and Grishman, 2015)</ref>, variations in the training/test split including all the domains has not been studied. We evaluate the sensitivity of model accuracy to changes in training and test set splits through a randomized study.</p><p>Given the limited amount of training data in comparison to other datasets used by neural net- work models, and the narrow margin between many high performance methods, the effect of the initialization of these methods needs to be consid- ered. In this paper, we conduct an empirical study of the sensitivity of the system performance to the model initialization.</p><p>Results show that our DAG-GRU method is competitive with other state-of-the-art methods. However, the performance of all methods is more sensitive to the random model initialization than expected. Importantly, the ranking of different methods based on the performance on the standard training-validation-test split is sometimes different from the ranking based on the average over mul- tiple splits, suggesting that the community should move away from single split evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Event detection and extraction are well-studied tasks with a long history of research. <ref type="bibr" target="#b14">Nguyen and Grishman (2015)</ref> used CNNs to represent windows around candidate triggers. Each word is represented by a concatenation of its word and entity type embeddings with the distance to candidate trigger. Global max-pooling summa- rizes the CNN filter and the result is passed to a linear classifier.</p><p>Nguyen and Grishman (2016) followed up with a skip-gram based CNN model which allows the filter to skip non-salient or otherwise unnecessary words in the middle of word sequences. <ref type="bibr" target="#b4">Feng et al. (2016)</ref> combined a CNN, simi- lar to <ref type="bibr" target="#b14">(Nguyen and Grishman, 2015)</ref>, with a bi- directional LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997</ref>) to create a hybrid network. The output of both networks was concatenated together and fed to a linear model for final predictions.  uses a bidirectional gated recurrent units (GRUs) for sentence level encod- ing, and in conjunction with a memory network, to jointly predict events and their arguments.  created a probablistic soft logic model incorporating the semantic frames from Framenet ( <ref type="bibr" target="#b0">Baker et al., 1998</ref>) in the form of ex- tra training examples. Based on the intuition that entity and argument information is important for event detection, <ref type="bibr" target="#b11">Liu et al. (2017)</ref> built an attention model over annotated arguments and the context surrounding event trigger candidates. <ref type="bibr" target="#b9">Liu et al. (2018)</ref> created a cross language atten- tion model for event detection and used it for event detection in both the English and Chinese portions of the ACE2005 data.</p><p>Recently, Nguyen and Grishman (2018) used graph-CNN (GCCN) where the convolutional fil- ters are applied to syntactically dependent words in addition to consecutive words. The addition of the entity information into the network structure produced the state-of-the-art CNN model. Another neural network model that includes syntactic dependency relationships is DAG-based LSTM ( <ref type="bibr" target="#b18">Qian et al., 2018)</ref>. It combines the syntac- tic hidden vectors by weighted average and adds them through a dependency gate to the output gate of the LSTM model. To the best of our knowledge, none of the neural models combine syntactic in- formation with attention, which motivates our re- search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DAG GRU Model</head><p>Event detection is often framed as a multi-class classification problem <ref type="bibr" target="#b1">(Chen et al., 2015;</ref><ref type="bibr" target="#b5">Ghaeini et al., 2016)</ref>. The task is to predict the event label for each word in the test documents, and NIL if the word is not an event. A sentence is a sequence of words x 1 . . . x n , where each word is represented by a k-length vector. The standard GRU model works as follows:</p><formula xml:id="formula_0">r t = σ(W r x t + U r h t−1 + b r ) z t = σ(W z x t + U z h t−1 + b z ) ˜ h t = tanh(W h x t + r t U h h t−1 + b h ) h t = (1 − z t ) h t−1 + z t ˜ h t</formula><p>The GRU model produces a hidden vector h t for each word x t by combining its representation with the previous hidden vector. Thus h t summarizes both the word and its prior context.</p><p>Our proposed DAG-GRU model incorporates syntactic information through dependency parse relationships and is similar in spirit to <ref type="bibr" target="#b16">(Nguyen and Grishman, 2018)</ref> and <ref type="bibr" target="#b18">(Qian et al., 2018)</ref>. However, unlike those methods, DAG-GRU uses attention to combine syntactic and temporal infor- mation. Rather than using an additional gate as in ( <ref type="bibr" target="#b18">Qian et al., 2018</ref>), DAG-GRU creates a sin- gle combined representation over previous hid- den vectors and then applies the standard GRU model. Each relationship is represented as an edge, (t, t , e), between words at index t and t with an edge type e. The standard GRU edges are included as (t, t − 1, temporal).</p><p>Each dependency relationship may be between any two words, which could produce a graph with cycles. However, back-propagation through time <ref type="bibr" target="#b12">(Mozer, 1995</ref>) requires a directed acyclic graph (DAG), Hence the sentence graph, consisting of temporal and dependency edges E, is split into two DAGs: a "forward" DAG G f that consists of only of edges (t, t , e) where t &lt; t and a corre- sponding "backward" DAG G b where t &gt; t. The dependency relation between t and t also includes the parent-child orientation, e.g., nsubj-parent or nsubj-child for a nsubj (subject) relation.</p><p>An attention mechanism is used to combine the multiple hidden vectors. The matrix D t is formed at each word x t by collecting and transforming all the previous hidden vectors coming into node t, one per each edge type e. α gives the attention, a distribution weighting importance over the edges.</p><p>At least three members of a family in Indias northeastern state of Tripura were hacked to death by a tribal mob nsubj auxpass tanh softmax x dot tanh dot <ref type="figure">Figure 1</ref>: The hidden state of "hacked" is a combination of previous output vectors. In this case, three vectors are aggregated with DAG-GRU's attention model. h t , is included in the input for the attention model since it is accessible through the "subj" dependency edge. h t is included twice because it is connected through a narrative edge and a dependency edge with type "auxpass." The input matrix is non-linearly transformed by U a and tanh. Next, w a determines the importance of each vector in D t . Finally, the attention a t is produced by tanh followed by softmax then applied to D t . The subject "members" would be distant under a standard RNN model, however the DAG-GRU model can focus on this important connection via dependency edges and attention.</p><p>Finally, the combined hidden vector h a is created by summing D t weighted by attention.</p><formula xml:id="formula_1">D T t = [tanh(U e h t )|(t, t , e) ∈ E] α = softmax(tanh(D t w a )) h a = D T t α</formula><p>However, having a set of parameters U e for each edge type e is over-specific for small datasets. In- stead a shared set of parameters U a is used in con- junction with an edge embedding v e .</p><formula xml:id="formula_2">D T t = [tanh(U a h t • v e )|(t, t , e) ∈ E]</formula><p>The edge type embedding v e is concatenated with the hidden vector h t and then transformed by the shared weights U a . This limits the number of parameters while flexibly weighting the differ- ent edge types. The new combined hidden vector h a is used instead of h t−1 in the GRU equations.</p><p>The model is run forward and backward with the output concatenated, h c,t = h f,t h b,t , for a representation that includes the entire sentence's context and dependency relations. After applying dropout ( <ref type="bibr" target="#b19">Srivastava et al., 2014</ref>) with 0.5 rate to h c,t , a linear model with softmax is used to make predictions for each word at index t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We use the ACE2005 dataset for evaluation. Each word in each document is marked with one of the thirty-three event types or Nil for non-triggers. Several high-performance models were repro- duced for comparison. Each is a good faith re- production of the original with some adjustments to level the playing field.</p><p>For word embeddings, Elmo was used to gen- erate a fixed representation for every word in ACE2005 ( <ref type="bibr" target="#b17">Peters et al., 2018</ref>). The three vectors produced per word were concatenated together for a single representation. We did not use entity type embeddings for any method. The models were trained to minimize the cross entropy loss with Adam ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>) with L2 regulariza- tion set at 0.0001. The learning rate was halved every five epochs starting from 0.0005 for a max- imum of 30 epochs or until convergence as deter- mined by F1 score on the development set.</p><p>The same training method and word embed- dings were used across all the methods. Based on preliminary experiments, these settings resulted in better performance than those originally specified. However, notably both GRU (  and DAG-LSTM ( <ref type="bibr" target="#b18">Qian et al., 2018)</ref> were not used as joint models. Further, the GRU implementation did not use a memory network, instead we used the final vectors from the forward and backward pass concatenated to each timestep's output for ad- ditional context. For CNNs <ref type="bibr" target="#b14">(Nguyen and Grishman, 2015</ref>) the number of filters was reduced to 50 per filter size. The CNN+LSTM (Feng et al., 2016) model had no other modifications. DAG- GRU used a hidden layer size of 128. Variant A of the DAG-GRU model utilized the attention mech- anism, while variant B used averaging, that is:</p><formula xml:id="formula_3">D t = 1 |E(t)| (t ,e)∈E(t) tanh(U a h t • v e )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effects of Random Initialization</head><p>Given that ACE2005 is small as far as neural net- work models are concerned, the effect of the ran- dom initialization of these models needs to be studied. Although some methods include tests of significance, the type of statistical test is often not reported. Simple statistical significance tests, such as the t-test, are not compatible with a single F1 score, instead the average of F1 scores should be tested ( <ref type="bibr" target="#b20">Wang et al., 2015)</ref>. We reproduced and evaluated five different sys- tems with different initializations to empirically assess the effect of initialization. The experiments were done on the standard ACE2005 split and the aggregated results over 20 random seeds were given in <ref type="table" target="#tab_0">Table 1</ref>. The random initializations of the models had a significant impact on their perfor- mance. The variation was large enough that the observed range of the F1 scores overlapped across almost all the models. However the differences in average performances of different methods, ex- cept for CNN and DAG-LSTM, were significant at p &lt; 0.05 according to the t-test, not controlling for multiple hypotheses.</p><p>Both the GRU ( ) and CNN ( <ref type="bibr" target="#b14">Nguyen and Grishman, 2015</ref>) models perform well with their best scores being close to the re- ported values. The CNN+LSTM model's results were significantly lower than the published val- ues, though this method has the highest variation. It is possible that there is some unknown factor such as the preprocessing of the data that signifi- cantly impacted the results or that the value is an outlier. Likewise, the DAG-LSTM model under- performed. However, the published results were based on a joint event and argument extraction model and probably benefited from the additional entity and argument information. DAG-GRU A consistently and significantly out- performs the other methods in this comparison. The best observed F1 score, 71.1%, for DAG- GRU is close to the published state-of-the-art scores of DAG-LSTM and GCNN at 71.9% and 71.4% respectively. With additional entity infor- mation, GCNN achieves a score of 73.1%. Also, the attention mechanism used in DAG-GRU A shows a significant improvement over the averag- ing method of DAG-GRU B. This indicates that some syntactic links are more useful than others and that the weighting attention applies is neces- sary to utilize that syntactic information.</p><p>Another source of variation was the distribu- tional differences between the development and testing sets. Further, the testing set only include newswire articles whereas the training and dev. sets contain informal writing such as web log (WL) documents. The two sets have different pro- portions of event types and each model saw at least a 2% drop in performance between dev. and test on average. At worst, the DAG-LSTM model's drop was 5.26%. This is a problem for model se- lection, since the dev. score is used to choose the best model, hyperparameters, or random initial- ization. The distributional differences mean that methods which outperform others on the dev. set do not necessarily perform as well on the test set. For example, DAG-GRU A performs worse that DAG-GRU B on the dev. set, however it achieves a higher mean score on the testing set.</p><p>One method of model selection over random initializations is to train the model k times and pick the best one based on the dev. score. Repeat- ing this model selection procedure many times for each model is prohibitively expensive, so the ex- periment was approximated by bootstrapping the twenty samples per model <ref type="bibr" target="#b3">(Efron, 1992</ref>). For each model, 5 dev. &amp; test score pairs were sampled with replacement from the twenty available pairs. The initialization with the best dev. score was selected and the corresponding test score was taken. This model selection process of picking the best of 5 random samples was repeated 1000 times and the results are shown in <ref type="table" target="#tab_1">Table 2</ref>. This process did not substantially increase the average performance be- yond the results in <ref type="table" target="#tab_0">Table 1</ref>, although it did reduce the variance, except for the CNN model. It ap-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev Mean Mean Min Max Std. Dev. Published DAG-GRU A 70.3%</p><p>69.2% ± 0.42 67.8% 71.1% 0.91% - DAG-GRU B 71.2%</p><p>68.4% ± 0.45 67.39% 70.53% 0.96% - GRU 70.3% 68.0% ± 0.42 66.4% 69.4% 0.86% 69.3% † CNN+LSTM 69.6%</p><p>66.4% ± 0.62 63.6% 68.1% 1.32% 73.4% DAG-LSTM 70.5%</p><p>65.2% ± 0.44 63.0% 66.8% 0.94% 71.9% † CNN 68.5% 64.7% ± 0.65 61.6% 67.2% 1.38% 67.6%  pears that using the dev. score for model selection is only marginally helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Randomized Splits</head><p>In order to explore the effect of the training/testing split popularized by ( <ref type="bibr" target="#b8">Li et al., 2013</ref>), a randomized cross validation experiment was conducted. From the set of 599 documents in ACE2005, 10 random splits were created maintaining the same 529, 30, 40 document counts per split, training, develop- ment, testing, respectively. This method was used to evaluate the effect of the standard split, since it maintains the same data proportions while only varying the split. The results of the experiment are found in <ref type="table" target="#tab_3">Table 3</ref>. The effect of the split is substantial. Almost all models' performance dropped except for DAG- LSTM, however the variance increased across all models. In the worst case, the standard deviation increased threefold from 0.86% to 2.60% for the GRU model. In fact, the increased variation of the splits means that the confidence intervals for all the models overlap. This aligns with cross domain analysis, some domains such as WL are known to be much more difficult than the newswire domain which comprises all of the test data under the stan- dard split <ref type="bibr" target="#b14">(Nguyen and Grishman, 2015)</ref>. Further, the effect of the difference in splits also negates the benefits of the attention mechanism of DAG-  GRU A. This is likely due to the test partitions' inclusion of WL and other kinds of informal writ- ing. The syntactic links are much more likely to be noisy for informal writing, reducing the syntactic information's usefulness and reliability. All these sources of variation are greater than most advances in event detection, so quantifying and reporting this variation is essential when as- sessing model performance. Further, understand- ing this variation is important for reproducibility and is necessary for making any valid claims about a model's relative effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduced and evaluated a DAG-GRU model along with four previous models in two different settings, the standard ACE2005 split with mul- tiple random initializations and the same dataset with multiple random splits. These experiments demonstrate that our model, which utilizes syntac- tic information through an attention mechanism, is competitive with the state-of-the-art. Further, they show that there are several significant sources of variation which had not been previously studied and quantified. Studying and mitigating this vari- ation could be of significant value by itself. At a minimum, it suggests that the community should move away from evaluations based on single ran- dom initializations and single training-test splits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A comparison of mean performance versus number of parameters.</figDesc><graphic url="image-8.png" coords="4,72.00,62.81,214.86,72.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The statistics of the 20 random initializations 
experiment.  † denotes results are from a joint event and 
argument extraction model. 

Model 
Dev Mean 
Mean 
Std. Dev. 
DAG-GRU A 
72.0% 
69.2% ± 0.04% 
0.68% 
DAG-GRU B 
72.0% 
67.9% ± 0.04% 
0.60% 
GRU 
71.5% 
68.4% ± 0.05% 
0.80% 
CNN+LSTM 
70.8% 
66.8% ± 0.07% 
1.08% 
DAG-LSTM 
70.4% 
65.5% ± 0.02% 
0.40% 
CNN 
69.6% 
65.4% ± 0.09% 
1.49% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Bootstrap estimates on 1000 samples for each 
model after model selection based on dev set scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Average results on 10 randomized splits.</head><label>3</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by grants from DARPA XAI (N66001-17-2-4030), DEFT (FA8750-13-2-0033), and NSF (IIS-1619433 and IIS-1406049).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Computational linguistics</title>
		<meeting>the 17th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bootstrap methods: another look at the jackknife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bradley Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="569" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A languageindependent neural network for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event nugget detection with forward-backward recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ghaeini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiaoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">369</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Event detection via gated multilingual attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging framenet to improve automatic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2134" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A focused backpropagation algorithm for temporal. Backpropagation: Theory, architectures, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mozer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling skip-grams for event detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph convolutional networks with argument-aware pooling for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jointly extracting event triggers and arguments by dependency-bridge rnn and tensor-based argument interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Confidence interval for f1 measure of algorithm performance based on blocked 32 crossvalidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wingli</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="651" to="659" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
