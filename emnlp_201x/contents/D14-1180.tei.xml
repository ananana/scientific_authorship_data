<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Type-based MCMC for Sampling Tree Fragments from Forests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Type-based MCMC for Sampling Tree Fragments from Forests</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1735" to="1745"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper applies type-based Markov Chain Monte Carlo (MCMC) algorithms to the problem of learning Synchronous Context-Free Grammar (SCFG) rules from a forest that represents all possible rules consistent with a fixed word alignment. While type-based MCMC has been shown to be effective in a number of NLP applications, our setting, where the tree structure of the sentence is itself a hidden variable, presents a number of challenges to type-based inference. We describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges. These methods lead to improvements in both log likelihood and BLEU score in our experiments .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In previous work, sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees <ref type="bibr" target="#b16">(Post and Gildea, 2009;</ref>) for TSG learning. Here, at each node in the derivation tree, there is a binary vari- able indicating whether the node is internal to a TSG rule or is a split point, which we refer to as a cut, between two rules. The problem of extract- ing machine translation rules from word-aligned bitext is a similar problem in that we wish to au- tomatically learn the best granularity for the rules with which to analyze each sentence. The prob- lem of rule extraction is more complex, however, because the tree structure of the sentence is also unknown.</p><p>In machine translation applications, most pre- vious work on joint alignment and rule extrac- tion models uses heuristic methods to extract rules from learned word alignment or bracketing struc- tures ( <ref type="bibr" target="#b18">Zhang et al., 2008;</ref><ref type="bibr" target="#b5">DeNero et al., 2008;</ref><ref type="bibr" target="#b12">Levenberg et al., 2012)</ref>. <ref type="bibr" target="#b2">Chung et al. (2014)</ref> present a MCMC algorithm schedule to learn Hiero-style SCFG rules <ref type="bibr" target="#b1">(Chiang, 2007)</ref> by sampling tree fragments from phrase de- composition forests, which represent all possible rules that are consistent with a set of fixed word alignments. Assuming fixed word alignments re- duces the complexity of the sampling problem, and has generally been effective in most state- of-the-art machine translation systems. The al- gorithm for sampling rules from a forest is as follows: from the root of the phrase decomposi- tion forest, one samples a cut variable, denoting whether the current node is a cut, and an edge vari- able, denoting which incoming hyperedge is cho- sen, at each node of the current tree in a top-down manner. This sampling schedule is efficient in that it only samples the current tree and will not waste time on updating variables that are unlikely to be used in any tree.</p><p>As with many other token-based Gibbs Sam- pling applications, sampling one node at a time can result in slow mixing due to the strong cou- pling between variables. One general remedy is to sample blocks of coupled variables. <ref type="bibr" target="#b3">Cohn and Blunsom (2010)</ref> and <ref type="bibr" target="#b17">Yamangil and Shieber (2013)</ref> used blocked sampling algorithms that sample the whole tree structure associated with one sentence at a time for TSG and TAG learning. However, this kind of blocking does not deal with the coupling of variables correlated with the same type of struc- ture across sentences. <ref type="bibr" target="#b13">Liang et al. (2010)</ref> intro- duced a type-based sampling schedule which up- dates a block of variables of the same type jointly. The type of a variable is defined as the combina- tion of new structural choices added when assign- ing different values to the variable. Type-based MCMC tackles the coupling issue by assigning the same type to variables that are strongly coupled.</p><p>In this paper, we follow the phrase decompo- sition forest construction procedures of <ref type="bibr" target="#b2">Chung et al. (2014)</ref> and present a type-based MCMC algo- rithm for sampling tree fragments from phrase de- composition forests which samples the variables of the same type jointly. We define the type of the cut variable for each node in our sampling sched- ule. While type-based MCMC has been proven to be effective in a number of NLP applications, our sample-edge, sample-cut setting is more com- plicated as our tree structure is unknown. We need additional steps to maintain the cut type in- formation when the tree structure is changed as we sample the edge variable. Like other type- based MCMC applications, we need bookkeep- ing of node sites to be sampled in order to loop through sites of the same type efficiently. As noted by <ref type="bibr" target="#b13">Liang et al. (2010)</ref>, indexing by the complete type information is too expensive in some appli- cations like TSG learning. Our setting is different from TSG learning in that the internal structure of each SCFG rule is abstracted away when deriving the rule type from the tree fragment sampled.</p><p>We make the following contributions:</p><p>1. We apply type-based MCMC to the setting of SCFG learning and have achieved better log likelihood and BLEU score result.</p><p>2. We present an innovative way of storing the type information by indexing on partial type information and then filtering the retrieved nodes according to the full type information, which enables efficient updates to maintain the type information while the amount of bookkeeping is reduced significantly.</p><p>3. We replace the two-stage sampling schedule of <ref type="bibr" target="#b13">Liang et al. (2010)</ref> with a simpler and faster one-stage method.</p><p>4. We use parallel programming to do inexact type-based MCMC, which leads to a speed up of four times in comparison with non- parallel type-based MCMC, while the like- lihood result of the Markov Chain does not change. This strategy should also work with other type-based MCMC applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MCMC for Sampling Tree Fragments from Forests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Phrase Decomposition Forest</head><p>The phrase decomposition forest provides a com- pact representation of all machine translation rules that are consistent with our fixed input word align- ment ( <ref type="bibr" target="#b2">Chung et al., 2014</ref>), and our sampling algo- rithm selects trees from this forest. As in Hiero, our grammars will make use of a single nonterminal X, and will contain rules with a mixture of nonterminals and terminals on the righthand side (r.h.s.), with at most two nontermi- nal occurrences on the r.h.s. Under this restric- tion, the maximum number of rules that can be extracted from an input sentence pair is O(n 12 ) with respect to the length of the sentence pair, as the left and right boundaries of the lefthand side (l.h.s.) nonterminal and each of the two r.h.s. nonterminals can take O(n) positions in each of the two languages. This complexity leads us to explore sampling algorithms instead of using dy- namic programming.</p><p>A span <ref type="bibr">[i, j]</ref> is a set of contiguous word in- dices {i, i + 1, . . . , j − 1}. Given an aligned Chinese-English sentence pair, a phrase n is a pair of spans n = ([i 1 , j 1 ], [i 2 , j 2 ]) such that Chinese words in positions [i 1 , j 1 ] are aligned only to En- glish words in positions [i 2 , j 2 ], and vice versa. A phrase forest H = V, E is a hypergraph made of a set of hypernodes V and a set of hyperedges E. Each node n = ([i 1 , j 1 ], [i 2 , j 2 ]) ∈ V is a tight phrase as defined by <ref type="bibr" target="#b11">Koehn et al. (2003)</ref>, i.e., a phrase containing no unaligned words at its boundaries. A phrase n = ( </p><formula xml:id="formula_0">[i 1 , j 1 ], [i 2 , j 2 ]) covers n = ([i 1 , j 1 ], [i 2 , j 2 ]) if i 1 ≤ i 1 ∧ j 1 ≤ j 1 ∧ i 2 ≤ i 2 ∧ j 2 ≤ j 2 Each edge in E, written as T → n, is made of a ([0 1], [0 1]) X , I ([4 5], [1 2]) X , have ([5 6], [3 4]) X , date</formula><formula xml:id="formula_1">X XX, XX ([1 6], [1 7]) X XX, XX X XX, XX ([0 6], [0 7]) X XX, XX ([1 2], [6 7])</formula><p>X , today <ref type="figure">Figure 2</ref>: A phrase decomposition forest extracted from the sentence pair , I have a date with her today. Each edge is a min- imal SCFG rule, and the rules at the bottom level are phrase pairs. Unaligned word "a" shows up in the rule X → X 1 X 2 , X 1 aX 2 after unaligned words are put back into the alignment matrix. The highlighted portion of the forest shows an SCFG rule built by composing minimal rules. set of non-intersecting tail nodes T ⊂ V , and a single head node n ∈ V that covers each tail node. We say an edge T → n is minimal if there does not exist another edge T → n such that T covers T . A minimal edge is an SCFG rule that cannot be decomposed by factoring out some part of its r.h.s. as a separate rule. We define a phrase de- composition forest to be made of all phrases from a sentence pair, connected by all minimal SCFG rules. A phrase decomposition forest compactly represents all possible SCFG rules that are consis- tent with word alignments. For the example word alignment shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the phrase decom- position forest is shown in <ref type="figure">Figure 2</ref>. Each boxed phrase in <ref type="figure" target="#fig_0">Figure 1</ref> corresponds to a node in the forest of <ref type="figure">Figure 2</ref>, while hyperedges in <ref type="figure">Figure 2</ref> represent ways of building phrases out of shorter phrases.</p><p>A phrase decomposition forest has the impor- tant property that any SCFG rule consistent with the word alignment corresponds to a contiguous fragment of some complete tree found in the for- est. For example, the highlighted tree fragment of the forest in <ref type="figure">Figure 2</ref> corresponds to the SCFG rule:</p><formula xml:id="formula_2">X → X 2 X 1 , have a X 1 with X 2</formula><p>Thus any valid SCFG rule can be formed by se- lecting a set of adjacent hyperedges from the for- est and composing the minimal SCFG rules speci- fied by each hyperedge. Therefore, the problem of SCFG rules extraction can be solved by sampling tree fragments from the phrase decomposition for- est. We use a bottom-up algorithm to construct the phrase decomposition forest from the word align- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sampling Tree Fragments From Forest</head><p>We formulate the rule sampling procedure into two phases: first we select a tree from a forest, then we select the cuts in the tree to denote the split points between fragments, with each fragment cor- responding to a SCFG rule. A tree can be speci- fied by attaching a variable e n to each node n in the forest, indicating which hyperedge is turned on at the current node. Thus each assignment will specify a unique tree by tracing the edge variables from the root down to the leaves. We also attach a cut variable z n to each node, indicating whether the node is a split point between two adjacent frag- ments.</p><p>Let all the edge variables form the random vec- tor Y and all the cut variables form the random vector Z. Given an assignment y to the edge vari- ables and assignment z to the cut variables, our de- sired distribution is proportional to the product of weights of the rules specified by the assignment:</p><formula xml:id="formula_3">P t (Y = y, Z = z) ∝ r∈τ (y,z) w(r)<label>(1)</label></formula><p>where τ (y, z) is the set of rules identified by the assignment. We use a generative model based on a Dirichlet Process (DP) defined over composed rules. We draw a distribution G over rules from a DP, and then rules from G.</p><formula xml:id="formula_4">G | α, P 0 ∼Dir(α, P 0 ) r | G ∼G</formula><p>For the base distribution P 0 , we use a uniform distribution where all rules of the same size have equal probability:</p><formula xml:id="formula_5">P 0 (r) = V −|r f | f V −|re| e (2)</formula><p>where V f and V e are the vocabulary sizes of the source language and the target language, and |r f | and |r e | are the lengths of the source side and tar- get side of rule r. By marginalizing out G we get a simple posterior distribution over rules which can be described using the Chinese Restaurant Process (CRP). For this analogy, we imagine a restaurant has infinite number of tables that represent rule types and customers that represent translation rule instances. Each customer enters the restaurant and chooses a table to sit at. Let z i be the table chosen by the i-th customer, then the customer chooses a table k either having been seated or a new table with probability:</p><formula xml:id="formula_6">P (z i = k|z −i ) = n k i−1+α 1 ≤ k ≤ K α i−1+α k = K + 1 (3)</formula><p>where z −i is the current seating arrangement, n k is the number of customers at the table k, K is the to- tal number of occupied tables. If the customer sits at a new table, the new table will be assigned a rule label r with probability P 0 (r). We can see from Equation 3 that the only history related to the cur- rent table assignment is the counts in z −i . There- fore, we define a table of counts N = {N C } C∈I which memorizes different categories of counts in z −i . I is an index set for different categories of counts. Each N C is a vector of counts for category C. We have P (r i = r|z −i ) = P (r i = r|N ). If we marginalize over tables labeled with the same rule, we get the following probability over rule r given the previous count table N :</p><formula xml:id="formula_7">P (r i = r|N ) = N R (r) + αP 0 (r) n + α (4)</formula><p>here in the case of DP, I = {R}, where R is the index for the category of rule counts. N R (r) is the number of times that rule r has been observed in</p><formula xml:id="formula_8">z −i , n = r N R (r)</formula><p>is the total number of rules observed.</p><p>We also define a Pitman-Yor Process (PYP) <ref type="bibr" target="#b15">(Pitman and Yor, 1997</ref>) over rules of each length l. We draw the rule distribution G from a PYP, and then rules of length l are drawn from G.</p><formula xml:id="formula_9">G|α, d, P 0 ∼ P Y (α, d, P 0 ) r|G ∼ G</formula><p>The first two parameters, a concentration parame- ter α and a discount parameter d, control the shape of distribution G by controlling the size and the Algorithm 1 Top-down Sampling Algorithm 1: queue.push(root) 2: while queue is not empty do 3: n = queue.pop() 4: SAMPLEEDGE(n) 5:</p><p>SAMPLECUT(n) 6:</p><p>for each child c of node n do 7:</p><p>queue.push(c) 8:</p><p>end for 9: end while number of clusters. Integrating over G, we have the following PYP posterior probability:</p><formula xml:id="formula_10">P (r i = r|N ) = N R (r) − T r d + (T l d + α)P 0 (r) N L (l) + α<label>(5)</label></formula><p>here for the case of PYP, I = {R, L}. We have an additional index L for the category of rule length counts, and N L (l) is the total number of rules of length l observed in z −i . T r is the number of ta- bles labeled with r in z −i . The length of the rule is drawn from a Poisson distribution, so a rule length probability</p><formula xml:id="formula_11">P (l; λ) = λ l e −λ l!</formula><p>is multiplied by this probability to calculate the real posterior probabil- ity for each rule. In order to simplify the tedious book-keeping, we estimate the number of tables using the following equations <ref type="bibr" target="#b9">(Huang and Renals, 2010)</ref>:</p><formula xml:id="formula_12">T r = N R (r) d<label>(6)</label></formula><formula xml:id="formula_13">T l = r:|r|=l N R (r) d<label>(7)</label></formula><p>We use the top-down sampling algorithm of <ref type="bibr" target="#b2">Chung et al. (2014)</ref> (see Algorithm 1). Starting from the root of the forest, we sample a value for the edge variable denoting which incoming hyper- edge of the node is turned on in the current tree, and then we sample a cut value for the node de- noting whether the node is a split point between two fragments in the tree. For each node n, we de- note the composed rule type that we get when we set the cut of node n to 0 as r 1 and the two split rule types that we get when we set the cut to 1 as r 2 , r 3 . We sample the cut value z i of the current node according to the posterior probability:</p><formula xml:id="formula_14">P (z i = z|N ) = P (r1|N ) P (r1|N )+P (r2|N )P (r3|N ) if z = 0 P (r2|N )P (r3|N ) P (r1|N )+P (r2|N )P (r3|N ) otherwise (8)</formula><p>where the posterior probability P (r i |N ) is accord- ing to either a DP or a PYP, and N, N are tables of counts. In the case of DP, N, N differ only in the rule counts of r 2 , where N R (r 2 ) = N R (r 2 ) + 1. In the case of PYP, there is an extra difference that  These two non-split nodes are internal to the same composed rule: X → X 1 X 2 X 3 , X 3 X 2 X 1 . We keep these two sites with the same index. However, when we set the cut value of these two nodes to 1, as the rules imme- diately above and immediately below are different for these two sites, they are not of the same type.</p><formula xml:id="formula_15">X XX, XX X XX, XX ([0 6], [0 7]) X XX, XX ([1 2], [6 7]) X , today</formula><formula xml:id="formula_16">N L (l) = N L (l) + 1,</formula><p>where l is the rule length of r 2 .</p><p>As for edge variables e i , we refer to the set of composed rules turned on below n including the composed rule fragments having n as an internal or root node as {r 1 , . . . , r m }. We have the follow- ing posterior probability over the edge variable e i :</p><formula xml:id="formula_17">P (e i = e|N ) ∝ m i=1 P (r i |N i−1 ) v∈τ (e)∩in(n) deg(v) (9)</formula><p>where deg(v) is the number of incoming edges for node v, in(n) is the set of nodes in all subtrees under n, and τ (e) is the tree specified when we set e i = e. N 0 to N m are tables of counts where</p><formula xml:id="formula_18">N 0 = N , N i R (r i ) = N i−1 R (r i ) + 1 in the case of DP and additionally N i L (l i ) = N i−1 L (l i ) + 1</formula><p>in the case of PYP, where l i is the rule length of r i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Type-based MCMC Sampling</head><p>Our goal in this paper is to organize blocks of vari- ables that are strongly coupled into types and sam- ple variables of each type jointly. One major prop- erty of type-based MCMC is that the joint proba- bility of variables of the same type should be ex- changeable so that the order of the variables does not matter. Also, the choices of the variables to be sampled jointly should not interfere with each other, which we define as a conflict. In this section, we define the type of cut variables in our sampling schedule and explain that with the two priors we introduced before, the joint probability of the vari- ables will satisfy the exchangeability property. We will also discuss how to check conflict sites in our application.</p><p>In type-based MCMC, we need bookkeeping of sites as we need to loop through them to search for sites having the same type efficiently. In our two- stage sample-edge, sample-cut schedule, updating the edge variable would change the tree structure and trigger updates for the cut variable types in both the old and the new subtree. We come up with an efficient bookkeeping strategy to index on par- tial type information which significantly reduces the bookkeeping size, while updates are quite effi- cient when the tree structure is changed. The detail will become clear below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Type-based MCMC</head><p>We refer to each node site to be sampled as a pair (t, n), indicating node n of forest t. For each site (t, n) and the corresponding composed rule types r 1 obtained when we set n's cut value to 0 and r 2 , r 3 obtained when we set the cut value to 1, the cut variable type of site (t, n) is:</p><formula xml:id="formula_19">type(t, n) def = (r 1 , r 2 , r 3 )</formula><p>We say that the cut variables of two sites are of the same type if the composed rule types r 1 , r 2 and r 3 are exactly the same. For example, in <ref type="figure" target="#fig_3">Figure 3</ref>, assume that all the nodes in the hypergraph are currently set to be split points except for the two nodes marked in bold, ( <ref type="bibr">[2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6], [1 6]), ([1 4], [4 7]).</head><p>Considering these two non-split nodes, the com- posed rule types they are internal to (r 1 ) are ex- actly the same. However, the situation changes if we set the cut variables of these two nodes to be 1, i.e., all of the nodes in the hypergraph are now split points. As the rule type immediately above and the rule type immediately below the two nodes (r 2 and r 3 ) are now different, they are not of the same type.</p><p>We sample the cut value z i according to Equa- tion 8. As each rule is sampled according to a DP or PYP posterior and the joint probabili- ties according to both posteriors are exchangeable, we can see from Equation 8 that the joint prob-ability of a sequence of cut variables is also ex- changeable. Consider a set of sites S containing n cut variables z S = (z 1 , ..., z n ) of the same type. This exchangeability property leads to the fact that any sequence containing same number of cuts (cut value of 1) would have same probability. We have the following probability distribution:</p><formula xml:id="formula_20">P (z S |N ) ∝ n−m i=1 P (r 1 |N i−1 ) m i=1 P (r 2 | ¯ N i−1 )P (r 3 | ˆ N i−1 ) def = g(m) (10)</formula><p>where N is the count table for all the other vari- ables except for S. m = n i=1 z i is the number of cut sites. The variables N, ¯ N , andˆNandˆ andˆN keep track of the counts as the derivation proceeds step by step:</p><formula xml:id="formula_21">N 0 = N N i R (r 1 ) = N i−1 R (r 1 ) + 1 ¯ N 0 = N n−m ˆ N i−1 R (r 2 ) = ¯ N i−1 R (r 2 ) + 1 ¯ N i R (r 3 ) = ˆ N i−1 R (r 3 ) + 1</formula><p>For PYP, we add extra count indices for rule length counts similarly. Given the exchangeability property of the cut variables, we can calculate the posterior probabil- ity of m = n i=1 z i by summing over all n m combinations of the cut sites:</p><formula xml:id="formula_22">p(m|N ) ∝ z S :m= i z i p(z S |N ) = n m g(m) (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sampling Cut-types</head><p>Given Equation 11 and the exchangeability prop- erty, our sampling strategy falls out naturally: first we sample m according to Equation 11, then con- ditioned on m, we pick m sites of z S as cut sites out of the n m combinations with uniform proba- bility. Now we proceed to define conflict sites. In ad- dition to exchangeability, another important prop- erty of type-based MCMC is that the type of each site to be sampled should be independent of the assignment of the other sites sampled at the same time. That is, in our case, setting the cut value of each site should not change the (r 1 , r 2 , r 3 ) triple of another site. We can see that the cut value of the current site would have effect on and only on Algorithm 2 Type-based MCMC Algorithm for Sampling One Site 1: sample one type of sites, currently sample site (node, parent) 2: if parent is N one or node is sampled then  <ref type="bibr">29:</ref> mark all nodes in sites as sampled the nodes in the r 1 fragment. We denote nodes(r) as the node set for all nodes within fragment r. Then for ∀z, z ∈ S, z is not in conflict with z if and only if nodes(r 1 ) ∩ nodes(r 1 ) = ∅, where r 1 and r 1 are the corresponding composed rule types when we set z, z to 0.</p><p>Another crucial issue in type-based sampling is the bookkeeping of sampling sites, as we need to loop through all sites having the same type with the current node. We only maintain the type in- formation of nodes that are currently turned on in the chosen tree of the forest, as we only sample these nodes. It is common practice to directly use the type value of each variable as an index and maintain a set of sites for each type. However, maintaining a (r 1 , r 2 , r 3 ) triple for each node in the chosen tree is too memory heavy in our appli-cation.</p><p>In our two-stage sample-edge, sample-cut schedule, there is an additional issue that we must deal with efficiently: when we have chosen a new incoming edge for the current node, we also have to update the bookkeeping index as the current tree structure is changed. Cut variable types in the old subtree will be turned off and a new subtree of variable types will be turned on. In the extreme case, when we have chosen a new incoming edge at the root node, we have chosen a new tree in the forest. So, we need to remove appearances of cut variable types in the old tree and add all cut vari- able types in the newly chosen tree.</p><p>Our strategy to deal with these two issues is to build a small, simple index, at the cost of some additional computation when retrieving nodes of a specified type. To be precise, we build an index from (single) rule types r to all occurrences of r in the data, where each occurrence is represented as a pointer to the root of r in the forest. Our strategy has two important differences from the standard strategy of building an index having the complete type (r 1 , r 2 , r 3 ) as the key and having every node as an entry. Specifically:</p><p>1. We index only the roots of the current rules, rather than every node, and 2. We key on a single rule type, rather than a triple of rule types.</p><p>Differences (1) and (2) both serve to keep the in- dex small, and the dramatic savings in memory is essential to making our algorithm practical. Fur- thermore, difference (1) reduces the amount of work that needs to be done when an edge variable is resampled. While we must still re-index the en- tire subtree under the changed edge variable, we need only to re-index the roots of the current tree fragments, rather than all nodes in the subtree. Given this indexing strategy, we now proceed to describe the process for retrieving nodes of a specified type (r 1 , r 2 , r 3 ). These nodes fall into one of two cases:</p><p>1. Internal nodes, i.e., nodes whose cut variable is currently set to 0. These nodes must be contained in a fragment of rule type r 1 , and must furthermore have r 2 above them, and r 3 below them. We retrieve these nodes by look- ing up r 1 in the index, iterating over all nodes in each fragment retrieved, and retaining only those with r 2 above and r 3 below. <ref type="table" target="#tab_0">(Lines 13- 19 in Algorithm 2.)</ref> 2. Boundary nodes, i.e., nodes whose cut vari- able is currently set to 1. These nodes must form the root of a fragment r 3 , and have a fragment r 2 above them. We retrieve these nodes by looking up r 3 in the index, and then checking each node retrieved to retain only those nodes with r 2 above them in the current tree. (Lines 20-24 in Algorithm 2.)</p><p>This process of winnowing down the nodes re- trieved by the index adds some computational overhead to our algorithm, but we find that it is minimal in practice.</p><p>We still use the top-down sampling schedule of Algorithm 1, except that in the sample-edge step, when we choose a new incoming edge, we add additional steps to update the bookkeeping index. Furthermore, in the sample-cut step, we sample all non-conflict sites having the same type with n jointly. Our full algorithm for sampling one cut- type is shown in Algorithm 2. When sampling each site, we record a parent node of the near- est cut ancestor of the current node so that we can build r 1 and r 2 more quickly, as they are both rooted at parent. We first identify the type of the current site. Then we search the bookkeeping in- dex to find possible candidate sites of the same type, as described above. As for conflict check- ing, we keep a set of nodes that includes all nodes in the r 1 fragment of previous non-conflict sites. If the r 1 fragment of the current site has any node in common with this node set, we arrive at a conflict site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods of Further Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">One-stage Sampling Schedule</head><p>Instead of calculating the posterior of each m ac- cording to Equation 11 and then sampling m, we can build our real m more greedily.</p><formula xml:id="formula_23">P (z S |N ) = n i=1 P (z i |N i−1 )<label>(12)</label></formula><p>where N, N 0 , . . . , N n are count tables, and N 0 = N . N i is the new count table after we update N i−1 according to the assignment of z i . This equation gives us a hint to sample each z i according to P (z i |N i−1 ) and then update the count table N i−1 according to the assignment of z i . This greedy sampling saves us the effort to calculate each m by multiplying over each posterior of cut variables but directly samples the real m. In our exper- iment, this one-stage sampling strategy gives us a 1.5 times overall speed up in comparison with the two-stage sampling schedule of <ref type="bibr" target="#b13">Liang et al. (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parallel Implementation</head><p>As our type-based sampler involves tedious book- keeping and frequent conflict checking and mis- match of cut types, one iteration of the type-based sampler is slower than an iteration of the token- based sampler when run on a single processor. In order to speed up our sampling procedure, we used a parallel sampling strategy similar to that of  and <ref type="bibr" target="#b6">Feng and Cohn (2013)</ref>, who use multiple processors to perform inexact Gibbs Sampling, and find equivalent performance in comparison with an exact Gibbs Sampler with significant speed up. In our application, we split the data into several subsets and assign each sub- set to a processor. Each processor performs type- based sampling on its subset using local counts and local bookkeeping, and communicates the up- date of the local counts after each iteration. All the updates are then aggregated to generate global counts and then we refresh the local counts of each processor. We do not communicate the up- date on the bookkeeping of each processor. In this implementation, we have a slightly "out-of-date" counts at each processor and a smaller bookkeep- ing of sites of the same type, but we can perform type-based sampling independently on each pro- cessor. Our experiments show that, with proper division of the dataset, the final performance does not change, while the speed up is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We used the same LDC Chinese-English parallel corpus as <ref type="bibr" target="#b2">Chung et al. (2014)</ref>, 1 which is composed of newswire text. The corpus consists of 41K sen- tence pairs, which has 1M words on the English side. The corpus has a 392-sentence development set with four references for parameter tuning, and a 428-sentence test set with four references for testing. <ref type="bibr">2</ref> The development set and the test set have sentences with less than 30 words. A trigram lan- guage model was used for all experiments. We plotted the log likelihood graph to compare the convergence property of each sampling schedule and calculated BLEU ( <ref type="bibr" target="#b14">Papineni et al., 2002</ref>) for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>We use the top-down token-based sampling al- gorithm of Chung et al. <ref type="formula" target="#formula_3">(2014)</ref> as our baseline. We use the same SCFG decoder for translation with both the baseline and the grammars sam- pled using our type-based MCMC sampler. The features included in our experiments are differ- ently normalized rule counts and lexical weight- ings ( <ref type="bibr" target="#b11">Koehn et al., 2003</ref>) of each rule. Weights are tuned using Pairwise Ranking Optimization (Hop- kins and May, 2011) using a grammar extracted by the standard heuristic method <ref type="bibr" target="#b1">(Chiang, 2007)</ref> and the development set. The same weights are used throughout our experiments. First we want to compare the DP likelihood of the baseline with our type-based MCMC sam- pler to see if type-based sampling would converge to a better sampling result. In order to verify if type-based MCMC really converges to a good op- timum point, we use simulated annealing <ref type="bibr" target="#b10">(Kirkpatrick et al., 1983</ref>) to search possible better opti- mum points. We sample from the real distribution modified by an annealing parameter β:</p><formula xml:id="formula_24">z ∼ P (z) β</formula><p>We increase our β from 0.1 to 1.3, and then de- crease from 1.3 to 1.0, changing by 0.1 every 3 iterations. We also run an inexact parallel ap- proximation of type-based MCMC in comparison with the non-parallel sampling to find out if par- allel programming is feasible to speed up type- based MCMC sampling without affecting the per- formance greatly. We do not compare the PYP likelihood because the approximation renders it impossible to calculate the real PYP likelihood. We also calculate the BLEU score to compare the grammars extracted using each sampling sched- ule. We just report the BLEU result of grammars sampled using PYP as for all our schedules, since PYP always performs better than DP.</p><p>As for parameter settings, we used d = 0.5 for the Pitman-Yor discount parameter. Though we have a separate PYP for each rule length, we used same α = 5 for all rule sizes in all experiments, including experiments using DP. For rule length probability, a Poisson distribution where λ = 2 was used for all experiments. <ref type="bibr">3</ref> For each sentence sample, we initialize all the nodes in the forest to be cut sites and choose an incoming edge for each node uniformly. For each experiment, we run for 160 iterations. For each DP experiment, we draw the log likelihood graph for each sampling schedule before it finally con- verges. For each PYP experiment, we tried aver- aging the grammars from every 10th iteration to construct a single grammar and use this grammar for decoding. We tune the number of grammars included for averaging by comparing the BLEU score on the dev set and report the BLEU score result on the test with the same averaging of gram- mars.</p><p>As each tree fragment sampled from the for- est represents a unique translation rule, we do not need to explicitly extract the rules; we merely need to collect them and count them. However, the fragments sampled include purely non-lexical rules that do not conform to the rule constraints of Hiero, and rules that are not useful for trans- lation. In order to get rid of this type of rule, we prune every rule that has scope <ref type="bibr" target="#b7">(Hopkins and Langmead, 2010)</ref> greater than two. Whereas Hi- ero does not allow two adjacent nonterminals in the source side, our pruning criterion allows some rules of scope two that are not allowed by Hiero. For example, the following rule (only source side shown) has scope two but is not allowed by Hiero: <ref type="figure" target="#fig_5">Figure 4</ref> shows the log likelihood result of our type-based MCMC sampling schedule and the baseline top-down sampling. We can see that type- based sampling converges to a much better re- sult than non-type-based top-down sampling. This shows that type-based MCMC escapes some local optima that are hard for token-based methods to escape. This further strengthens the idea that sam- pling a block of strongly coupled variables jointly <ref type="bibr">3</ref> The priors are the same as the work of <ref type="bibr" target="#b2">Chung et al. (2014)</ref>. The priors are set to be the same because other priors turn out not to affect much of the final performance and add additional difficulty for tuning. Log likelihood: type-based vs non-type-base vs simulated annealing type-based + simulated annealing type-based non-type-based Log likelihood: parallel type-based vs non-parallel type-based parallel type-based non-parallel type-based <ref type="figure">Figure 5</ref>: parallelization result for type-based MCMC helps solve the slow mixing problem of token- based sampling methods. Another interesting ob- servation is that, even though theoretically these two sampling methods should finally converge to the same point, in practice a worse sampling al- gorithm is prone to get trapped at local optima, and it will be hard for its Markov chain to es- cape it. We can also see from <ref type="figure" target="#fig_5">Figure 4</ref> that the log likelihood result only improves slightly using simulated annealing. One possible explanation is that the Markov chain has already converged to a very good optimum point with type-based sam- pling and it is hard to search for a better optimum. <ref type="figure">Figure 5</ref> shows the parallelization result of type- based MCMC sampling when we run the program on five processors. We can see from the graph that when running on five processors, the likelihood fi-Sampling Schedule iteration dev test Non-type-based averaged (0-90) 25.62 24.98 Type-based averaged (0-100) 25.88 25.20 Parallel Type-based averaged  25.75 25.04 <ref type="table">Table 1</ref>: Comparisons of BLEU score results nally converges to the same likelihood result as non-parallel type-based MCMC sampling. How- ever, when we use more processors, the likelihood eventually becomes lower than with non-parallel sampling. This is because when we increase the number of processors, we split the dataset into very small subsets. As we maintain the bookkeep- ing for each subset separately and do not com- municate the updates to each subset, the power of type-based sampling is weakened with bookkeep- ing for very few sites of each type. In the extreme case, when we use too many processors in parallel, the bookkeeping would have a singleton site for each type. In this case, the approximation would degrade to the scenario of approximating token- based sampling. By choosing a proper size of divi- sion of the dataset and by maintaining local book- keeping for each subset, the parallel approxima- tion can converge to almost the same point as non- parallel sampling. As shown in our experimental results, the speed up is very significant with the running time decreasing from thirty minutes per iteration to just seven minutes when running on five processors. Part of the speed up comes from the smaller bookkeeping since with fewer sites for each index, there is less mismatch or conflict of sites. <ref type="table">Table 1</ref> shows the BLEU score results for type- based MCMC and the baseline. For non-type- based top-down sampling, the best BLEU score re- sult on dev is achieved when averaging the gram- mars of every 10th iteration from the 0th to the 90th iteration, while our type-based method gets the best result by averaging over every 10th itera- tion from the 0th to the 100th iteration. We can see that the BLEU score on dev for type-based MCMC and the corresponding BLEU score on test are both better than the result for the non-type-based method, though not significantly. This shows that the better likelihood of our Markov Chain using type-based MCMC does result in better transla- tion.</p><formula xml:id="formula_25">X → w 1 X 1 X 2 w 2 X 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Results</head><p>We have also done experiments calculating the BLEU score result of the inexact parallel imple- mentation. We can see from <ref type="table">Table 1</ref> that, while the likelihood of the approximation does not change in comparison with the exact type-based MCMC, there is a gap between the BLEU score results. We think this difference might come from the incon- sistency of the grammars sampled by each proces- sor within each iteration, as they do not communi- cate the update within each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a novel type-based MCMC algo- rithm for sampling tree fragments from phrase de- composition forests. While the hidden tree struc- ture in our settings makes it difficult to maintain the constantly changing type information, we have come up with a compact way to store the type in- formation of variables and proposed efficient ways to update the bookkeeping index. Under the addi- tional hidden structure limitation, we have shown that type-based MCMC sampling still works and results in both better likelihood and BLEU score. We also came with techniques to speed up the type-based MCMC sampling schedule while not affecting the final sampling likelihood result. A re- maining issue with parallelization is the inconsis- tency of the grammar within an iteration between processors. One possible solution would be using better averaging methods instead of simply aver- aging over every few iterations. Another interest- ing extension for our methods would be to also de- fine types for the edge variables, and then sample both cut and edge types jointly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example word alignment, with boxes showing valid phrase pairs. In this example, all individual alignment points are also valid phrase pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of cut type: Consider the two nodes marked in bold, ([2 6], [1 6]), ([1 4], [4 7]). These two non-split nodes are internal to the same composed rule: X → X 1 X 2 X 3 , X 3 X 2 X 1. We keep these two sites with the same index. However, when we set the cut value of these two nodes to 1, as the rules immediately above and immediately below are different for these two sites, they are not of the same type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Log likelihood result of type-based MCMC sampling against non-type-based MCMC sampling, simulated annealing is used to verify if type-based MCMC converges to a good likelihood</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>3 :</head><label>3</label><figDesc></figDesc><table>return 
4: end if 
5: old = node.cut 
6: node.cut = 0 
7: r 1 = composed rule(parent) 
8: node.cut = 1 
9: r 2 = composed rule(parent) 
10: r 3 = composed rule(node) 
11: node.cut = old 
12: sites = 
13: for sites s ∈ index[r 1 ] do 

14: 

for sites s in rule rooted at s do 

15: 

if s of type (r 1 , r 2 , r 3 ) and no conflict 
then 

16: 

add s to sites 

17: 

end if 

18: 

end for 
19: end for 
20: for sites s ∈ index[r 3 ] do 

21: 

if s of type (r 1 , r 2 , r 3 ) and no conflict then 

22: 

add s to sites 

23: 

end if 
24: end for 
25: sample m according to Equation 11 
26: remove sites from index 
27: uniformly choose m in sites to be cut sites. 
28: add new cut sites to index 
</table></figure>

			<note place="foot" n="1"> The data are randomly sampled from various different sources (LDC2006E86, LDC2006E93, LDC2002E18, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T08, LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E26, LDC2005E83, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006E24, LDC2006E92, LDC2006E24) The language model is trained on the English side of entire data (1.65M sentences, which is 39.3M words.)</note>

			<note place="foot" n="2"> They are from newswire portion of NIST MT evaluation data from 2004, 2005, and 2006.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the assistance of Licheng Fang and Tagyoung Chung. This work was partially funded by NSF grant IIS-0910611.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Gibbs sampler for phrasal synchronous grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="782" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sampling tree fragments from forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielštefankovičdanielˇdanielštefankovič</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="203" to="229" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Blocked inference in Bayesian tree substitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inducing compact but accurate treesubstitution grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sampling alignment structure under a Bayesian translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A markov model of machine translation using non-parametric bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SCFG decoding without binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Langmead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="646" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Power law discounting for n-gram language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP&apos;10)</title>
		<meeting>IEEE International Conference on Acoustic, Speech, and Signal essing (ICASSP&apos;10)<address><addrLine>Dallas, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="5178" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimization by Simulated Annealing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="issue">4598</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-03</title>
		<meeting>NAACL-03<address><addrLine>Edmonton, Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Bayesian model for learning SCFGs with discontiguous rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Type-based mcmc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="573" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-02</title>
		<meeting>ACL-02<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Yor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="855" to="900" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian learning of a tree substitution grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
	<note>short paper</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian inference and efficient parsing for tree-adjoining grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elif</forename><surname>Yamangil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association of Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics. Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extracting synchronous grammar rules from wordlevel alignments in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-08</title>
		<meeting><address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
