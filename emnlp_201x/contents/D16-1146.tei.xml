<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifted Rule Injection for Relation Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
							<email>tdmeeste@intec.ugent.be</email>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -iMinds Ghent</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University College London London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lifted Rule Injection for Relation Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1389" to="1399"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach reg-ularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly , we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current successful methods for automated knowl- edge base construction tasks heavily rely on learned distributed vector representations ( <ref type="bibr" target="#b14">Nickel et al., 2012;</ref><ref type="bibr" target="#b18">Riedel et al., 2013;</ref><ref type="bibr" target="#b22">Socher et al., 2013;</ref><ref type="bibr" target="#b4">Chang et al., 2014;</ref><ref type="bibr" target="#b13">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b23">Toutanova et al., 2015;</ref><ref type="bibr" target="#b15">Nickel et al., 2015;</ref>. Although these mod- els are able to learn robust representations from large amounts of data, they often lack common- sense knowledge. Such knowledge is rarely explic- itly stated in texts but can be found in resources like PPDB ( <ref type="bibr" target="#b8">Ganitkevitch et al., 2013)</ref> or WordNet <ref type="bibr" target="#b12">(Miller, 1995)</ref>.</p><p>Combining neural methods with symbolic com- monsense knowledge, for instance in the form of implication rules, is in the focus of current research <ref type="bibr" target="#b20">(Rocktäschel et al., 2014;</ref><ref type="bibr" target="#b28">Wang et al., 2014;</ref><ref type="bibr" target="#b2">Bowman et al., 2015;</ref><ref type="bibr" target="#b29">Wang et al., 2015;</ref><ref type="bibr" target="#b24">Vendrov et al., 2016;</ref><ref type="bibr" target="#b9">Hu et al., 2016;</ref>. A recent approach <ref type="bibr" target="#b21">(Rocktäschel et al., 2015</ref>) regularizes entity-tuple and relation em- beddings via first-order logic rules. To this end, ev- ery first-order rule is propositionalized based on ob- served entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule ∀x : isMan(x) ⇒ isMortal(x) would result in a very large number of loss terms on a large database.</p><p>In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts. This is achieved by minimizing an upper bound of the loss that encourages the implication between rela- tions to hold, entirely independent from the num- ber of entity pairs. It only involves representa- tions of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space. In the example given above, if we require that every component of the vector representation of isMan is smaller than the corresponding component of relation isMortal, then we can show that the rule holds for any non- negative representation of an entity-tuple. Hence our method avoids the need for separate loss terms for every ground atom resulting from propositionaliz- ing rules. In statistical relational learning this type of approach is often referred to as lifted inference or learning <ref type="bibr" target="#b16">(Poole, 2003;</ref><ref type="bibr" target="#b3">Braz, 2007)</ref> because it deals with groups of random variables at a first-order level. In this sense our approach is a lifted form of rule injection. This allows for imposing large num- bers of rules while learning distributed representa- tions of relations and entity-tuples. Besides drasti- cally lower computation time, an important advan- tage of our method over <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> is that when these constraints are satisfied, the injected rules always hold, even for unseen but inferred facts. While the method presented here only deals with im- plications and not general first-order rules, it does not rely on the assumption of independence between relations, and is hence more generally applicable.</p><p>Our contributions are fourfold: (i) we develop a very efficient way of regularizing relation represen- tations to incorporate first-order logic implications ( §3), (ii) we reveal that, against expectation, map- ping entity-tuple embeddings to non-negative space does not hurt but instead improves the generaliza- tion ability of our model ( §5.1) (iii) we show im- provements on a knowledge base completion task by injecting mined commonsense rules from WordNet ( §5.3), and finally (iv) we give a qualitative analysis of the results, demonstrating that implication con- straints are indeed satisfied in an asymmetric way and result in a substantially increased structuring of the relation embedding space ( §5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section we revisit the matrix factorization re- lation extraction model by <ref type="bibr" target="#b18">Riedel et al. (2013)</ref> and introduce the notation used throughout the paper. We choose the matrix factorization model for its simplicity as the base on which we develop impli- cation injection. <ref type="bibr" target="#b18">Riedel et al. (2013)</ref> represent every relation r ∈ R (selected from Freebase ( <ref type="bibr" target="#b1">Bollacker et al., 2008)</ref> or extracted as textual surface pattern) by a k- dimensional latent representation r ∈ R k . A par- ticular relation instance or fact is the combination of a relation r and a tuple t of entities that are en- gaged in that relation, and is written as r, t. We write O as the set of all such input facts available for training. Furthermore, every entity-tuple t ∈ T is represented by a latent vector t ∈ R k (with T the set of all entity-tuples in O).</p><p>Model F by <ref type="bibr" target="#b18">Riedel et al. (2013)</ref> measures the compatibility between a relation r and an entity- tuple t using the dot product r t of their respec- tive vector representations. During training, the representations are learned such that valid facts re- ceive high scores, whereas negative ones receive low scores. Typically no negative evidence is available at training time, and therefore a Bayesian Personal- ized Ranking (BPR) objective <ref type="bibr" target="#b17">(Rendle et al., 2009)</ref> is used. Given a pair of facts f p := r p , t p ∈ O and f q := r q , t q ∈ O, this objective requires that</p><formula xml:id="formula_0">r p t p ≤ r q t q .<label>(1)</label></formula><p>The embeddings can be trained by minimizing a convex loss function R that penalizes violations of that requirement when iterating over the training set. In practice, each positive training fact r, t q is compared with a randomly sampled unobserved fact r, t p for the same relation. The overall loss can hence be written as</p><formula xml:id="formula_1">L R = r,tq∈O tp∈T , r,tp ∈O R r [t p − t q ] .<label>(2)</label></formula><p>and measures how well observed valid facts are ranked above unobserved facts, thus reconstructing the ranking of the training data. We will hence- forth call L R the reconstruction loss, to make a dis- tinction with the implication loss that we will intro- duce later. <ref type="bibr" target="#b18">Riedel et al. (2013)</ref> use the logistic loss R (s) := − log σ(−s), where σ(s) := (1 + e −x ) −1 denotes the sigmoid function. In order to avoid over- fitting, an L 2 regularization term on the r and t em- beddings is added to the reconstruction loss. The overall objective to minimize hence is</p><formula xml:id="formula_2">L F = L R + α r r 2 2 + t t 2 2 (3)</formula><p>where α is the regularization strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Lifted Injection of Implications</head><p>In this section, we show how an implication ∀t ∈ T : r p , t ⇒ r q , t,</p><p>can be imposed independently of the entity-tuples. For simplicity, we abbreviate such implications as r p ⇒ r q (e.g., professorAt ⇒ employeeAt).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Grounded Loss Formulation</head><p>The implication rule can be imposed by requiring that every tuple t ∈ T is at least as compatible with relation r p as with r q . Written in terms of the latent representations, eq. (4) therefore becomes</p><formula xml:id="formula_4">∀t ∈ T : r p t ≤ r q t<label>(5)</label></formula><p>If r p , t is a true fact with a high score r p t, and the fact r q , t has an even higher score, it must also be true, but not vice versa. We can therefore inject an implication rule by minimizing a loss term with a separate contribution from every t ∈ T , adding up to the total loss if the corresponding inequality is not satisfied. In order to make the contribution of every tuple t to that loss independent of the magni- tude of the tuple embedding, we divide both sides of the above inequality by t 1 . With˜tWith˜ With˜t := t/t 1 , the implication loss for the rule r p ⇒ r q can be written as</p><formula xml:id="formula_5">L I = ∀t∈T I [r p − r q ] ˜ t<label>(6)</label></formula><p>for an appropriate convex loss function I , similarly to eq. (2). In practice, the summation can be reduced to those tuples that occur in combination with r p or r q in the training data. Still, the propositionalization in terms of training facts leads to a heavy computa- tional cost for imposing a single implication, simi- lar to the technique introduced in <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref>. Moreover, with that simplification there is no guarantee that the implication between both re- lations would generalize towards inferred facts not seen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lifted Loss Formulation</head><p>The problems mentioned above can be avoided if instead of L I , a tuple-independent upper bound is minimized. Such a bound can be constructed, pro- vided all components of t are restricted to a non- negative embedding space, i.e., T ⊆ R k,+ . If this holds, Jensen's inequality allows us to transform eq. <ref type="formula" target="#formula_5">(6)</ref> as follows</p><formula xml:id="formula_6">L I = ∀t∈T I k i=1˜t i=1˜ i=1˜t i [r p − r q ] 1 i (7) ≤ k i=1 I [r p − r q ] 1 i ∀t∈T˜t ∀t∈T˜ ∀t∈T˜t i (8)</formula><p>where 1 i is the unit vector along dimension i in tuple-space. This is allowed because the</p><formula xml:id="formula_7">{ ˜ t i } k i=1</formula><p>form convex coefficients ( ˜ t i &gt; 0, and</p><formula xml:id="formula_8">i ˜ t i = 1)</formula><p>, and I is a convex function. If we define</p><formula xml:id="formula_9">L U I := k i=1 I [r p − r q ] 1 i (9)</formula><p>we can write</p><formula xml:id="formula_10">L I ≤ βL U I (10)</formula><p>in which β is an upper bound on t ˜ t i . One such bound is |T |, but others are conceivable too. In prac- tice we rescale β to a hyper-parameter˜βparameter˜ parameter˜β that we use to control the impact of the upper bound to the over- all loss. We call L U I the lifted loss, as it no longer depends on any of the entity-tuples; it is grounded over the unit tuples 1 i instead.</p><p>The implication r p ⇒ r q can thus be imposed by minimizing the lifted loss L U I . Note that by mini- mizing L U I , the model is encouraged to satisfy the constraint r p ≤ r q on the relation embeddings, where ≤ denotes the component-wise comparison. In fact, a sufficient condition for eq. <ref type="formula" target="#formula_4">(5)</ref> to hold, is r p ≤ r q and ∀t ∈ T : t ≥ 0 (11) with 0 the k-dimensional null vector. This corre- sponds to a single relation-specific loss term, and the general restriction T ⊆ R k,+ on the tuple- embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approximately Boolean Entity Tuples</head><p>In order to impose implications by minimizing a lifted loss L U I , the tuple-embedding space needs to be restricted to R k,+ . We have chosen to restrict the tuple space even more than required, namely to the hypercube t ∈ [0, 1] k , as approximately Boolean embeddings ( <ref type="bibr" target="#b11">Kruszewski et al., 2015)</ref>. The tuple embeddings are constructed from real-valued vec- tors e, using the component-wise sigmoid function</p><formula xml:id="formula_11">t = σ(e), e ∈ R k .<label>(12)</label></formula><p>For minimizing the loss, the gradients are hence computed with respect to e, and the L 2 regulariza- tion is applied to the components of e instead of t.</p><p>Other choices for ensuring the restriction t ≥ 0 in eq. <ref type="formula" target="#formula_0">(11)</ref> are possible, but we found that our ap- proach works better in practice than those (e.g., the exponential transformation proposed by <ref type="bibr" target="#b6">Demeester et al. (2016)</ref>). It can also be observed that the unit tuples over which the implication loss is grounded, form a special case of approximately Boolean em- beddings.</p><p>In order to investigate the impact of this restric- tion even when not injecting any rules, we introduce model FS: the original model F, but with sigmoidal entity-tuples:</p><formula xml:id="formula_12">L F S = r,tq∈O tp∈T , r,tp ∈O R r [σ(e p ) − σ(e q )] + α r r 2 2 + e e 2 2 (13)</formula><p>Here, e p and e q are the real-valued representations as in eq. (12), for tuples t p and t q , respectively. With the above choice of a non-negative tuple- embedding space we can now state the full lifted rule injection model (FSL):</p><formula xml:id="formula_13">L F SL = L F S + ˜ β I∈I L U I<label>(14)</label></formula><p>L U I denotes a lifted loss term for every rule in a set I of implication rules that we want to inject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Convex Implication Loss</head><p>The logistic loss R (see §2) is not suited for im- posing implications because once the inequality in eq. <ref type="formula" target="#formula_0">(11)</ref> is satisfied, the components of r p and r q do not need to be separated any further. However, with R this would continue to happen due to the small non-zero gradient. In the reconstruction loss L R this is a desirable effect which further separates the scores for positive from negative examples. How- ever, if an implication is imposed between two re- lations that are almost equivalent according to the training data, we still want to find almost equivalent embedding vectors. Hence, we propose to use the loss</p><formula xml:id="formula_14">I (s) = max(0, s + δ)<label>(15)</label></formula><p>with δ a small positive margin to ensure that the gra- dient does not disappear before the inequality is ac- tually satisfied. We use δ = 0.01 in all experiments.</p><p>The main advantage of the presented approach over earlier methods that impose the rules in a grounded way <ref type="bibr" target="#b21">(Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b29">Wang et al., 2015</ref>) is the computational efficiency of impos- ing the lifted loss. Evaluating L U I or its gradient for one implication rule is comparable to evaluating the reconstruction loss for one pair of training facts. In typical applications there are much fewer rules than training facts and the extra computation time needed to inject these rules is therefore negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Recent research on combining rules with learned vector representations has been important for new developments in the field of knowledge base com- pletion. <ref type="bibr" target="#b20">Rocktäschel et al. (2014)</ref> and <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> provided a framework to jointly maxi- mize the probability of observed facts and proposi- tionalized first-order logic rules. <ref type="bibr" target="#b29">Wang et al. (2015)</ref> demonstrated how different types of rules can be incorporated using an Integer Linear Programming approach. <ref type="bibr" target="#b27">Wang and Cohen (2016)</ref> learned em- beddings for facts and first-order logic rules using matrix factorization. Yet, all of these approaches ground the rules in the training data, limiting their scalability towards large rule sets and KBs with many entities. As argued in the introduction, this forms an important motivation for the lifted rule in- jection model put forward in this work, which by construction does not suffer from that limitation. <ref type="bibr" target="#b30">Wei et al. (2015)</ref> proposed an alternative strategy to tackle the scalability problem by reasoning on a fil- tered subset of grounded facts. <ref type="bibr" target="#b31">Wu et al. (2015)</ref> proposed to use a path ranking approach for capturing long-range interactions be- tween entities, and to add these as an extra loss term, besides the loss that models pairwise relations. Our model FSL differs substantially from their approach, in that we consider tuples instead of separate enti- ties, and we inject a given set of rules. Yet, by cre-ating a partial ordering in the relation embeddings as a result of injecting implication rules, model FSL can also capture interactions beyond direct relations. This will be demonstrated in §5.3 by injecting rules between surface patterns only and still measuring an improvement on predictions for structured Freebase relations.</p><p>Combining logic and distributed representations is also an active field of research outside of au- tomated knowledge base completion. Recent ad- vances include the work by <ref type="bibr" target="#b7">Faruqui et al. (2014)</ref>, who injected ontological knowledge from WordNet into word representations. Furthermore, <ref type="bibr" target="#b24">Vendrov et al. (2016)</ref> proposed to enforce a partial ordering in an embeddings space of images and phrases. Our method is related to such order embeddings since we define a partial ordering on relation embeddings. However, to ensure that implications hold for all entity-tuples we also need a restriction on the entity- tuple embedding space and derive bounds on the loss. Another important contribution is the recent work by <ref type="bibr" target="#b9">Hu et al. (2016)</ref>, who proposed a frame- work for injecting rules into general neural network architectures, by jointly training on the actual targets and on the rule-regularized predictions provided by a teacher network. Although quite different at first sight, their work could offer a way to use our model in various neural network architectures, by integrat- ing the proposed lifted loss into the teacher network.</p><p>This paper builds upon our previous workshop paper <ref type="bibr" target="#b6">(Demeester et al., 2016)</ref>. In that work, we tested different tuple embedding transforma- tions in an ad-hoc manner. We used approxi- mately Boolean representations of relations instead of entity-tuples, strongly reducing the model's de- grees of freedom. We now derive the FSL model from a carefully considered mathematical transfor- mation of the grounded loss. The FSL model only restricts the tuple embedding space, whereby rela- tion vectors remain real valued. Furthermore, previ- ous experiments were performed on small-scale ar- tificial datasets, whereas we now test on a real-world relation extraction benchmark.</p><p>Finally, we explicitly discuss the main differ- ences with respect to the strongly related work from <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref>. Their method is more gen- eral, as they cover a wide range of first-order logic rules, whereas we only discuss implications. Lifted rule injection beyond implications will be studied in future research contributions. However, albeit less general, our model has a number of clear advan- tages:</p><p>Scalability -Our proposed model of lifted rule injection scales according to the number of implica- tion rules, instead of the number of rules times the number of observed facts for every relation present in a rule.</p><p>Generalizability -Injected implications will hold even for facts not seen during training, because their validity only depends on the order relation im- posed on the relation representations. This is not guaranteed when training on rules grounded in train- ing facts by <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref>.</p><p>Training Flexibility -Our method can be trained with various loss functions, including the rank-based loss as used in <ref type="bibr" target="#b18">Riedel et al. (2013)</ref>. This was not possible for the model of <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> and already leads to an improved accuracy as seen from the zero-shot learning experiment in §5.2.</p><p>Independence Assumption -In Rocktäschel et al. <ref type="formula" target="#formula_0">(2015)</ref> an implication of the form a p ⇒ a q for two ground atoms a p and a q is modeled by the log- ical equivalence ¬(a p ∧ ¬a q ), and its probability is approximated in terms of the elementary proba- bilities π(a p ) and π(a q ) as 1 − π(a p )</p><p>1 − π(a q ) .</p><p>This assumes the independence of the two atoms a p and a q , which may not hold in practice. Our ap- proach does not rely on that assumption and also works for cases of statistical dependence. For ex- ample, the independence assumption does not hold in the trivial case where the relations r p and r q in the two atoms are equivalent, whereas in our model, the constraints r p ≤ r q and r p ≥ r q would simply reduce to r p = r q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>We now present our experimental results. We start by describing the experimental setup and hyperpa- rameters. Before turning to the injection of rules, we compare model F with model FS, and show that restricting the tuple embedding space has a regu- larization effect, rather than limiting the expressive- ness of the model ( §5.1 leads to an improved precision ( §5.3). We proceed with a visual illustration of the relation embeddings with and without injected rules ( §5.4), provide de- tails on time efficiency of the lifted rule injection method ( §5.5), and show that it correctly captures the asymmetry of implication rules ( §5.6).</p><p>All models were implemented in Tensor- Flow ( <ref type="bibr">Abadi et al., 2015)</ref>. We use the hyperparam- eters of <ref type="bibr" target="#b18">Riedel et al. (2013)</ref>, with k = 100 hidden dimensions and a weight of α = 0.01 for the L 2 regularization loss. We use ADAM <ref type="bibr" target="#b10">(Kingma and Ba, 2014</ref>) for optimization with an initial learning rate of 0.005 and a mini-batch size of 8192. The embeddings are initialized by sampling uniformly from [−0.1, 0.1] and we use˜βuse˜ use˜β = 0.1 for the implication loss throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Restricted Embedding Space</head><p>Before incorporating external commonsense knowl- edge into relation representations, we were curious how much we lose by restricting the entity-tuple space to approximately Boolean embeddings. We evaluate our models on the New York Times dataset introduced by <ref type="bibr" target="#b18">Riedel et al. (2013)</ref>. Surprisingly, we find that the expressiveness of the model does not suffer from this strong restriction. <ref type="table">From Table 1</ref> we see that restricting the tuple-embedding space seems to perform slightly better (FS) as opposed to a real- valued tuple-embedding space (F), suggesting that this restriction has a regularization effect that im- proves generalization. We also provide the original results for model F by <ref type="bibr" target="#b18">Riedel et al. (2013)</ref> (denoted as R13-F) for comparison. Due to a different im- plementation and optimization procedure, the results for our model F and R13-F are not identical.</p><p>Inspecting the top relations for a sampled dimen- sion in the embedding space reveals that the rela- tion space of model FS more closely resembles clus- ters than that of model F <ref type="table" target="#tab_1">(Table 2)</ref>. We hypothesize that this might be caused by approximately Boolean entity-tuple representations in model FS, resulting in attribute-like entity-tuple vectors that capture which relation clusters they belong to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Zero-shot Learning</head><p>The zero-shot learning experiment performed in <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> leads to an important find- ing: when injecting implications with right-hand sides for Freebase relations for which no or very lim- ited training facts are available, the model should be able to infer the validity of Freebase facts for those relations based on rules and correlations between textual surface patterns.</p><p>We inject the same hand-picked relations as used by <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref>, after removing all Freebase training facts. The lifted rule injection (model FSL) reaches a weighted MAP of 0.35, comparable with 0.38 by the Joint model from <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> (denoted R15-Joint). Note that for this experiment we initialized the Freebase relations implied by the rules with negative random vectors (sampled uniformly from [−7.9, −8.1]). The reason is that without any negative training facts for these relations, their components can only go up due to the implication loss, and we do not want to get values that are too high before optimization. <ref type="figure" target="#fig_0">Figure 1</ref> shows how the relation extraction perfor- mance improves when more Freebase relation train- ing facts are added. It effictively measures how well the proposed models, matrix factorization (F), propositionalized rule injection (R15-Joint), and our model (FSL), can make use of the provided rules and correlations between textual surface form pat- </p><note type="other">: Top patterns for a randomly sampled dimension in non-restricted and restricted embedding space .</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model F (non-restricted)</head><p>Model FS (restricted)</p><p>nsubj&lt;-represent-&gt;dobj rcmod-&gt;return-&gt;prep-&gt;to-&gt;pobj appos-&gt;member-&gt;prep-&gt;of-&gt;pobj-&gt;team-&gt;nn nn&lt;-return-&gt;prep-&gt;to-&gt;pobj nsubj&lt;-die-&gt;dobj nsubj&lt;-return-&gt;prep-&gt;to-&gt;pobj nsubj&lt;-speak-&gt;prep-&gt;about-&gt;pobj rcmod-&gt;leave-&gt;dobj appos-&gt;champion-&gt;poss nsubj&lt;-quit-&gt;dobj terns and increased fractions of Freebase training facts. Although FSL starts at a lower performance than R15-Joint when no Freebase training facts are present, it outperforms R15-Joint and a plain matrix factorization model by a substantial margin when provided with more than 7.5% of Freebase train- ing facts. This indicates that, in addition to being much faster than R15-Joint, it can make better use of provided rules and few training facts. We at- tribute this to the Bayesian personalized ranking loss instead of the logistic loss used in <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref>. The former is compatible with our rule- injection method, but not with the approach of max- imizing the expectation of propositional rules used by R15-Joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Injecting Knowledge from WordNet</head><p>The main purpose of this work is to be able to incorporate rules from external resources for aid- ing relation extraction. We use WordNet hyper- nyms to generate rules for the NYT dataset. To this end we iterate over all surface form patterns in the dataset and attempt to replace words in the pattern by their hypernyms. If the result- ing pattern is contained in the dataset, we gen- erate the corresponding rule. For instance, we generate a rule appos-&gt;diplomat-&gt;amod ⇒ appos-&gt;official-&gt;amod since both patterns are contained in the NYT dataset and we know from WordNet that a diplomat is an official. This leads to 427 rules from WordNet that we subsequently anno- tate manually to obtain 36 high-quality rules. Note that none of these rules directly imply a Freebase re- lation. Although the test relations all originate from Freebase, we still hope to see improvements by tran- sitive effects, i.e., better surface form representations that in turn help to predict Freebase facts.</p><p>We show results obtained by injecting these WordNet rules in <ref type="table">Table 1</ref> (column FSL). The weighted MAP measure increases by 2% with respect to model FS, and 4% compared to our reim- plementation of the matrix factorization model F. This demonstrates that imposing a partial ordering based on implication rules can be used to incorpo- rate logical commonsense knowledge and increase the quality of information extraction systems. Note that our evaluation setting guarantees that only indirect effects of the rules are measured, i.e., we do not use any rules directly implying test relations. This shows that injecting such rules influences the relation embedding space beyond only the relations explicitly stated in the rules. For example, injecting the rule appos&lt;-father-&gt;appos ⇒ poss&lt;-parent-&gt;appos can contribute to improved predictions for the test relation parent/child. Figure 2: Visualization of embeddings (columns) for the relations that appear in the high-quality Word- Net rules, (a) without and (b) with injection of these rules. Values range from -1 (orange) via 0 (white) to 1 (purple). Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualizing Relation Embeddings</head><p>We provide a visual inspection of how the structure of the relation embedding space changes when rules are imposed. We select all relations involved in the WordNet rules, and gather them as columns in a sin- gle matrix, sorted by increasing 1 norm (values in the 100 dimensions are similarly sorted). <ref type="figure">Figures 2a  and 2b</ref> show the difference between model F (with- out injected rules) and FSL (with rules). The val- ues of the embeddings in model FSL are more po- larized, i.e., we observe stronger negative or posi- tive components than for model F. Furthermore, FSL also reveals a clearer difference between the left- most (mostly negative, more specific) and right-most (predominantly positive, more general) embeddings (i.e., a clearer separation between positive and nega- tive values in the plot), which results from imposing the order relation in eq. (11) when injecting implica- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Efficiency of Lifted Injection of Rules</head><p>In order to get an idea of the time efficiency of in- jecting rules, we measure the time per epoch when restricting the program execution to a single 2.4GHz CPU core. We measure on average 6.33s per epoch without rules (model FS), against 6.76s and 6.97s when injecting the 36 high-quality WordNet rules and the unfiltered 427 rules (model FSL), respec- tively. Increasing the amount of injected rules from 36 to 427 leads to an increase of only 3% in compu- tation time, even though in our setup all rule losses are used in every training batch. This confirms the high efficiency of our lifted rule injection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Asymmetric Character of Implications</head><p>In order to demonstrate that injecting implications conserves their asymmetric nature, we perform the following experiment. After incorporating high- quality Wordnet rules r p ⇒ r q into model FSL we select all of the tuples t p that occur with relation r p in a training fact r p , t p . Matching these with re- lation r q should result in high values for the scores r q t p , if the implication holds. If however the tuples t q are selected from the training facts r q , t q , and matched with relation r p , the scores r p t q should be much lower if the inverse implication does not hold (in other words, if r q and r p are not equiva- lent). <ref type="table">Table 3</ref> lists the averaged results for 5 example rules, and the average over all relations in WordNet rules, both for the case with injected rules (model FSL), and without rules (model FS). For easier com- parison, the scores are mapped to the unit interval via the sigmoid function. This quantity σ(r t) is often interpreted as the probability that the corre- sponding fact holds ( <ref type="bibr" target="#b18">Riedel et al., 2013</ref>), but be- cause of the BPR-based training, only differences between scores play a role here. After injecting rules, the average scores of facts inferred by these rules (i.e., column σ(r q t p ) for model FSL) are al- ways higher than for facts (incorrectly) inferred by the inverse rules (column σ(r p t q ) for model FSL). In the fourth example, the inverse rule leads to high scores as well (on average 0.79, vs. 0.98 for the ac- tual rule). This is due to the fact that the daily and newspaper relations are more or less equivalent, such that the components of r p are not much below those of r q . For the last example (the ambassador ⇒ diplomat rule), the asymmetry in the implica- tion is maintained, although the absolute scores are rather low for these two relations.</p><p>The results for model FS reflect how strongly the implications in either direction are latently present in the training data. We can only conclude that model FS manages to capture the similarity be-rule model FSL model FS r p ⇒ r q σ(r q t p ) σ(r p t q ) σ(r q t p ) σ(r p t q ) appos-&gt;party-&gt;amod ⇒ appos-&gt;organization-&gt;amod 0.99 0.22 0.70 0.86 poss&lt;-father-&gt;appos ⇒ poss&lt;-parent-&gt;appos 0.96 0.00 0.72 0.89 appos-&gt;prosecutor-&gt;nn ⇒ appos-&gt;lawyer-&gt;nn 0.99 0.01 0.87 0.80 appos-&gt;daily-&gt;amod ⇒ appos-&gt;newspaper-&gt;amod 0.98 0.79 0.90 0.86 appos-&gt;ambassador-&gt;amod ⇒ appos-&gt;diplomat-&gt;amod  <ref type="table">Table 3</ref>: Average of σ(r q t) over all inferred facts r q , t p for tuples t p from training items for relation r p , and vice versa, for Wordnet implications r p ⇒ r q , and model FSL (injected rules) vs. model FS (no rules).</p><p>tween relations, but not the asymmetric character of implications. For example, purely based on the training data, it appears to be more likely that the parent relation implies the father relation, than vice versa. This again demonstrates the importance and added value of injecting external rules capturing commonsense knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a novel, fast approach for incorporat- ing first-order implication rules into distributed rep- resentations of relations. We termed our approach 'lifted rule injection', as it avoids the costly ground- ing of first-order implication rules and is thus inde- pendent of the size of the domain of entities. By construction, these rules are satisfied for any ob- served or unobserved fact. The presented approach requires a restriction on the entity-tuple embedding space. However, experiments on a real-world dataset show that this does not impair the expressiveness of the learned representations. On the contrary, it ap- pears to have a beneficial regularization effect.</p><p>By incorporating rules generated from WordNet hypernyms, our model improved over a matrix fac- torization baseline for knowledge base completion. Especially for domains where annotation is costly and only small amounts of training facts are avail- able, our approach provides a way to leverage exter- nal knowledge sources for inferring facts.</p><p>In future work, we want to extend the proposed ideas beyond implications towards general first- order logic rules. We believe that supporting con- junctions, disjunctions and negations would enable to debug and improve representation learning based knowledge base completion. Furthermore, we want to integrate these ideas into neural methods beyond matrix factorization approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Weighted MAP for injecting hand-picked rules as a function of the fraction of Freebase training facts. Comparison between model F (lowest, in blue), R15-Joint (middle, in green) and model FSL (highest, in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Research Founda-tion-Flanders (FWO), Ghent University-iMinds, Microsoft Research through its PhD Scholarship Programme, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xiaoqiang Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org</title>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan</editor>
		<meeting><address><addrLine>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner; Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and</addrLine></address></meeting>
		<imprint>
			<publisher>Fernanda Viégas</publisher>
		</imprint>
	</monogr>
	<note>Oriol Vinyals</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recursive neural networks can learn logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lifted First-order Probabilistic Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo De Salvo</forename><surname>Braz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Champaign, IL, USA. AAI3290183</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1568" to="1579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
		<title level="m">TensorLog: A Differentiable Deductive Database. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regularizing relation representations by first-order implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Automated Knowledge Base Construction (AKBC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ppdb: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06318</idno>
		<title level="m">Harnessing deep neural networks with logic rules</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deriving boolean structures from distributional vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06662</idno>
		<title level="m">Compositional vector space models for knowledge base completion</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A review of relational machine learning for knowledge graphs: From multirelational link prediction to automated knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00759</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">First-order probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 18th International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="985" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)<address><addrLine>Arlington, Virginia, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLTNAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning knowledge base inference with neural theorem provers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Automated Knowledge Base Construction (AKBC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-dimensional embeddings of logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Semantic Parsing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Injecting Logical Background Knowledge into Embeddings for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<idno>abs/1511.06361</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Sanja Fidler, and Raquel Urtasun</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Row-less universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Automated Knowledge Base Construction (AKBC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL)</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning first-order logic embeddings via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence (IJCAI 2015)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structure learning via parameter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1859" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale knowledge base completion: Inferring via grounding network sampling over selected instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengya</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhua</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured embedding via pairwise relations and long-range interactions in knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1663" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
