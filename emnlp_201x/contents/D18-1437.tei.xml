<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Hallucination in Image Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Object Hallucination in Image Captioning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="4035" to="4045"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Despite continuously improving performance, contemporary image captioning models are prone to &quot;hallucinating&quot; objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination , explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image caption-ing benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image captioning performance has dramatically improved over the past decade. Despite such impressive results, it is unclear to what extent captioning models actually rely on image con- tent: as we show, existing metrics fall short of fully capturing the captions' relevance to the im- age. In <ref type="figure">Figure 1</ref> we show an example where a competitive captioning model, Neural Baby Talk (NBT) ( <ref type="bibr" target="#b13">Lu et al., 2018)</ref>, incorrectly generates the object "bench." We refer to this issue as object hallucination.</p><p>While missing salient objects is also a failure mode, captions are summaries and thus generally * Denotes equal contribution. Figure 1: Image captioning models often "hallucinate" objects that may appear in a given context, like e.g. a bench here. Moreover, the sentence metrics do not al- ways appropriately penalize such hallucination. Our proposed metrics (CHAIRs and CHAIRi) reflect hallu- cination. For CHAIR lower is better.</p><p>not expected to describe all objects in the scene. On the other hand, describing objects that are not present in the image has been shown to be less preferable to humans. For example, the LSMDC challenge ( <ref type="bibr">Rohrbach et al., 2017a</ref>) documents that correctness is more important to human judges than specificity. In another study, ( <ref type="bibr">MacLeod et al., 2017</ref>) analyzed how visually impaired people re- act to automatic image captions. They found that people vary in their preference of either coverage or correctness. For many visually impaired who value correctness over coverage, hallucination is an obvious concern.</p><p>Besides being poorly received by humans, ob- ject hallucination reveals an internal issue of a cap- tioning model, such as not learning a very good representation of the visual scene or overfitting to its loss function.</p><p>In this paper we assess the phenomenon of object hallucination in contemporary captioning models, and consider several key questions. The first question we aim to answer is: Which mod- els are more prone to hallucination? We analyze this question on a diverse set of captioning models, spanning different architectures and learning ob- jectives. To measure object hallucination, we pro- pose a new metric, CHAIR (Caption Hallucination Assessment with Image Relevance), which cap- tures image relevance of the generated captions. Specifically, we consider both ground truth object annotations (MSCOCO Object segmentation ( <ref type="bibr" target="#b12">Lin et al., 2014)</ref>) and ground truth sentence annota- tions <ref type="bibr">(MSCOCO Captions (Chen et al., 2015)</ref>). In- terestingly, we find that models which score best on standard sentence metrics do not always hallu- cinate less.</p><p>The second question we raise is: What are the likely causes of hallucination? While hallucina- tion may occur due to a number of reasons, we believe the top factors include visual misclassifi- cation and over-reliance on language priors. The latter may result in memorizing which words "go together" regardless of image content, which may lead to poor generalization, once the test distri- bution is changed. We propose image and lan- guage model consistency scores to investigate this issue, and find that models which hallucinate more tend to make mistakes consistent with a language model. Finally, we ask: How well do the standard metrics capture hallucination? It is a common practice to rely on automatic sentence metrics, e.g. <ref type="bibr">CIDEr (Vedantam et al., 2015)</ref>, to evaluate captioning performance during development, and few employ human evaluation to measure the fi- nal performance of their models. As we largely rely on these metrics, it is important to under- stand how well they capture the hallucination phe- nomenon. In <ref type="figure">Figure 1</ref> we show how two sen- tences, from NBT with hallucination and from TopDown model ( <ref type="bibr" target="#b1">Anderson et al., 2018</ref>) -with- out, are scored by the standard metrics. As we see, hallucination is not always appropriately pe- nalized. We find that by using additional ground truth data about the image in the form of object la- bels, our metric CHAIR allows us to catch discrep- ancies that the standard captioning metrics cannot fully capture. We then investigate ways to assess object hallucination risk with the standard metrics. Finally, we show that CHAIR is complementary to the standard metrics in terms of capturing human preference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Caption Hallucination Assessment</head><p>We first introduce our image relevance metric, CHAIR, which assesses captions w.r.t. objects that are actually in an image. It is used as a main tool in our evaluation. Next we discuss the notions of image and language model consistency, which we use to reason about the causes of hallucination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The CHAIR Metric</head><p>To measure object hallucination, we propose the CHAIR (Caption Hallucination Assessment with Image Relevance) metric, which calculates what proportion of words generated are actually in the image according to the ground truth sentences and object segmentations. This metric has two vari- ants: per-instance, or what fraction of object in- stances are hallucinated (denoted as CHAIRi), and per-sentence, or what fraction of sentences include a hallucinated object (denoted as CHAIRs):</p><formula xml:id="formula_0">CHAIR i = |{hallucinated objects}| |{all objects mentioned}| CHAIR s = |{sentences with hallucinated object}| |{ all sentences}|</formula><p>For easier analysis, we restrict our study to the 80 MSCOCO objects which appear in the MSCOCO segmentation challenge. To determine whether a generated sentence contains halluci- nated objects, we first tokenize each sentence and then singularize each word. We then use a list of synonyms for MSCOCO objects (based on the list from <ref type="bibr" target="#b13">Lu et al. (2018)</ref>) to map words (e.g., "player") to MSCOCO objects (e.g., "per- son"). Additionally, for sentences which in- clude two word compounds (e.g., "hot dog") we take care that other MSCOCO objects (in this case "dog") are not incorrectly assigned to the list of MSCOCO objects in the sentence. For each ground truth sentence, we determine a list of MSCOCO objects in the same way. The MSCOCO segmentation annotations are used by simply relying on the provided object labels.</p><p>We find that considering both sources of an- notation is important. For example, MSCOCO contains an object "dining table" annotated with segmentation maps. However, humans refer to many different kinds of objects as "table" (e.g., "coffee table" or "side table"), though these ob- jects are not annotated as they are not specifically "dining table". By using sentence annotations to scrape ground truth objects, we account for vari- ation in how human annotators refer to different objects. Inversely, we find that frequently humans will not mention all objects in a scene. Qualita- tively, we observe that both annotations are impor- tant to capture hallucination. Empirically, we ver- ify that using only segmentation labels or only ref- erence captions leads to higher hallucination (and practically incorrect) rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Consistency</head><p>We define a notion of image consistency, or how consistent errors from the captioning model are with a model which predicts objects based on an image alone. To measure image consistency for a particular generated word, we train an image model and record P (w|I) or the probability of pre- dicting the word given only the image. To score the image consistency of a caption we use the av- erage of P (w|I) for all MSCOCO objects, where higher values mean that errors are more consis- tent with the image model. Our image model is a multi-label classification model with labels corre- sponding to MSCOCO objects (labels determined the same way as is done for CHAIR) which shares the visual features with the caption models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Language Consistency</head><p>We also introduce a notion of language consis- tency, i.e. how consistent errors from the cap- tioning model are with a model which predicts words based only on previously generated words. We train an LSTM (Hochreiter and Schmidhu- ber, 1997) based language model which pre- dicts a word w t given previous words w 0:t−1 on MSCOCO data. We report language consis- tency as 1/R(w t ) where R(w t ) is the rank of the predicted word in the language model. Again, for a caption we report average rank across all MSCOCO objects in the sentence and higher lan- guage consistency implies that errors are more consistent with the language model.</p><p>We illustrate image and language consistency in <ref type="figure" target="#fig_1">Figure 2</ref>, i.e. the hallucination error ("fork") is more consistent with the Language Model predic- tions than with the Image Model predictions. We use these consistency measures in Section 3.3 to help us investigate the causes of hallucination.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section we present the findings of our study, where we aim to answer the questions posed in Section 1: Which models are more prone to hal- lucination? What are the likely causes of halluci- nation? How well do the standard metrics capture hallucination?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Captioning Models</head><p>We compare object hallucination across a wide range of models. We define two axes for compari- son: model architecture and learning objective.</p><p>Model architecture. Regarding model architec- ture, we consider models both with and without attention mechanisms. In this work, we use "at- tention" to refer to any mechanism which learns to focus on different image regions, whether im- age regions be determined by a high level feature map, or by object proposals from a trained de- tector. All models are end-to-end trainable and use a recurrent neural network (LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) in our case) to output text. For non-attention based methods we consider the FC model from <ref type="bibr">Rennie et al. (2017)</ref> which incorporates visual information by initializing the LSTM hidden state with high level image features. We also consider LRCN ( <ref type="bibr" target="#b5">Donahue et al., 2015)</ref> which considers visual information at each time step, as opposed to just initializing the LSTM hid- den state with extracted features.</p><p>For attention based models, we consider Att2In ( <ref type="bibr">Rennie et al., 2017)</ref>, which is similar to the original attention based model proposed by ( <ref type="bibr" target="#b16">Xu et al., 2015)</ref>, except the image feature is only input into the cell gate as this was shown to lead to better performance. We then consider the attention model proposed by <ref type="bibr" target="#b1">(Anderson et al., 2018</ref>) which proposes a specific "top-down at- tention" LSTM as well as a "language" LSTM.</p><p>Generally attention mechanisms operate over high level convolutional layers. The attention mecha- nism from <ref type="bibr" target="#b1">(Anderson et al., 2018)</ref> can be used on such feature maps, but Anderson et al. also con- sider feature maps corresponding to object pro- posals from a detection model. We consider both models, denoted as TopDown (feature map ex- tracted from high level convolutional layer) and TopDown-BB (feature map extracted from object proposals from a detection model). Finally, we consider the recently proposed Neural Baby Talk (NBT) model ( <ref type="bibr" target="#b13">Lu et al., 2018</ref>) which explicitly uses object detections (as opposed to just bound- ing boxes) for sentence generation.</p><p>Learning objective. All of the above models are trained with the standard cross entropy (CE) loss as well as the self-critical (SC) loss pro- posed by <ref type="bibr">Rennie et al. (2017)</ref> (with an exception of NBT, where only the CE version is included). The SC loss directly optimizes the CIDEr metric with a reinforcement learning technique. We ad- ditionally consider a model trained with a GAN loss ( <ref type="bibr">Shetty et al., 2017</ref>) (denoted GAN), which applies adversarial training to obtain more diverse and "human-like" captions, and their respective non-GAN baseline with the CE loss.</p><p>TopDown deconstruction. To better evaluate how each component of a model might influ- ence hallucination, we "deconstruct" the Top- Down model by gradually removing components until it is equivalent to the FC model. The interme- diate networks are NoAttention, in which the atten- tion mechanism is replaced by mean pooling, No- Conv in which spatial feature maps are not input into the network (the model is provided with fully connected feature maps), SingleLayer in which only one LSTM is included in the model, and fi- nally, instead of inputting visual features at each time step, visual features are used to initialize the LSTM embedding as is done in the FC model. By deconstructing the TopDown model in this way, we ensure that model design choices and hyperpa- rameters do not confound results.</p><p>Implementation details. All the baseline mod- els employ features extracted from the fourth layer of ResNet-101 ( <ref type="bibr" target="#b6">He et al., 2016)</ref>, except for the GAN model which employs ResNet-152. Mod- els without attention traditionally use fully con- nected layers as opposed to convolutional layers. However, as ResNet-101 does not have intermedi- ate fully connected layers, it is standard to average pool convolutional activations and input these fea- tures into non-attention based description models. Note that this means the difference between the NoAttention and NoConv model is that the NoAt- tention model learns a visual embedding of spatial feature maps as opposed to relying on pre-pooled feature maps. All models except for TopDown- BB, NBT, and GAN are implemented in the same open source framework from <ref type="bibr" target="#b14">Luo et al. (2018)</ref>. <ref type="bibr">1</ref> Training/Test splits. We evaluate the captioning models on two MSCOCO splits. First, we con- sider the split from <ref type="bibr" target="#b9">Karpathy et al. (Karpathy and Fei-Fei, 2015)</ref>, specifically in that case the mod- els are trained on the respective Karpathy Training set, tuned on Karpathy Validation set and the re- ported numbers are on the Karpathy Test set. We also consider the Robust split, introduced in ( <ref type="bibr" target="#b13">Lu et al., 2018)</ref>, which provides a compositional split for MSCOCO. Specifically, it is ensured that the object pairs present in the training, validation and test captions do not overlap. In this case the cap- tioning models are trained on the Robust Training set, tuned on the Robust Validation set and the re- ported numbers are on the Robust Test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Which Models Are More Prone To</head><p>Hallucination?</p><p>We first present how well competitive models perform on our proposed CHAIR metric (Ta- ble 1). We report CHAIR at sentence-level and at instance-level <ref type="table">(CHs and CHi in the table)</ref>. In general, we see that models which perform bet- ter on standard evaluation metrics, perform bet- ter on CHAIR, though this is not always true. In particular, models which optimize for CIDEr fre- quently hallucinate more. Out of all generated captions on the Karpathy Test set, anywhere be- tween 7.4% and 17.7% include a hallucinated ob- ject. When shifting to more difficult training sce- narios in which new combinations of objects are seen at test time, hallucination consistently in- creases <ref type="table" target="#tab_2">(Table 2)</ref>. Karpathy Test set. <ref type="table">Table 1</ref> presents object hal- lucination on the Karpathy Test set. All sentences are generated using beam search and a beam size of 5. We note a few important trends. First, mod- els with attention tend to perform better on the CHAIR metric than models without attention. As we explore later, this is likely because they have  decreases, implying that the GAN loss actually helps decrease hallucination. Unlike the self crit- ical loss, the GAN loss encourages sentences to be human-like as opposed to optimizing a metric. Human-like sentences are not likely to hallucinate objects, and a hallucinated object is likely a strong signal to the discriminator that a sentence is gen- erated, and is not from a human. We also assess the effect of beam size on CHAIR. We find that generally beam search de- creases hallucination. We use beam size of 5, and for all models trained with cross entropy, it out- performs lower beam sizes on CHAIR. However, when training models with the self-critical loss, beam size sometimes leads to worse performance on CHAIR. For example, on the Att2In model trained with SC loss, a beam size of 5 leads to 12.8 on CHAIRs and 8.7 on CHAIRi, while a beam size of 1 leads to 10.8 on CHAIRs and 8.1 on CHAIRi.</p><p>Robust Test set. Next we review the hallucina- tion behavior on the Robust Test set (  hallucination is more critical in scenarios where test examples can not be assumed to have the same distribution as train examples. We again note that attention is helpful for decreasing hallucination. We note that the NBT model actually has lower hallucination scores on the robust split. This is in part because when generating sentences we use the detector outputs provided by <ref type="bibr" target="#b13">Lu et al. (2018)</ref>. Separate detectors on the Karpathy test and robust split are not available and the detector has access to images in the robust split during training. Con- sequently, the comparison between NBT and other models is not completely fair, but we include the number for completeness.</p><p>In addition to the Robust Test set, we also con- sider a set of MSCOCO in which certain ob- jects are held out, which we call the Novel Ob- ject split <ref type="bibr" target="#b7">(Hendricks et al., 2016)</ref>. We train on the training set outlined in ( <ref type="bibr" target="#b7">Hendricks et al., 2016)</ref> and test on the Karpathy test split, which includes ob- jects unseen during training. Similarly to the Ro- bust Test set, we see hallucination increase sub- stantially on this split. For example, for the Top- Down model hallucination increases from 8.4% to 12.1% for CHAIRs and 6.0% to 9.1% for CHAIRi.</p><p>We find no obvious correlation between the av- erage length of the generated captions and the hal- lucination rate. Moreover, vocabulary size does not correlate with hallucination either, i.e. mod- els with more diverse descriptions may actually hallucinate less. We notice that hallucinated ob- jects tend to be mentioned towards the end of the sentence (on average at position 6, with average sentence length 9), suggesting that some of the preceding words may have triggered hallucination. We investigate this below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Which objects are hallucinated and in what context?</head><p>Here we analyze which MSCOCO ob- jects tend to be hallucinated more often and what are the common preceding words and image con- text. Across all models the super-category Fur- niture is hallucinated most often, accounting for 20 − 50% of all hallucinated objects. Other com- mon super-categories are Outdoor objects, Sports and Kitchenware. On the Robust Test set, Ani- mals are often hallucinated. The dining table is the most frequently hallucinated object across all models (with an exception of GAN, where person is the most hallucinated object). We find that often words like "sitting" and "top" precede the "din- ing table" hallucination, implying the two com- mon scenarios: a person "sitting at the table" and an object "sitting on top of the table" <ref type="figure" target="#fig_1">(Figure 3,  row 1, examples 1, 2)</ref>. Similar observations can be made for other objects, e.g. word "kitchen" of- ten precedes "sink" hallucination ( <ref type="figure" target="#fig_2">Figure 3</ref>, row 1, example 3) and "laying" precedes "bed" <ref type="figure" target="#fig_2">(Fig- ure 3, row 1, example 4)</ref>. At the same time, if we look at which objects are actually present in the image (based on MSCOCO object annotations), we can similarly identify that presence of a "cat" co-occurs with hallucinating a "laptop" <ref type="figure" target="#fig_2">(Figure 3</ref>, row 2, example 1), a "dog" -with a "chair" <ref type="figure" target="#fig_1">(Fig- ure 3, row 2, example 2)</ref> etc. In most cases we observe that the hallucinated objects appear in the relevant scenes (e.g. "surfboard" on a beach), but there are cases where objects are hallucinated out of context (e.g. "bed" in the bathroom, <ref type="figure" target="#fig_2">Figure 3</ref>, row 1, example 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">What Are The Likely Causes Of</head><p>Hallucination?</p><p>In this section we investigate the likely causes of object hallucination. We have earlier described how we deconstruct the TopDown model to en- able a controlled experimental setup. We rely on the deconstructed TopDown models to analyze the impact of model components on hallucination. First, we summarize the hallucination analysis on the deconstructed TopDown models <ref type="table" target="#tab_5">(Table 3)</ref>. Interestingly, the NoAttention model does not do substantially worse than the full model (w.r.t. sen- tence metrics and CHAIR). However, removing Conv input (NoConv model) and relying only on FC features, decreases the performance dramati- cally. This suggests that much of the gain in at- tention based models is primarily due to access to feature maps with spatial locality, not the actual attention mechanism. Also, similar to LRCN vs. FC in <ref type="table">Table 1</ref>, initializing the LSTM hidden state with image features, as opposed to inputting image features at each time step, leads to lower halluci- nation (Single Layer vs. FC). This is somewhat surprising, as a model which has access to image information at each time step should be less likely to "forget" image content and hallucinate objects. However, it is possible that models which include image inputs at each time step with no access to spatial features overfit to the visual features.</p><p>Now we investigate what causes hallucination using the deconstructed TopDown models and the image consistency and language consistency scores, introduced in Sections 2.2 and 2.3 which capture how consistent the hallucinations errors are with image-/ language-only models.   <ref type="figure" target="#fig_3">Figure 4</ref> shows the CHAIR metric, image con- sistency and language consistency for the decon- structed TopDown models on the Karpathy Test set (left) and the Robust Test set (right). We note that models with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make er- rors consistent with the language model. This im- plies that models with less hallucination are bet- ter at integrating knowledge from an image into the sentence generation process. When looking at the Robust Test set, <ref type="figure" target="#fig_3">Figure 4</ref> (right), which is more challenging, as we have shown earlier, we see that image consistency decreases when com- paring to the same models on the Karpathy split, whereas language consistency is similar across all models trained on the Robust split. This is perhaps because the Robust split contains novel composi- tions of objects at test time, and all of the models are heavily biased by language.</p><p>Finally, we measure image and language con- sistency during training for the FC model and note that at the beginning of training errors are more consistent with the language model, whereas to- wards the end of training, errors are more con- sistent with the image model. This suggests that models first learn to produce fluent language be- fore learning to incorporate visual information. capture hallucination. All three metrics do penal- ize sentences for mentioning incorrect words, ei- ther via an F score (METEOR and SPICE) or co- sine distance (CIDEr). However, if a caption men- tions enough words correctly, it can have a high METEOR, SPICE, or CIDEr score while still hal- lucinating specific objects. Our first analysis tool is the TD-Restrict model. This is a modification of the TopDown model, where we enforce that MSCOCO objects which are not present in an image are not generated in the caption. We determine which words refer to objects absent in an image following our approach in Section 2.1. We then set the log probability for such words to a very low value. We generate sen- tences with the TopDown and TD-Restrict model with beam search of size 1, meaning all words pro- duced by both models are the same, until the Top- Down model produces a hallucinated word.</p><p>We compare which scores are assigned to such captions in <ref type="figure" target="#fig_4">Figure 5</ref>. TD-Restrict generates cap- tions that do not contain hallucinated objects, while TD hallucinates a "cat" in both cases. In <ref type="figure" target="#fig_4">Figure 5</ref> (left) we see that CIDEr scores the more correct caption much lower. In <ref type="figure" target="#fig_4">Figure 5</ref> (right), the TopDown model incorrectly calls the animal a "cat." Interestingly, it then correctly identifies the "frisbee," which the TD-Restrict model fails to mention, leading to lower SPICE and CIDEr.</p><p>In <ref type="table" target="#tab_7">Table 4</ref> we compute Pearson correlation co- efficient between individual sentence scores and   We further analyze the metrics in terms of their predictiveness of hallucination risk. Predictive- ness means that a certain score should imply a cer- tain percentage of hallucination. Here we show the results for SPICE and the captioning models FC and TopDown. For each model and a score in- terval (e.g. 10 − 20) we compute the percentage of captions without hallucination (1−CHAIRs). We plot the difference between the percentages from both models (TopDown -FC) in <ref type="figure" target="#fig_5">Figure 6</ref>. Com- paring the models, we note that even when scores are similar (e.g., all sentences with SPICE score in the range of 10 − 20), the TopDown model has fewer sentences with hallucinated objects. We see similar trends across other metrics. Consequently, object hallucination can not be always predicted based on the traditional sentence metrics.</p><p>Is CHAIR complementary to standard met- rics? In order to measure usefulness of our pro- posed metrics, we have conducted the following   human evaluation (via the Amazon Mechanical Turk). We have randomly selected 500 test images and respective captions from 5 models: non-GAN baseline, GAN, NBT, TopDown and TopDown - Self Critical. The AMT workers were asked to score the presented captions w.r.t. the given image based on their preference. They could score each caption from 5 (very good) to 1 (very bad). We did not use ranking, i.e. different captions could get the same score; each image was scored by three annotators, and the average score is used as the fi- nal human score. For each image we consider the 5 captions from all models and their correspond- ing sentence scores (METEOR, CIDEr, SPICE). We then compute Pearson correlation between the human scores and sentence scores; we also con- sider a simple combination of sentence metrics and 1-CHAIRs or 1-CHAIRi by summation. The final correlation is computed by averaging across all 500 images. The results are presented in Ta- ble 5. Our findings indicate that a simple combi- nation of CHAIRs or CHAIRi with the sentence metrics leads to an increased correlation with the human scores, showing the usefulness and com- plementarity of our proposed metrics.</p><p>Does hallucination impact generation of other words? Hallucinating objects impacts sentence quality not only because an object is predicted in- correctly, but also because the hallucinated word impacts generation of other words in the sen- tence. Comparing the sentences generated by Top- Down and TD-Restrict allows us to analyze this phenomenon. We find that after the hallucinated word is generated, the following words in the sen- tence are different 47.3% of the time. This im- plies that hallucination impacts sentence quality beyond simply naming an incorrect object. We ob- serve that one hallucination may lead to another, e.g. hallucinating a "cat" leading to hallucinating a "chair", hallucinating a "dog" -to a "frisbee".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this work we closely analyze hallucination in object captioning models. Our work is similar to other works which attempt to characterize flaws of different evaluation metrics ( <ref type="bibr" target="#b10">Kilickaya et al., 2016)</ref>, though we focus specifically on halluci- nation. Likewise, our work is related to other work which aims to build better evaluation tools <ref type="bibr" target="#b3">((Vedantam et al., 2015)</ref>, <ref type="bibr" target="#b0">(Anderson et al., 2016)</ref>, <ref type="bibr" target="#b4">(Cui et al., 2018)</ref>). However, we focus on carefully quantifying and characterizing one important type of error: object hallucination. A significant number of objects are hallucinated in current captioning models (between 5.5% and 13.1% of MSCOCO objects). Furthermore, hal- lucination does not always agree with the output of standard captioning metrics. For instance, the popular self critical loss increases CIDEr score, but also the amount of hallucination. Addition- ally, we find that given two sentences with similar CIDEr, SPICE, or METEOR scores from two dif- ferent models, the number of hallucinated objects might be quite different. This is especially appar- ent when standard metrics assign a low score to a generated sentence. Thus, for challenging cap- tion tasks on which standard metrics are currently poor (e.g., the LSMDC dataset ( <ref type="bibr">Rohrbach et al., 2017b)</ref>), the CHAIR metric might be helpful to tease apart the most favorable model. Our results indicate that CHAIR complements the standard sentence metrics in capturing human preference.</p><p>Additionally, attention lowers hallucination, but it appears that much of the gain from attention models is due to access to the underlying convo- lutional features as opposed the attention mecha- nism itself. Furthermore, we see that models with stronger image consistency frequently hallucinate fewer objects, suggesting that strong visual pro- cessing is important for avoiding hallucination.</p><p>Based on our results, we argue that the de- sign and training of captioning models should be guided not only by cross-entropy loss or standard sentence metrics, but also by image relevance. Our CHAIR metric gives a way to evaluate the phe- nomenon of hallucination, but other image rele- vance metrics e.g. those that incorporate missed salient objects, should also be investigated. We believe that incorporating visual information in the form of ground truth objects in a scene (as opposed to only reference captions) helps us better under- stand the performance of captioning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>NBT: A woman talking on a cell phone while sitting on a bench. CIDEr: 0.87, METEOR: 0.23, SPICE: 0.22, CHs: 1.00, CHi: 0.33 TopDown: A woman is talking on a cell phone. CIDEr: 0.54, METEOR: 0.26, SPICE: 0.13, CHs: 0.00, CHi: 0.00</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of image and language consistency. The hallucination error ("fork") is more consistent with the Language Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of object hallucination from two state-of-the-art captioning models, TopDown and NBT, see Section 3.2.</figDesc><graphic url="image-7.png" coords="6,309.14,170.83,94.86,71.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Image and Language model consistency (IM, LM) and CHAIRi (instance-level, CHi) on deconstructed TopDown models. Images with less hallucination tend to make errors consistent with the image model, whereas models with more hallucination tend to make errors consistent with the language model, see Section 3.3.</figDesc><graphic url="image-13.png" coords="7,72.00,62.93,225.41,101.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of how TopDown (TD) sentences change when we enforce that objects cannot be hallucinated: SPICE (S), Meteor (M), CIDEr (C), see Section 3.4.</figDesc><graphic url="image-17.png" coords="8,312.73,157.61,207.38,175.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Difference in percentage of sentences with no hallucination for TopDown and FC models when SPICE scores fall into specific ranges. For sentences with low SPICE scores, the hallucination is generally larger for the FC model, even though the SPICE scores are similar, see Section 3.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Metric</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Hallucination Analysis on the Robust Test set:</head><label>2</label><figDesc></figDesc><table>Spice (S), CIDEr (C) and METEOR (M) scores across dif-
ferent image captioning models as well as CHAIRs (sen-
tence level, CHs) and CHAIRi (instance level, CHi). * are 
trained/evaluated within the same implementation (Luo et al., 
2018),  † are trained/evaluated with implementation publicly 
released with corresponding papers. All models trained with 
cross-entropy loss. See Section 3.2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 )</head><label>2</label><figDesc>. For almost all models the hallucination increases on the Robust split (e.g. for TopDown from 8.4% to 11.4% of sentences), indicating that the issue of</figDesc><table>TopDown: A pile of luggage sitting on top of a table. 
NBT: Several pieces of luggage sitting on a table. 

TopDown: A group of people sitting around a 
table with laptops. 
NBT: A group of people sitting around a table 
with laptop. 

TopDown: A couple of cats laying on top of a bed. 
NBT: A couple of cats laying on top of a bed. 

TopDown: A kitchen with a stove and a sink. 
NBT: A kitchen with a stove and a sink. 

TopDown: Aa man and a woman are playing 
with a frisbee. 
NBT: A man riding a skateboard down a street. 

TopDown: A cat sitting on top of a laptop 
computer. 
NBT: A cat sitting on a table next to a computer. 

TopDown: A small b 
of a chair. 
NBT: A brown dog s 
chair. 

TopDown: A brown dog sitting on top of a chair. 
NBT: A brown and white dog sitting under an 
umbrella. 

TopDown: A man standing on a beach holding a 
surfboard. 
NBT: A man standing on top of a sandy beach. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Hallucination analysis on deconstructed TopDown 

models with sentence metrics, CHAIRs (sentence level, CHs) 
and CHAIRi (instance level, CHi). See Section 3.3. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 : Pearson correlation coefficients between 1-CHs and</head><label>4</label><figDesc></figDesc><table>CIDEr, METEOR, and SPICE scores, see Section 3.4. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Pearson correlation coefficients between individ-

ual/combined metrics and human scores. See Section 3.4. 

</table></figure>

			<note place="foot" n="1"> https://github.com/ruotianluo/ self-critical.pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to evaluate image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Re-evaluating automatic metrics for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Kilickaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Nazli Ikizler-Cinbis, and Erkut Erdem</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminability objective for training descriptive captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
