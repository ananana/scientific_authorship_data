<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Segment to Segment Neural Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Segment to Segment Neural Transduction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1307" to="1316"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginaliza-tion of the latent segmentation during training , and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of mapping from one sequence to an- other is an importance challenge of natural language processing. Common applications include machine translation and abstractive sentence summarisation. Traditionally this type of problem has been tackled by a combination of hand-crafted features, align- ment models, segmentation heuristics, and language models, all of which are tuned separately.</p><p>The recently introduced encoder-decoder paradigm has proved very successful for machine translation, where an input sequence is encoded into a fixed-length vector and an output sequence is then decoded from said vector <ref type="bibr" target="#b13">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Cho et al., 2014</ref>). This architecture is appealing, as it makes it possible to tackle the problem of sequence- to-sequence mapping by training a large neural network in an end-to-end fashion. However it is difficult for a fixed-length vector to memorize all the necessary information of an input sequence, especially for long sequences. Often a very large encoding needs to be employed in order to capture the longest sequences, which invariably wastes capacity and computation for short sequences. While the attention mechanism of <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> goes some way to address this issue, it still requires the full input to be seen before any output can be produced.</p><p>In this paper we propose an architecture to tackle the limitations of the vanilla encoder-decoder model, a segment to segment neural transduction model (SSNT) that learns to generate and align simul- taneously. Our model is inspired by the HMM word alignment model proposed for statistical ma- chine translation ( <ref type="bibr" target="#b26">Vogel et al., 1996</ref>; <ref type="bibr" target="#b24">Tillmann et al., 1997</ref>); we impose a monotone restriction on the alignments but incorporate recurrent dependencies on the input which enable rich locally non-monotone alignments to be captured. This is similar to the se- quence transduction model of <ref type="bibr" target="#b8">Graves (2012)</ref>, but we propose alignment distributions which are parame- terised separately, making the model more flexible and allowing online inference.</p><p>Our model introduces a latent segmentation which determines correspondences between tokens of the input sequence and those of the output sequence. The aligned hidden states of the encoder and de- coder are used to predict the next output token and to calculate the transition probability of the alignment. We carefully design the input and output RNNs such that they independently update their respective hid- den states. This enables us to derive an exact dy- namic programme to marginalize out the hidden segmentation during training and an efficient beam search to generate online the best alignment path to- gether with the output sequence during decoding. Unlike previous recurrent segmentation models that only capture dependencies in the input ( <ref type="bibr" target="#b6">Graves et al., 2006;</ref><ref type="bibr" target="#b15">Kong et al., 2016</ref>), our segmentation model is able to capture unbounded dependencies in both the input and output sequences while still permitting polynomial inference.</p><p>While attentive models treat the attention weights as output of a deterministic function, our model as- signs attention weights to a sequential latent variable which can be marginalized out. Our model is gen- eral and could be incorporated into any RNN-based encoder-decoder architecture, such as Neural Turing Machines ( <ref type="bibr" target="#b7">Graves et al., 2014</ref>), memory networks ( <ref type="bibr" target="#b16">Kumar et al., 2016)</ref> or stack- based networks ( <ref type="bibr" target="#b9">Grefenstette et al., 2015)</ref>, enabling such models to process data online.</p><p>We conduct experiments on two different trans- duction tasks, abstractive sentence summarisation (sequence to sequence mapping at word level) and morphological inflection generation (sequence to se- quence mapping at character level). We evaluate our proposed algorithms in both the online setting, where the input is encoded with a unidirectional LSTM, and where the whole input is available such that it can be encoded with a bidirectional network. The experimental results demonstrate the effective- ness of SSNT -it consistently output performs the baseline encoder-decoder approach while requir- ing significantly smaller hidden layers, thus show- ing that the segmentation model is able to learn to break one large transduction task into a series of smaller encodings and decodings. When bidirec- tional encodings are used the segmentation model outperforms an attention-based benchmark. Quali- </p><formula xml:id="formula_0">&lt;/s&gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>h i n e s e m a r k e t s c l o s e d f o r p u b l i c h o l i d a y .</head><p>&lt; / s &gt; <ref type="figure">Figure 1</ref>: Example output of our recurrent segmenta- tion model on the task of abstractive sentence sum- marisation. The path highlighted is the alignment found by the model during decoding.</p><p>tative analysis shows that the alignments found by our model are highly intuitive and demonstrates that the model learns to read ahead the required number of tokens before producing output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Let x I 1 be the input sequence of length I and y J 1 the output sequence of length J. Let y j denote the j- th token of y. Our goal is to model the conditional distribution</p><formula xml:id="formula_1">p(y|x) = J j=1 p(y j |y j−1 1 , x).<label>(1)</label></formula><p>We introduce a hidden alignment sequence a J 1 where each a j = i corresponds to an input position i ∈ {1, . . . , I} that we want to focus on when gener- ating y j . Then p(y|x) is calculated by marginalizing over all the hidden alignments,</p><formula xml:id="formula_2">p(y|x) = a p(y, a|x) (2) ≈ a J j=1 p(a j |a j−1 , y j−1 1 , x) transition probability · p(y j |y j−1 1 , a j , x).</formula><p>word prediction <ref type="figure">Figure 1</ref> illustrates the model graphically. Each path from the top left node to the right-most column in the graph corresponds to an alignment. We con- strain the alignments to be monotone, i.e. only for- ward and downward transitions are permitted at each point in the grid. This constraint enables the model to learn to perform online generation. Additionally, the model learns to align input and output segments, which means that it can learn local reorderings by memorizing phrases. Another possible constraint on the alignments would be to ensure that the entire in- put sequence is consumed before last output word is emitted, i.e. all valid alignment paths have to end in the bottom right corner of the grid. However, we do not enforce this constraint in our setup.</p><p>The probability contributed by an alignment is ob- tained by accumulating the probability of word pre- dictions at each point on the path and the transition probability between points. The transition probabil- ities and the word output probabilities are modeled by neural networks, which are described in detail in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Probabilities of Output Word Predictions</head><p>The input sentence x is encoded with a Recur- rent Neural Network (RNN), in particular an LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>). The encoder can either be a unidirectional or bidirectional LSTM. If a unidirectional encoder is used the model is able to read input and generate output symbols online. The hidden state vectors are computed as</p><formula xml:id="formula_3">h → i = RNN(h → i−1 , v (e) (x i )),<label>(3)</label></formula><formula xml:id="formula_4">h ← i = RNN(h ← i+1 , v (e) (x i )),<label>(4)</label></formula><p>where v (e) (x i ) denotes the vector representation of the token x, and h → i and h ← i are the forward and backward hidden states, respectively. For a bidi- rectional encoder, they are concatenated as</p><formula xml:id="formula_5">h i = [h → i ; h ← i ]</formula><p>; and for unidirectional encoder h i = h → i . The hidden state s j of the RNN for the output se- quence y is computed as</p><formula xml:id="formula_6">s j = RNN(s j−1 , v (d) (y j−1 )),<label>(5)</label></formula><p>where v (d) (y j−1 ) is the encoded vector of the pre- viously generated output word y j−1 . That is, s j en- codes y j−1 1 . To calculate the probability of the next word, we concatenate the aligned hidden state vectors s j and h a j and feed the result into a softmax layer,</p><formula xml:id="formula_7">p(y j = l|y j−1 1 , a j , x) = p(y j = l|h a j , s j ) = softmax(W w [h a j ; s j ] + b w ) l . (6)</formula><p>The word output distribution in Graves <ref type="formula" target="#formula_1">(2012)</ref> is pa- rameterised in similar way. <ref type="figure">Figure 2</ref> illustrates the model structure. Note that the hidden states of the input and output decoders are kept independent to permit tractable inference, while the output distributions are conditionally dependent on both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transition Probabilities</head><p>As the alignments are constrained to be monotone, we can treat the transition from timestep j to j +1 as a sequence of shift and emit operations. Specif- ically, at each input position, a decision of shift or emit is made by the model; if the operation is emit then the next output word is generated; other- wise, the model will shift to the next input word. While the multinomial distribution is an alternative for parameterising alignments, the shift/emit param- eterisation does not place an upper limit on the jump size, as a multinomial distribution would, and biases the model towards shorter jump sizes, which a multi- nomial model would have to learn.</p><p>We describe two methods for modelling the align- ment transition probability. The first approach is in- dependent of the input or output words. To parame- terise the alignment distribution in terms of shift and emit operations we use a geometric distribution,</p><formula xml:id="formula_8">p(a j |a j−1 ) = (1 − e) a j −a j−1 e,<label>(7)</label></formula><p>where e is the emission probability. This transition probability only has one parameter e, which can be</p><formula xml:id="formula_9">x 3 x 2 x 1 s 1 h 1 &lt;s&gt; y 1 y 2 y 3 y 1</formula><p>Figure 2: The structure of our model. (x 1 , x 2 , x 3 ) and (y 1 , y 2 , y 3 ) denote the input and output se- quences, respectively. The points, e.g. (i, j), in the grid represent an alignment between x i and y j . For each column j, the concatenation of the hidden states</p><formula xml:id="formula_10">[h i , s j ] is used to predict y j .</formula><p>estimated directly by maximum likelihood as</p><formula xml:id="formula_11">e = n J n n I n + n J n ,<label>(8)</label></formula><p>where I n and J n are the lengths of the input and out- put sequences of training example n, respectively. For the second method we model the transition probability with a neural network,</p><formula xml:id="formula_12">p(a 1 = i) = i−1 d=1 (1 − p(e d,1 ))p(e i,1 ), p(a j = i|a j−1 = k) = i−1 d=k (1 − p(e d,j ))p(e i,j ),<label>(9)</label></formula><p>where p(e i,j ) denotes the probability of emit for the alignment a j = i. This probability is obtained by feeding [h i ; s j ] into a feed forward neural network,</p><formula xml:id="formula_13">p(e i,j ) = σ(MLP(W t [h i ; s j ] + b t )).<label>(10)</label></formula><p>For simplicity, p(a j = i|a j−1 = k, s j , h i k ) is abbre- viated as p(a j = i|a j−1 = k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training and Decoding</head><p>Since there are an exponential number of possi- ble alignments, it is computationally intractable to explicitly calculate every p(y, a|x) and then sum them to get the conditional probability p(y|x). We instead approach the problem using a dynamic- programming algorithm similar to the forward- backward algorithm for HMMs <ref type="bibr" target="#b20">(Rabiner, 1989)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>For an input x and output y, the forward variable α(i, j) = p(a j = i, y j 1 |x). The value of α(i, j) is computed by summing over the probabilities of ev- ery path that could lead to this cell. Formally, α(i, j) is defined as follows:</p><p>For i ∈ <ref type="bibr">[1, I]</ref>:</p><formula xml:id="formula_14">α(i, 1) = p(a 1 = i)p(y 1 |h i , s 1 ).<label>(11)</label></formula><formula xml:id="formula_15">For j ∈ [2, J], i ∈ [1, I]:</formula><formula xml:id="formula_16">α(i, j) = p(y j |h i , s j )· (12) i k=1 α(k, j − 1)p(a j = i|a j−1 = k).</formula><p>The backward variables, defined as</p><formula xml:id="formula_17">β(i, j) = p(y J j+1 |a j = i, y j 1 , x), are computed as: For i ∈ [1, I]:</formula><formula xml:id="formula_18">β(i, J) = 1.<label>(13)</label></formula><p>For</p><formula xml:id="formula_19">j ∈ [1, J − 1], i ∈ [1, I]:</formula><formula xml:id="formula_20">β(i, j) = I k=i p(a j+1 = k|a j = i)β(k, j + 1)· p(y j+1 |h k , s j+1 ).<label>(14)</label></formula><p>During training we estimate the parameters by minimizing the negative log likelihood of the train- ing set S:</p><formula xml:id="formula_21">L(θ) = − (x,y)∈S log p(y|x; θ) = − (x,y)∈S log I i=1 α(i, J).<label>(15)</label></formula><p>Let θ j be the neural network parameters w.r.t. the model output at position j. The gradient is computed as:</p><formula xml:id="formula_22">∂ log p(y|x; θ) ∂θ = J j=1 I i=1 ∂ log p(y|x; θ) ∂α(i, j) ∂α(i, j) ∂θ j .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1310</head><p>The derivative w.r.t. the forward weights is</p><formula xml:id="formula_23">∂ log p(y|x; θ) ∂α(i, j) = β(i, j) p(y|x; θ) .<label>(17)</label></formula><p>The derivative of the forward weights w.r.t. the model parameters at position j is</p><formula xml:id="formula_24">∂α(i, j) ∂θ j = ∂p(y j |h i , s j ) ∂θ j α(i, j) p(y j |h i , s j ) + p(y j |h i , s j ) i k=1 α(j − 1, k) ∂ ∂θ j p(a j =i|a j−1 =k).<label>(18)</label></formula><p>For the geometric distribution transition probabil-</p><formula xml:id="formula_25">ity model ∂ ∂θ j p(a j = i|a j−1 = k) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DP search algorithm</head><p>Input: source sentence x Output: best output sentence y * Initialization:</p><formula xml:id="formula_26">Q ∈ R I×Jmax , bp ∈ N I×Jmax , W ∈ N I×Jmax , I end , J end . for i ∈ [1, I] do Q[i, 1] ← max y∈V p(a 1 = i)p(y|h i , s 1 ) bp[i, 1] ← 0 W [i, 1] ← arg max y∈V p(a 1 = i)p(y|h i , s 1 ) end for for j ∈ [2, J max ] do for i ∈ [1, I] do Q[i, j] ← max y∈V,k∈[1,i] Q[k, j − 1]· p(a j = i|a j−1 = k)p(y|h i , s j ) bp[i, j], W [i, j] ← arg max y∈V,k∈[1,i] · Q[k, j − 1]p(a j = i|a j−1 = k)p(y|h i , s j ) end for I end ← arg max i Q[i, j] if W [I end , j] = EOS then J end ← j break end if</formula><p>end for return a sequence of words stored in W by fol- lowing backpointers starting from (I end , J end ).</p><p>For decoding, we aim to find the best output se- quence y * for a given input sequence x:</p><formula xml:id="formula_27">y * = arg max y p(y|x)<label>(19)</label></formula><p>The search algorithm is based on dynamic program- ming ( <ref type="bibr" target="#b24">Tillmann et al., 1997</ref>). The main idea is to create a path probability matrix Q, and fill each cell Q[i, j] by recursively taking the most probable path that could lead to this cell. We present the greedy search algorithm in Algorithm 1. We also imple- mented a beam search that tracks the k best partial sequences at position (i, j). The notation bp refers to backpointers, W stores words to be predicted, V denotes the output vocabulary, J max is the maximum length of the output sequences that the model is al- lowed to predict.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the effectiveness of our model on two representative natural language processing tasks, sentence compression and morphological inflection. The primary aim of this evaluation is to assess whether our proposed architecture is able to outper- form the baseline encoder-decoder model by over- coming its encoding bottleneck. We further bench- mark our results against an attention model in order to determine whether our alternative alignment strat- egy is able to provide similar benefits while process- ing the input online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Abstractive Sentence Summarisation</head><p>Sentence summarisation is the task of generating a condensed version of a sentence while preserv- ing its meaning. In abstractive sentence summari- sation, summaries are generated from the given vo- cabulary without the constraint of copying words in the input sentence. <ref type="bibr" target="#b22">Rush et al. (2015)</ref> compiled a data set for this task from the annotated Gigaword data set ( <ref type="bibr" target="#b5">Graff et al., 2003;</ref><ref type="bibr" target="#b18">Napoles et al., 2012)</ref>, where sentence-summary pairs are obtained by pair- ing the headline of each article with its first sentence.  <ref type="table">Table 1</ref>: ROUGE F1 scores on the sentence sum- marisation test set. Seq2seq refers to the vanilla encoder-decoder and attention denotes the attention- based model. SSNT denotes our model with align- ment transition probability modelled as geometric distribution. SSNT+ refers to our model with tran- sition probability modelled using neural networks. The prefixes uni-and bi-denote using unidirectional and bidirectional encoder LSTMs, respectively. and summaries are 50 and 25, respectively.</p><p>2. For each sentence-summary pair, the product of the input and output lengths should be no greater than 500.</p><p>We use the filtered 172k pairs for validation and sample 1m pairs for training. While this training set is smaller than that used in previous work (and there- fore our results cannot be compared directly against reported results), it serves our purpose for evaluat- ing our algorithm against sequence to sequence and attention-based approaches under identical data con- ditions. Following from previous work <ref type="bibr" target="#b22">(Rush et al., 2015;</ref><ref type="bibr" target="#b2">Chopra et al., 2016;</ref><ref type="bibr" target="#b10">Gülçehre et al., 2016)</ref>, we report results on a randomly sampled test set of 2000 sentence-summary pairs. The quality of the generated summaries are evaluated by three ver- sions of ROUGE for different match lengths, namely ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring). For training, we use Adam (Kingma and Ba, 2015) for optimization, with an initial learning rate of 0.001. The mini-batch size is set to 32. The number of hidden units H is set to 256 for both our model and the baseline models, and dropout of 0.2 is applied to the input of LSTMs. All hyperparameters were optimised via grid search on the perplexity of the validation set. We use greedy decoding to gener- ate summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Configuration Perplexity</p><p>Seq2seq <ref type="table">Table 2</ref>: Perplexity on the validation set with 172k sentence-summary pairs. <ref type="table">Table 1</ref> displays the ROUGE-F1 scores of our models on the test set, together with baseline mod- els, including the attention-based model. Our models achieve significantly better results than the vanilla encoder-decoder and outperform the attention-based model. The fact that SSNT+ per- forms better is in line with our expectations, as the neural network-parameterised alignment model is more expressive than that modelled by geometric distribution.</p><formula xml:id="formula_28">H = 128, L = 1 48.5 H = 256, L = 1 35.6 H = 256, L = 2 32.1 H = 256, L = 3 31.0 biSSNT+ H = 128, L = 1 26.7 H = 256, L = 1 22.6</formula><p>To make further comparison, we experimented with different sizes of hidden units and adding more layers to the baseline encoder-decoder. <ref type="table">Table 2</ref> lists the configurations of different models and their cor- responding perplexities on the validation set. We can see that the vanilla encoder-decoder tends to get bet- ter results by adding more hidden units and stacking more layers. This is due to the limitation of com- pressing information into a fixed-size vector. It has to use larger vectors and deeper structure in order to memorize more information. By contrast, our model can do well with smaller networks. In fact, even with 1 layer and 128 hidden units, our model works much better than the vanilla encoder-decoder with 3 layers and 256 hidden units per layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Morphological Inflection</head><p>Morphological inflection generation is the task of predicting the inflected form of a given lexical item based on a morphological attribute. The transforma- tion from a base form to an inflected form usually in- cludes concatenating it with a prefix or a suffix and substituting some characters. For example, the in- flected form of a German stem abgang is abgängen when the case is dative and the number is plural.</p><p>In our experiments, we use the same dataset as     Our model is trained separately for each type of inflection, the same setting as the factored model described in <ref type="bibr" target="#b4">Faruqui et al. (2016)</ref>. The model is trained to predict the character sequence of the in- flected form given that of the stem. The output is evaluated by accuracies of string matching. For all the experiments on this task we use 128 hidden units for the LSTMs and apply dropout of 0.5 on the input and output of the LSTMs. We use Adam ( <ref type="bibr" target="#b14">Kingma and Ba, 2015)</ref> for optimisation with initial learning rate of 0.001. During decoding, beam search is em- ployed with beam size of 30. <ref type="table" target="#tab_2">Table 3</ref> gives the average accuracy of the uniSSNT+, biSSNT+, vanilla encoder-decoder, and attention-based models. The model with the best previous average result -denoted as adapted- seq2seq (FTND16) <ref type="bibr" target="#b4">(Faruqui et al., 2016</ref>) -is also included for comparison. Our biSSNT+ model out- performs the vanilla encoder-decoder by a large margin and almost matches the state-of-the-art result on this task. As mentioned earlier, a characteristic of these datasets is that the stems and their corre-  <ref type="table">Table 4</ref>: Comparison of the performance of our model (biSSNT+) against the previous state-of-the- art on each morphological inflection dataset.</p><p>sponding inflected forms mostly overlap. Compare to the vanilla encoder-decoder, our model is better at copying and finding correspondences between pre- fix, stem and suffix segments. <ref type="table">Table 4</ref> compares the results of biSSNT+ and pre- vious models on each individual dataset. DDN13 and NCK15 denote the models of Durrett and DeN- ero (2013) and <ref type="bibr" target="#b19">Nicolai et al. (2015)</ref>, respectively. Both models tackle the task by feature engineering. FTND16 <ref type="bibr" target="#b4">(Faruqui et al., 2016</ref>) adapted the vanilla encoder-decoder by feeding the i-th character of the encoded string as an extra input into the i-th position of the decoder. It can be considered as a special case of our model by forcing a fixed diagonal alignment between input and output sequences. Our model achieves comparable results to these models on all the datasets. Notably it outperforms other models on the Finnish noun and adjective, and verbs datasets, whose stems and inflected forms are the longest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Alignment Quality</head><p>Figure 3 presents visualisations of segment align- ments generated by our model for sample instances from both tasks. We see that the model is able to learn the correct correspondences between segments of the input and output sequences. For instance, the alignment follows a nearly diagonal path for the ex- ample in <ref type="figure" target="#fig_4">Figure 3c</ref>, where the input and output se- quences are identical. In <ref type="figure" target="#fig_4">Figure 3b</ref>, it learns to add the prefix 'ge' at the start of the sequence and replace 'en' with 't' after copying 'zock'. We observe that the model is robust on long phrasal mappings. As  shown in <ref type="figure" target="#fig_4">Figure 3a</ref>, the mapping between 'the wall street journal asia, the asian edition of the us-based business daily' and 'wall street journal asia' demon- strates that our model learns to ignore phrasal mod- ifiers containing additional information. We also find some examples of word reordering, e.g., the phrase 'industrial production in france' is reordered as 'france industrial output' in the model's predicted output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our work is inspired by the seminal HMM align- ment model ( <ref type="bibr" target="#b26">Vogel et al., 1996;</ref><ref type="bibr" target="#b24">Tillmann et al., 1997</ref>) proposed for machine translation. In contrast to that work, when predicting a target word we addi- tionally condition on all previously generated words, which is enabled by the recurrent neural models. This means that the model also functions as a con- ditional language model. It can therefore be applied directly, while traditional models have to be com- bined with a language model through a noisy chan- nel in order to be effective. Additionally, instead of EM training on the most likely alignments at each iteration, our model is trained with direct gradient descent, marginalizing over all the alignments.</p><p>Latent variables have been employed in neural network-based models for sequence labelling tasks in the past. Examples include connectionist tem- poral classification (CTC) ( <ref type="bibr" target="#b6">Graves et al., 2006</ref>) for speech recognition and the more recent segmental recurrent neural networks (SRNNs) ( <ref type="bibr" target="#b15">Kong et al., 2016)</ref>, with applications on handwriting recogni- tion and part-of-speech tagging. Weighted finite- state transducers (WFSTs) have also been aug- mented to encode input sequences with bidirectional LSTMs ( <ref type="bibr" target="#b21">Rastogi et al., 2016)</ref>, permitting exact in- ference over all possible output strings. While these models have been shown to achieve appealing per- formance on different applications, they have com- mon limitations in terms of modelling dependencies between labels. It is not possible for CTCs to model explicit dependencies. SRNNs and neural WFSTs model fixed-length dependencies, making it is diffi- cult to carry out effective inference as the dependen- cies become longer.</p><p>Our model shares the property of the sequence transduction model of <ref type="bibr" target="#b8">Graves (2012)</ref> in being able to model unbounded dependencies between output tokens via an output RNN. This property makes it possible to apply our model to tasks like summarisa- tion and machine translation that require the tokens in the output sequence to be modelled highly depen- dently. Graves (2012) models the joint distribution over outputs and alignments by inserting null sym- bols (representing shift operations) into the output sequence. During training the model uses dynamic programming to marginalize over permutations of the null symbols, while beam search is employed during decoding. In contrast our model defines a separate latent alignment variable, which adds flex- ibility to the way the alignment distribution can be defined (as a geometric distribution or parameterised by a neural network) and how the alignments can be constrained, without redefining the dynamic pro- gram. In addition to marginalizing during training, our decoding algorithm also makes use of dynamic programming, allowing us to use either no beam or small beam sizes.</p><p>Our work is also related to the attention- based models first introduced for machine transla- tion ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. <ref type="bibr" target="#b17">Luong et al. (2015)</ref> proposed two alternative attention mechanisms: a global method that attends all words in the input sen- tence, and a local one that points to parts of the input words. Another variation on this theme are pointer networks ( <ref type="bibr" target="#b25">Vinyals et al., 2015)</ref>, where the outputs are pointers to elements of the variable-length in- put, predicted by the attention distribution. <ref type="bibr">Jaitly et al. (2016)</ref> propose an online sequence to sequence model with attention that conditions on fixed-sized blocks of the input sequence and emits output tokens corresponding to each block. The model is trained with alignment information to generate supervised segmentations.</p><p>Although our model shares the same idea of joint training and aligning with the attention-based mod- els, our design has fundamental differences and ad- vantages. While attention-based models treat the at- tention weights as output of a deterministic func- tion (soft-alignment), in our model the attention weights correspond to a hidden variable, that can be marginalized out using dynamic programming. Fur- ther, our model's inherent online nature permits it the flexibility to use its capacity to chose how much input to encode before decoding each segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have proposed a novel segment to segment neu- ral transduction model that tackles the limitations of vanilla encoder-decoders that have to read and mem- orize an entire input sequence in a fixed-length con- text vector before producing any output. By intro- ducing a latent segmentation that determines corre- spondences between tokens of the input and output sequences, our model learns to generate and align jointly. During training, the hidden alignment is marginalized out using dynamic programming, and during decoding the best alignment path is gener- ated alongside the predicted output sequence. By employing a unidirectional LSTM as encoder, our model is capable of doing online generation. Exper- iments on two representative natural language pro- cessing tasks, abstractive sentence summarisation and morphological inflection generation, showed that our model significantly outperforms encoder- decoder baselines while requiring much smaller hid- den layers. For future work we would like to incor- porate attention-based models to our framework to enable such models to process data online.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Faruqui et al.</head><label></label><figDesc>(2016). This dataset was originally created by Durrett and DeNero (2013) from Wik- tionary, containing inflections for German nouns (de-N), German verbs (de-V), Spanish verbs (es- V), Finnish noun and adjective (fi-NA), and Finnish verbs (fi-V). It was further expanded by Nicolai et al. (2015) by adding Dutch verbs (nl-V) and French verbs (fr-V). The number of inflection types for each language ranges from 8 to 57. The number of base forms, i.e. the number of instances in each dataset, ranges from 2000 to 11200. The predefined split is 200/200 for dev and test sets, and the rest of the data for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example alignments found by BiSSNT+. Highlighted grid cells represent the correspondence between the input and output tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average accuracy over all the morpho-
logical inflection datasets. The baseline results for 
Seq2Seq variants are taken from (Faruqui et al., 
2016). 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Chris Dyer, Karl Moritz Hermann, Ed-ward Grefenstette, Tomáš Kˇocisk´yKˇocisk´Kˇocisk´y, Gabor Melis, Yishu Miao and many others for their helpful com-ments. The first author is funded by EPSRC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>English gigaword. Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>abs/1410.5401</idno>
		<title level="m">Neural turing machines. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1819" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1603.08148</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Samy Bengio. 2016. A neural transducer. In Proceedings of NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIR</title>
		<meeting>ICIR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Segmental recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inflection generation as discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence R Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A DP-based search using monotone alignments in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HMM-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
