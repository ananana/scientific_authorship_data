<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering and Computer Science</orgName>
								<orgName type="institution">Queen Mary University of London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="912" to="918"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An exciting outcome of research at the intersection of language and vision is that of zero-shot learning (ZSL). ZSL promises to scale visual recognition by borrowing distributed semantic models learned from linguistic corpora and turning them into visual recognition models. However the popular word-vector DSM embeddings are relatively impoverished in their expressivity as they model each word as a single vector point. In this paper we explore word-distribution embeddings for ZSL. We present a visual-linguistic mapping for ZSL in the case where words and visual categories are both represented by distributions. Experiments show improved results on ZSL benchmarks due to this better exploiting of intra-concept variability in each modality</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning vector representations of word meaning is a topical area in computational linguistics. Based on the distributional hypothesis <ref type="bibr" target="#b8">(Harris, 1954)</ref> -that words in similar context have similar meanings - distributed semantic models (DSM)s build vector representations based on corpus-extracted context. DSM approaches such as topic models ( <ref type="bibr">Blei et al., 2003)</ref>, and more recently neural networks <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013)</ref> have had great success in a variety of lexical and semantic tasks ( <ref type="bibr" target="#b1">Arora et al., 2015;</ref><ref type="bibr" target="#b14">Schwenk, 2007)</ref>.</p><p>However despite their successes, classic DSMs are severely impoverished compared to humans due to learning solely from word cooccurrence without grounding in the outside world. This has motivated a wave of recent research into multi-modal and cross- modal learning that aims to ground DSMs in non- linguistic modalities ( <ref type="bibr">Kiela and Bottou, 2014;</ref><ref type="bibr">Silberer and Lapata, 2014;</ref><ref type="bibr">?)</ref>. Such multi-modal DSMs are attractive because they learn richer representations than language-only models (e.g., that bananas are yellow fruit ( <ref type="bibr" target="#b2">Bruni et al., 2012b)</ref>), and thus often outperform language only models in various lexical tasks <ref type="bibr" target="#b2">(Bruni et al., 2012a)</ref>.</p><p>In this paper, we focus on a key unique and prac- tically valuable capability enabled by cross-modal DSMs: that of zero-shot learning (ZSL). Zero-shot recognition aims to recognise visual categories in the absence of any training examples by cross-modal transfer from language. The idea is to use a lim- ited set of training data to learn a linguistic-visual mapping and then apply the induced function to map images from novel visual categories (unseen during training) to a linguistic embedding: thus enabling recognition in the absence of visual training exam- ples. ZSL has generated big impact <ref type="bibr" target="#b10">(Lampert et al., 2009;</ref><ref type="bibr" target="#b15">Socher et al., 2013;</ref><ref type="bibr" target="#b10">Lazaridou et al., 2014</ref>) due to the potential of leveraging language to help visual recognition scale to many categories without labor intensive image annotation.</p><p>DSMs typically generate vector embeddings of words, and hence ZSL is typically realised by vari- ants of vector-valued cross-modal regression. How- ever, such vector representations have limited ex- pressivity -each word is represented by a point, with no notion of intra-class variability. In this paper, we consider ZSL in the case where both visual and linguistic concepts are represented by Gaussian dis- tribution embeddings. Specifically, our Gaussian-embedding approach to ZSL learns concept distri- butions in both domains: Gaussians representing in- dividual words (as in <ref type="bibr" target="#b16">(Vilnis and McCallum, 2015)</ref>) and Gaussians representing visual concepts. Simul- taneously, it learns a cross-domain mapping that warps language-domain Gaussian concept represen- tations into alignment with visual-domain concept Gaussians. Some existing vector DSM-based cross- modal ZSL mappings ( <ref type="bibr" target="#b0">Akata et al., 2013;</ref> can be seen as special cases of ours where the within-domain model is pre-fixed as vector cor- responding to the Gaussian means alone, and only the cross-domain mapping is learned. Our results show that modeling linguistic and visual concepts as Gaussian distributions rather than vectors can signif- icantly improve zero-shot recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Vector Word Embeddings In a typical setup for unsupervised learning of word-vectors, we observe a sequence of tokens {w i } and their context words {c(w) i }. The goal is to map each word w to a d- dimensional vector e w reflecting its distributional properties. Popular skip-gram and CBOW models ( <ref type="bibr" target="#b11">Mikolov et al., 2013)</ref>, learn a matrix W ∈ R |V |×d of word embeddings for each of V vocabulary words (e w = W (w,:) ) based on the objective of predicting words given their contexts.</p><p>Another way to formalise a word vector represen- tation learning problem is to search for a representa- tion W so that words w have high representational similarity with co-occuring words c(w), and low similarity with representations of non-co-occurring words ¬c(w). This could be expressed as optimisa- tion of max-margin loss J; requiring that each word w's representation e w is more similar to that of con- text words e p than non-context words e n .</p><formula xml:id="formula_0">J(W ) = w,wp∈c(w),wn∈¬c(w) max(0, δ−E(e w , e wp )+E(e w , e wn )) (1)</formula><p>where similarity measure E(·, ·) is a distance in R d space such as cosine or euclidean.</p><p>Gaussian Word Embeddings Vector-space mod- els are successful, but have limited expressivity in terms of modelling the variance of a concept, or asymmetric distances between words, etc. This has motivated recent work into distribution-based em- beddings ( <ref type="bibr" target="#b16">Vilnis and McCallum, 2015)</ref>. Rather than learning word-vectors e w , the goal here is now to learn a distribution for each word, represented by a per-word mean µ w and covariance Σ w .</p><p>In order to extend word representation learning approaches such as Eq. <ref type="formula">(1)</ref> to learning Gaussians, we need to replace vector similarity measure E(·, ·) with a similarity measure for Gaussians. We fol- low ( <ref type="bibr" target="#b16">Vilnis and McCallum, 2015</ref>) in using the inner product between distributions f and g -the proba- bility product kernel ( <ref type="bibr" target="#b9">Jebara et al., 2004</ref>).</p><formula xml:id="formula_1">E(f, g) = x∈R n f (x)g(x).<label>(2)</label></formula><p>The probability product kernel (PPK) has a conve- nient closed form in the case of Gaussians:</p><formula xml:id="formula_2">E(f, g) = x∈R n N (x; µ f , Σ f )N (x; µ g , Σ g )dx = N (0; µ f − µ g , Σ f + Σ g )<label>(3)</label></formula><p>where µ f , µ g are the means and Σ f , Σ g are the co- variances of the probability distribution f and g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Modal Distribution Mapping</head><p>Gaussian models of words can be learned as in the previous section, and that Gaussian models of im- age categories can be trivially obtained by maximum likelihood. The central task is therefore to estab- lish a mapping between word-and image-Gaussians, which will be of different dimensions d w and d x .</p><p>We aim to find a projection matrix A ∈ R dx×dw such that a word w generates an image vector as e x = Ae w . Working with distributions, this im- plies that we have µ x = Aµ w and Σ x = AΣ w A T . We can now evaluate the similarity of concept dis- tributions across modalities. The similarity between image-and text-domain Gaussians f and g is:</p><formula xml:id="formula_3">E(f, g) = N (0; µ f − Aµ g , Σ f + AΣ g A T )<label>(4)</label></formula><p>Using this metric, we can train our cross-modal pro- jection A via the cross-domain loss:</p><formula xml:id="formula_4">J(A) = f,g∈P, h,k∈N max(0, δ − E(f, g) + E(h, k)) (5)</formula><p>where P is the set of matching pairs that should be aligned (e.g., the word Gaussian 'plane' and the Gaussian of plane images) and N is the set of mis- matching pairs that should be separated (e.g., 'plane' and images of dogs). This can be optimised with SGD using the gradient:</p><formula xml:id="formula_5">∂E ∂A = 1 2 ((Σ f + AΣ g A T ) −1 A(Σ g + Σ T g )) + ((µ T g (Σ f + AΣ g A T ) −1 (µ f − Aµ g ) + (µ f − Aµ g ) T (Σ f + AΣ g A T ) −1 µ T g + (µ f − Aµ g ) T (Σ f + AΣ g A T ) −1 A T (Σ g + Σ T j )(Σ f + AΣ g A T ) −1 (µ f − Aµ g ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Joint Representation and Mapping</head><p>The cross-domain mapping A can be learned (Eq. 5) for fixed within-domain representations (word and image Gaussians). It is also possible to simulta- neously learn the text and image-domain gaussians</p><formula xml:id="formula_6">({µ i , Σ i } text , {µ j , Σ j } img )</formula><p>by optimising the sum of three coupled losses: Eq. 1 with Eq. 3, Eq. 5 and max-margin image-classification using Gaussians.</p><p>We found jointly learning the image-classification Gaussians did not bring much benefit over the MLE Gaussians, so we only jointly learn the text Gaus- sians and cross-domain mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Application to Zero-Shot Recognition</head><p>Once the text-domain Gaussians and cross-domain mapping have been trained for a set of known words/classes, we can use the learned model to recognise any novel/unseen but name-able visual category w as follows: 1. Get the word-Gaussians of target categories w, N (µ w , Σ w ). 2. Project those Gaussians to image modality, N (Aµ w , AΣ w A T ). 3. Classify a test image x by evaluating its likelihood under each Gaussian, and picking the most likely Gaussian: p(w|x) ∝ N (x|Aµ w , AΣ w A T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Contextual Query</head><p>To illustrate our approach, we also experiment with a new variant of the ZSL setting. In conventional ZSL, a novel word can be matched against images by projecting it into image space, and sorting images by their distance to the word (vector), or likelihood under the word (Gaussian). However, results may be unreliable when used with polysemous words, or words with large appearance variability. In this case we may wish to enrich the query with contex- tual words that disambiguate the visual meaning of the query. With regular vector-based queries, the typical approach is to sum the word-vectors. For example: For contextual disambiguation of poly- semy, we may hope that vec('bank')+vec('river') may retrieve a very different set of images than vec('bank')+vec('finance'). For specification of a specific subcategory or variant, we may hope that vec('plane')+vec('military') retrieves a different set of images than vec('plane')+vec('passenger'). By using distributions rather than vectors, our frame- work provides a richer means to make such queries that accounts for the intra-class variability of each concept. When each word is represented by a Gaussian, a two-word query can be represented by their product, which is the new Gaussian   ES-ZSL <ref type="bibr" target="#b13">(Romera-Paredes and Torr, 2015)</ref>, and a max-margin cross-modal energy function method (CME, ( <ref type="bibr" target="#b0">Akata et al., 2013;</ref>).</p><formula xml:id="formula_7">N ( Σ −1 1 µ 1 +Σ −1 2 µ 2 Σ −1 1 +Σ −1 2 , (Σ −1 1 + Σ −1 2 ) −1</formula><p>Note that the CME strategy is the most closely re- lated to ours in that it also trains a d x × d w matrix with max-margin loss, but uses it in a bilinear en- ergy function with vectors E(x, y) = x T Ay; while our energy function operates on Gaussians. <ref type="table" target="#tab_1">Table 1</ref> compares our results on the AWA bench- mark against alternatives using the same visual fea- tures, and word vectors trained on the same corpus. We observe that: (i) Our Gaussian-embedding ob- tains the best performance overall. (ii) Our method outperforms CME which shares an objective func- tion and optimisation strategy with ours, but oper- ates on vectors rather than Gaussians. This sug- gests that our new distribution rather than vector- embedding does indeed bring significant benefit. A comparison to published results obtained by other studies on the same ZSL splits is given in Ta- ble 2, where we see that our results are competitive despite exploitation of supervised embeddings such as attributes <ref type="figure" target="#fig_0">(Fu et al., 2014</ref>), or combinations of embeddings ( <ref type="bibr" target="#b0">Akata et al., 2013</ref>) by other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We next demonstrate our approach qualitatively by means of the contextual query idea introduced in ImageNet ConSE ( <ref type="bibr" target="#b11">Norouzi et al., 2014)</ref> 28.5% DeVISE (  31.8% Large Scale Metric. ( <ref type="bibr">Mensink et al., 2012)</ref> 35.7% Semantic Manifold. ( <ref type="bibr" target="#b7">Fu et al., 2015)</ref> 41.0% Gaussian Embedding 45.7% AwA DAP (CNN feat) ( <ref type="bibr" target="#b10">Lampert et al., 2009)</ref> 53.2% ALE ( <ref type="bibr" target="#b0">Akata et al., 2013)</ref> 43.5% TMV-BLP ( <ref type="bibr">Fu et al., 2014)</ref> 47.1% ES-ZSL (Romera-Paredes and Torr, 2015) 49.3% Gaussian Embedding 65.4% Sec 2.5. <ref type="figure" target="#fig_0">Fig. 1</ref> shows examples of how the top re- trieved images differ intuitively when querying Im- ageNet for zero-shot categories 'plane' and 'horse' with different context words. To ease interpretation, we constrain the retrieval to the true target class, and focus on the effect of the context word. Our learned Gaussian method retrieves more relevant im- ages than the word-vector sum baseline. E.g., with the Gaussian model all of the top-4 retrieved images for Passenger+Plane are relevant, while only two are relevant with the vector model. Similarly, the re- trieved black horses are more clearly black.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Further Analysis</head><p>To provide insight into our contribution, we repeat the analysis of the AwA dataset and evaluate several variants of our full method. These use our features, and train the same cross-domain max-margin loss in Eq 5, but vary in the energy function and representa-AwA Bilinear-WordVec 43.1% Bilinear-MeanVec 52.2% PPK-MeanVec 52.6% PPK-Gaussian 65.4% From the results in <ref type="table" target="#tab_3">Table 3</ref>, we make the observa- tions: (i) Bilinear-MeanVec outperforming Bilinear- WordVec shows that cross-modal (Sec 2.3) train- ing of word Gaussians learns better point esti- mates of words than conventional word-vector train- ing, since these only differ in the choice of vector representation of class names. (ii) PPK-Gaussian outperforming PPK-MeanVec shows that having a model of intra-class variability (as provided by the word-Gaussians) allows better zero-shot recogni- tion, since these differ only in whether covariance is used at testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Related Work and Discussion</head><p>Our approach models intra-class variability in both images and text. For example, the variability in visual appearance of military versus passenger 'plane's, and the variability in context according to whether a the word 'plane' is being used in a military or civilian sense. Given distribution-based represen- tations in each domain, we find a cross-modal map that warps the two distributions into alignment.</p><p>Concurrently with our work, Ren et al (2016) present a related study on distribution-based visual- text embeddings. Methodologically, they benefit from end-to-end learning of deep features as well as cross-modal mapping, but they only discrimi- natively train word covariances, rather than jointly training both means and covariances as we do.</p><p>With regards to efficiency, our model is fast to train if fixing pre-trained word-Gaussians and op- timising only the cross-modal mapping A. How- ever, training the mapping jointly with the word- Gaussians comes at the cost of updating the repre- sentations of all words in the dictionary, and is thus much slower.</p><p>In terms of future work, an immediate improve- ment would be to generalise our of Gaussian embed- dings to model concepts as mixtures of Gaussians or other exponential family distributions <ref type="bibr" target="#b13">(Rudolph et al., 2016;</ref><ref type="bibr">Chen et al., 2015)</ref>. This would for exam- ple, allow polysemy to be represented more cleanly as a mixture, rather than as a wide-covariance Gaussian as happens now. We would also like to explore distribution-based embeddings of sen- tences/paragraphs for class description (rather than class name) based zero-shot recognition <ref type="bibr">(Reed et al., 2016)</ref>. Finally, besides end-to-end deep learning of visual features, training non-linear cross-modal mappings is also of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we advocate using distribution-based embeddings of text and images when bridging the gap between vision and text modalities. This is in contrast to the common practice of point vector- based embeddings. Our distribution-based approach provides a representation of intra-class variability that improves zero-shot recognition, allows more meaningful retrieval by multiple keywords, and also produces better point-estimates of word vectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Qualitative visualisation of zero-shot query with context words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Zero-shot recognition results on AWA (% accuracy).</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Comparison of our ZSL results with state of the art.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Impact of training and testing with distribution rather</head><label>3</label><figDesc></figDesc><table>than vector-based representations 

tions used. Variants include: (i) Bilinear-WordVec: 
Max-margin training on word vector representations 
of words and images with a bilinear energy func-
tion. (ii) Bilinear-MeanVec: As before, but using 
our Gaussian means as vector representations in im-
age and text domains. (iii) PPK-MeanVec: Train 
the max-margin model with Gaussian representa-
tion and PPK energy function as in our full model, 
but treat the resulting means as point estimates for 
conventional vector-based ZSL matching at testing-
time. (v) PPK-Gaussian: Our full model with Gaus-
sian PPK training and testing by Gaussian matching. 
</table></figure>

			<note place="foot" n="1"> Code and datasets kept at http://bit.ly/2cI64Zf</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label-embedding for attributebased classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<idno>abs/1502.03520</idno>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. JMLR</title>
		<editor>David M. Blei, Andrew Y. Ng, and Michael I. Jordan</editor>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Blei et al.2003</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bruni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06246</idno>
	</analytic>
	<monogr>
		<title level="m">Xinchi Chen, Xipeng Qiu, Jingxiang Jiang, and Xuanjing Huang. 2015. Gaussian mixture embeddings for multiple word prototypes</title>
		<editor>ACM Multimedia. [Bruni et al.2014] Elia Bruni, Nam Khanh Tran, and Marco Baroni</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing and evaluating ukwac, a very large web-derived corpus of english</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Web as Corpus Workshop</title>
		<meeting>the 4th Web as Corpus Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>ICLR Workshop Paper. WAC-4</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Frome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>MarcAurelio Ranzato, and Tomas Mikolov</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
	</analytic>
	<monogr>
		<title level="m">Yanwei Fu, Timothy Hospedales, Tony Xiang, Zhenyong Fu, and Shaogang Gong</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-shot object recognition by semantic manifold distance</title>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributional structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Word</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning image embeddings using convolutional neural networks for improved multi-modal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Kiela and Bottou2014] Douwe Kiela and Léon Bottou</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Is this a wampimuk? crossmodal mapping between distributional semantics and the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. 2012. Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>European Conference on Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>ICLR. [Reed et al.2016] Scott Reed, Zeynep Akata, Honglak Lee, and Bernt Schiele</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint image-text representation by gaussian visual semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of ACM International Conference on Multimedia</title>
		<meeting>eeding of ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM MM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Paredes And Torr2015] Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romeraparedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Torr ; Maja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Exponential Family Embeddings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Silberer and Lapata2014] Carina Silberer and Mirella Lapata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. [Simonyan and Zisserman2015] Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Learning grounded meaning representations with autoencoders</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Zero Shot Learning Through Cross-Modal Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccallum2015] Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
