<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Empirical Analysis of Edit Importance between Document Versions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
							<email>tanyagoyal.93@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Kelkar</surname></persName>
							<email>sachinkel19@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeenu</forename><surname>Grover</surname></persName>
							<email>groverjeenu@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Big Data Experience Lab Adobe Research</orgName>
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<addrLine>Roorkee Manas Agarwal</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Roorkee</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Empirical Analysis of Edit Importance between Document Versions</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2780" to="2784"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
					<note>Indian Institute of Technology, Kharagpur Abstract</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In collaborative content authoring, multiple au- thors make changes to the same document, which results in the final version being significantly dif- ferent from the base draft. Often there is a need to review the edits made to the original document, which can be a long and arduous task. Tools like Microsoft Word (mic) and Adobe Acrobat (ado) provide reviewers with a list of edits, in the form of insertions and deletions. While helpful, these tools do not differentiate between the different types of edits, or consider the varying impact of edits. For instance, change from numeric '18' to word 'eigh- teen' may be a minor change and less crucial for the author to review, as compared to an edit that All authors have equal contribution in this paper. This work was done as part of an internship at Adobe Research alters the facts of the document. Thus, in our work, we focus on automatically inferring the im- pact/change introduced by edits, and predict the perceived importance of such edits by authors. In this paper, we perform a linguistic analysis of how humans evaluate the significance of edits while reviewing documents. Our algorithm as- signs scores to edits between two versions of a document, which indicate the significance of the specified edit as perceived by the reviewer. We demonstrate the efficacy of our approach by com- paring our algorithm generated edit importance scores with the human perceived ground truth im- portance, established through a Mechanical Turk survey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been significant amount of work on defining the importance of a keyword or a sentence in the context of document summarization <ref type="bibr" target="#b5">(Mihalcea and Tarau, 2004</ref>). Some prior work has also been done on inferring the type of edits between Wikipedia versions. <ref type="bibr" target="#b1">Bronner et al. (Bronner and Monz, 2012)</ref> proposed a supervised approach to classify Wikipedia edits as factual or fluency. <ref type="bibr" target="#b2">Daxenberger et al. (Daxenberger and Gurevych, 2013)</ref> propose an approach to classify these edits into a 21-category taxonomy. However, none of the the prior work studies the impact or significance of the edit to the content of the document. They do not take the context of the change into account, neither do they study how edits are perceived by reviewers and the significance associated to each edit type. To the best of our knowledge, there is no prior work that evaluates the importance of an edit between document versions as perceived by human reviewers, which is the novel contribution of our work here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion on types of edits</head><p>Before trying to automatically infer the impor- tance of individual edits, we first identified the broad categories of textual edits made by authors. <ref type="bibr" target="#b1">Bronner et al. (Bronner and Monz, 2012</ref>) broadly classify text edits into two categories, namely, Factual Edits and Fluency Edits. Factual edits re- fer to those that modify, add or delete information in the document while fluency edits mainly deal with changes in writing styles or paraphrasing. To obtain finer granularity edit categories, we sub- classified factual edits into Information Modify, In- formation Delete and Information Insert. To fur- ther classify fluency changes, we looked at linguis- tic literature <ref type="bibr" target="#b4">(Honeck, 1971)</ref> and identified sub- categories of paraphrase changes. Based on this, fluency edits were further classified into Lexical Paraphrase (change of textual elements by syn- onymous words/phrases/numbers) and Transfor- mational Paraphrase (change in the structure of the sentence, e.g. active to passive voice). For the purpose of this paper, we assume these edit categories to be exhaustive and consequently clas- sify all changes as belonging to one of these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and Annotation</head><p>Due to the unavailability of an appropriately annotated dataset, we performed an online survey on Amazon Mechanical Turk 1 to capture people's perception of edit importance. To achieve this, we used an available corpus of news articles 2 . We created newer versions for these articles by manually introducing multiple changes to each article. <ref type="figure" target="#fig_1">Fig 1 provides</ref> statistics for the types of edits (based on the discussion in the previous section) across this entire corpus of 52 article pairs. There are a total of 523 changed sentence pairs in the document corpus, and an average of 1.2 edits per sentence pair.</p><p>For annotation of edit importance, we asked Mechanical Turk workers to assign an importance score to each pair of changed sentences. We first provide each turker with the initial (original) version of a news article. After the turker finishes reading the article, he is presented with a list of sentences that were changed between the initial and the final version, along with the changed sentences. The worker classifies each of these  Moderately Important, (c) Important, (d) Neutral, (e) Not necessary for review. To avoid intro- ducing biases based on our own notions of edit importance, we provide only a brief description of the task and encourage annotators to follow their own intuitions of importance. Each sentence pair is annotated by 3 annotators and the final importance score is calculated as the mean of the three scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Solution Description</head><p>This section describes the methodology followed to obtain importance scores for text edits to doc- uments. <ref type="figure">Fig 2 shows</ref> the overall workflow of the proposed approach.</p><p>The input to the algorithm is a corpus of doc- uments D, which consists of pairs of documents (d, d ) corresponding to the initial and final doc- ument versions respectively. We use the sen- tence alignment module proposed by <ref type="bibr" target="#b7">Zhang et al. (Zhang and Litman, 2014</ref>) to obtain the mapped pairs of sentences (s, s ), where s represents a sen- tence in the first version d and s is its modified variant in d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification of Edits</head><p>The first step of the algorithm is to determine the number and types of the various edits between each sentence pair (s, s ). In this module, we as- sign an edit type label, as discussed in Section 3, to all edits between a pair of sentences. Our analysis of the document corpus revealed that most text edits to sentences do not significantly change the structure of the sentence. Thus, a simple heuristic based approach can be used to identify edit types. We use the Stanford Parser to extract the POS tag sequences of the two sen- tences, with words backed off to their named en- tities wherever possible. We identify the longest common subsequence between these to obtain a word to word mapping for the sentence pair. If the ratio of the LCS and the mean of the sen- tence lengths (original and the modified) is above a threshold, we assume that the sentence struc- ture is preserved. A simple word comparison be- tween the similarly tagged words reveals instances of Information-Modify and Lexical changes; addi- tions and deletions are identified as Information- Insert and Information-Delete respectively. In case the structure of the sentence is not preserved, the above heuristic fails, and we tag the sentence pair as Transformational Paraphrase. For such sentence pairs, we employ the method outlined by <ref type="bibr" target="#b1">Bronner et al. (Bronner and Monz, 2012</ref>) and train a supervised classifier to differentiate between fac- tual and fluency edits, without bothering about the subtype. Following the outlined heuristic, we were able to correctly classify 92% of all edits in the document corpus, without using the supervised classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Extraction</head><p>Next, we extract linguistic features for supervised modeling of edit importance. We hypothesized that the importance of edits would be affected by both the nature of the edits, characterized by the aforementioned categories, as well as the relevance of the sentence to the content of the document. Thus, we chose features that capture both these aspects and have divided them into two groups, namely, change-related features and relevance-related features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Change-related features</head><p>These set of features account for the factual dif- ferences between sentence pairs caused due to the edits. The complete list of such features is as fol- lows:</p><p>• One-hot feature for type of edits identified in the Edit Classification module. We conjec- tured that different types of changes will have different perceived importance. For example, factual changes may be more important for the author to review compared to paraphras- ing changes.</p><p>• One-hot feature for the POS tags and Named Entities whose count changes between the initial and the final version of the sentences. We also include one-hot features for those tags whose corresponding word changes be- tween the two versions. These aim to capture the importance associated with deletion, in- sertion or modification of specific POS tags and Named Entities.</p><p>• One-hot features for the following depen- dency tuples that change between the two versions, with lexical items backed off to POS tags: (gov,typ, dep), (gov, typ), (typ, dep), (gov, dep).</p><p>• Count for the number of edits between the two versions.</p><p>• Absolute difference in the Flesch Kinkaid readability scores of the two sentences. We hypothesized that human perception of de- gree of change may be correlated with the change in ease of readability of content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Relevance-related features</head><p>These features aim to score the sentences where the edit occurred. We conjectured that edit im- portance must also depend on the relevance of the underlying sentence to the content of the docu- ment. For instance, in an article about the monar- chy in the United Kingdom, an edit that occurs in a sentence discussing the Queen may potentially be more important than one that provides generic facts about the country. The features we consider are :</p><p>• TextRank Score: We use the TextRank al- gorithm ( <ref type="bibr" target="#b5">Mihalcea and Tarau, 2004</ref>) to ex- tract keywords from the document along with the PageRank score attached to them. Each sentence is scored based on the cumulative scores of all keywords that occur in it. Ex- plicitly, the score of a particular sentence is calculated as:</p><formula xml:id="formula_0">Score(s) = w∈W ∩S KeywordScore(w) |S| (1)</formula><p>where S is the set of words in the sentence and W is the set of keywords extracted by the TextRank algorithm.</p><p>• Position of the sentence in the document:</p><p>The importance of sentence position has been studied in <ref type="bibr" target="#b3">(Edmundson, 1969)</ref>. We expect more important sentences to have a higher edit importance score attached to them.</p><p>We train a ridge regression model with the model parameters tuned using cross validation on the training data. We report the Spearman ρ correlation <ref type="bibr" target="#b6">(Spearman, 1904)</ref> of the predicted edit importance scores with the human annotated scores on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>In this section, we discuss the various experiments performed, and the results obtained. Baselines: To the best of our knowledge, our work is the first that attempts to infer importance/impact of text ed- its between document versions. Thus, we did not have established baselines to compare against. In- stead we use the following features as baselines:</p><p>• Sentence Order -Sentences are ordered ac- cording to their position in the document, with the first sentence assigned most impor- tance. This is also the order in which a re- viewer would normally view edits.</p><p>• Readability Score -Sentence edit impor- tance scores are calculated as being pro- portional to the change in their readability scores.</p><p>• Text Rank -We expect sentences with higher TextRank score to have higher edit impor- tance attached to them. <ref type="table" target="#tab_1">Table 1</ref> outlines the Spearman ρ correlation of our model and the above baselines with human judg- ments. We are able to achieve significant improve- ment over the baselines using the full set of fea- tures. An interesting observation was that sen- tence position correlates poorly with the human   <ref type="table">Table 2</ref>: Performance of each feature group in iso- lation. Numbers reflect the performance (Spear- man ρ) of the model when using only the specified feature group, relative to the performance when using all features.</p><p>annotated importance scores. This indicates that the order/position of sentences has negligible ef- fect on the perceived significance. Both readabil- ity score and TextRank have reasonable influence on edit importance, though neither of them is able to match the performance of the full set of fea- tures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of feature groups</head><p>In order to gain better insight into individual fea- ture performance, we look more closely at the per- formance of each feature group in isolation. <ref type="table">Table  2</ref> shows the performance of the model when us- ing only a specific feature group, relative to the performance when using all features. This pro- vides us with a number of interesting insights. First, it is evident that change-related features con- tribute more to edit importance than relevance- related features. According to our results, humans perceive change in number and types of POS tags to be the most significant indicator of edit importance. For fur- ther insight, we looked at the coefficient values of individual POS tags in the ridge regression model trained using only POS tags as features. Our in- vestigations revealed that change in proper nouns, nouns, present participle verbs and modal are most highly correlated with edit importance. Contrary to our expectation, modification of named entities does not significantly influence edit importance. This may be due to the fact that named entity changes occur in only a small subset of sentences, and hence cannot be good predictors of edit im- portance when used as a feature by themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper we present a novel approach to infer the importance of text edit between two document versions. We present an empirical analysis of the relevance of various linguistic features for the task of scoring edit importance and model it using a regression model. AMT is used to collect human annotated data for edit importance, and a compar- ison against those establish the superiority of our proposed approach over several baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://www.mturk.com/mturk/ 2 http://literacynet.org/cnnsf/archives.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of edits of each type as a percentage of the total number of edits in the entire document corpus</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,218.27,87.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Spearman ρ of the predicted impor- tance score with the human annotated importance scores.</head><label>1</label><figDesc></figDesc><table>Feature 
Spearman ρ 
Type of Change 
0.47907 
Readability Score 
0.311989 
Change in POS tags 
0.978821 
Change in NE 
0.189176 
Change in Dependency Tuples 
0.96417 
Sentence Position 
0.06846 
Text Rank 
0.209058 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adobe</title>
		<ptr target="https://acrobat.adobe.com/in/en/acrobat/pdf-reader.html" />
		<imprint>
			<biblScope unit="page" from="2017" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">User edits classification using document revision histories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bronner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="356" to="366" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatically classifying edit categories in wikipedia revisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="578" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New methods in automatic extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold P Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A study of paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard P Honeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="381" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The proof and measurement of association between two things. The American journal of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="72" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sentence-level rewriting detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Grantee Submission</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
