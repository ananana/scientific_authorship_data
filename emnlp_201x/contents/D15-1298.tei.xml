<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PhraseRNN: Phrase Recursive Neural Network for Aspect-based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Thien</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science</orgName>
								<orgName type="institution">Japan Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>1-1 Asahidai</addrLine>
									<postCode>923-1292</postCode>
									<settlement>Nomi</settlement>
									<region>Ishikawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoaki</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science</orgName>
								<orgName type="institution">Japan Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>1-1 Asahidai</addrLine>
									<postCode>923-1292</postCode>
									<settlement>Nomi</settlement>
									<region>Ishikawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shirai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science</orgName>
								<orgName type="institution">Japan Advanced Institute of Science and Technology</orgName>
								<address>
									<addrLine>1-1 Asahidai</addrLine>
									<postCode>923-1292</postCode>
									<settlement>Nomi</settlement>
									<region>Ishikawa</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PhraseRNN: Phrase Recursive Neural Network for Aspect-based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a new method to identify sentiment of an aspect of an entity. It is an extension of RNN (Recursive Neu-ral Network) that takes both dependency and constituent trees of a sentence into account. Results of an experiment show that our method significantly outperforms previous methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-based sentiment analysis (ABSA) has been found to play a significant role in many applica- tions such as opinion mining on product or restau- rant reviews. It is a task to determine an attitude, opinion and emotions of people toward aspects in a sentence. For example, given a sentence "Except the design, the phone is bad for me", the system should classify positive and negative as the senti- ments for the aspects 'design' and 'phone', respec- tively.</p><p>The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence ( <ref type="bibr" target="#b7">Liu and Zhang, 2012;</ref><ref type="bibr" target="#b12">Pang and Lee, 2008)</ref>. This method is further improved by identifying the aspect-opinion relations using tree kernel method <ref type="bibr">(Nguyen and Shirai, 2015a</ref>).</p><p>Other researches have attempted to use unsuper- vised topic modeling methods. To identify the sen- timent category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA <ref type="bibr">(Nguyen and Shirai, 2015b</ref>), ASUM (Jo and Oh, 2011), JST ( <ref type="bibr" target="#b6">Lin and He, 2009)</ref> and FACTS model ( <ref type="bibr" target="#b5">Lakkaraju et al., 2011</ref>).</p><p>Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed represen- tations of words (aka word embedding) ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Hinton, 1986)</ref>, RNN merges word rep- resentations to represent phrases or sentences. It is one of the best methods to predict sentiment la- bels for the phrases <ref type="bibr" target="#b14">(Socher et al., 2011;</ref><ref type="bibr" target="#b15">Socher et al., 2012;</ref><ref type="bibr" target="#b16">Socher et al., 2013)</ref>. AdaRNN (Adap- tive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification ( <ref type="bibr" target="#b1">Dong et al., 2014a;</ref><ref type="bibr" target="#b2">Dong et al., 2014b)</ref>. This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic infor- mation from both the dependency and constituent trees of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recursive Neural Networks for ABSA</head><p>In RNN and AdaRNN, given a sentence contain- ing a target aspect, "binary dependency tree" is built from a dependency tree of the sentence. Intu- itively, it represents syntactic relations associated with the aspect. Each word (leaf) or phrase (inter- nal node) in the binary dependency tree is repre- sented as a d-dimensional vector. From bottom to up, the representations of a parent node v is calcu- lated by combination of left and right child vector representations (v l and v r ) using a global function g in RNN:</p><formula xml:id="formula_0">g(v l , v r ) = W v l v r + b (1)</formula><p>where W ∈ d×2d is the composition matrix and b ∈ d is the bias vector.</p><formula xml:id="formula_1">Then v = f (g(v l , v r ))</formula><p>where f is a nonlinear function such as tanh.</p><p>Instead of using only a global function g, AdaRNN employed n compositional functions G = {g 1 , · · · , g n } and selected them depending on the linguistic tags and combined vectors as fol- lows: <ref type="figure">Figure 1</ref>: Example of a Constituent Tree where P (g i |v l , v r , e) is the probability of function g i given the child vectors v l , v r and external fea- ture vector e. The probabilities are estimated as Equation <ref type="formula">(3)</ref>.</p><formula xml:id="formula_2">v = f n i=1 P (g i |v l , v r , e)g i (v l , v r ) (2)</formula><formula xml:id="formula_3">  P (g 1 |v l , v r , e) · · · P (g n |v l , v r , e)   = sof tmax   βR   v l v r e     (3)</formula><p>where β ∈ is a hyper-parameter, and R ∈ n×(2d+|e|) is the parameter matrix.</p><p>The vector of the root node of the binary depen- dency tree is regarded as a representation of the target aspect. It is fed to a logistic regression to predict the sentiment category of the aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PhraseRNN: Phrase Recursive Neural Network</head><p>In this model, a representation of an aspect will be obtained from a "target dependent binary phrase dependency tree" constructed by combining the constituent and dependency trees. In addition, in- stead of using a list of global functions G as in AdaRNN, two kinds of composition functions G in inner-phrase and H in outer-phrase are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Building Hierarchical Structure</head><p>First, the basic phrases (noun phrases, verb phrases, preposition phrases and so on) are ex- tracted from the constituent tree of the sentence. For example, a list of phrases P = {PP[Except the design], NP[the phone], VP[is bad for me]} is ex- tracted from the constituent tree in <ref type="figure">Figure 1</ref>. Given a dependency tree and a list of phrases, a phrase dependency tree is created by Algorithm 1. The input is a dependency tree T = (V, E) consisting of a set of vertices V = {v 1 , · · · , v |V | } and a set of relation edges E = {(r ji , v i , v j )} between two vertices, and a list of phrases P = {p 1 , · · · , p K } extracted from the constituent tree. The output is a phrase dependency tree pT = (pV, pE) where</p><formula xml:id="formula_4">pV = {T 1 , · · · , T K } (T i = (V i , E i )</formula><p>is a subtree) and pE = {(r ji , T i , T j )} (a set of relations between two subtrees). With the dependency tree and the phrase list in <ref type="figure" target="#fig_0">Figure 2</ref>(a), the algorithm will output a phrase dependency tree in <ref type="figure" target="#fig_0">Figure 2</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Convert to Phrase Dependency Tree</head><p>Input: dependency tree T = (V, E),</p><formula xml:id="formula_5">phrase list P = {p 1 , · · · , p K } Output: phrase dependency tree: pT = (pV, pE) where pV = {T 1 , · · · , T K }, T i = (V i , E i ) and pE = {(r ji , T i , T j )} 1 for each phrase p i ∈ P do 2 V i ← {v j |v j ∈ p i } 3 end 4 for each edge (r nm , v m , v n ) ∈ E do 5 v m ∈ p k , v n ∈ p l 6 if k = l then 7 E k ← E k ∪ {(r nm , v m , v n )} 8 else 9 pE ← pE ∪ {(r nm , T k , T l )} 10 end 11 end</formula><p>The phrase dependency tree is transformed into a target dependent binary phrase dependency tree bpT by Algorithm 2. The input of the algorithm is a phrase dependency tree pT = (pV, pE) and a target word v t (the aspect word we want to predict the sentiment category). The output is the binary tree bpT . Note that the leaves of the binary tree bpT are binary subtrees bT 1 , · · · , bT K which are the binary versions of subtrees T 1 , · · · , T K . On the other hand, the leaves of binary subtree bT i are the words in phrase p i . bpT and bT i are obtained by convert function defined as Algorithm 3. It can convert an arbitrary tree to a binary tree <ref type="bibr">1</ref> . <ref type="figure" target="#fig_0">Figure  2</ref>(c) and <ref type="figure" target="#fig_1">Figure 3</ref> show the outputs for the aspect 'design' and 'phone', respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing the Aspect Representation</head><p>Each node in the binary tree is represented as a d- dimensional vector. In this research, we use the pre-trained Google News dataset 2 by word2vec algorithms ( <ref type="bibr" target="#b9">Mikolov et al., 2013)</ref>. Each word is Input: phrase dependency tree: pT = (pV, pE), target v t Output: target dependent binary phrase dependency tree: bpT  The vector of the parent node v in in the binary subtree bT i , where v l and v r are the vectors of the left and right children, is computed as:</p><formula xml:id="formula_6">1 for T i = (V i , E i ) ∈ pV do 2 if v t ∈ V i then 3 h ← v</formula><formula xml:id="formula_7">2 v ← v t 3 for v i → v t , v t → v i in E do 4 if v t → v i then 5 E ← E \ {v t → v i } 6 w ← [convert(E, v i ), v] 7 else 8 E ← E \ {v i → v t } 9 w ← [v, convert(E, v i )]</formula><formula xml:id="formula_8">v in = f n i=1 P (g i |v l , v r , e in )g i (v l , v r )<label>(4)</label></formula><p>where e in is the external feature vector. P (g i |v l , v r , e in ) is the probability of function g i given the child vectors v l , v r and e in . It is defined as Equation <ref type="formula" target="#formula_9">(5)</ref>.</p><formula xml:id="formula_9">  P (g 1 |v l , v r , e in ) · · · P (g n |v l , v r , e in )   = sof tmax   βR   v l v r e in    <label>(5)</label></formula><p>where β ∈ is a hyper-parameter, and R ∈ n×(2d+|e in |) is the parameter matrix.</p><p>In the target dependent binary phrase depen- dency tree bpT , the vector of the parent node v out , where the vectors of the left and right children are v l and v r , is computed as:</p><formula xml:id="formula_10">v out = f m i=1 P (h i |v l , v r , e out )h i (v l , v r )<label>(6)</label></formula><p>P (h i |v l , v r , e out ) is the probability of function h i given the child vectors v l , v r and external feature vector e out as shown in Equation <ref type="formula" target="#formula_11">(7)</ref>.</p><formula xml:id="formula_11">  P (h 1 |v l , v r , e out ) · · · P (h m |v l , v r , e out )   = sof tmax   βS   v l v r e out    <label>(7)</label></formula><p>where S ∈ m×(2d+|eout|) is the parameter matrix.</p><p>The external features e i (e in and e out ) of the node v i consists of three types of features: Label l , Label r and DepT ype i . Label l and Label r are the labels of the left and right child nodes, respec- tively. If node v l is a leaf word, Label l is the POS of the word v l . Otherwise, it is the non-terminal symbol of the lowest common parent of descen- dants of v l in the constituent tree. For example, the Label of the node combined from 'the' and 'design' in <ref type="figure" target="#fig_0">Figure 2</ref>(c) is 'NP' which is the low- est common parent of these two words in the con- stituent tree in <ref type="figure">Figure 1</ref>. DepT ype i is the depen- dency relation for node v i . If the left and right children of v i are leaf nodes, it is the direct relation in the dependency tree between them. Otherwise, DepT ype i is the relation between head words of the left and right nodes. For instance, in <ref type="figure" target="#fig_0">Figure  2</ref>(c), let a be the parent of 'is' and 'bad', b is the parent of 'for' and 'me', c is the parent of a and b. DepT ype of a and b are 'COP' and 'POBJ' that are direct relations between two child nodes in the dependency tree in <ref type="figure" target="#fig_0">Figure 2(a)</ref>. While, DepT ype of c is 'PREP' that is the dependency relation be- tween two head words 'bad' and 'for'. e i is a bi- nary vector where the weight of the vector repre- sents the presence of each feature.</p><p>We suppose a batch training data consist- ing of B instances {(x (1) , t (1) ), · · · , (x (B) , t (B) )}, where x (b) and t (b) are the aspect and its sentiment category of b-th instance. Let y (b) be the predicted sentiment category for aspect x (b) by PhraseRNN. The goal is to minimize the loss function which is the sum of the mean of negative log likelihood and L2 regularization penalty in a batch training set as in Equation <ref type="formula">(8)</ref>.</p><formula xml:id="formula_12">L = − 1 B B b=1 log(P (y (b) = t (b) |x (b) , θ)) + λ θ i ∈θ θ i 2 (8)</formula><p>where λ is a constant controlling the degree of penalty, θ is all the parameters in the model. Stochastic gradient descent is used to optimize the loss function. Backpropagation is employed to propagate the errors from the top node to the leaf nodes. The derivatives of parameters are used to update the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We use the restaurant reviews dataset in Se- mEval2014 Task 4 consisting of over 3000 English sentences. For each aspect, "positive", "negative" or "neutral" is annotated as its polarity. Dataset is divided into three parts: 70% training, 10% devel- opment and 20% test.</p><p>We compare the following methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect ( <ref type="bibr" target="#b7">Liu and Zhang, 2012;</ref><ref type="bibr" target="#b12">Pang and Lee, 2008)</ref>.</p><p>ASA with RE: It improves "ASA w/o RE" by firstly identifying the aspect-opinion relations us- ing tree kernel, then integrating them to the senti- ment calculation <ref type="bibr">(Nguyen and Shirai, 2015a)</ref>.</p><p>RNN: It uses only one global function g 1 over the binary dependency tree.</p><p>AdaRNN: It uses multi-composition functions G = {g 1 , · · · , g n } over a binary dependency tree ( <ref type="bibr" target="#b1">Dong et al., 2014a</ref>).</p><p>PhraseRNN-1: our PhraseRNN with only one global function: G = H = g 1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g 1 and H = h 1Stanford CoreNLP ( <ref type="bibr" target="#b8">Manning et al., 2014</ref>) is used to parse the sentence and obtain constituent and dependency trees. For RNN, AdaRNN and PhraseRNN, the optimal parameters, which mini- mize the error in the development set, are used for the sentiment classification of the test set. We set β = 1 for AdaRNN and PhraseRNN since it is re- ported that β = 1 is the best parameter ( <ref type="bibr" target="#b1">Dong et al., 2014a</ref>). The optimized number of composition functions n and m = n 2 are selected by grid search with n = {2, 4, 6, 8, 10} on the development set. λ = 0.0001 is employed. Accuracy (A), Preci- sion (P), Recall (R) and F-measure (F) are used as evaluation metrics <ref type="bibr">3</ref> . <ref type="table" target="#tab_1">Table 1</ref> shows the results of the methods. Dif- ferences of PhraseRNN and RNN are verified by statistical significance tests. We use the paired randomization test because it does not require additional assumption about distribution of out- puts ( <ref type="bibr" target="#b13">Smucker et al., 2007)</ref>. The results indi- cate that four variations of our PhraseRNN out- perform "ASA w/o RE", "ASA with RE", RNN and AdaRNN methods from 5.35% to 19.44% ac- curacy and 8% to 16.48% F-measure. Among four variations, PhraseRNN-2 and PhraseRNN- 3 achieved the best performance. By using dif- ferent global functions in the inner and outer phrases, PhraseRNN-2 improves PhraseRNN-1 by 2.54% F-measure while keeping the comparable accuracy. Using multi-composition functions is also effective since PhraseRNN-3 was better than PhraseRNN-1 by 1.55% accuracy. PhraseRNN-4 improved PhraseRNN-3 by 6.38% precision while keeping comparable in other metrics.</p><p>Since our PhraseRNN-1 and PhraseRNN-3 out- perform RNN and AdaRNN (the models rely- ing on the binary dependency tree) respectively, we can conclude that our target dependent binary phrase dependency tree is much effective than bi- nary dependency tree for ABSA.</p><p>In the data used in ( <ref type="bibr" target="#b1">Dong et al., 2014a</ref>), one sen- tence contains only one aspect. On the other hand, two or more aspects can be appeared in one sen- tence in SemEval 2014 data. It is common in the real text. To examine in which cases our method is better than the others, we conduct an additional ex- periment by dividing the test set into three disjoint subsets. The first subset (S1) contains sentences having only one aspect. The second subset (S2) <ref type="bibr">3</ref> Precision, Recall and F-measure are the average for three polarity categories weighted by the number of true instances.   <ref type="bibr">90 (48.13)</ref> and third subset (S3) have two or more aspects in each sentence. All aspects in a sentence in S2 have the same sentiment category, while different sen- timent categories in S3. The number of aspects in S1, S2 and S3 are 200, 323 and 187, respectively. <ref type="table" target="#tab_2">Table 2</ref> shows the number of aspects where their sentiments are correctly identified by the methods in the subsets S1, S2 and S3. The accuracies are also shown in parentheses. Among three subsets, S3 is the most difficult and ambiguous case. In all methods, the performance in S3 is worse than S1 and S2. Comparing with other methods in each subset, PhraseRNN improves the accuracy in S2 more than in S1 and S3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed PhraseRNN to identify the sentiment of the aspect in the sentence. Propagating the semantics through the binary dependency tree in RNN and AdaRNN could not be enough to rep- resent the sentiment of the aspect. A new hierar- chical structure was constructed by integrating the dependency relations and phrases. The results in- dicated that our PhraseRNN outperformed "ASA w/o RE", "ASA with RE", RNN and AdaRNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical Structures in PhraseRNN: (a) Dependency Tree, (b) Phrase Dependency Tree and (c) Target Dependent Binary Phrase Dependency Tree</figDesc><graphic url="image-2.png" coords="3,72.00,62.81,460.16,171.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Another Target Dependent Binary Phrase Dependency Tree (Target Aspect 'phone')</figDesc><graphic url="image-3.png" coords="3,307.28,286.87,224.61,161.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Results of ABSA</head><label>1</label><figDesc></figDesc><table>Methods 
A 
P 
R 
F 

ASA w/o RE 46.76 54.63 46.76 48.06 
ASA with RE 52.39 53.91 52.39 52.54 
RNN 
60.85 53.59 60.85 54.21 
AdaRNN 
60.42 36.78 60.42 45.73 
PhraseRNN-1 64.65  † 58.59  † 64.65  † 59.67 * 
PhraseRNN-2 63.94  † 62.40 * 63.94  † 62.21 * 
PhraseRNN-3 66.20 * 53.88 66.20 * 59.32 * 
PhraseRNN-4 65.92 * 60.26  † 65.92 * 59.80 * 

Notes: Statistical significance test of PhraseRNN compar-
ing to RNN. 
* Significant at the 1 percent level. 
 † Significant at the 5 percent level. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The Number of Correctly Identified As-
pects in Subsets S1, S2 and S3 

Methods 
S1 
S2 
S3 

ASA w/o RE 
98 (49.00) 156 (48.30) 78 (41.71) 
ASA with RE 111 (55.50) 176 (54.49) 85 (45.45) 
RNN 
123 (61.50) 226 (69.97) 83 (44.39) 
AdaRNN 
117 (58.50) 234 (72.45) 78 (41.71) 
PhraseRNN-1 129 (64.50) 248 (76.78) 82 (43.85) 
PhraseRNN-2 125 (62.50) 247 (76.47) 82 (43.85) 
PhraseRNN-3 125 (62.50) 257 (79.57) 88 (47.06) 
PhraseRNN-4 128 (64.00) 250 (77.40) </table></figure>

			<note place="foot" n="1"> Note that convert function returns a tree represented by nested brackets such as [PP,[NP,VP]]. 2 https://code.google.com/p/word2vec/</note>

			<note place="foot">PhraseRNN-3: our PhraseRNN with multiple global functions: G = H = {g 1 , · · · , g n } PhraseRNN-4: our PhraseRNN with two lists of global functions. One for inner-phrase, the other for outer-phrase: G = {g 1 , · · · , g n } and H = {h 1 , · · · , h m }</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1537" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference of the cognitive science society</title>
		<meeting>the eighth annual conference of the cognitive science society<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aspect and sentiment unification model for online review analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting coherence for the simultaneous discovery of latent facets and associated sentiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indrajit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srujana</forename><surname>Merugu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the Eleventh SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM / Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="498" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management (CIKM)</title>
		<meeting>the 18th ACM conference on Information and knowledge management (CIKM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of opinion mining and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="415" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aspectbased sentiment analysis using tree kernel based relation extraction</title>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing (CICLing)</title>
		<editor>Alexander Gelbukh</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9042</biblScope>
			<biblScope unit="page" from="114" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic modeling based sentiment analysis on social media for stock market prediction</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1354" to="1364" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management (CIKM)</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management (CIKM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
