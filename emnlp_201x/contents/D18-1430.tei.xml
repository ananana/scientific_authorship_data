<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="institution">Jiangsu Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3960" to="3969"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3960</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ability to write diverse poems in different styles under the same poetic imagery is an important characteristic of human poetry writing. Most previous works on automatic Chi-nese poetry generation focused on improving the coherency among lines. Some work explored style transfer but suffered from expensive expert labeling of poem styles. In this paper , we target on stylistic poetry generation in a fully unsupervised manner for the first time. We propose a novel model which requires no supervised style labeling by incorporating mutual information, a concept in information theory , into modeling. Experimental results show that our model is able to generate stylistic poems without losing fluency and coherency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classical Chinese poetries are great heritages of the history of Chinese culture. One of the most popular genres of classical Chinese poems, i.e. quatrains, contains four lines with five or seven characters each and additional rhythm and tune re- strictions. During 1,000 years history of quatrains, various styles, e.g. pastoral, descriptive and ro- mantic, have been developed to express different feelings of poets. In human poetry writing, poets are able to write completely different poems in di- verse styles even given the same keyword or first sentence. For example, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, when a poet mentioned " 月" (the moon), she/he may write about the Great Wall in the battlefield style or the sleepless feeling in the romantic style. Such ability to write stylistic poems under the same po- etic imagery is an important characteristic of hu- man poetries.</p><p>Automatic poetry generation is one of the first attempts towards computer writing. Chinese qua- train generation has also attracted much atten- * corresponding author: sms@tsinghua.edu.cn tion in recent years. Early works inspired by statistical machine translation explored rule-based and template-based methods <ref type="bibr" target="#b5">(He et al., 2012;</ref><ref type="bibr">Yan et al., 2013)</ref>, while recent works ( <ref type="bibr" target="#b24">Zhang and Lapata, 2014;</ref><ref type="bibr" target="#b3">Yan, 2016;</ref><ref type="bibr" target="#b23">Zhang et al., 2017;</ref><ref type="bibr" target="#b22">Yi et al., 2017</ref>) employed neural network based sequence- to-sequence approaches which have shown their effectiveness in neural machine translation for poem generation. Most works target on improving the coherency among all lines and the conformity between the theme and subsequent lines by plan- ning ( , polishing schema <ref type="bibr" target="#b3">(Yan, 2016)</ref>, poem block ( <ref type="bibr" target="#b22">Yi et al., 2017</ref>) and condi- tional variational autoencoder ( . Different from these previous works, we aim to learn the ability of diverse stylistic poetry genera- tion which can generate multiple outputs (poems) in various styles under the same input (keywords or the first sentence). This ability enables a poetry generation system to be closer to a real poet and allows the model to generate more expressive and creative poems.</p><p>However, there is no explicit label about what style or category a poem or a sentence is for thousands of poems in the database. Therefore, traditional supervised sequence-to-sequence mod-els ( <ref type="bibr" target="#b23">Zhang et al., 2017)</ref> are not capable to gener- ate stylistic poems without expert labeling. To the best of our knowledge, we are the first effort at stylistic poetry generation in a fully unsupervised manner.</p><p>In this paper, we propose a novel poetry gen- eration model which can disentangle the poems in different styles and generate style-specific out- puts conditioned on the manually selected style input. We employ sequence-to-sequence model with attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref> as our basis and maximize the mutual information which measures the dependency between two ran- dom variables in information theory to strengthen the relationship between manually selected style inputs and generated style-specific outputs. Exper- imental results show that our model is able to gen- erate poems in various styles without losing flu- ency and coherency.</p><p>To summarize, our effort provides the following three contributions:</p><p>• To the best of our knowledge, we are the first effort at stylistic poetry generation which is an im- portant characteristic of human poetries in a fully unsupervised manner.</p><p>• We innovatively incorporate mutual infor- mation, a measurement in information theory, for unsupervised style disentanglement and style- specific generation.</p><p>• Experimental results show that our model is able to generate diverse poems in various styles without losing fluency and coherency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works and Motivations</head><p>Poetry generation is a classic task in computer writing <ref type="bibr" target="#b4">(Gervás, 2001;</ref><ref type="bibr" target="#b10">Levy, 2001;</ref><ref type="bibr" target="#b12">Netzer et al., 2009;</ref><ref type="bibr" target="#b13">Oliveira, 2012)</ref>. Chinese poetry generation has also attracted much attention during the last decade. Early works are mostly rule-based and template-based ( <ref type="bibr" target="#b18">Wu et al., 2009)</ref>. ( <ref type="bibr" target="#b5">He et al., 2012</ref>) employed statistical machine translation and ( <ref type="bibr">Yan et al., 2013</ref>) adopted automatic summarization techniques for classical poem generation.</p><p>As neural network based approaches have been successfully applied in various applications such as machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), people came up with the idea to apply sequence- to-sequence models for poem generation. ( <ref type="bibr" target="#b24">Zhang and Lapata, 2014</ref>) employed the Recurrent Neural Network (RNN) as their basis and further consid- ered the global context using Convolutional Neu- ral Network (CNN). They also incorporated other hand-crafted features into the model to improve the coherency. <ref type="bibr" target="#b3">(Yan, 2016)</ref> proposed an itera- tive polishing schema based on two RNNs, which can refine the generated poems for several times. ( ) generated poetries in a two- stage process: planning the keywords of each line first and then generating each line sequentially. ( <ref type="bibr" target="#b22">Yi et al., 2017)</ref> proposed poem blocks to learn semantic meaning within a single line and seman- tic relevance among lines in a poem. ( ) employed a conditional variational autoen- coder with augmented word2vec architecture to enhance the conformity between the theme and generated poems. All previous works above tar- get on improving the coherency and conformity of poetry generation while a recent work ( <ref type="bibr" target="#b23">Zhang et al., 2017</ref>) was proposed to improve the nov- elty of generated poems using external memories. Their model can also transfer the style of a poem into three predefined topics. However, they need to manually label many poems in these three pre- defined topics to learn the patterns.</p><p>Formally, a traditional sequence-to-sequence model actually learns a conditional probability distribution Pr(s output |s input ) where s output is the generated poems and s input is the input keyword or the first sentence. Different from machine translation tasks where the output sentence s output is rather certain and compact given input sen- tence s input , the conditional probability distribu- tion Pr(s output |s input ) has strong uncertainty in literary creation scenarios such as poem genera- tion. To support this point, we further train a topic model based on LDA ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>) by treat- ing each poem as a document. We train a 10- topic and a 20-topic LDA model and then reana- lyze the largest topic component of each sentence in a poem. Surprisingly, we find that only 20% and 10% consecutive sentences in a poem have the same largest topic components respectively though we assume that each poem is generated by the same topic when training the LDA model. The fact indicates that even given the former sen- tence, the style of the latter one could still be di- verse and flexible. Intuitively, poets will choose a style or topic in their minds and then write the next sentence based on the style they choose in poetry creation. Therefore, we propose to learn a stylistic poetry generation model which can disen- tangle the poems in different styles and generate style-specific outputs as real human poets could do. Note that no explicit style labels are given to the training data and thus previous supervised al- gorithms cannot be adapted for the purpose easily.</p><p>Recently, a few works have been proposed for disentangled representation learning. Info- GAN ( ) was proposed for gen- erating continuous image data conditioned on im- age labels and disentangled text generation ( <ref type="bibr" target="#b7">Hu et al., 2017</ref>) focused on controllable generation in the semi-supervised setting. Though inspired by them, the motivation and proposed models of our work differ from these methods by a large mar- gin. To the best of our knowledge, we are the first effort at stylistic poetry generation in a fully unsu- pervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We hope our generation model can generate mul- tiple outputs in various styles. Formally, our model takes two arguments as input: input sen- tence s input and style id k ∈ 1, 2 . . . K where K is the total number of different styles. Then we can enumerate each style id k and generate style- specific output sentence s k output respectively. In this section, we will start by introducing the background knowledge about mutual information and sequence-to-sequence model with attention mechanism. Then we propose our framework of decoder model for style disentanglement by tak- ing the mutual information as an additional regu- larization term. Finally, we will present the details of our implementations of each component in the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mutual Information</head><p>Inspired by previous works on image genera- tion ( ) and semi-supervised gen- eration ( <ref type="bibr" target="#b7">Hu et al., 2017)</ref>, we propose to incorpo- rate the concept of mutual information in informa- tion theory for style disentanglement. Given two random variables X and Y , the mutual informa- tion I(X, Y ) measures "the amount of informa- tion" obtained about one random variable given another one <ref type="bibr">1</ref> . Mutual information can also be interpreted as a measurement about how similar the joint probability distribution p(X, Y ) is to the product of marginal distributions p(X)p(Y ). The definition of mutual information is</p><formula xml:id="formula_0">I(X, Y ) = Y X p(X, Y ) log p(X, Y ) p(X)p(Y ) dXdY. (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequence-to-Sequence Model with Attention Mechanism</head><p>We employ a widely used Encoder-Decoder framework ) which was firstly proposed in machine translation as our ba- sis. Suppose sentence X = (x 1 x 2 . . . x T ) and Y = (y 1 y 2 . . . y T ) are the input sentence (source sentence) and output sentence (target sentence) re- spectively where x i , y i for i = 1, 2 . . . T are char- acters and T is the total number of characters in the sentence. We denote the character vocabulary as V . Specifically, we use bidirectional LSTM (bi- LSTM) <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b14">Schuster and Paliwal, 1997</ref>) model as the encoder to project the input sentence X into the vector space. Formally, the hidden state of LSTM are computed by</p><formula xml:id="formula_1">− → hi = LST M f orward ( − − → hi−1, e(xi)),<label>(2)</label></formula><formula xml:id="formula_2">← − hi = LST M backward ( ← − − hi−1, e(xT −i+1)),<label>(3)</label></formula><p>for i = 1, 2 . . . T where − → h i and ← − h i are the i-th hidden state of forward and backward LSTM re- spectively, e(x i ) ∈ R d is the character embedding of character x i and d is the dimension of charac- ter embeddings. Then we concatenate correspond- ing hidden states of forward and backward LSTM</p><formula xml:id="formula_3">h i = [ − → h i , ← −−− − h T −i+1</formula><p>] as the i-th hidden state of bi- LSTM. In specific, we use the last hidden state h T as the embedding vector and feed it to the decoder.</p><p>The decoder module contains an LSTM decoder with attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) which computes a context vector as a weighted sum of all encoder hidden states to represent the most relevant information at each stage. The char- acter probability distribution when decoding the i- th character can be expressed as</p><formula xml:id="formula_4">p(yi|y1y2 . . . yi−1, X) = g(yi|si),<label>(4)</label></formula><p>where g(·) is a linear projection function with soft- max regularization, s i is the i-th hidden state in the decoder LSTM:</p><formula xml:id="formula_5">si = LST M decoder (si−1, [e(yi−1), ai]),<label>(5)</label></formula><p>for i = 2, . . .  learned by attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>)</p><formula xml:id="formula_6">ai = attention(si−1, h1:T ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder Model with Style Disentanglement</head><p>To accept two arguments, i.e., input sentence X and style id k, we can directly concatenate the one-hot representation of style id one hot(k) and the embedding vector h T obtained by bi-LSTM encoder and then feed the concatenated vector [one hot(k), h T ] instead of h T into the decoder model without changing anything else. However, there is no theoretical guarantee that the output sentence generated by the de- coder is strongly correlated to the style id input one hot(k). In other words, when the input style id is changed, the output sentence would probably be the same because no supervised loss is given to force the output sentences to follow the one- hot style representation. Therefore, we naturally come up with the idea to add a regularization term to force a strong dependency relationship between the input style id and generated sentence.</p><p>Without loss of generality, we assume that the input style id is a uniformly distributed random variable Sty and Pr(Sty = k) = 1 K for k = 1, 2 . . . K where K is the total number of styles. Recall that mutual information quantifies the mu- tual dependency between two random variables. Hence we propose to maximize the mutual in- formation between the style distribution Pr(Sty) and the generated sentence distribution Pr(Y ; X) given input sentence X to strengthen the depen- dency between them as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> Pr(Sty = k|Y )log P (Sty = k|Y )dY + logK.</p><p>Note that the input sequence X and style Sty are both input arguments and thus independent with each other. Therefore, posterior distribution Pr(Sty = k|Y ; X) = Pr(Sty = k|Y ). But the posterior probability distribution Pr(Sty = k|Y ) is actually unknown, we cannot compute the integration directly. Fortunately, with the help of variational inference maximiza- tion ( <ref type="bibr" target="#b1">Barber and Agakov, 2003)</ref>, we can train a parameterized function Q(Sty = k|Y ) which estimates the posterior distribution and maximize a lower bound of mutual information in Eq. Pr(Sty = k|Y ) log Q(Sty = k|Y )dY</p><formula xml:id="formula_8">+ Y ;X KL(Pr(Sty|Y ), Q(Sty|Y ))dY ≥ Y ;X K k=1 Pr(Sty = k|Y ) log Q(Sty = k|Y )dY = K k=1 Pr(Sty = k) Y |k;X log Q(Sty = k|Y )dY.<label>(8)</label></formula><p>Here KL(Pr(·)||Q(·)) indicates the KL- divergence distance between probability dis- tribution Pr(·) and Q(·). The inequality comes from the fact that the KL-divergence is always no less than zero and is tight when Pr(·) = Q(·).</p><p>We use the lower bound parameterized by func- tion Q in Eq. 8 as an additional maximization term. Intuitively, the lower bound is maximized if we can perfectly infer the style id from generated style-specific outputs by inference function Q. It also indicates that generated outputs will heavily depend on the input style id and outputs generated by different styles are distinguishable, which fol- lows our motivation of stylistic poetry generation. Now we will introduce how to design the poste- rior distribution estimation function Q and com- pute the integration in the lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Posterior Distribution Estimation</head><p>Given an output sequence Y , the function Q esti- mates the probability distribution of the style of sequence Y and therefore disentangles different styles. In this paper, we employ neural network to parametrize the posterior distribution estima- tion function Q. Specifically, we first compute the average character embeddings of sequence Y and then use a linear projection with softmax nor- malizer to get the style distribution. Formally, Q(Sty|Y ) is computed as</p><formula xml:id="formula_9">Q(Sty|Y ) = softmax(W · 1 T T i=1 e(yi)),<label>(9)</label></formula><p>where W ∈ R K×d is the linear projection matrix. Then the last thing to do is to compute the in- tegration over Y |k; X. However, the search space of sequence Y is exponential to the size of vocab- ulary. Hence it's impossible to enumerate all pos- sible sequence Y for computing the integration. Also, it is not differentiable if we sample sequence Y according to generation probability for approx- imation. Therefore, we propose to use expected character embedding to approximate the integra- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Expected Character Embedding</head><p>Inspired by previous works <ref type="bibr" target="#b9">(Kočisk´Kočisk´y et al., 2016)</ref>, we use expected character embedding to approxi- mate the probabilistic space of output sequences: we only generate an expected embedding se- quence and suppose Y |k; X has one hundred per- cent probability generating this one. Formally, Eq. 4 gives a probability distribution of generat- ing the i-th character given previous ones. Then the "expected" generated character embedding is</p><formula xml:id="formula_10">expect(i; k, X) = c∈V g(c|si)e(c),<label>(10)</label></formula><p>where expect(i; k, X) ∈ R d represents the ex- pected character embedding at i-th output given style id k and input sequence X and c ∈ V enu- merates all characters in the vocabulary. Then expect(i; k, X) is fed into the LSTM in decoder to update the hidden state for generating next expected character embedding:</p><formula xml:id="formula_11">si+1 = LST M decoder (si, [expect(i; k, X), ai+1]). (11)</formula><p>Finally, we use the expected embeddings expect(i; k, X) for i = 1, 2 . . . T as an approxi- mation of the whole probability space of Y |k; X. The lower bound in Eq. 8 can be rewritten as</p><formula xml:id="formula_12">Lreg = 1 K K k=1 log{softmax(W · 1 T T i=1 expect(i; k, X))[k]},<label>(12)</label></formula><p>where x[j] represents the j-th dimension of vector x.</p><p>We add the lower bound L reg to the overall training objective as a regularization term. The computing process is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Then for each training pair (X, Y ), we aim to maximize</p><formula xml:id="formula_13">Train(X, Y ) = T i=1 log p(yi|y1y2 . . . yi−1, X) + λLreg,<label>(13)</label></formula><p>where p(y i |y 1 y 2 . . . y i−1 , X) is style irrelevant generation likelihood and computed by setting one-hot style representation to an all-zero vector, and λ is a harmonic hyperparameter to balance the log-likelihood of generating sequence Y and the lower bound of mutual information. The first term ensures that the decoder can generate fluent and coherent outputs and the second term guaran- tees the style-specific output has a strong depen- dency on the one-hot style representation input.</p><p>Note that unlike machine translation task where the trained model is desired to generate exactly the same as the target, poetry generation encourages novelty as an important requirement. In Eq. 13, we can also enumerate the style id representation as a one-hot vector instead of an all-zero one. How- ever, this will force the generation of all styles to be close to the training target and discourage the diversity and novelty. Moreover, our model is not task-specific: the regularization term L reg can be added to other models conveniently for diverse or stylistic gen- erations. A Chinese quatrain has at most 4*7=28 characters and clear rhyme requirements. As the first attempt on unsupervised style disentangle- ment, we find Chinese poetry generation is an ideal choice for evaluation because we can bet- ter focus on stylistic generation rather than deal- ing with genre requirements. We will explore the applicability of our model on other languages and tasks for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Following the experimental settings in previous works ( <ref type="bibr" target="#b23">Zhang et al., 2017)</ref>, we conduct human judgment to evaluate the performance of our model and state-of-the-art baseline methods. We will first introduce the dataset, our model details, baseline methods and evaluation settings. Then we will present the experimental results and give fur- ther analysis about the evaluation. Finally, we will present some example generations for case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Model Details</head><p>We collect 168,000 poems (half wuyan: five char- acters per line, half qiyan: seven characters per line) over 1,000 years history of classical Chinese poetries as our corpus. We randomly select 80% of the corpus as training set, 10% as validation set and leave the rest for test. We extract all consec- utive sentences in a poem as training pairs (X, Y ) and feed them into our model. For hyperparameter settings of our model, i.e. stylistic poetry generation (SPG), we set the di- mensions of character embeddings and encoder hidden states as d = 512. The total number of styles is set to K = 10. Hence the dimension of decoder hidden states is 512 + 512 + 10 = 1034. We pack the training pairs to mini-batches and the size of mini-batches is set to 50. The harmonic hy- perparameter is set to λ = 0 for the first 50, 000 mini-batches as pretraining and λ = 1.0 for subse- quent batches. We use Adam optimizer <ref type="bibr" target="#b8">(Kingma and Ba, 2014</ref>) for stochastic gradient descent. We also employ dropout ( <ref type="bibr" target="#b15">Srivastava et al., 2014</ref>) strat- egy with dropout rate 0.2 to avoid overfitting prob- lem. We terminate the training process when the optimization loss on validation set is stable, i.e. 300, 000 mini-batches in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We consider the following state-of-the-art poetry generation models for comparison:</p><p>• Seq2seq ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) is the sequence-to-sequence model with attention mech- anism. Note that seq2seq is the basis of our SPG model. We can better analyze the improvement by style disentanglement from the comparison with seq2seq.</p><p>• Polish <ref type="bibr" target="#b3">(Yan, 2016)</ref> proposed an iterative schema to polish and refine the generated sen- tences for several times instead of a one-pass gen- eration.</p><p>• Memory (Zhang et al., 2017) incorporated external memory into poem generation for better novelty. The memory can be seen as a regulariza- tion to constrain the behavior of the neural model.</p><p>Rule-based and template-based methods are not considered as our baselines as they have been al- ready thoroughly compared in ( <ref type="bibr" target="#b5">He et al., 2012</ref>; Yan, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Settings</head><p>Following the settings in ( <ref type="bibr" target="#b23">Zhang et al., 2017)</ref>, we employ human judgment for evaluation. To better compare the ability to generate fluent and coherent poems, we fix the first sentence as input and let all models generate three subsequent sentences. The first sentences are randomly chosen from the po- ems in the test set. Therefore we also consider the original poems written by real poets for compari- son.</p><p>Note that our SPG model need a manually spec- ified style id as input. For fully automatic poem generation, we use the posterior style estimation function Q(·) to infer the style of the first sen- tence and then generate next three sentences se- quentially using the same style.</p><p>As previous works <ref type="bibr" target="#b11">(Manurung, 2004;</ref><ref type="bibr" target="#b22">Yi et al., 2017</ref>) did, we design four criteria for human judg- ment: Fluency (are the generated poems fluent and well-formed?), Coherence (is the topic of the whole quatrain consistent?), Meaningfulness</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group</head><p>Methods Fluency Coherence Meaningfulness Poeticness wuyan qiyan wuyan qiyan wuyan qiyan wuyan qiyan  (does the poem convey some certain messages?) and Poeticness (does the poem have some poetic features?). Each criterion needs to be scored on a 5-point scale ranging from 1 to 5. For each model, we generate 20 wuyan and 20 qiyan quatrains given the first sentence. We invite 10 experts who major in Chinese literature or are members of a poetry association to evaluate these quatrains. We divide the baseline methods into two groups: In the first group, we compare SPG with seq2seq ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) to present the advantages of style disentanglement; In the sec- ond group, we compare SPG with state-of-the-art poetry generation methods, memory <ref type="bibr" target="#b23">(Zhang et al., 2017)</ref> and polish <ref type="bibr" target="#b3">(Yan, 2016)</ref>, and the original po- ems written by poets to demonstrate the effective- ness of our algorithm. Note that the scores of hu- man judgment are relative, not absolute. Thus we compare with seq2seq separately to avoid mutual interference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>We report the average scores of expert judgments in <ref type="table">Table 1</ref>. From the experimental results, we have the following observations:</p><p>Firstly, the experimental results in group 1 demonstrate that SPG outperforms seq2seq, the basis of our model, by learning a style- disentangled generation model. The only differ- ence between seq2seq and our model is that we append a one-hot style id to the encoder state and add mutual information regularization in Eq. 12 to the loss function. Note that our model is fully un- supervised: the one-hot style id conveys no mean- ingful message unless the mutual information reg- ularization is considered. In other words, the one- hot style id and mutual information regularization cannot be torn apart. Therefore, the improvements over seq2seq all come from the part of style dis- entanglement modeling because seq2seq and our model share all the other components. The com- parisons in group 1 of <ref type="table">Table 1</ref> are sufficient to show the effectiveness of style modeling.</p><p>Secondly, the experimental results in group 2 show that SPG consistently outperforms two state- of-the-art poem generation methods. SPG is able to generate poetries in diverse styles without los- ing fluency and coherency. Our advantages are two-fold: On one hand, SPG better fits the diver- sity characteristic of human poetry writing. In- tuitively, seq2seq learns a generation model that mixes poems in various styles and is more likely to generate meaningless common sentences. By disentangling poems in different styles, our model can learn a more compact generation model for each style and write more fluent and meaningful poems. On the other hand, by fixing the style when generating three lines in a quatrain, SPG is able to generate more coherent poems.</p><p>One can also imagine that the generation prob- ability distribution actually consists of several peaks where each peak identifies a cluster of po- ems in similar styles. A traditional model mixes all the stuff together and learns a one-peak genera- tion model which is more likely to generate mean- ingless common sentences. In contrast, our model disentangles different clusters (styles) and learns a more accurate and compact generation model for each cluster (style). Hence our model can generate poems with higher quality and beat the baselines in terms of human evaluation. This observation demonstrates the effectiveness and robustness of our model. Finally, there is still a large margin between SPG and human poets. In this paper, we focus on poem generation in diverse styles. Other poetry characteristics are also important for the quality of generations. We will consider applying our style- disentangle regularization on other poem models for better performance in the future. <ref type="table" target="#tab_2">Style id  Keywords  1  loneliness, melancholy  2  the portrait of landscape  3  sorrow during roaming  4  hermit, rural scenes  5  grand scenery, regrets about old events  6  sorrow during drinking  7  emotions towards life experience  8  the portrait of hazy sceneries  9</ref> reminiscence, homesickness 10 sadness about seasons  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Interpretability of Learned Styles</head><p>In this subsection, we conduct experiments to un- derstand the semantic meanings of each learned style. To this end, we first generate 50 poems in each style and then manually select two or three keywords to sketch the overall feelings of every style. The keywords of all 10 styles are listed in <ref type="table" target="#tab_2">Table 2</ref>. Note that the learned styles may not strictly align the traditional writing styles recog- nized by human such as romantic or pastoral be- cause our model is fully unsupervised. Then we generate another 50 poems (5 poems per style) and ask the experts to classify these po- ems into the 10 styles according to the relationship between generated poems and style keywords in <ref type="table" target="#tab_2">Table 2</ref>. The human annotation results are shown in <ref type="figure" target="#fig_5">Fig. 3</ref>.</p><p>We can see that many learned styles can be suc- cessfully recognized by human with a higher prob- ability, e.g. the first style can be correctly clas- sified by 80% probability. This observation indi- cates that the learned styles of SPG are meaningful and recognizable through only two or three key- words. Also, the generated poems are diverse oth- erwise they cannot be differentiated and correctly classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>We present three poems generated by SPG with the same first sentence for case study. We only list the results of three most representative styles in <ref type="figure" target="#fig_6">Fig. 4</ref> and put other generation examples in sup- plementary materials (in Chinese). We can see that the poems generated by different style inputs dif- fer a lot from each other and follow the style key- words. To conclude, SPG can generate fluent and coherent poetries in diverse styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a stylistic poetry gen- eration (SPG) model to learn the ability to write poems in different styles under the same poetic imagery. To the best of our knowledge, we are the first effort at stylistic poetry generation in a fully unsupervised manner. Therefore, our model requires no expensive expert style annotation for thousands of poems in the database. We innova- tively employ mutual information, a concept in in- formation theory, to develop our model. Exper- imental results show that SPG is able to gener- ate fluent and coherent poetries in diverse styles without losing fluency and coherency. The learned styles are meaningful and recognizable given only two or three keywords. Our algorithm has been in- corporated into Jiuge 2 poetry generation system.</p><p>For future works, we will consider adopting the mutual information regularization for other text generation tasks which encourage stylistic gener- ation or diversity to improve their performances. Another intriguing direction is to refine our model for more task-specific scenarios. Besides, our model simplifies the prior of style distribution as a uniform distribution. This assumption could be further improved by an iteratively updating ap- proach. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of poems in diverse styles under the same keyword.</figDesc><graphic url="image-1.png" coords="1,312.73,222.54,207.35,109.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of style disentanglement by mutual information maximization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>7 instead: I(Pr(Sty), Pr(Y ; X)) − log K = Y ;X K k=1 Pr(Sty = k|Y ) log Pr(Sty = k|Y )dY = Y ;X K k=1 Pr(Sty = k|Y ) log Q(Sty = k|Y )dY + Y ;X K k=1 Pr(Sty = k|Y ) log Pr(Sty = k|Y ) Q(Sty = k|Y ) dY = Y ;X K k=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Group</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results on style recognition. Each row represents the human annotation of corresponding style generations. The diagonal blocks are correct classifications. Darker block indicates higher probability.</figDesc><graphic url="image-2.png" coords="8,73.09,219.49,216.05,215.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples generated by style 1,4 and 8 given the same first sentence. The keywords of the three styles are listed for convenience.</figDesc><graphic url="image-3.png" coords="9,74.73,68.97,145.14,131.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Representative keywords for poems generated 
by each learned style. 

</table></figure>

			<note place="foot" n="1"> https://en.wikipedia.org/wiki/Mutual_ information</note>

			<note place="foot" n="2"> https://jiuge.thunlp.cn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Chongxuan Li, Huimin Chen, Jiannan Liang, Zhipeng Guo and anony-mous reviewers for their insightful comments. This research is funded by the National 973 project <ref type="bibr">(No. 2014CB340501)</ref>. It is also par-tially supported by the NExT++ project, the Na-tional Research Foundation, Prime Minister's Of-fice, Singapore under its IRC@Singapore Funding Initiative.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The im algorithm: a variational approach to information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Neural Information Processing Systems</title>
		<meeting>the 16th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An expert system for the composition of formal spanish poetry. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gervás</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="181" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating chinese classical poems with statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00955</idno>
		<title level="m">Controllable text generation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A computational model of poetic creativity with neural network as measure of adaptive fitness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCBR-01 Workshop on Creative Systems</title>
		<meeting>the ICCBR-01 Workshop on Creative Systems</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An evolutionary algorithm approach to poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisar</forename><surname>Manurung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gaiku: Generating haiku with word associations norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Poetryme: a versatile platform for poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo Gonçalo</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Concept Invention, and General Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09889</idno>
		<title level="m">Chinese poetry generation with planning based neural network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoko</forename><surname>Tosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Entertainment Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="191" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2016. i, poet: Automatic poetry composition through recurrent neural networks with iterative polishing schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2238" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2013. i, poet: Automatic chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2197" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunda</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07632</idno>
		<title level="m">Generating thematic chinese poetry with conditional variational autoencoder</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating chinese classical poems with rnn encoderdecoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="211" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Flexible and creative chinese poetry generation using neural memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03773</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
