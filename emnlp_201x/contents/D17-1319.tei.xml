<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Compute Science</orgName>
								<orgName type="department" key="dep2">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Compute Science</orgName>
								<orgName type="department" key="dep2">Center for Language and Speech Processing</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<postCode>21218</postCode>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2945" to="2950"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce Zipporah, a fast and scal-able data cleaning system. We propose a novel type of bag-of-words translation feature , and train logistic regression models to classify good data and synthetic noisy data in the proposed feature space. The trained model is used to score parallel sentences in the data pool for selection. As shown in experiments, Zipporah selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one noisy dataset, Zipporah achieves a 2.1 BLEU score improvement with using 1/5 of the data over using the entire corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical machine translation (SMT) systems re- quire the use of parallel corpora for training the internal model parameters. Data quality is vital for the performance of the SMT system <ref type="bibr" target="#b15">(Simard, 2014)</ref>. To acquire a massive parallel corpus, many researchers have been using the Internet as a re- source, but the quality of data acquired from the Internet usually has no guarantee, and data clean- ing/data selection is needed before the data is used in actual systems. Usually data cleaning refers to getting rid of a small amount of very noisy data from a large data pool, and data selection refers to selecting a small subset of clean (or in-domain) data from the data pool; both have the objective of improving translation performances. For practi- cal purposes, it is highly desirable to perform data selection in a very fast and scalable manner. In this paper we introduce Zipporah 1 , a fast and scal- able system which can select an arbitrary size of good data from a large noisy data pool to be used in SMT model training. <ref type="bibr">1</ref> https://github.com/hainan-xv/zipporah</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Many researchers have studied the data clean- ing/selection problem. For data selection, there have been a lot of work on selecting a sub- set of data based on domain-matching. <ref type="bibr" target="#b3">Duh et al. (2013)</ref> used a neural network based lan- guage model trained on a small in-domain cor- pus to select from a larger data pool. <ref type="bibr" target="#b11">Moore and Lewis (2010)</ref> computed cross-entropy between in- domain and out-of-domain language models to se- lect data for training language models. XenC <ref type="bibr" target="#b13">(Rousseau, 2013)</ref>, an open-source tool, also se- lects data based on cross-entropy scores on lan- guage models. <ref type="bibr" target="#b0">Axelrod et al. (2015)</ref> utilized part- of-speech tags and used a class-based n-gram lan- guage model for selecting in-domain data. There are a few works that utilize other metrics. <ref type="bibr">Lü et al. (2007)</ref> redistributed different weights for sen- tence pairs/predefined sub-models. Shah and Spe- cia (2014) described experiments on quality esti- mation which, given a source sentence, select the best translation among several options. The qe- clean system <ref type="bibr" target="#b2">(Denkowski et al., 2012;</ref><ref type="bibr" target="#b5">Dyer et al., 2010;</ref><ref type="bibr" target="#b7">Heafield, 2011</ref>) uses word alignments and language models to select sentence pairs that are likely to be good translations of one another.</p><p>For data cleaning, a lot of researchers worked on getting rid of noising data. <ref type="bibr" target="#b17">Taghipour et al. (2011)</ref> proposed an outlier detection algorithm which leads to an improved translation quality when trimming a small portion of data. <ref type="bibr" target="#b1">Cui et al. (2013)</ref> used a graph-based random walk algorithm to do bilingual data cleaning. BiTextor <ref type="bibr">(EspláGomis and Forcada, 2009</ref>) utilizes sentence align- ment scores and source URL information to filter out bad URL pairs and selects good sentence pairs.</p><p>In this paper we propose a novel way to eval- uate the quality of a sentence pair which runs efficiently. We do not make a clear distinction between data selection and data cleaning in this work, because under different settings, our method can perform either based on the computed quality scores of sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The method in this paper works as follows: we first map all sentence pairs into the proposed fea- ture space, and then train a simple logistic regres- sion model to separate known good data and (syn- thetic) bad data. Once the model is trained, it is used to score sentence pairs in the noisy data pool. Sentence pairs with better scores are added to the selected subset until the desired size constraint is met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features</head><p>Since good adequacy and fluency are the major two elements that constitute a good parallel sen- tence pair, we propose separate features to address both of them. For adequacy, we propose bag-of- words translation scores, and for fluency we use n- gram language model scores. For notational sim- plicity, in this section we assume the sentence pair is French-English in describing the features, and we will use subscripts f and e to indicate the lan- guages. In designing the features, we prioritize efficiency as well as performance since we could be dealing with corpora of huge sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Adequacy scores</head><p>We view each sentence as a bag of words, and de- sign a "distance" between the sentence pairs based on a bag-of-words translation model. To do this, we first generate dictionaries from an aligned cor- pus, and represent them as sets of triplets. For- mally,</p><formula xml:id="formula_0">D f2e = {(w f i , w e i , p(w e i |w f i )), i = 1, ..., m}.</formula><p>Given a sentence pair (s f , s e ) in the noisy data pool, we represent the two sentence as two sparse word-frequency vectors v f and v e . For exam- ple for any French word w f , we have</p><formula xml:id="formula_1">v f [w f ] = c(w f ,s f ) l(s f )</formula><p>, where c(w f , s f ) is the number of occur- rences of w f in s f and l(s f ) is the length of s f . We do the same for v e . Notice that by construction, both vectors add up to 1 and represent a proper probability distribution on their respective vocab- ularies. Then we "translate" v f into v e , based on the probabilistic f2e dictionary, where</p><formula xml:id="formula_2">v e [w e ] = w f v f [w f ]p(w e |w f )</formula><p>For a French word w that does not appear in the dictionary, we keep it as it is in the translated vec- tor, i.e. assume there is an entry of (w, w, 1.0) in the dictionary. Since the dictionary is probabilis- tic, the elements in v e also add up to 1, and v e represents another probability distribution on the English vocabulary. We compute the (smoothed) cross-entropy between v e and v e ,</p><formula xml:id="formula_3">xent(v e , v e ) = we v e [w e ] log 1 v e [w e ] + c (1)</formula><p>where c is a smoothing constant to prevent the de- nominator from being zero, and set c = 0.0001 for all experiments in this paper (more about this in Section 4). We perform similar procedures for English-to- French, and compute xent(v f , v f ). We define the adequacy score as the sum of the two:</p><formula xml:id="formula_4">adequacy(s f , s e ) = xent(v e , v e ) + xent(v f , v f ) 3.1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Fluency scores</head><p>We train two n-gram language models with a clean French and English corpus, and then for each sentence pair (s f , s e ), we score each sentence with the corresponding model, F ngram (s f ) and F ngram (s e ), each computed as the ratio between the sentence negative log-likelihood and the sentence length. We define the fluency score as the sum of the two:</p><formula xml:id="formula_5">fluency(s f , s e ) = F ngram (s f ) + F ngram (s e )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synthetic noisy data generation</head><p>We generate synthetic noisy data from good data, and make sure the generated noisy data include sentence pairs with a) good fluency and bad ad- equacy, b) good adequacy and bad fluency and c) bad both.</p><p>Respectively, we generate 3 types of "noisy" sentence pairs from a good corpus: a) shuffle the sentences in the target language file (each sentence in the source language would be aligned to a ran- dom sentence in the target language); b) shuffle the words within each sentence (each sentence will be bad but the pairs are good translations in the "bag- of-words" sense); c) shuffle both the sentences and words. We emphasize that, while the synthetic data might not represent "real" noisy data, it has the following advantages: 1) each type of noisy data is equally represented so the classifier has to do well on all of them; 2) the data generated this way would be among the hardest to classify, espe- cially type a and type b, so if a classifier separates such hard data with good performance, we expect it to also be able to do well in real world situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Logistic regression feature mapping</head><p>Figure 1: newstest09 fr-en data in the feature space We plot the newstest09 data (original and auto- generated noisy ones as described in Section 3.2) into the proposed feature space in <ref type="figure">Figure 1</ref>. We observe that the clusters are quite separable, though the decision function would not be linear. We map the features into higher order forms of (x n , y n ) in order for logistic regression to train a non-linear decision boundary. <ref type="bibr">2</ref> We use n = 8 in this work since it gives the best classification per- formance on the newstest09 fr-en corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Hyper-parameter Tuning</head><p>We conduct experiments to determine the value of the constant c in the smoothed cross-entropy computation in equation 1. We choose the new- stest09 German-English corpus, and shuffle the sentences in the English file and combine the orig- inal (clean) corpus with the shuffled (noisy) cor- pus into a larger corpus, where half of them are good sentence pairs. We set different values of c and use the adequacy scores to pick the better half, <ref type="bibr">2</ref> We avoid using multiple mappings of one feature be- cause we want the scoring function to be monotonic both w.r.t x and y, which could break if we allow multiple higher-order mappings of the same feature and they end up with weights with different signs. and compute the retrieval accuracy. <ref type="table" target="#tab_0">Table 1</ref> shows that the best value for c is 0.0001, and we use that in all experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluate Zipporah on 3 language pairs, French-English, German-English and Spanish- English. The noisy web-crawled data comes from an early version of http://statmt.org/ paracrawl. The number of words are (in mil- lions) 340, 487 and 70 respectively. To generate the dictionaries for computing the adequacy scores, we use fast align <ref type="bibr" target="#b4">(Dyer et al., 2013</ref>) to align the Europarl ( <ref type="bibr" target="#b8">Koehn, 2005</ref>) cor- pus and generate probabilistic dictionaries from the alignments. We set the n-gram order to be 5 and use SRILM <ref type="bibr" target="#b16">(Stolcke et al., 2011</ref>) to train lan- guage models on the Europarl corpus and generate the n-gram scores.</p><p>For each language pair, we use scikit-learn (Pe- dregosa et al., 2011) to train a logistic regression model to classify between the original and the syn- thetic noisy corpus of newstest09, and the trained model is used to score all sentence pairs in the data pool. We keep selecting the best ones until the de- sired number of words is reached.</p><p>To evaluate the quality, we train a Moses ( <ref type="bibr" target="#b9">Koehn et al., 2007)</ref> SMT system on selected data, and evaluate each trained SMT system on 3 test corpora: newstest2011 which contains 3003 sen- tence pairs, and a random subset of the TED-talks corpus and the movie-subtitle corpus from OPUS ( <ref type="bibr" target="#b18">Tiedemann, 2012)</ref>, each of which contains 3000 sentence pairs. <ref type="table" target="#tab_2">Tables 2, 3 and 4</ref> show the BLEU performance of the selected subsets of the Zipporah system compared to the baseline, which selects sentence pairs at random; for comparison, we also give the BLEU performance of systems trained on Eu- roparl. The Zipporah system gives consistently better performance across multiple datasets and multiple languages than the baseline. <ref type="bibr">3</ref>     <ref type="table">Table 4</ref>: BLEU Performance, Spanish-English</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head><p>In particular, for the Germen-English corpus, when selecting less than 2% of the data (10 mil- lion words), on the TED-talk dataset, Zipporah achieves a 5.5 BLEU score improvement over the baseline; by selecting less than 4% of the data (20 million words) the system gives better perfor- mance than using all data. Peak performance is achieved when selecting 100 million words, where an improvement of 2.1 BLEU score over all data is achieved on the movie-subtitle dataset, despite only using less than 1/5 of the data.  <ref type="bibr" target="#b2">Denkowski et al., 2012;</ref><ref type="bibr" target="#b5">Dyer et al., 2010;</ref><ref type="bibr" target="#b7">Heafield, 2011</ref>) and the random baseline. We use the same data when running qe- clean, with Europarl for training and newstest09 for dev. While they both perform comparably and better than the baseline, Zipporah achieves a bet- ter peak in all the datasets, and the peak is usu- ally achieved when selecting a smaller number of words compared to qe-clean, Another advantage of Zipporah is it allows the user to select an arbi- subsets of the Zipporah system can surpass that of Europarl, although the Europarl corpus acts like an "oracle" in the sys- tem, upon which the dictionaries and language models for feature computations are trained. <ref type="figure">Figure 4</ref>: BLEU performance of Zipporah, qe- clean and random on TED-talks, Spanish-English trary size from the pool. <ref type="bibr">4</ref> We also want to empha- size that unlike qe-clean, which requires running word-alignments for all sentence pairs in the noisy corpus, Zipporah's feature computation is simple, fast and can easily be scaled for huge datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we introduced Zipporah, a fast data selection system for noisy parallel corpora. SMT results demonstrate that Zipporah can select a high-quality subset of the data and significantly improve SMT performance.</p><p>Zipporah currently selects sentences based on the "individual quality" only, and we plan in future work to also consider other factors, e.g. encourage selection of a subset that has a better n-gram cov- erage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU performance of Zipporah, qeclean and random on TED-talks, French-English</figDesc><graphic url="image-2.png" coords="4,311.41,224.86,210.00,129.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Tuning cross-entropy constant c 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : BLEU Performance, French-English</head><label>2</label><figDesc></figDesc><table>BLEU 
newstest11 ted-talk 
subtitle 

num-words rand zipp rand zipp rand zipp 

10 million 13.6 17.6 17.0 22.5 11.4 15.8 
20 million 14.8 18.4 18.9 23.7 12.7 16.9 
50 million 16.3 19.2 20.8 24.8 13.9 17.8 
100 million 16.9 19.5 21.3 25.0 14.0 18.3 
200 million 18.0 19.2 22.9 24.2 15.3 17.9 
487 mil (all) 
18.7 
23.5 
16.2 

Europarl 
17.5 
21.5 
14.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : BLEU Performance, German-English</head><label>3</label><figDesc></figDesc><table>BLEU newstest11 ted-talk 
subtitle 

num-words rand zipp rand zipp rand zipp 

10 million 24.2 25.5 25.9 28.3 17.9 19.8 
20 million 25.3 26.2 28.2 29.7 19.3 21.2 
50 million 26.6 26.5 29.9 30.4 21.3 21.4 
70 mil (all) 
27.1 
30.3 
21.8 

Europarl 
25.4 
28.4 
19.8 

</table></figure>

			<note place="foot" n="3"> We also point out that the performance of the selected</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project was funded by Google Faculty Re-search Award. The authors would like to thank Shuoyang Ding, Tongfei Chen, Matthew Wiesner, Winston Wu, Huda Khayrallah and Adi Renduch-intala for their help during this project. The au-thors would also like to thank Penny Peng for her moral support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-based n-gram language difference models for data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Martindale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johns</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT (International Workshop on Spoken Language Translation)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bilingual data cleaning for smt using graphbased random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cmu-avenue french-english translation system. In 4 In the plots the data points of Zipporah and qe-clean are not aligned because we always select multiples of million words, but it is hard to do so with qe-clean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hanneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL 2012 Workshop on Statistical Machine Translation</title>
		<meeting>the NAACL 2012 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptation data selection using neural language models: Experiments in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bitextor, a free/open-source software to harvest translation memories from multilingual websites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miquel</forename><surname>Esplá</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Gomis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forcada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit XII, Ottawa, Canada. Association for Machine Translation in the Americas</title>
		<meeting>MT Summit XII, Ottawa, Canada. Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation performance by training data selection and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 conference short papers</title>
		<meeting>the ACL 2010 conference short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Xenc: An open-source tool for data selection in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quality estimation for translation selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Conference of the European Association for Machine Translation</title>
		<meeting>the 17th Annual Conference of the European Association for Machine Translation<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clean data for training statistical mt: The case of mt contamination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the Association for Machine Translation in the Americas (AMTA)</title>
		<meeting>the 11th Conference of the Association for Machine Translation in the Americas (AMTA)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Srilm at sixteen: Update and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel corpus refinement as an outlier detection algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Machine Translation Summit (MT Summit XIII)</title>
		<meeting>the 13th Machine Translation Summit (MT Summit XIII)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="414" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
