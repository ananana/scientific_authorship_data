<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Context-Aware Convolutional Filters for Text Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
							<email>dinghan.shen@duke.edu, renqiang@nec-labs.com, yitong.li@duke.edu,</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
							<email>lcarin@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Context-Aware Convolutional Filters for Text Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1839" to="1848"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-aware convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis , answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-aware filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-aware filters, we further validate and rationalize the effectiveness of proposed framework.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last few years, convolutional neural net- works (CNNs) have demonstrated remarkable progress in various natural language process- ing applications <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>), includ- ing sentence/document classification <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b41">Zhang et al., 2015;</ref>), text se- quence matching ( <ref type="bibr" target="#b11">Hu et al., 2014;</ref><ref type="bibr" target="#b40">Yin et al., 2016;</ref><ref type="bibr" target="#b28">Shen et al., 2017)</ref>, generic text representations ( <ref type="bibr" target="#b8">Gan et al., 2016;</ref>, language modeling ( , machine trans- lation ( <ref type="bibr" target="#b9">Gehring et al., 2017</ref>) and abstractive sen- tence summarization <ref type="bibr" target="#b9">(Gehring et al., 2017)</ref>. CNNs are typically applied to tasks where feature extrac- tion and a corresponding supervised task are ap- proached jointly ( <ref type="bibr" target="#b16">LeCun et al., 1998)</ref>. As an en- coder network for text, CNNs typically convolve a set of filters, of window size n, with an input- sentence embedding matrix obtained via word2vec ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) or Glove ( <ref type="bibr" target="#b22">Pennington et al., 2014)</ref>. Different filter sizes n may be used within the same model, exploiting meaningful semantic features from different n-gram fragments.</p><p>The learned weights of CNN filters, in most cases, are assumed to be fixed regardless of the in- put text. As a result, the rich contextual informa- tion inherent in natural language sequences may not be fully captured. As demonstrated in <ref type="bibr" target="#b3">Cohen and Singer (1999)</ref>, the context of a word tends to greatly influence its contribution to the final super- vised tasks. This observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leverag- ing the fact that the same words or phrases may have different meaning or imply different things, depending on context.</p><p>Several research efforts have sought to incor- porate contextual information into CNNs to adap- tively extract text representations. One common strategy is the attention mechanism, which is typ- ically employed on top of a CNN (or Long Short- Term Memory (LSTM)) layer to guide the extrac- tion of semantic features. For the embedding of a single sentence, <ref type="bibr" target="#b17">Lin et al. (2017)</ref> proposed a self- attentive model that attends to different parts of a sentence and combines them into multiple vector representations. However, their model needs con- siderably more parameters to achieve performance gains over traditional CNNs. To match sentence pairs, <ref type="bibr" target="#b40">Yin et al. (2016)</ref> introduced an attention- based CNN model, which re-weights the convo- lution inputs or outputs, to extract interdepen-dent sentence representations. <ref type="bibr" target="#b37">Wang et al. (2016)</ref>; <ref type="bibr" target="#b34">Wang and Jiang (2017)</ref> explore a compare and ag- gregate framework to directly capture the word- by-word matching between two paired sentences. However, these approaches suffer from the prob- lem of high matching complexity, since a simi- larity matrix between pairwise words needs to be computed, and thus it is computationally ineffi- cient or even prohibitive when applied to long sen- tences ( <ref type="bibr" target="#b21">Mou et al., 2016)</ref>.</p><p>In this paper, we propose a generic approach to learn context-aware convolutional filters for nat- ural language understanding. In contrast to tra- ditional CNNs, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger model- ing flexibility and capacity. Specifically, we intro- duce a meta network to generate a set of context- aware filters, conditioned on specific input sen- tences; these filters are adaptively applied to either the same (Section 3.2) or different (Section 3.3) text sequences. In this manner, the learned filters vary from sentence to sentence and allow for more fine-grained feature abstraction.</p><p>Moreover, since the generated filters in our framework can adapt to different conditional infor- mation available (labels or paired sentences), they can be naturally generalized to model sentence pairs. In this regard, we propose a novel bidirec- tional filter generation mechanism to allow inter- actions between sentence pairs while constructing context-aware representations.</p><p>We investigate the effectiveness of our Adap- tive Context-sensitive CNN (ACNN) framework on several text processing tasks: ontology classi- fication, sentiment analysis, answer sentence se- lection and paraphrase identification. We show that the proposed methods consistently outper- forms the standard CNN and attention-based CNN baselines. Our work provides a new perspective on how to incorporate contextual information into text representations, which can be combined with more sophisticated structures to achieve even bet- ter performance in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning deep text representations has attracted much attention recently, since they can potentially benefit a wide range of NLP applications <ref type="bibr" target="#b4">(Collobert et al., 2011;</ref><ref type="bibr" target="#b14">Kim, 2014;</ref><ref type="bibr" target="#b35">Wang et al., 2017a;</ref><ref type="bibr" target="#b26">Shen et al., 2018a;</ref><ref type="bibr" target="#b31">Tang and de Sa, 2018;</ref>. CNNs have been extensively in- vestigated as the encoder networks of natural lan- guage. Our work is in line with previous efforts on improving the adaptivity and flexibility of con- volutional neural networks <ref type="bibr" target="#b12">(Jeon and Kim, 2017;</ref><ref type="bibr" target="#b7">De Brabandere et al., 2016)</ref>. <ref type="bibr" target="#b12">Jeon and Kim (2017)</ref> proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation. De <ref type="bibr" target="#b7">Brabandere et al. (2016)</ref> introduced an architecture to gen- erate the future frames conditioned on given im- age(s), by adapting the CNN filter weights to the motion within previous video frames. Although CNNs have been widely adopted in a large number of NLP applications, improving the adaptivity of vanilla CNN modules has been considerably less studied. To the best of our knowledge, the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences.</p><p>Our use of a meta network to generate pa- rameters for another network is directly inspired by the recent success of hypernetworks for text- generation tasks ( <ref type="bibr" target="#b10">Ha et al., 2017)</ref>, and dynamic parameter-prediction for video-frame generation <ref type="bibr" target="#b7">(De Brabandere et al., 2016)</ref>. In contrast to these works that focus on generation problems, our model is based on context-aware CNN fil- ters and is aimed at abstracting more informa- tive and predictive sentence features. Most sim- ilar to our work, <ref type="bibr" target="#b18">Liu et al. (2017)</ref> designed a meta network to generate compositional functions over tree-structured neural networks for encapsu- lating sentence features. However, their model is only suitable for encoding individual sentences, while our framework can be readily generalized to capture the interactions between sentence pairs. Moreover, our framework is based on CNN mod- els, which is advantageous due to fewer parame- ters and highly parallelizable computations rela- tive to sequential-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic CNN for text representations</head><p>The CNN architectures in <ref type="bibr" target="#b14">(Kim, 2014;</ref><ref type="bibr" target="#b4">Collobert et al., 2011</ref>) are typically utilized for extracting sentence representations, by a composition of a convolutional layer and a max-pooling operation over all resulting feature maps. Let the words of a sentence of length T (padded where necessary) be x 1 , x 2 , ... , x T . The sentence can be represented as a matrix X ∈ R d×T , where each column rep- resents a d-dimensional embedding of the corre- sponding word.</p><p>In the convolutional layer, a set of filters with weights W ∈ R K×h×d is convolved with ev- ery window of h words within the sentence, i.e., {x 1:h , x 2:h+1 , . . . , x T −h+1:T }, where K is the number of output feature maps (and filters). In this manner, feature maps p for these h-gram text frag- ments are generated as:</p><formula xml:id="formula_0">p i = f (W × x i:i+h−1 + b)<label>(1)</label></formula><p>where i = 1, 2, ..., T − h + 1 and × denotes the convolution operator at the ith shift location. Parameter b ∈ R K is the bias term and f (·) is a non-linear function, implemented as a rectified linear unit (ReLU) in our experiments. The out- put feature maps of the convolutional layer, i.e., p ∈ R K×(T −h+1) are then passed to the pooling layer, which takes the maximum value in every row of p, forming a K-dimensional vector, z. This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments. Moreover, the max-over-time nature of the pooling operation <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>) guarantees that the size of the obtained representation is independent of the sentence length. Note that in basic CNN sentence encoders, fil- ter weights are the same for different inputs, which may be suboptimal for feature extraction <ref type="bibr" target="#b7">(De Brabandere et al., 2016)</ref>, especially in the case where conditional information is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning context-sensitive filters</head><p>The proposed architecture to learn context- sensitive filters is composed of two principal mod- ules: (i) a filter generation module, which pro- duces a set of filters conditioned on the input sen- tence; and (ii) an adaptive convolution module, which applies the generated filters to an input sen- tence (this sentence may be either the same as or different from the first input, as discussed further in Section 3.3). The two modules are jointly differ- entiable, and the overall architecture can be trained in an end-to-end manner. Since the generated fil- ters are sample-specific, our ACNN feature extrac- tor for text tends to have stronger predictive power than a basic CNN encoder. The general ACNN framework is shown schematically in <ref type="figure">Figure 1</ref>. Filter generation module Instead of utilizing fixed filter weights W for different inputs (as <ref type="formula" target="#formula_0">(1))</ref>, our model generates a set of filters conditioned on the input sentence X. Given an input X, the filter-generation module can be implemented, in principle, as any deep (differentiable) architecture. However, in order to handle input sentences of variable length common in natural language, we design a generic filter generation module to pro- duce filters with a predefined size. First, the input X is encapsulated into a fixed- length vector (code) z with the dimension of l, via a basic CNN model, where one convolutional layer is employed along with the pooling opera- tion (as described in Section 3.1). On top of this hidden representation z, a deconvolutional layer, which performs transposed operations of convolu- tions ( <ref type="bibr" target="#b23">Radford et al., 2016)</ref>, is further applied to produce a unique set of filters for X (as illustrated in <ref type="figure">Figure 1</ref>):</p><formula xml:id="formula_1">z = CNN(X; θ e ), (2) f = DCNN(z; θ d ) ,<label>(3)</label></formula><p>where θ e and θ d are the learned parameters in each layer of the filter-generating module, respec- tively. Specifically, we convolve z with a filter of size (f s , l, k x , k y ), where f s is the number of generated filters and the kernel size is (k x , k y ). The output will be a tensor of shape (f s , k x , k y ).</p><p>Since the dimension of hidden representation z is independent of input-sentence length, this frame- work guarantees that the generated filters are of the same shape and size for every sentence. Intu- itively, the encoding part of filter generation mod- ule abstracts the information from sentence X into z. Based on this representation, the deconvolu- tional up-sampling layer determines a set of fixed- size, fine-grained filters f for the specific input.</p><p>Adaptive convolution module The adaptive convolution module takes as inputs the generated filters f and an input sentence. This sentence and the input to the filter-generation module may be identical (as in <ref type="figure">Figure 1</ref>) or different (as in <ref type="figure" target="#fig_1">Fig- ure 2</ref>). With the sample-specific filters, the input sentence is adaptively encoded, again, via a basic CNN architecture as in Section 3.1, i.e., one con- volutional and one pooling layer. Notably, there are no additional parameters in the adaptive con- volution module (no bias term is employed).</p><p>Our ACNN framework can be seen as a gen- eralization of the basic CNN, which can be rep- resented as an ACNN by setting the outputs of the filter-generation module to a constant, regard- less of the contextual information from input sen- tence(s). Because of the learning-to-learn (Thrun and Pratt, 2012) nature of the proposed ACNN framework, it tends to have greater representa- tional power than the basic CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extension to text sequence matching</head><p>Considering the ability of our ACNN framework to generate context-aware filters, it can be nat- urally generalized to the task of text sequence matching. In this section, we will describe the proposed Adaptive Question Answering (AdaQA) model in the context of answer sentence selection task. Note that the corresponding model can be readily adapted to other sentence matching prob- lems as well (see Section 5.2).</p><p>Given a factual question q (associated with a list of candidate answers {a 1 , a 2 , . . . , a m } and their corresponding labels y = {y 1 , y 2 , . . . , y m }), the goal of the model is to identify the correct answers from the set of candidates. For i = 1, 2, . . . , m, if a i correctly answers q, then y i = 1, and other- wise y i = 0. Therefore, the task can be cast as a classification problem where, given an unlabeled question-answer pair (q i , a i ), we seek to predict the judgement y i .</p><p>Conventionally, a question q and an answer a are independently encoded by two basic CNNs to fixed-length vector representations, denoted h q and h a , respectively. They are then directly em- ployed to predict the judgement y. This strategy could be suboptimal, since no communication (in- formation sharing) occurs between the question- answer pair until the top prediction layer. Intu- itively, while the model is inferring the representa- tion for a question, if the meaning of the answer is  The AdaQA model can be divided into three mod- ules: filter generation, adaptive convolution, and matching modules, as depicted schematically in <ref type="figure" target="#fig_1">Figure 2</ref>. Assume there is a question-answer pair to be matched, represented by word-embedding matrices, i.e. Q ∈ R Tq×d and A ∈ R Ta×d , where d is the embedding dimension and T q and T a are respective sentence lengths. First, they are passed to two filter-generation modules, to produce two sets of filters that encapsulate features of the cor- responding input sentences. Similar to the setup in Section 3.2, we also employ a two-step process to produce the filters. For a question Q, the generat- ing process is:</p><formula xml:id="formula_2">z q = CNN(Q; θ q e ),<label>(4)</label></formula><formula xml:id="formula_3">f q = DCNN(z q ; θ q d )<label>(5)</label></formula><p>where CNN and DCNN denote the basic CNN unit and deconvolution layer, respectively, as discussed in Section 2.1. Parameters θ q e and θ q d are to be learned. The same process can be utilized to pro- duce encodings z a and filters f a for the answer input, A, with parameters θ a e and θ a d , respectively. The two sets of filter weights are then passed to adaptive convolution modules, along with Q and A, to obtain the extracted question and answer embeddings. That is, the question embedding is convolved with the filters produced by the answer and vise versa (ψ q and ψ a are the bias terms to be learned). The key idea is to abstract informa- tion from the answer (or question) that is perti- nent to the corresponding question (or answer).</p><p>Compared to a Siamese CNN architecture <ref type="bibr" target="#b2">(Bromley et al., 1994)</ref>, our model selectively encapsu- lates the most important features for judgement prediction, removing less vital information. We then employ the question and answer representa- tions h q ∈ R n h , h a ∈ R n h as inputs to the match- ing module (where n h is the dimension of ques- tion/answer embeddings). Following <ref type="bibr" target="#b21">Mou et al. (2016)</ref>, the matching function is defined as:</p><formula xml:id="formula_4">t = [h q ; h a ; h q − h a ; h q h a ] (6) p(y = 1|h q , h a ) = MLP(t; η )<label>(7)</label></formula><p>where − and denote an element-wise sub- traction and element-wise product, respectively.</p><p>[h a ; h b ] indicates that h a and h b are stacked as column vectors. The resulting matching vector t ∈ R 4n h is then sent through an MLP layer (with sigmoid activation function and parameters η to be learned) to model the desired conditional dis- tribution p(y i = 1|h q , h a ).</p><p>Notably, we share the weights of filter gener- ating networks for both the question and answer, so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters. All three modules in AdaQA model are jointly trained end-to-end. Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks, such as paraphrase identification (see Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Connections to attention mechanism</head><p>The adaptive context-aware filter generation mechanism proposed here bears close resem- blance to attention mechanism ( <ref type="bibr" target="#b40">Yin et al., 2016;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b38">Xiong et al., 2017</ref>) widely adopted in the NLP community, in the sense that both methods intend to incorporate rich contextual information into text representations. However, at- tention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers, and assigns different weights to each unit according to a context vector. By contrast, in our context-aware filter generation mechanism, the contextual infor- mation is inherently encoded into the convolu- tional filters, which directly interact with the input sentence during the convolution encoding opera- tion. Notably, according to our experiments, the proposed filter generation module can be readily combined with (standard) attention mechanisms to further enhance the modeling expressiveness of CNN encoder.  , we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset. We also consider the task of paraphrase identification with the Quora Question Pairs dataset, with the same data splits as in ( <ref type="bibr" target="#b36">Wang et al., 2017b)</ref>. A summary of all datasets is pre- sented in <ref type="table">Table 1</ref>.</p><p>Training Details For the document classifica- tion experiments, we randomly initialize the word embeddings uniformly within [−0.001, 0.001] and update them during training. For the generated filters, we set the window size as h = 5, with K = 100 feature maps (the dimension of z is set as 100). For direct comparison, we employ the same filter shape/size settings as in our ba- sic CNN implementation. A one-layer architec- ture is utilized for both the CNN baseline and the ACNN model, since we did not observe significant performance gains with a multilayer architecture. The minibatch size is set as 128, and a dropout rate of 0.2 is utilized on the embedding layer. We observed that a larger dropout rate (e.g., 0.5) will hurt performance on document classifications and make training significantly slower. For the sentence matching tasks, we initialized the word embeddings with 50-dimensional Glove ( <ref type="bibr" target="#b22">Pennington et al., 2014</ref>) word vectors pretrained from <ref type="bibr">Wikipedia 2014 and</ref><ref type="bibr">Gigaword 5 (Pennington et al., 2014</ref>) for all model variants. As for the filters, we set the window size as h = 5, with K = 300 feature maps. As described in Section 3.3, the vector t, output from the match- ing module, is fed to the prediction layer, imple- mented as a one-layer MLP followed by the sig- moid function. We use Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2014)</ref> to train the models, with a learning rate of 3 × 10 −4 . Dropout ( <ref type="bibr" target="#b29">Srivastava et al., 2014</ref>), with a rate of 0.5, is employed on the word embed- ding layer. The hyperparameters are selected by choosing the best model on the validation set. All models are implemented with <ref type="bibr">TensorFlow (Abadi et al., 2016)</ref> and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12GB mem- ory.</p><p>Baselines For document classification, we con- sider several baseline models: (i) ngrams <ref type="bibr" target="#b41">(Zhang et al., 2015)</ref>, a bag-of-means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams (up to 5-grams) from the training set and use their correspond- ing counts as features; (ii) small/large word CNN ( <ref type="bibr" target="#b41">Zhang et al., 2015</ref> Evaluation Metrics For document categoriza- tion and paraphrase identification tasks, we em- ploy the percentage of correct predictions on the test set to evaluate and compare different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Yelp For the answer sentence selection task, mean av- erage precision (MAP) and mean reciprocal rank (MRR) are utilized as the corresponding evalua- tion metrics.</p><note type="other">P. DBpedia CNN-based Baseline Models ngrams * 4.36 1.37 ngrams TFIDF * 4.56 1.31 Small word CNN * 5.54 1.85 Large word CNN * 4.89 1.72 Self-attentive Embedding ‡ 3.92 1.14 Deep CNN (9 layer) † 4.88 1.35 Deep CNN (17 layer) † 4.50 1.40 Deep CNN (29 layer) † 4.28 1.29 Our Implementations S-CNN 14.48 22.35 S-ACNN 6.41 5.16 M-CNN 4.58 1.66 M-ACNN 3.89 1.07</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Document Classification</head><p>To explicitly explore whether our ACNN model can leverage the input-aware filter weights for bet- ter sentence representation, we perform a compar- ison between the basic CNN and ACNN models with only a single filter, which are denoted as S- CNN, S-ACNN, respectively (this setting may not yield best overall performance, since only a sin- gle filter is used, but it allows us to isolate the im- pact of adaptivity). As illustrated in <ref type="table" target="#tab_2">Table 2</ref>, S- ACNN significantly outperforms S-CNN on both datasets, demonstrating the advantage of the filter- generation module in our ACNN framework. As a result, with only one convolutional filter and thus very limited modeling capacity, our S-ACNN model tends to be much more expressive than the basic CNN model, due to the flexibility of apply- ing different filters to different sentences. We further experiment on both ACNN and CNN models with multiple filters. The corresponding document categorization accuracies are presented in <ref type="table" target="#tab_2">Table 2</ref>. Although we only use one convolu- tion layer for our ACNN model, it already out- performs other CNN baseline methods with much deeper architectures. Moreover, our method ex-  <ref type="figure">6890  ACNN (self-adaptive)</ref> 0.6897 0.7032 AdaQA (one-way) 0.7005 0.7161 AdaQA (two-way) 0.7107 0.7304 AdaQA (two-way) + att.</p><p>0.7325 0.7428 hibits higher accuracy than n-grams, which is a very strong baseline as shown in ( <ref type="bibr" target="#b41">Zhang et al., 2015)</ref>. We attribute the superior performance of the ACNN framework to its stronger (adap- tive) feature-extraction ability. Moreover, our M- ACNN also achieves slightly better performance than self-attentive sentence embeddings proposed in <ref type="bibr" target="#b17">Lin et al. (2017)</ref>, which requires significant more parameters than our method.</p><p>Effect of number of filters To further demon- strate that the performance gains in document cat- egorization experiments originates from the im- proved adaptivity of our ACNN framework, we implement the basic CNN model with different numbers of filter sizes, ranging from 1 to 1000. As illustrated in <ref type="figure" target="#fig_4">Figure 3(a)</ref>, when the filter size is larger than 100, the test accuracy of the stan- dard CNN model does not show any noticeable improvement with more filters. More importantly, even with a filter size of 1000, the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100. Given these observations, we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature ex- traction of our ACNN framework.  are generated conditioned on the question; (iv) a two-way AdaQA model as described in Sec- tion 2.4, where both sentences are adaptively en- coded, with filters generated conditioned on the other sequence; (v) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks (see Section 3.4), we experiment with another model variant that combines the pro- posed context-aware filter generation mechanism with the multi-perspective attention layer intro- duced in ( <ref type="bibr" target="#b36">Wang et al., 2017b)</ref>. <ref type="table" target="#tab_4">Tables 3 and 4</ref> show experimental results of our models on WikiQA and SelQA datasets, along with other state-of-the-art methods. Note that the self-adaptive ACNN model variant, which gen- erates filters only for the input itself (without any interactions before the top matching module), slightly outperforms the vanilla CNN Siamese model. Combined with the results in document categorization experiments, we believe that our ACNN framework, in its simplest form, can be uti- lized as a powerful feature extractor for transform- ing natural language sentences into fixed-length vectors. More importantly, our two-way AdaQA model exhibits superior results compared with the one-way variant as well as other CNN-based base- line models on the WikiQA dataset. This obser- vation indicates that the bidirectional filter gener- ation mechanism is strongly associated with the performance gains. While combined with the multi-perspective attention layers, adopted after the ACNN encoding layer, our two-way AdaQA model achieves even better performance. This suggests that the proposed strategy is complemen-   tary, in terms of the incorporation of rich contex- tual information, to the standard attention mech- anism. The same trend is also observed on the SelQA dataset (as shown in <ref type="table" target="#tab_6">Table 4</ref>), which is a much larger dataset than WikiQA. Notably, our model yields significantly bet- ter results than an attentive pooling network and ABCNN (attention-based CNN) baselines. We at- tribute the improvement to two potential advan- tages of our AdaQA model: (i) for the two pre- vious baseline methods, the interaction between question and answer takes place either before or after convolution. However, in our AdaQA model, the communication between two sentences is in- herent in the convolution operation, and thus can provide the abstracted features with more flexibil- ity; (ii) the bidirectional filter generation mecha- nism in our AdaQA model generates co-dependent representations for the question and candidate an- swer, which could enable the model to recover from initial local maxima corresponding to incor- rect predictions ( <ref type="bibr" target="#b38">Xiong et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Answer Sentence Selection</head><p>Paragraph Identification Considering that the proposed AdaQA model can be readily general- ized to other text sequence matching problems, we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset. To ensure a fair compari- son, we employ the same data splits as in ( <ref type="bibr" target="#b36">Wang et al., 2017b</ref>). As illustrated in <ref type="table" target="#tab_8">Table 5</ref>, our two- way AdaQA model again exhibits superior perfor- mances compared with basic CNN models (as re- ported in ( <ref type="bibr" target="#b36">Wang et al., 2017b</ref>)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Reasoning ability To associate the improved answer sentence selection results with the reason- ing capabilities of our AdaQA model, we further categorize the questions in the WikiQA test set into 5 types containing: 'What', 'Where', 'How', 'When' or 'Who'. We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types. Similar to the find- ings in ( <ref type="bibr" target="#b19">Miao et al., 2016)</ref>, we observe that the 'How' question is the hardest to answer, with the lowest MAP scores. However, our AdaQA model improves most over the basic CNN on the 'How' type question, see <ref type="figure" target="#fig_4">Figure 3</ref>(b). Further compar- ing our results with NASM in ( <ref type="bibr" target="#b19">Miao et al., 2016</ref>), our AdaQA model (with a MAP score of 0.579) outperforms their reported 'How' question MAP scores (0.524) by a large margin, indicating that the adaptive convolutional filter-generation mech- anism improves the model's ability to read and reason over natural language sentences.</p><p>Filter visualization To better understand what information has been encoded into our context- aware filters, we visualize one of the filters for sentences within the test set (on the DBpedia dataset) with t-SNE. The corresponding results are shown in <ref type="figure" target="#fig_4">Figure 3(c)</ref>. It can be observed that the filters for documents with the same label (ontol- ogy) are grouped into clusters, indicating that for different types of document, ACNN has leveraged distinct convolutional filters for better feature ex- traction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a context-aware convolutional filter- generation mechanism, introducing a meta net- work to adaptively produce a set of input-aware filters. In this manner, the filter weights vary from sample to sample, providing the CNN encoder net- work with more modeling flexibility and capacity. This framework is further generalized to model question-answer sentence pairs, leveraging a two- way feature abstraction process. We evaluate our models on several document-categorization and sentence matching benchmarks, and they consis- tently outperform the standard CNN and attention- based CNN baselines, demonstrating the effective- ness of our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: The general ACNN framework. Notably, the input sentences to filter generating module and convolution module could be different (see Section 3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic description of Adaptive Question Answering (AdaQA) model. taken into account, those features that are relevant for final prediction are more likely to be extracted. So motivated, we propose an adaptive CNN-based question-answer (AdaQA) model for this problem. The AdaQA model can be divided into three modules: filter generation, adaptive convolution, and matching modules, as depicted schematically in Figure 2. Assume there is a question-answer pair to be matched, represented by word-embedding matrices, i.e. Q ∈ R Tq×d and A ∈ R Ta×d , where d is the embedding dimension and T q and T a are respective sentence lengths. First, they are passed to two filter-generation modules, to produce two sets of filters that encapsulate features of the corresponding input sentences. Similar to the setup in Section 3.2, we also employ a two-step process to produce the filters. For a question Q, the generating process is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>): 6 layer word-based convo- lutional networks, with 256/1024 features at each layer, denoted as small/large, respectively; (iii) deep CNN (Conneau et al., 2016): deep con- volutional neural networks with 9/17/29 layers. To evaluate the effectiveness of proposed AdaQA model, we compare it with several CNN-based sequence matching baselines, including Vanilla CNN (Jurczyk et al., 2016; Santos et al., 2017), at- tentive pooling networks (dos Santos et al., 2016), and ABCNN (Yin et al., 2016) (where an attention mechanism is employed over the two sentence rep- resentations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comprehensive study of the proposed ACNN framework, including (a) the number of filters (Yelp dataset), and (b) performance vs question types (WikiQA dataset), and (c) t-SNE visualization of learned filter weights (DBpedia dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test error rates on document classification tasks (in 

percentages). S-model indicates that the model has one single 
convolutional filter, while M-model indicates that the model 
has multiple convolutional filters. Results marked with  *  are 
reported by (Zhang et al., 2015),  † are reported by (Conneau 
et al., 2016), and  ‡ are reported by (Lin et al., 2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of our models on WikiQA dataset, com-

pared with previous CNN-based methods. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 : Results of our models on SelQA dataset, compared</head><label>4</label><figDesc></figDesc><table>with previous CNN-based methods. Results marked with  *  
are from (Jurczyk et al., 2016), and marked with  ‡ are from 
(Santos et al., 2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : Results on the Quora Question Pairs dataset.</head><label>5</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contextsensitive learning methods for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="173" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07897</idno>
		<title level="m">Learning generic sentence representations using convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Hypernetworks. ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunho</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Selqa: A new benchmark for selectionbased question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Jurczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho D</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICTAI</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic compositional neural networks over tree structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning loss functions for semi-supervised learning via discriminative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kahini</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Wadhawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02198</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nash: Toward end-to-end neural architecture for generative semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paidamoyo</forename><surname>Chapfuwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deconvolutional latent-variable model for text sequence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07109</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Speeding up contextbased sentence representation learning with nonautoregressive convolutional decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Representation Learning for NLP</title>
		<meeting>The Third Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multi-view sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia R De</forename><surname>Sa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07443</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04174</idno>
		<title level="m">Joint embedding of words and labels for text classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09783</idno>
		<title level="m">Topic compositional neural language model</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sentence similarity learning by lexical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>COLING</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Diffusion maps for textual network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09906</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
