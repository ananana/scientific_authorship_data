<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Language Model based on PMI Matrix Approximations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
							<email>oren.melamud@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Dept. Bar</orgName>
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Engineering Bar</orgName>
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
							<email>jacob.goldberger@biu.ac.il</email>
							<affiliation key="aff2">
								<orgName type="institution">Ilan University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Language Model based on PMI Matrix Approximations</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1860" to="1865"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec&apos;s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) learn to estimate the prob- ability of a word given a context of preceding words. Recurrent Neural Network (RNN) language models recently outperformed traditional n-gram LMs across a range of tasks ( <ref type="bibr" target="#b8">Jozefowicz et al., 2016)</ref>. However, an important practical issue asso- ciated with such neural-network LMs is the high computational cost incurred. The key factor that limits the scalability of traditional neural LMs is the computation of the normalization term in the softmax output layer, whose cost is linearly propor- tional to the size of the word vocabulary.</p><p>Several methods have been proposed to cope with this scaling issue by replacing the softmax with a more computationally efficient component at train time. <ref type="bibr">1</ref> These include importance sam-pling <ref type="bibr" target="#b1">(Bengio and et al, 2003</ref>), hierarchical softmax ( <ref type="bibr" target="#b13">Minh and Hinton, 2008</ref>), BlackOut ( <ref type="bibr" target="#b7">Ji et al., 2016)</ref> and Noise Contrastive Estimation (NCE) ( <ref type="bibr" target="#b6">Gutmann and Hyvarinen, 2012)</ref>. NCE has been applied to train neural LMs with large vocabularies <ref type="bibr" target="#b14">(Mnih and Teh, 2012</ref>) and more recently was also suc- cessfully used to train LSTM-RNN LMs ( <ref type="bibr" target="#b17">Vaswani et al., 2013;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Zoph et al., 2016)</ref>. NCE-based language models achieved near state- of-the-art performance on language modeling tasks ( <ref type="bibr" target="#b8">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2016)</ref>, and as we later show, are closely related to the method presented in this paper.</p><p>Continuous word embeddings were initially in- troduced as a 'by-product' of learning neural lan- guage models <ref type="bibr" target="#b1">(Bengio and et al, 2003</ref>). However, they were later adopted in many other NLP tasks, and the most popular recent word embedding learn- ing models are no longer proper language models. In particular, the skip-gram with negative sampling (NEG) embedding algorithm (  as implemented in the word2vec toolkit, has be- come one of the most popular such models today. This is largely attributed to its scalability to huge volumes of data, which is critical for learning high- quality embeddings. Recently, <ref type="bibr" target="#b10">Levy and Goldberg (2014)</ref> offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to fac- torize a word-context pointwise mutual information (PMI) matrix. <ref type="bibr" target="#b11">Melamud and Goldberger (2017)</ref> rederived this result by offering an information- theory interpretation of NEG.</p><p>The NEG objective function is considered a sim- plification of the NCE's objective, unsuitable for learning language models <ref type="bibr" target="#b5">(Dyer, 2014)</ref>. However, in this study, we show that despite its simplicity, it can be used in a principled way to effectively train a language model, based on PMI matrix fac- torization. More specifically, we use NEG to train a model for estimating the PMI between words and their preceding contexts, and then derive con- ditional probabilities from PMI at test time. The obtained PMI-LM can be viewed as a simple vari- ant of word2vec's algorithm, where the context of a predicted word is the preceding sequence of words, rather than a single word within a context window (skip-gram), or a bag-of-context-words (CBOW).</p><p>Our analysis shows that the proposed PMI-LM is very closely related to NCE language models (NCE-LMs). Similar to NCE-LMs, PMI-LM avoids the dependency of train run-time on the size of the word vocabulary by sampling from a negative (noise) distribution. Furthermore, conveniently, it also has a notably more simplified objective func- tion formulation inherited from word2vec, which allows it to avoid the heuristic components and initialization procedures used in various implemen- tations of NCE language models ( <ref type="bibr" target="#b17">Vaswani et al., 2013;</ref><ref type="bibr" target="#b4">Chen et al., 2015;</ref><ref type="bibr" target="#b20">Zoph et al., 2016)</ref>.</p><p>Finally, we report on a perplexity evaluation of PMI and NCE language models on two stan- dard language modeling datasets. The evaluation yielded comparable results, supporting our theoret- ical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NCE-based Language Modeling</head><p>Noise Contrastive Estimation (NCE) has recently been used to learn language models efficiently. NCE transforms the parameter learning problem into a binary classifier training problem. Let p(w|c) be the probability of a word w given a context c that represents its entire preceding context, and let p(w) be a 'noise' word distribution (e.g. a uni- gram distribution). The NCE approach assumes that the word w is sampled from a mixture distri- bution 1 k+1 (p(w|c) + kp(w)) such that the noise samples are k times more frequent than samples from the 'true' distribution p(w|c). Let y be a bi- nary random variable such that y = 0 and y = 1 correspond to a noise sample and a true sample, respectively, i.e. p(w|c, y = 0) = p(w) and p(w|c, y = 1) = p(w|c). Assume the distribution p(w|c) has the following parametric form:</p><formula xml:id="formula_0">p nce (w|c) = 1 Z c exp( w · c + b w )<label>(1)</label></formula><p>such that w and c are vector representations of the word w and its context c. Applying Bayes rule, it can be easily verified that:</p><formula xml:id="formula_1">p nce (y = 1|w, c) = (2) σ( w · c + b w − log Z c − log(p(w)k))</formula><p>where σ() is the sigmoid function. NCE uses Eq. <ref type="formula">(2)</ref> and the following objective function to train a binary classifier that decides which distribution was used to sample w:</p><formula xml:id="formula_2">S nce = w,c∈D log p(1|w, c) + k i=1 log p(0|u i , c)<label>(3)</label></formula><p>such that w, c go over all the word-context co- occurrences in the learning corpus D and u 1 , ..., u k are 'noise' samples drawn from the word unigram distribution.</p><p>Note that the normalization factor Z c is not a free parameter and to obtain its value, one needs to com-</p><formula xml:id="formula_3">pute Z c = w∈V exp( w · c + b w ) for each context c,</formula><p>where V is the word vocabulary. This computa- tion is typically not feasible due to the large vocab- ulary size and the exponentially large number of possible contexts and therefore it was heuristically circumvented by prior work. <ref type="bibr" target="#b14">Mnih and Teh (2012)</ref> found empirically that setting Z c = 1 didn't hurt the performance (see also discussion in ( <ref type="bibr" target="#b0">Andreas and Klein, 2015)</ref>). <ref type="bibr" target="#b4">Chen et al. (2015)</ref> reported that setting log(Z c ) = 9 gave them the best results. Re- cent works ( <ref type="bibr" target="#b17">Vaswani et al., 2013;</ref><ref type="bibr" target="#b20">Zoph et al., 2016)</ref> used Z c = 1 and also initialized NCE's bias term from Eq. (2) to b w = − log |V |. They reported that without these heuristics the training procedure did not converge to a meaningful model.</p><p>In the following section, we describe our pro- posed language model, which is derived from word2vec's interpretation as a low-rank PMI matrix approximation. Interestingly, this model turns out to be a close variant of NCE language models, but with a simplified objective function that avoids the need for the normalization factor Z c and the bias terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PMI-based Language Modeling</head><p>The skip-gram negative sampling word embedding algorithm represents each word w and each context word c as d-dimensional vectors, with the purpose that words that are "similar" to each other will have similar vector representations. The algorithm optimizes the following NEG objective function ( :</p><formula xml:id="formula_4">S neg = w,c∈D log σ( w · c) + k i=1 log σ(− u i · c)<label>(4)</label></formula><p>such that w, c go over all the word-context co- occurrences in the learning corpus D, u 1 , ..., u k are words independently sampled from the word unigram distribution, x is the embedding of x and σ() is the sigmoid function. The objective function S neg can be viewed as a log-likelihood function of a binary logistic regression classifier that treats a sample from a joint word-context distribution as a positive instance, and two independent samples from the word and context unigram distributions as a negative instance, while k is the proportion between negative and positive instances. <ref type="bibr" target="#b10">Levy and Goldberg (2014)</ref> showed that this objective func- tion achieves its maximal value when for every word-context pair w, c:</p><formula xml:id="formula_5">w · c = pmi k (w, c) = log p(w|c) kp(w)<label>(5)</label></formula><p>where pmi k (w, c) is the word-context PMI matrix. Actually achieving this maximal value is typically infeasible, since the embedding dimensionality is intentionally limited. Therefore, learning word and context embeddings that optimize skip-gram's NEG objective function (4) can be viewed as find- ing a low-rank approximation of the word-context PMI matrix. An explicit expression of the approxi- mation criterion optimized by the skip-gram algo- rithm can be found in ( <ref type="bibr" target="#b11">Melamud and Goldberger, 2017)</ref>. Our study is based on two simple observations regarding this finding of <ref type="bibr" target="#b10">Levy and Goldberg (2014)</ref>. First, Equation (5) can be reformulated as follows to derive an estimate of the conditional distribution p(w|c):</p><formula xml:id="formula_6">ˆ p(w|c) ∝ exp( w · c)p(w)<label>(6)</label></formula><p>where the constant k is dropped since p(w|c) is a distribution. Second, while the above analysis had been originally applied to the case of word-context joint distributions p(w, c), it is easy to see that the PMI matrix approximation analysis also holds for every Euclidean embedding of a joint distribution p(x, y) of any two given random variables X and Y . In particular, we note that it holds for word- context joint distributions p(w, c), where w is a single word, but c represents its entire preceding context, rather than just a single context word, and c is a vector representation of this entire context. Altogether, this allows us to use word2vec's NEG objective function (4) to approximate the language modeling conditional probabilityˆpprobabilityˆ probabilityˆp(w|c) (6), with c being the entire preceding context of the predicted word w. We next describe the design details of the pro- posed PMI-based language modeling. We use a simple lookup table for the word representation w, and an LSTM recurrent neural network to obtain a low dimensional representation of the entire preced- ing context c. These representations are trained to maximize the NEG objective in Eq. <ref type="formula" target="#formula_4">(4)</ref>, where this time w goes over every word token in the corpus, and c is its preceding context. We showed above that optimizing this objective seeks to obtain the best low-dimensional approximation of the PMI matrix associated with the joint distribution of the word and its preceding context (Eq. <ref type="formula" target="#formula_5">(5)</ref>). Hence, based on Eq. (6), for a reasonable embedding di- mensionality and a good model for representing the preceding context, we expectˆpexpectˆ expectˆp(w|c) to be a good estimate of the language modeling conditional dis- tribution.</p><p>At test time, to obtain a proper distribution, we perform a normalization operation as done by all other comparable models. The train and test steps of the proposed language modeling algorithm are shown in algorithm box 1.</p><p>Note that while the NCE approach (1) learns to explicitly estimate normalized conditional distri- butions, our model learns to approximate the PMI matrix. Hence, we have no real motivation to in- clude additional learned normalization parameters, as considered in comparable NCE language models <ref type="bibr" target="#b14">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b20">Zoph et al., 2016)</ref>.</p><p>The NEG and NCE objective functions share a similar form:</p><formula xml:id="formula_7">S = w,c log s(w, c)+ k i=1 log(1−s(u i , c))<label>(7)</label></formula><p>with the differences summarized in  <ref type="table" target="#tab_0">Table 1</ref>: Comparison of the training objective functions (see Eq. <ref type="formula" target="#formula_7">(7)</ref>) and the respective test-time conditional word probability functions for NCE-LM and PMI-LM algorithms.</p><formula xml:id="formula_8">(w, c) = σ( w · c + b w −log Z c −log(kp(w))) ˆ p(w|c) ∝ exp( w · c + b w ) PMI-LM s(w, c) = σ( w · c) ˆ p(w|c) ∝ exp( w · c)p(w)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 PMI Language Modeling</head><p>Training phase: -Use a simple lookup table for the word repre- sentation and an LSTM recurrent neural network to obtain the preceding context representation. -Train the word and preceding context embed- dings to maximize the objective:</p><formula xml:id="formula_9">S neg = w,c∈D log σ( w· c)+ k i=1 log σ(− u i · c)</formula><p>such that w and c go over every word and it pre- ceding context in the corpus D, and u 1 , ..., u k are words independently sampled from the uni- gram distribution p(w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test phase:</head><p>The conditional probability estimate for a word w given a preceding context c is:</p><formula xml:id="formula_10">ˆ p(w|c) = exp( w · c)p(w) v∈V exp( v · c)p(v)</formula><p>where V is the word vocabulary. therefore potentially more difficult to concentrate around zero with low variance to facilitate effec- tive back-propagation. This may explain heuristics used by prior work for initializing the values of b w ( <ref type="bibr" target="#b17">Vaswani et al., 2013;</ref><ref type="bibr" target="#b20">Zoph et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The goal of the evaluation described in this sec- tion is to empirically establish PMI-LM as a sound language model. We do so by comparing its perfor- mance with the well-established NCE-LM, using the popular perplexity measure on two standard datasets, under the same terms. We describe our hyperparameter choices below and stress that for a fair comparison, we followed prior best practices and avoided hyperparameter optimization in favor of PMI-LM. All of the models described hereafter were implemented using the Chainer toolkit ( <ref type="bibr" target="#b16">Tokui et al., 2015</ref>).</p><p>For our NCE baseline, we used the heuristics that worked well in ( <ref type="bibr" target="#b17">Vaswani et al., 2013;</ref><ref type="bibr" target="#b20">Zoph et al., 2016)</ref>, initializing NCE's bias term from Eq. <ref type="formula">(2)</ref> to b w = − log |V |, where V is the word vocabulary, and using Z c = 1.</p><p>The first dataset we used is a version of the Penn Tree Bank (PTB), commonly used to evalu- ate language models. <ref type="bibr">2</ref> It consists of 929K training words, 73K validation words and 82K test words with a 10K word vocabulary. To build and train the compared models in this setting, we followed the work of <ref type="bibr" target="#b19">Zaremba et al. (2014)</ref>, who achieved excellent results on this dataset. Specifically, we used a 2-layer 300-hidden-units LSTM with a 50% dropout ratio to represent the preceding (left-side) context of a predicted word. <ref type="bibr">3</ref> We represented end- of-sentence as a special &lt;eos&gt; token and predicted this token like any other word. During training, we performed truncated back-propagation-through- time, unrolling the LSTM for 20 steps at a time without ever resetting the LSTM state. We trained our model for 39 epochs using Stochastic Gradient Descent (SGD) with a learning rate of 1, which is decreased by a factor of 1.2 after every epoch starting after epoch 6. We clipped the norms of the gradient to 5 and used a mini-batch size of 20. We set the negative sampling parameter to k = 100 following <ref type="bibr" target="#b20">Zoph et al. (2016)</ref>, who showed highly competitive performance with NCE LMs trained with this number of samples.</p><p>As the second dataset, we used the much larger WMT 1B-word benchmark introduced by <ref type="bibr" target="#b2">Chelba et al. (2013)</ref>. This dataset comprises about 0.8B train- ing words and has a held-out set partitioned into 50 subsets. The test set is the first subset in the held- out, comprising 159K words, including the &lt;eos&gt; tokens. We used the second subset as the validation set with 165K words. The original vocabulary size of this dataset is 0.8M words after converting all PMI-LM NCE-LM PTB 98.35 104.33 WMT 65.84 69.28 <ref type="table">Table 2</ref>: Perplexity results on test sets.</p><p>words that occur less than 3 times in the corpus to an &lt;unk&gt; token. However, we followed previous works <ref type="bibr">(Williams et al., 2015;</ref><ref type="bibr" target="#b7">Ji et al., 2016)</ref> and trimmed the vocabulary further down to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources. To build and train our models, we used a similar method to the one used with PTB, with the following differences. We used a single-layer 512-hidden-unit LSTM to represent the preceding context. We followed <ref type="bibr" target="#b8">Jozefowicz et al. (2016)</ref>, who found a 10% dropout rate to be suf- ficient for relatively small models fitted to this large training corpus. We trained our model for only one epoch using the Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2014</ref>) with default parameters, which we found to converge more quickly and effectively than SGD. We used a mini-batch size of 1000. The perplexity results achieved by the compared models appear in <ref type="table">Table 2</ref>. As can be seen, the per- formance of our PMI-LM is competitive, slightly outperforming the NCE-LM on both test sets. To put these numbers in a broader context, we note that state-of-the-art results on these datasets are no- tably better. For example, on the small PTB test set, <ref type="bibr" target="#b19">Zaremba et al. (2014)</ref> achieved 78.4 perplexity with a larger LSTM model and using the more costly softmax component. On the larger WMT dataset, <ref type="bibr" target="#b8">Jozefowicz et al. (2016)</ref> achieved 46.1 and 43.7 per- plexity numbers using NCE and importance sam- pling respectively, and with much larger LSTM models trained over the full vocabulary, rather than our trimmed one. They also achieved 23.7 with an ensemble method, which is the best result on this dataset to date. Yet, as intended, we argue that our experimental results affirm the claim that PMI-LM is a sound language model on par with NCE-LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we have shown that word2vec's nega- tive sampling objective function, popularized in the context of learning word representations, can also be used to effectively learn parametric language models. These language models are closely re- lated to NCE language models, but utilize a simpler, potentially more robust objective function. More generally, our theoretical analysis shows that any word2vec model trained with negative sampling can be used in a principled way to estimate the conditional distribution p(w|c), by following our proposed procedure at test time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>The 
comparison shows that PMI-LM's NEG objective 
function is much simpler. Furthermore, due to the 
component log(p(w)k)) in NCE's objective func-
tion, its input to the sigmoid function is sensitive to 
the variable values in the unigram distribution, and </table></figure>

			<note place="foot" n="1"> An alternative recent approach for coping with large word vocabularies is to represent words as compositions of subword units, such as individual characters. This approach has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper.</note>

			<note place="foot" n="2"> Available from Tomas Mikolov at: http: //www.fit.vutbr.cz/ ˜ imikolov/rnnlm/ simple-examples.tgz 3 Zaremba et al. (2014) used larger models with more units and also applied dropout to the output of the top LSTM layer, which we did not.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">When and why are loglinear models self-normalizing? In NAACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quick training of probabilistic neural nets by importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senecal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Strategies for training large vocabulary neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1512.04906</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural network language model training with noise contrastive estimation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8251</idno>
		<title level="m">Notes on noise contrastive estimation and negative sampling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Blackout: Speeding up recurrent neural network language models with very large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nadathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Informationtheory interpretation of the skip-gram negativesampling objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno>abs/1508.07909</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning Systems (LearningSys) in The 29th Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robinson. 2015. Scaling recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mrva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple, fast noise-contrastive estimation for large RNN vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
