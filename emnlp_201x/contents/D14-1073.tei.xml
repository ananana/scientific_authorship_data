<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Good Enough: A Task-Based Evaluation of Query Biased Summarization for Cross Language Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Williams</surname></persName>
							<email>jennifer.williams@ll.mit.edu,</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory Human Language Technology Group</orgName>
								<address>
									<addrLine>244 Wood Street</addrLine>
									<postCode>02420</postCode>
									<settlement>Lexington</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Tam</surname></persName>
							<email>sharontam@alum.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory Human Language Technology Group</orgName>
								<address>
									<addrLine>244 Wood Street</addrLine>
									<postCode>02420</postCode>
									<settlement>Lexington</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory Human Language Technology Group</orgName>
								<address>
									<addrLine>244 Wood Street</addrLine>
									<postCode>02420</postCode>
									<settlement>Lexington</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Good Enough: A Task-Based Evaluation of Query Biased Summarization for Cross Language Information Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="657" to="669"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we present our task-based evaluation of query biased summarization for cross-language information retrieval (CLIR) using relevance prediction. We describe our 13 summarization methods each from one of four summarization strategies. We show how well our methods perform using Farsi text from the CLEF 2008 shared-task, which we translated to English automtatically. We report preci-sion/recall/F1, accuracy and time-on-task. We found that different summarization methods perform optimally for different evaluation metrics, but overall query biased word clouds are the best summariza-tion strategy. In our analysis, we demonstrate that using the ROUGE metric on our sentence-based summaries cannot make the same kinds of distinctions as our evaluation framework does. Finally, we present our recommendations for creating much-needed evaluation standards and datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite many recent advances in query biased summarization for cross-language information re- trieval (CLIR), there are no existing evaluation standards or datasets to make comparisons among different methods, and across different languages <ref type="bibr" target="#b48">(Tombros and Sanderson, 1998;</ref><ref type="bibr" target="#b42">Pingali et al., 2007;</ref><ref type="bibr" target="#b33">McCallum et al., 2012;</ref>. Consider that creating this kind of summary requires familiarity with tech- niques from machine translation (MT), summa- rization, and information retrieval (IR). In this This work was sponsored by the Federal Bureau of Inves- tigation under Air Force Contract FA8721-05-C-0002. Opin- ions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government.</p><p>paper, we arrive at the intersection of each of these research areas. Query biased summariza- tion (also known as query-focused, query-relevant, and query-dependent) involves automatically cap- turing relevant ideas and content from a document with respect to a given query, and presenting it as a condensed version of the original document. This kind of summarization is mostly used in search en- gines because when search results are tailored to a user's information need, the user can find texts that they are looking for more quickly and more ac- curately ( <ref type="bibr" target="#b48">Tombros and Sanderson, 1998;</ref><ref type="bibr" target="#b34">Mori et al., 2004</ref>). Query biased summarization is a valu- able research area in natural language processing (NLP), especially for CLIR. Users of CLIR sys- tems meet their information needs by submitting their queries in L 1 to search through documents that have been composed in L 2 , even though they may not be familiar with L 2 ( <ref type="bibr">Hovy et al., 1999;</ref><ref type="bibr" target="#b42">Pingali et al., 2007)</ref>.</p><p>There are no standards for objectively evaluat- ing summaries for CLIR -a research gap that we begin to address in this paper. The problem we explore is two-fold: what kinds of summaries are well-suited for CLIR applications, and how should the summaries be evaluated. Our evaluation is ex- trinsic, that is to say we are interested in how sum- marization affects performance on a different task ( <ref type="bibr" target="#b31">Mani et al., 2002;</ref><ref type="bibr" target="#b32">McKeown et al., 2005;</ref><ref type="bibr" target="#b17">Dorr et al., 2005;</ref><ref type="bibr" target="#b35">Murray et al., 2009;</ref><ref type="bibr" target="#b33">McCallum et al., 2012</ref>). We use relevance prediction as our ex- trinsic task: a human must decide if a summary for a given document is relevant to a particular in- formation need, or not. Relevance prediction is known to be useful as it correlates with some au- tomatic intrinsic methods as well <ref type="bibr" target="#b43">(President and Dorr, 2006;</ref><ref type="bibr" target="#b22">Hobson et al., 2007)</ref>. To the best of our knowledge, we are the first to apply this eval- uation framework to cross language query biased summarization.</p><p>Each one of the summarization methods that we present in this paper belongs to one of the fol- lowing strategies: (1) unbiased full machine trans- lated text, (2) unbiased word clouds, (3) query bi- ased word clouds, and (4) query biased sentence summaries. The methods and strategies that we present are fast, cheap, and language-independent. All of these strategies are extractive, meaning that we used existing parts of a document to create the condensed version, or summary. We approach our task as an engineering prob- lem: the goal is to decide if summaries are good enough to help CLIR system users find what they are looking for. We have simplified the task by as- suming that a set of documents has already been retrieved from a search engine, as CLIR tech- niques are outside the scope of this paper. We predict that showing the full MT English text as a summarization strategy would not be particu- larly helpful in our relevance prediction task be- cause the words in the text could be mixed-up, or sentences could be nonsensical, resulting in poor readability. For the same reasons, we expect that showing the full MT English text would take longer to arrive at a relevance decision. Finally, we predict that query biased summaries will result in faster, more accurate decisions from the partic- ipants <ref type="bibr" target="#b48">(Tombros and Sanderson, 1998)</ref>.</p><p>We treat the actual CLIR search engine as if it were a black box so that we can focus on evaluat- ing if the summaries themselves are useful. As a starting point, we begin with some principles that we expect to hold true when we evaluate. These principles provide us with the kind of framework that we need for a productive and judicious dis- cussion about how well a summarization method works. We encourage the NLP community to consider the following concepts when developing evaluation standards for this problem:</p><p>• End-user intelligiblity</p><p>• Query-salience</p><p>• Retrieval-relevance Summaries should be presented to the end-user in a way that is both concise and intelligible, even if the machine translated text is difficult to under- stand. Our notions of query-salience and retrieval- relevance capture the expectation that good sum- maries will be efficient enough to help end-users fulfill their information needs. For query-salience, we want users to positively identify relevant doc- uments. Similarly, for retrieval-relevance we want users to be able to find as many relevant docu- ments as possible.</p><p>This paper is structured as follows: Section 2 presents related work; Section 3 describes our data and pre-processing; Section 4 details our sum- marization methods and strategies; Section 5 de- scribes our experiments; Section 6 shows our re- sults and analysis; and in Section 7, we conclude and discuss some future directions for the NLP community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic summarization is generally a well- investigated research area. Summarization is a way of describing the relationships of words in documents to the information content of that doc- ument <ref type="bibr" target="#b27">(Luhn, 1958;</ref><ref type="bibr">Edmunson, 1969;</ref><ref type="bibr" target="#b47">Salton and Yang, 1973;</ref><ref type="bibr" target="#b46">Robertson and Walker, 1994;</ref><ref type="bibr" target="#b12">Church and Gale, 1999;</ref><ref type="bibr" target="#b45">Robertson, 2004)</ref>. Recent work has looked at creating summaries of single and multiple documents <ref type="bibr" target="#b19">Erkan and Radev, 2004;</ref><ref type="bibr">Wan et al., 2007;</ref><ref type="bibr" target="#b53">Yin et al., 2012;</ref><ref type="bibr" target="#b11">Chatterjee et al., 2012)</ref>, as well as summary eval- uation ( <ref type="bibr" target="#b23">Jing et al., 1998;</ref><ref type="bibr" target="#b48">Tombros and Sanderson 1998;</ref><ref type="bibr" target="#b28">Mani et al., 1998;</ref><ref type="bibr" target="#b29">Mani et al., 1999;</ref><ref type="bibr" target="#b30">Mani, 2001;</ref><ref type="bibr">Lin and Hovy, 2003;</ref><ref type="bibr" target="#b25">Lin, 2004;</ref><ref type="bibr">Nenkova et al., 2007;</ref><ref type="bibr" target="#b22">Hobson et al., 2007;</ref><ref type="bibr" target="#b40">Owczarzak et al., 2012</ref>), query and topic biased summariza- tion <ref type="bibr" target="#b4">(Berger and Mittal, 2000;</ref><ref type="bibr" target="#b38">Otterbacher et al., 2005;</ref><ref type="bibr">Daume and Marcu, 2006;</ref><ref type="bibr" target="#b10">Chali and Joty, 2008;</ref><ref type="bibr" target="#b39">Otterbacher et al., 2009;</ref><ref type="bibr">Bando et al., 2010;</ref><ref type="bibr" target="#b21">Harwath and Hazen, 2012;</ref><ref type="bibr" target="#b53">Yin et al., 2012)</ref>, and summarization across languages ( <ref type="bibr" target="#b42">Pingali et al., 2007;</ref><ref type="bibr" target="#b37">Or˘ asan and Chiorean, 2008;</ref><ref type="bibr">Wan et al., 2010;</ref><ref type="bibr" target="#b2">Azarbonyad et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query Biased Summarization</head><p>Previous work most closely related to our own comes from Pingali et al., <ref type="bibr">(2007)</ref>. In their work, they present their method for cross-language query biased summarization for Telugu and En- glish. Their work was motivated by the need for people to have access to foreign-language docu- ments from a search engine even though the users were not familiar with the foreign language, in their case English. They used language model- ing and translation probability to translate a user's query into L 2 , and then summarized each docu- ment in L 2 with respect to the query. In their final step, they translated the summary from L 2 back to L 1 for the user. They evaluated their method on the DUC 2005 query-focused summarization shared-task with ROUGE scores. We compare our methods to this work also on the DUC 2005 task. Our work demonstrates the first attempt to draw at a comparison between user-based studies and in- trinsic evaluation with ROUGE. However, one of the limitations with evaluating this way is that the shared-task documents and queries are monolin- gual.</p><p>Bhaskar and Bandyopadhyay (2012) tried a subjective evaluation of extractive cross-language query biased summarization for 7 different lan- guages. They extracted sentences, then scored and ranked the sentences to generate query dependent snippets of documents for their cross lingual in- formation access (CLIA) system. However, the snippet quality was determined subjectively based on scores on a scale of 0 to 1 (with 1 being best). Each score indicated annotator satisfaction for a given snippet. Our evaluation methodology is ob- jective: we ask users to decide if a given document is relevant to an information need, or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Machine Translation Effects</head><p>Machine translation quality can affect summa- rization quality. <ref type="bibr">Wan et al. (2010)</ref> researched the effects of MT quality prediction on cross- language document summarization. They gener- ated 5-sentence summaries in Chinese using En- glish source documents. To select sentences, they used predicted translation quality, sentence posi- tion, and sentence informativeness. In their eval- uation, they employed 4 Chinese-speakers to sub- jectively rate summaries on a 5-point scale (5 be- ing best) along the dimensions of content, read- ability, and overall impression. They showed that their approach of using MT quality scores did im- prove summarization quality on average. While their findings are important, their work did not ad- dress query biasing or objective evaluation of the summaries. We attempt to overcome limitations of machine translation quality by using word clouds as one of our summarization strategies.</p><p>Knowing when to translate is another challenge for cross-language query biased summarization. Several options exist for when and what to trans- late during the summarization process: (1) the source documents can be translated, (2) the user's query can be translated, (3) the final summary can be translated, or (4) some combination of these.</p><p>An example of translating only the summaries themselves can be found in <ref type="bibr">Wan et al., (2010)</ref>. On the other hand, <ref type="bibr" target="#b42">Pingali et al. (2007)</ref> translated the queries and the summaries. In our work, we used gold-translated queries from the CLEF 2008 dataset, and machine translated source documents. We briefly address this in our work, but note that a full discussion of when and what to translate, and those effects on summarization quality, is outside of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Summarization Evaluation</head><p>There has been a lot of work towards developing metrics for understanding what makes a summary good. Evaluation metrics are either intrinsic or ex- trinsic. Intrinsic metrics, such as ROUGE, mea- sure the quality of a summary with respect to gold human-generated summaries <ref type="bibr" target="#b25">(Lin, 2004;</ref><ref type="bibr">Lin and Hovy, 2003)</ref>. Generating gold standard summaries is expensive and time-consuming, a problem that persists with cross-language query biased summa- rization because those summaries must be query biased as well as in a different language from the source documents.</p><p>On the other hand, extrinsic metrics measure the quality of summaries at the system level, by look- ing at overall system performance on downstream tasks ( <ref type="bibr" target="#b23">Jing et al, 1998;</ref><ref type="bibr" target="#b48">Tombros and Sanderson, 1998)</ref>. One of the most important findings for query biased summarization comes from Tombros and Sanderson (1998). In their monolingual task- based evaluation, they measured user speed and accuracy at identifying relevant documents. They found that query biased summarization improved the user speed and accuracy when the user was asked to make relevance judgements for IR tasks. We also expect that our evaluation will demon- strate that user speed and accuracy is better when summaries are query biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data and Pre-Processing</head><p>We used data from the Farsi CLEF 2008 ad hoc task ( <ref type="bibr" target="#b0">Agirre et al., 2009</ref>). Each of the queries in- cluded in this dataset consisted of a title, narrative, and description. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the elements of a CLEF 2008 query. All of the au- tomatic query-biasing in this work was based on the query titles. For our human relevance predic- tion task on Mechanical Turk, we used the nar- rative version. The CLEF 2008 dataset included a ground-truth answer key indicating which docu-ments were relevant to each query. For each query, we randomly selected 5 documents that were rele- vant as well as 5 documents that were not relevant. The subset of CLEF 2008 data that we used there- fore consisted of 500 original Farsi documents and 50 parallel English-Farsi queries. Next we will de- scribe our text pre-processing steps for both lan- guages as well as how we created our parallel En- glish documents. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">English Documents</head><p>All of our English documents were created auto- matically by translating the original Farsi docu- ments into English ( <ref type="bibr" target="#b16">Drexler et al., 2012</ref>). The translated documents were sentence-aligned with one sentence per line. For all of our summariza- tion experiments (except unbised full MT text), we processed the text as follows: removed extra spaces, removed punctuation, folded to lowercase, and removed digits. We also removed common English stopwords 2 from the texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Farsi Documents</head><p>We used the original CLEF 2008 Farsi docu- ments for two of our summarization methods. We stemmed words in each document using automatic morphological analysis with Morfessor CatMAP. We note that within-sentence punctuation was re- moved during this process <ref type="bibr" target="#b13">(Creutz and Lagus, 2007)</ref>. We also removed Farsi stopwords and dig- its.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Summarization Strategies</head><p>All of our summarization methods were extrac- tive except for unbiased full machine translated text. In this section, we describe each of our 13 summarization methods which we have orga- nized into one of the following strategies: <ref type="formula">(1)</ref>   cloud summaries, (3) query biased word cloud summaries, and (4) query biased sentence summaries. Regardless of which summarization method used, we highlighted words in yellow that also appeard in the query. Let t be a term in document d where d ∈ D L and D L is a collec- tion of documents in a particular language. Note that for our summarization methods, term weight- ings were calculated separately for each language. While |D| = 1000, we calculated term weightings based on |D E | = 500 and |D F | = 500. Finally, let q be a query where q ∈ Q and Q is our set of 50 parallel English-Farsi CLEF queries. Assume that log refers to log 10 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unbiased Full Machine Translated English</head><p>Our first baseline approach was to use all of the raw machine translation output (no subsets of the sentences were used). Each summary there- fore consisted of the full text of an entire doc- ument automatically translated from Farsi to En- glish ( <ref type="bibr" target="#b16">Drexler et al., 2012</ref>). <ref type="figure" target="#fig_2">Figure 2</ref> shows an ex- ample full text document translated from Farsi to English and a gold-standard English CLEF query. Note that we use this particular document-query pair as an example throughout this paper (docu- ment: H-770622-42472S8, query: 10.2452/552- AH). According to the CLEF answer key, the sam- ple document is relevant to the sample query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unbiased Word Clouds</head><p>For our second baseline approach, we ranked terms in a document and displayed them as word clouds. Word clouds are one a way to arrange a collection of words where each word can vary in size. We used word clouds as a summariza- tion strategy to overcome any potential disfluen- cies from the machine translation output and also to see if they are feasible at all for summarization. All of our methods for word clouds used words from machine translated English text. Each term- ranking method below generates different ranked lists of terms, which we used to create different word clouds. We created one word cloud per doc- ument using the top 12 ranked words. We used the raw term scores to scale text font size, so that words with a highter score appeared larger and more prominent in a word cloud. Words were shuffled such that the exact ordering of words was at random.</p><p>I: Term Frequency (TF) Term frequency is very commonly used for finding important terms in a document. Given a term t in a document d, the number of times that term occurs is:</p><formula xml:id="formula_0">tf t,d = |t ∈ d| II: Inverse Document Frequency (IDF)</formula><p>The idf term weighting is typically used in IR and other text categorization tasks to make distinc- tions between documents. The version of idf that we used throughout our work came from Erkan and Radev (2004) and <ref type="bibr" target="#b39">Otterbacher et al. (2009)</ref>, in keeping consistent with theirs. Let N be the number of documents in the collection, such that N = |D| and n t is the number of documents that contain term t, such that n t = |{d ∈ D : t ∈ d}|, then: idf t = log N + 1 0.5 × n t While idf is usually thought of as a type of heuristic, there have been some discussions about its theoretical basis <ref type="bibr" target="#b45">(Robertson, 2004;</ref><ref type="bibr" target="#b46">Robertson and Walker, 1994;</ref><ref type="bibr" target="#b12">Church and Gale, 1999;</ref><ref type="bibr" target="#b47">Salton and Yang, 1973</ref>). An example of this summary is shown in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>III: Term Frequency Inverse Document Fre- quency (TFIDF) We use tf idf t,d term weight- ing to find terms which are both rare and impor- tant for a document, with respect to terms across all other documents in the collection:</p><formula xml:id="formula_1">tf idf t,d = tf t,d × idf t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Query Biased Word Clouds</head><p>We generated query biased word clouds following the same principles as our unbiased word clouds, namely the text font scaling and highlighting re- mained the same. <ref type="figure" target="#fig_4">Figure 4</ref> we show a sample word cloud summary based on query biased term frequency. We define query biased term frequency tf Q at the document level, as: V. Query Biased Inverse Document Frequency (IDFQ) Since idf helps with identifying terms that discriminate documents in a collection, we would expect that query biased idf would help to identify documents that are relevant to a query:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Query Biased Term Frequency (TFQ) In</head><formula xml:id="formula_2">tf Q t,d,q = 2tf t,d , if t ∈ q tf t,d , otherwise</formula><formula xml:id="formula_3">idf Q t,q = 2idf t , if t ∈ q idf t , otherwise</formula><p>VI. Query Biased TFIDF (TFIDFQ) We de- fine query biased tf × idf similarly to our TFQ and IDFQ, at the document level:</p><formula xml:id="formula_4">tf idf Q t,d,q = 2tf t,d × idf t , if t ∈ q tf t,d × idf t , otherwise</formula><p>Figure 5: Word cloud summary for scaled query biased term frequency (SFQ) for query "Tehran's stock market".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Query Biased Scaled Frequency (SFQ)</head><p>This term weighting scheme, which we call scaled query biased term frequency or sf Q, is a variant of the traditional tf ×idf weighting. First, we project the usual term frequency into log-space, for a term t in document d with:</p><formula xml:id="formula_5">tf S t,d = log(tf t,d )</formula><p>We let tf S t,d ≈ 0 when tf t,d = 1. We believe that singleton terms in a document provide no indica- tion that a document is query-relevant, and trea- ment of singleton terms in this way would have the potential to reduce false-positives in our relevance prediction task. Note that scaled term frequency differs from Robertson's (2004) inverse total term frequency in the sense that our method involves no consideration of term position within a document. Scaled query biased term frequency, shown in <ref type="figure">Fig- ure 5</ref>, is defined as:</p><formula xml:id="formula_6">sf Q t,d,q = 2tf S t,d × idf t , if t ∈ q tf S t,d × idf t , otherwise</formula><p>VIII. Word Relevance (W) We adapted an existing relevance weighting from Allan et al., <ref type="bibr">(2003)</ref>, that was originally formulated for ranking sentences with respect to a query. However, we modified their originaly ranking method so that we could rank individual terms in a document instead of sentences. Our method for word relevance, W is defined as:</p><formula xml:id="formula_7">W t,d,q = log(tf t,d + 1) × log(tf t,q + 1) × idf t</formula><p>In W , term frequency values are smoothed by adding 1. The smoothing could especially af- fect rare terms and singletons, when tf t,d is very low. All terms in a query or a document will be weighted and each term could potentially con- tribute to summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Query Biased Sentence Summaries</head><p>Sentences are a canonical unit to use in extractive summaries. In this section we describe four differ- ent sentence scoring methods that we used. These methods show how to calculate sentence scores for a given document with respect to a given query. Sentences for a document were always ranked us- ing the raw score value output generated from a scoring method. Each document summary con- tained the top 3 ranked sentences where the sen- tences were simply listed out. Each of these meth- ods used sentence-aligned English machine trans- lated documents, and two of them also used the original Farsi text.</p><p>IX. Sentence Relevance (REL) Our sentence relevance scoring method comes from <ref type="bibr">Allan et al. (2003)</ref>. The sentence weight is a summation over words that appear in the query. We provide their sentence scoring formula here. This calculates the relevance score for a sentence s from document d, to a query q: rel (s|q) = t∈s log(tf t,s + 1) × log(tf t,q + 1) × idf t Terms will occur in either the sentence or the query, or both. We applied this method to machine tranlsated English text. The output of this method is a relevance score for each sentence in a given document. We used those scores to rank sentences in each document from our English machine trans- lated text.</p><p>X. Query Biased Lexrank (LQ) We imple- mented query biased LexRank, a well-known graph-based summarization method <ref type="bibr" target="#b39">(Otterbacher et al., 2009)</ref>. It is a modified version of the orig- inal LexRank algorithm ( <ref type="bibr" target="#b19">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b41">Page et al., 1998</ref>). The similarity metric, sim x,y , also known as idf-modified cosine similarity, mea- sures the distance between two sentences x and y in a document d, defined as: We used sim x,y to score the similarity of sentence-to-sentence, resulting in a similarity graph where each vertex was a sentence and each edge was the cosine similarity between sentences.</p><formula xml:id="formula_8">sim x,y = t∈x</formula><p>We normalized the cosine matrix with a similarity threshold (t = 0.05), so that sentences above this threshold were given similarity 1, and 0 otherwise. We used rel (s|q) to score sentence-to-query. The LexRank score for each sentence was then calcu- lated as:</p><formula xml:id="formula_9">LQ s|q = d × rel s|q z∈C rel z|q + (1 − d)× v∈adj[s] sim s,v r∈adj[v] sim v,r LQ v|q</formula><p>where C is the set of all sentences in a given doc- ument. Here the parameter d is just a damper to designate a probability of randomly jumping to one of the sentences in the graph (d = 0.7). We found the stationary distribution by applying the power method ( = 5), which is guaranteed to converge to a stationary distribution <ref type="bibr" target="#b39">(Otterbacher et al., 2009</ref>). The output of LQ is a score for each sentence from a given document with respect to a query. We used that score to rank sentences in each document from our English machine trans- lated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XI. Projected Cross-Language Query Biased</head><p>Lexrank (LQP) We introduce LQP to describe a way of scoring and ranking sentences such that the L 1 (English) summaries are biased from the L 2 (Farsi) query and source document. Our gold- standard Farsi queries were included with our CLEF 2008 data, making them more reliable than what we could get from automatic translation. First, sentences from each Farsi document were scored with Farsi queries using LQ, described above. Then each LQ score was projected onto sentence-aligned English. We demonstrate LQP <ref type="figure">Figure 7</ref>: LQC -Farsi sentence scores are com- bined with parallel English sentence scores to ob- tain sentence re-ranking.</p><p>in <ref type="figure" target="#fig_5">Figure 6</ref>. By doing this, we simulated trans- lating the user's English query into Farsi with the best possible query translation, before proceed- ing with summarization. This approach to cross- language summarization could be of interest for CLIR systems that do query translation on-the-fly. It is also of interest for summarization systems that need to utilize previously translated source docu- ments the capability is lacking to translate sum- maries from L 2 to L 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XII. Combinatory Query Biased Lexrank (LQC)</head><p>Another variation of LexRank that we introduce in this work is LQC, which combines LexRank scores from both languages to re-rank sentences. A visual summary of this method is shown in <ref type="figure">Figure 7</ref>. We accomplished our re- ranking by first running LQ on Farsi and English separately, then adding the two scores together. This combination of Farsi and English scores pro- vided us with a different way to score and rank sentences, compared with LQ and LQP . The idea behind combinatory query biased LexRank is to take advantage of sentences which are high- ranking in Farsi but not in English. The LQC method exploits all available resources in our dataset: L 1 and L 2 queries as well as L 1 and L 2 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We tested each of our summarization methods and overall strategies in a task-based evaluation frame- work using relevance prediction. We used Me- chanical Turk for our experiments since it has been shown to be useful for evaluating NLP systems <ref type="bibr" target="#b9">(Callison-Burch 2009;</ref><ref type="bibr" target="#b20">Gillick and Liu, 2010</ref>). We obtained human judgments for whether or not a document was considered relevant to a query, or information need. We measured the relevance judgements by precision/recall/F1, accuracy, and also time-on-task based on the average response time per Human Intelligence Task (HIT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mechanical Turk</head><p>In our Mechanical Turk experiment, we used ter- minology from CLEF 2008 to describe a query as an "information need". All of the Mechanical Turk workers were presented with the following for their individual HIT: instructions, an informa- tion need and one summary for a document. Work- ers were asked to indicate if the given summary for a document was relevant to the given informa- tion need <ref type="bibr" target="#b22">(Hobson et al., 2007)</ref>. Workers were not shown the original Farsi source documents. We paid workers $0.01 per HIT. We obtained 5 HITs for each information need and summary pair. We used a built-in approval rate qualification pro- vided by Mechanical Turk to restrict which work- ers could work on our tasks. Each worker had an approval rate of at least 95</p><p>Instructions: Each image below consists of a statement summarizing the informa- tion you are trying to find from a set of documents followed by a summary of one of the documents returned when you query the documents. Based on the summary, choose whether you think the document returned is relevant to the in- formation need. NOTE: It may be diffi- cult to distinguish whether the document is relevant as the text may be difficult to understand. Just use your best judg- ment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>We present our experiment results and additional analysis. First, we report the results of our rel- evance prediction task, showing performance for individual summarization methods as well as per- formance for the overall strategies. Then we show analysis of our results from the monolin- gual question-biased shared-task for DUC 2005, as well as a comparison to previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Results for Individual Methods</head><p>Our results are shown in <ref type="table">Table 1</ref>. We report perfor- mance for 13 individual methods as well as over- all peformance on the 4 different summarization strategies. To calculate the performance for each strategy, we used the arithmetic mean of the corre- sponding individual methods. We measured preci- sion, recall and F1 to give us a sense of our sum- maries might influence document retrieval in an actual CLIR system. We also measured accuracy and time-on-task. For these latter two metrics, we distinguish between summaries that were relevant (R) and non-relevant (NR).</p><p>All of the summarization-based methods fa- vored recall over precision: documents were marked 'relevant' more often than 'non-relevant'. For many of the methods shown in <ref type="table">Table 1</ref>, work- ers spent more time correctly deciding 'relevant' than correctly deciding 'non-relevant'. This sug- gests some workers participated in our Mechanical Turk task purposefully. For many of the summa- rization methods, workers were able to positively identify relevant documents.</p><p>From <ref type="table">Table 1</ref> we see that Full MT performed better on precision than all of the other methods and strategies, but we note that performance on precision was generally very low. This might be due to Mechanical Turk workers overgeneraliz- ing by marking summaries as relevant when they were not. Some individual methods preserve our principle of retrieval-relevance, as indicated by the higher recall scores for SQF, LQEF, and TFQ. That is to say, these particular query biased sum- marization methods can be used to assist users with identifying more relevant documents. The ac- curacy on relevant documents addresses our prin- ciple of query-salience, and it is especially high for our query-biased methods: LQEF, SQF, LQ, and TFQ. The results also seem to fit our intuition that the summary in <ref type="figure" target="#fig_3">Figure 3</ref>  Overall, query biased word clouds outperform the other summarization strategies for 5 out of 7 metrics. This could be due to the fact that word clouds provide a very concise and overview of a document, which is one of the main goals for automatic summarization. Along these lines, word clouds are probably not subject to the effects of MT quality and we believe it is possible that MT quality could have had a negative impact on our query biased extracted sentence summaries, as well as our full MT English texts. <ref type="table">Table 1</ref>: Individual method results: precision/recall/F1, time-on-task, and accuracy. Note that results for time-on-task and accuracy scores are distinguished for relevant (R) and non-relevant (NR) documents.</p><p>Precision, Recall, F1 Time-on-Task Accuracy Summarization Strategy Prec. Rec.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Analysis with DUC 2005</head><p>We analysed our summarization methods by comparing two of our sentence-based methods (LQ and REL) with peers from the monolin- gual question-biased summarization shared-task for DUC 2005. Even though DUC 2005 is a mono- lingual task, we decided to use it as part of our analysis for two reasons: (1) to see how well we could do with query/question biasing while ignor- ing the variables introduced by MT and cross- language text, and (2) to make a comparison to previous work. <ref type="bibr" target="#b42">Pingali et al., (2007)</ref> also used this the same DUC task to assess their cross-language query biased summarization system. Systems from the DUC 2005 question-biased summariza- tion task were evaluated automatically against hu- man gold-standard summaries using ROUGE ( <ref type="bibr">Lin and Hovy, 2003</ref>) . Our results from the DUC 2005 shared-task are shown in <ref type="table" target="#tab_3">Table 2</ref>, reported as ROUGE-2 and ROUGE-SU4 f-scores, as these two variations of ROUGE are the most helpful <ref type="bibr" target="#b14">(Dang, 2005;</ref><ref type="bibr" target="#b42">Pingali et al., 2007)</ref>. <ref type="table" target="#tab_3">Table 2</ref> shows scores for several top peer sys- tems, as well as results for the Tel-Eng-Sum method from <ref type="bibr" target="#b42">Pingali et al., (2007)</ref>. While we have reported f-scores in our analysis, we also note that our implementations of LQ and REL outperform all of the DUC 2005 peer systems for precision, as shown in <ref type="table" target="#tab_4">Table 3</ref>. We also know that ROUGE can- not be used for comparing sentence summaries to ranked lists of words and there are no existing in- trinsic methods to make that kind of comparison. Therefore we were able to successfully compare just 2 of our sentence-based methods to previous work using ROUGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Future Work</head><p>Cross-language query biased summarization is an important part of CLIR, because it helps the user decide which foreign-language documents they might want to read. But, how do we know if a query biased summary is "good enough" to be used in a real-world CLIR system? We want to be able to say that we can do query biased sum- marization just as well for both monolingual and cross-language IR systems. From previous work, there has been some variability with regard to when and what to translate -variables which have no impact on monolingual summarization. We at- tempted to address this issue with two of our meth- ods: LQP and LQC. To fully exploit the MT vari- able, we would need many more relevance pre- diction experiments using humans who know L 1 and others who know L 2 . Unfortunately in our case, we were not able to find Farsi speakers on Mechanical Turk. Access to these speakers would have allowed us to try further experiments as well as other kinds of analysis.</p><p>Our results on the relevance prediction task tell us that query biased summarization strategies help users identify relevant documents faster and with better accuracy than unbiased summaries. Our findings support the findings of <ref type="bibr" target="#b48">Tombros and Sanderson (1998)</ref>. Another important finding is that now we can weigh tradeoffs so that different summarization methods could be used to optimize over different metrics. For example, if we want to optimize for retrieval-relevance we might select a summarization method that tends to have higher recall, such as scaled query biased term frequency (SFQ). Similarly, we could optimize over accu- racy on relevant documents, and use Combinatory LexRank (LQC) with Farsi and English together.</p><p>We have shown that the relevance prediction tasks can be crowdsourced on Mechanical Turk with reasonable results. The data we used from the Farsi CLEF 2008 ad-hoc task included an an- swer key, but there were no parallel English docu- ments. However, in order for the NLP community to make strides in evaluating cross-language query biased summarization for CLIR, we will need star- dards and data. Optimal data would be parallel datasets consisting of documents in L 1 and L 2 with queries in L 1 and L 2 along with an answer key specifying which documents are relevant to the queries. Further we would also need sets of human gold-standard query biased summaries in L 1 and L 2 . These standards and data would al- low us to compare method-to-method across dif- ferent languages, while simultaneously allowing us to tease apart other variables such as: when and what to translate, translation quality, methods for biasing, and type of summarization strategy (sen- tences, words, etc). And of course it would be bet- ter if this standard dataset was multilingual instead of billingual, for obvious reasons.</p><p>We have approached cross-language query bi- ased summarization as a stand-alone problem, treating the CLIR system and document retrieval as a black box. However, summaries need to pre- serve query-salience: summaries should not make it more difficult to positively identify relavant doc- uments. And they should also preserve retrieval- relevance: summaries should help users identify as many relevant documents as possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Full MT English summary and CLEF 2008 English query (title, description, narrative).</figDesc><graphic url="image-1.png" coords="4,79.09,199.87,204.09,76.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>word</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Full MT English summary and CLEF 2008 English query.</figDesc><graphic url="image-2.png" coords="4,314.36,269.32,204.09,136.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Word cloud summary for inverse document frequency (IDF), for query "Tehran's stock market".</figDesc><graphic url="image-3.png" coords="5,314.36,66.19,204.10,101.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Word cloud summary for query biased term frequency (TFQ), for query "Tehran's stock market".</figDesc><graphic url="image-4.png" coords="5,314.36,406.34,204.07,88.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: LQP-projecting Farsi sentence scores onto parallel English sentences.</figDesc><graphic url="image-6.png" coords="7,75.39,66.20,213.17,106.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>seems less relevant to the summaries shown in Figures 4 &amp; 5 even though these are the same documents biased on the same query "Tehran stock market".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of peer systems on DUC 
2005 shared-task for monolingual question-biased 
summarization, f-scores from ROUGE-2 and 
ROUGE-SU4. 
Peer ID ROUGE-2 ROUGE-SU4 
17 
0.07170 
0.12970 
8 
0.06960 
0.12790 
4 
0.06850 
0.12770 
Tel-Eng-Sum 
0.06048 
0.12058 
LQ 
0.05124 
0.09343 
REL 
0.04914 
0.09081 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top 3 system precision scores for 
ROUGE-2 and ROUGE-SU4. 
Peer ID ROUGE-2 ROUGE-SU4 
LQ 
0.08272 
0.15197 
REL 
0.0809 
0.15049 
15 
0.07249 
0.13129 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to express thanks to David Har-wath at MIT Computer Science and Artificial In-telligence Laboratory (CSAIL), who helped us de-velop and implement ideas in this paper. We also want to thank Terry Gleason from MIT Lincoln Laboratory for providing machine translations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ad hoc track overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><forename type="middle">Maria</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Nunzio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retrieval and Novelty Detection at the Sentence Level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bolivar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, (SIGIR &apos;03)</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, (SIGIR &apos;03)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting Multiple Translation Resources for English-Persian Cross Language Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosein</forename><surname>Azarbonyad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Shakery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heshaam</forename><surname>Faili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Access Evaluation. Multilinguality, Multimodality, and Visualization</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">8138</biblScope>
			<biblScope unit="page" from="93" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constructing Query-biased Summaries: A Comparison of Human and System Generated Snippets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorena</forename><surname>Leal Bando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falk</forename><surname>Scholer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Turpin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Symposium on Information Interaction in Context (IIiX &apos;10)</title>
		<meeting>the Third Symposium on Information Interaction in Context (IIiX &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
	<note>ACM 2010</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Query-Relevant Summarization Using FAQs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vibhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CrossLingual Query Dependent Snippet Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Science and Information Technology (IJCSIT)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language Independent Query Focused Snippet Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinaki</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Access Evaluation. Multilinguality, Multimodality, and Visual Analytics</title>
		<editor>T. Catarci, P. Forner, D. Hiemstra, A. Peñas, and G. Santucci</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7488</biblScope>
			<biblScope unit="page" from="138" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Robustness of Centrality Measures Under Conditions of Imperfect Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Borgatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krackhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="124" to="136" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Graph-Based Approach to CrossLanguage Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Torres-Moreno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="113" to="118" />
		</imprint>
	</monogr>
<note type="report_type">Polibits</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluating Translation Quality Using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheap</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Creative</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Singapore, ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised Approach for Selecting Sentences in Query-Based Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First International FLAIRS Conference</title>
		<meeting>the Twenty-First International FLAIRS Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single Document Extractive Text Summarization Using Genetic Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niladri</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amol</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Applications of Information Technology (EAIT), 2012 Third International Conference</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inverse Document Frequency (IDF): A Measure of Deviations From Poisson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language processing using very large corpora</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="283" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised Models for Morpheme Segmentation and Morphology Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
		<idno>3:1-3:34</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Overview of DUC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Document Understanding Conference</title>
		<meeting>the Document Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian QueryFocused Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The MIT-LL/AFRL IWSLT2012 MT System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Drexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">P</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Slyh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Ore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">G</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><surname>President</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">New Methods in Automatic Extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Günesgünes¸günes¸erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-Expert Evaluation of Summarization Systems is Risky</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk<address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="148" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Topic Identification Based Extrinsic Evaluation of Summarization Techniques Applied to Conversational Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5073" to="5076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Task-Eased Evaluation of Text Summarization Using Relevance Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Hobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1482" to="1499" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Summarization Evaluation Methods: Experiments and Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of American Association for Artificial Ingelligence (AAAI)</title>
		<meeting>American Association for Artificial Ingelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving Persian Information Retrieval Systems Using Stemming and Part of Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Karimpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amineh</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Pishdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename><surname>Mohtarami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abolfazl</forename><surname>Aleahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Oroumchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Crosslanguage Evaluation Forum Conference on Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<meeting>the 9th Crosslanguage Evaluation Forum Conference on Evaluating Systems for Multilingual and Multimodal Information Access<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Looking For A Few Good Metrics: Automatic Summarization Evaluation-How Many Samples Are Enough?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR Workshop</title>
		<meeting>NTCIR Workshop<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic Summary Evaluation without Human Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Automatic Creation of Literature Abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Luhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="165" />
			<date type="published" when="1958-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using Cohesion and Coherence Models for Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bloedorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Gates</surname></persName>
		</author>
		<idno>SS-989-06</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI Symposium</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The TIPSTER SUMMAC Text Summarization Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Therese</forename><surname>Firmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Association for Coputational Linguistics</title>
		<meeting>European Association for Coputational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Summarization Evaluation: An Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NTCIR Workshop</title>
		<meeting>the NTCIR Workshop</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SUMMAC: A Text Summarization Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>House</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Therese</forename><surname>Firmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="68" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Do Summaries Help? A Task-Based Evaluation of MultiDocument Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="210" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ecological Validity and the Evaluation of Speech Summarization Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosmin</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>Workshop on Evaluation Metrics and System Comparison for Automatic Summarization<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-Answer Focused Multi-Document Summarization Using a Question-Answering Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Nozawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiaki</forename><surname>Asada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics, COLING &apos;04</title>
		<meeting>the 20th International Conference on Computational Linguistics, COLING &apos;04<address><addrLine>Stroudsburg, PA, USA, ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Extrinsic Summarization Evaluation: A Decision Audit Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kleinbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Poller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kilgour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Survey of Text Summarization Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Text Data</title>
		<editor>C. C. Aggarwal and C. Zhai</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="43" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluation of a Cross-Lingual Romanian-English MultiDocument Summariser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Or˘ Asan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Andreea Chiorean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Language Resources and Evaluation Conference, LREC</title>
		<meeting>Language Resources and Evaluation Conference, LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using Random Walks for Question-focused Sentence Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günesgünes¸günes¸erkan</forename><surname>Dragomir R Ravev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Biased LexRank: Passage Retrieval Using Random Walks With Question-Based Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günesgünes¸günes¸erkan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Ravev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="54" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An Assessment of the Accuracy of Automatic Evaluation in Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization<address><addrLine>Montréal, Canada, ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Pagerank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Stanford Digital Library Technologies Project</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Experiments in Cross Language Query Focused Multi-Document Summarization In Workshop on Cross Lingual Information Access Addressing the Information Need of Multilingual Societies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Text Summarization Evaluation: Correlating Human Performance on an Extrinsic Task With Automatic Intrinsic Metrics. No. LAMP-TR-133</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>President</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland College Park Language and Media Processing Laboratory Institute for Advanced Computer Studies (UMIACS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Centroid-Based Summarization of Multiple Documents. InProceedings of Informaion Processing Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malgorzata</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sty´ssty´s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-11" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="919" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Understanding Inverse Document Frequency: on Theoretical Arguments for IDF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="503" to="520" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 17th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the Specification of Term Values in Automatic Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Shu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="372" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Advantages of Query Biased Summaries in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tombros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Graph-Based</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning for Topic-Focused MultiDocument Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multi-Modality</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international jont conference on Artifical intelligence (IJCAI&apos;09)</title>
		<meeting>the 21st international jont conference on Artifical intelligence (IJCAI&apos;09)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1586" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">CrossLanguage Document Summarization Based on Machine Translation Quality Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL &apos;10). Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL &apos;10). Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="917" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Summarizing the Differences in Multilingual News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houping</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="735" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SentTopic-MultiRank: A Novel Ranking Model for Multi-Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian&amp;apos;en</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2977" to="2992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Using the Web Corpus to Translate the Queries in CrossLingual Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinming</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings in 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering</title>
		<meeting>in 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="493" to="498" />
		</imprint>
	</monogr>
	<note>IEEE NLP-KE &apos;05</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
