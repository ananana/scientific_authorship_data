<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Coupled Sequence Labeling on Heterogeneous Annotations via Context-aware Pruning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="753" to="762"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The recently proposed coupled sequence labeling is shown to be able to effectively exploit multiple labeled data with heterogeneous annotations but suffer from severe inefficiency problem due to the large bundled tag space (Li et al., 2015). In their case study of part-of-speech (POS) tagging, Li et al. (2015) manually design context-free tag-to-tag mapping rules with a lot of effort to reduce the tag space. This paper proposes a context-aware pruning approach that performs token-wise constraints on the tag space based on contextual evidences, making the coupled approach efficient enough to be applied to the more complex task of joint word segmentation (WS) and POS tagging for the first time. Experiments show that using the large-scale People Daily as auxiliary heterogeneous data, the coupled approach can improve F-score by 95.55 − 94.88 = 0.67% on WS, and by 90.58 − 89.49 = 1.09% on joint WS&amp;POS on Penn Chinese Treebank. All codes are released at http://hlt.suda.edu.cn/~zhli.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In statistical natural language processing, manually labeled data is inevitable for model supervision, but is also very expensive to build. However, due to the long-debated differences in underlying linguistic theories or emphasis of application, there often exist multiple labeled corpora for the same or similar tasks following different annotation guidelines (Jiang et * Correspondence author   <ref type="bibr" target="#b19">Yu et al., 2003)</ref>. <ref type="table" target="#tab_0">Table 1</ref> gives an example with both CTB and PD annotations. We can see that CTB and PD differ in both word boundary standards and POS tag sets. Previous work on exploiting heterogeneous data mainly focuses on indirect guide-feature methods. The basic idea is to use one resource to generate extra guide features on another resource ( <ref type="bibr">Jiang et al., 2009;</ref><ref type="bibr" target="#b12">Sun and Wan, 2012)</ref>, which is similar to stacked learning <ref type="bibr" target="#b7">(Nivre and McDonald, 2008)</ref>. <ref type="bibr" target="#b5">Li et al. (2015)</ref> propose a coupled sequence labeling approach that can directly learn and predict two het- erogeneous annotations simultaneously. The basic idea is to transform a single-side tag into a set of bundled tags for weak supervision based on the idea of ambiguous labeling. Due to the huge size of the bundled tag space, their coupled model is extremely inefficient. They then carefully design tag-to-tag mapping rules to constrain the search space. Their case study on POS tagging shows that the coupled model outperforms the guide-feature method. How- ever, the requirement of manually designed mapping rules makes their approach less attractive, since such mapping rules may be very difficult to construct for more complex tasks such as joint word segmentation (WS) and POS tagging.</p><p>This paper proposes a context-aware pruning ap- proach that can effectively solve the inefficiency problem of the coupled model, making coupled se- quence labeling more generally applicable. Specifi- cally, this work makes the following contributions:</p><p>(1) We propose and systematically compare two ways for realizing context-aware pruning, i.e., online and offline pruning. Experiments on POS tagging show that both online and offline pruning can greatly improve the model effi- ciency with little accuracy loss.</p><p>(2) We for the first time apply coupled sequence labeling to the more complex task of joint WS&amp;POS tagging. Experiments show that online pruning works badly due to the much larger tag set while offline pruning works well. Further analysis gives a clear explanation and leads to more insights in learning from ambiguous labeling.</p><p>(3) Experiments on joint WS&amp;POS tagging show that our coupled approach with offline pruning improves F-score by 95.55 − 94.88 = 0.67% on WS, and by 90.58 − 89.49 = 1.09% on joint WS&amp;POS on CTB5-test over the baseline, and is also consistently better than the guide-feature method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Coupled Sequence Labeling</head><p>Given an input sequence of n tokens, denoted by x = w 1 ...w n , coupled sequence tagging aims to si- multaneously predict two tag sequences t a = t a 1 ...t a n and t b = t b 1 ...t b n , where t a i ∈ T a and t b i ∈ T b (1 ≤ i ≤ n), and T a and T b are two different predefined tag sets. Alternatively, we can view the two tag sequences as one bundled tag sequence t = [t a ,</p><formula xml:id="formula_0">t b ] = [t a 1 , t b 1 ]...[t a n , t b n ], where [t a i , t b i ] ∈ T a × T b is called a bundled tag.</formula><p>In this work, we treat CTB as the first-side anno- tation and PD as the second-side. For POS tagging, T a is the set of POS tags in CTB, and T b is the set of POS tags in PD, and we ignore the word boundary differences in the two datasets, following <ref type="bibr" target="#b5">Li et al. (2015)</ref>. We have |T a | = 33 and |T b | = 38.</p><p>For joint WS&amp;POS tagging, we employ the stan- dard four-tag label set to mark word boundaries, among which B, I, E respectively represent that the concerned character situates at the begining, inside, end position of a word, and S represents a single- character word. Then, we concatenate word bound- ary labels with POS tags. For instance, the first three characters in <ref type="table" target="#tab_0">Table 1</ref> correspond to "/B@AD /I@AD /E@AD" in CTB, and to "/B@d /E@d /S@v" in PD. We have |T a | = 99 and |T b | = 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Coupled Conditional Random Field (CRF)</head><p>Following <ref type="bibr" target="#b5">Li et al. (2015)</ref>, we build the coupled sequence labeling model based on a bigram linear- chain CRF ( <ref type="bibr" target="#b2">Lafferty et al., 2001</ref>). The conditional probability of a bundled tag sequence t is:</p><formula xml:id="formula_1">p(t|x, ˜ S; θ) = e Score(x,t;θ) Z(x, ˜ S; θ) Z(x, ˜ S; θ) = ∑ t∈˜St∈˜ t∈˜S e Score(x,t;θ)<label>(1)</label></formula><p>where θ is the feature weights; Z(x, ˜ S; θ) is the normalization factor; ˜ S is the search space including all legal tag sequences for x. We use˜Tuse˜ use˜T i ⊆ T a × T b to denote the set of all legal tags for token w i , sõsõ S = ˜ T 1 × · · · × ˜ T n . According to the linear-chain Markovian assump- tion, the score of a bundled tag sequence is:</p><formula xml:id="formula_2">Score(x, t; θ) = θ · f(x, [t a , t b ]) n+1 ∑ i=1 θ ·    f joint (x, i, [t a i−1 , t b i−1 ], [t a i , t b i ]) f sep_a (x, i, t a i−1 , t a i ) f sep_b (x, i, t b i−1 , t b i )    (2)</formula><p>where f(x, [t a , t b ]) is the accumulated sparse feature vector; f joint/sep_a/sep_b (x, i, t ′ , t) share the same list of feature templates, and return local feature vectors for tagging w i−1 as t ′ and w i as t.</p><p>Traditional single-side tagging models can only exploit a single set of separate features f sep_a (.) or f sep_b (.). In contrast, the coupled model makes use of all three sets of features. <ref type="bibr" target="#b5">Li et al. (2015)</ref> demonstrate that the joint features f joint (.) capture the implicit mappings between heterogeneous anno- tations, and the separate features function as back-off features for alleviating the data sparseness problem of the joint features.</p><p>For the feature templates, we follow <ref type="bibr" target="#b5">Li et al. (2015)</ref> and adopt those described in <ref type="bibr" target="#b20">Zhang and Clark (2008)</ref> for POS tagging, and use those described in <ref type="bibr" target="#b22">Zhang et al. (2014b)</ref> for joint WS&amp;POS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learn from Incomplete Data</head><p>The key challenge for coupled sequence labeling is that both CTB and PD are non-overlapping and each contains only one-side annotations. Based on the idea of ambiguous labeling, <ref type="bibr" target="#b5">Li et al. (2015)</ref> first concatenate a single-side tag with many possible second-side tags, and then use the set of bundled tags as possibly-correct references during training.</p><p>Suppose x = w 1 ...w n is a training sentence from CTB, and t a = ˇ t a 1 ... ˇ t a n is the manually labeled tag sequence. Then we define T i = { ˇ t a i } × T b as the set of possibly-correct bundled tags, and S = T 1 × · · · × T n as a exponential-size set of possibly-correct bundled tag sequences used for model supervision.</p><p>Given x and the whole legal search space˜Sspace˜ space˜S, the probability of the possibly-correct space S ⊆ ˜ S is:</p><formula xml:id="formula_3">p(S|x, ˜ S; θ) = ∑ t∈V p(t|x, ˜ S; θ) = Z(x, S; θ) Z(x, ˜ S; θ)<label>(3)</label></formula><p>where Z(x, S; θ) is analogous to Z(x, ˜ S; θ) in Eq. (3) but only sums over S.</p><formula xml:id="formula_4">Given D = {(x j , S j , ˜ S j )} N j=1</formula><p>, the gradient of the log likelihood is:</p><formula xml:id="formula_5">∂LL(D; θ) ∂θ = ∂log ∑ j p(S j |x j , ˜ S j ; θ) ∂θ = ∑ j ( ∂log Z(x j , S j ; θ) ∂θ − ∂log Z(x j , ˜ S j ; θ) ∂θ ) = ∑ j ( E t|x j ,S j ;θ [f(x j , t)] − E t|x j , ˜ S j ;θ [f(x j , t)] )<label>(4)</label></formula><p>where the two terms are the feature expectations under S j and˜Sand˜ and˜S j respectively. And the detailed derivations are as follows:</p><formula xml:id="formula_6">∂log Z(x, S; θ) ∂θ = 1 Z(x, S; θ) × ∂ ∑ t∈S e Score(x,t;θ) ∂θ = ∑ t∈S ( e Score(x,t;θ) Z(x, S; θ) × ∂Score(x, t; θ) ∂θ ) = ∑ t∈S p(t|x, S; θ) × f(x, t) =E t|x,S;θ [f(x, t)]<label>(5)</label></formula><p>Please notice that t = [t a , t b ] denotes a bundled tag sequence in this context of coupled sequence labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Efficiency Issue</head><p>Under complete mapping, each one-side tag is mapped to all the-other-side tags for constructing bundled tags, producing a very huge set of legal bundled tags˜Ttags˜ tags˜T i = T a × T b . Using the classic Forward-Backward algorithm, we still need</p><formula xml:id="formula_7">O(n × |T a | 2 × |T b | 2 ) time complexity to compute E t|x, ˜ S;θ [f(x, t)]</formula><p>, which is prohibitively expensive. 2 In order to improve efficiency, <ref type="bibr" target="#b5">Li et al. (2015)</ref> pro- pose to use a set of context-free tag-to-tag mapping rules for reducing the search space. For example, we may specify that the CTB POS tag "NN" can only be concatenated with a set of PD tags like "{n, vn, ns}". 3 With much effort, they propose a set of relaxed mapping rules that greatly reduces the number of bundled tags from |T a | × |T b | = 33 × 38 = 1, 254 to 179 for POS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B@AD I@AD E@AD S@PN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[I@AD,E@d][I@AD,E@v] [I@NN,E@d][I@NN,E@v] Bundled tags</head><p>Figure 1: Illustration of context-aware pruning with r = 2 on a CTB training sentence.</p><p>suggest that the coupled model can best learn the implicit context-sensitive mapping relationships between annotations under complete mapping, and imposing strict tag-to-tag mapping constraints usually hurts tagging accuracy. In this work, our intuition is that the mapping relationships between heterogeneous annotations are highly context-sensitive. Therefore, we propose a context-aware pruning approach to more accurately capture such mappings, thus solving the efficiency issue. The basic idea is to consider only a small set of most likely bundled tags, instead of the whole bundled tag space T a × T b , based on evidences of surrounding contexts. Specifically, for each token w i , we only keep r one-side tags according to sep- arate features f sep_a/b (.) for each side, and then use the remaining single-side tags to construct˜Tconstruct˜ construct˜T i and T i .</p><p>We use the second character "/I@AD" in <ref type="figure">Fig.  1</ref> as an example. We list the single-side tags in the descending order of their marginal probabilities according to f sep_a/b (.). Then we only keep r = 2 single-side tags, used as T a i and T b i . Theñ T i = T a × T b contains the four bundled tags shown in the upper box, known as the whole possible tag set for search- ing. And T i = { ˇ t a } × T b contains two bundled tags, as marked in bold, knowns as the possibly-correct tag set, sincě t a is the manually labeled tag. The case when the word has the second-side manually-labeled tag { ˇ t b } can be similarly handled. Beside r, we use another hyper-parameter λ to further reduce the number of one-side tag candidates. The intuition is that in many cases, we may only need to use a smaller number r ′ &lt; r of possible candi- dates, since the remaining tags are very unlikely ones according to the marginal probabilities. Therefore, for each item w i , we define r ′ as the smallest number of most likely candidate tags whose accumulative probability is larger than λ. Then, we only keep the min(r ′ , r) most likely candidate tags.</p><p>We have | ˜ T i | = r 2 without considering the ac- cumulated probability threshold λ. Thus, it requires O(nr 4 ) time complexity to compute E t|x, ˜ S;θ [f(x, t)] using the Forward-Backward algorithm.</p><p>In the following, we propose two ways for real- izing context-aware pruning, i.e., online and offline pruning. Their comparison and analysis are given in the experiment parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Online Pruning</head><p>The online pruning approach directly uses the cou- pled model to perform pruning. Given a sentence, we first use a subset of features f sep_a (.) and corre- sponding feature weights trained so far to compute marginal probabilities of first-side tags, and then analogously process the second-side tags based on f sep_b (.). This requires roughly the same time com- plexity as two baseline models. Then the marginal probabilities are used for pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Offline Pruning</head><p>The offline pruning approach is a little bit more complex, and uses many additional single-side tag- ging models for pruning. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the work- flow. Particularly, n-fold jack-knifing is adopted to perform pruning on the same-side training data. Finally, all training/dev/test datasets of CTB and PD are preprocessed in an offline way, so that each word in a sentence has a set of most likely CTB tags (T a i ) and another set of most likely PD tags (T b i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Settings</head><p>Data. Following <ref type="bibr" target="#b5">Li et al. (2015)</ref>, we use CTB5 and PD for the heterogeneous data. Under the standard data split of CTB5, the training/dev/test datasets contain 16, 091/803/1, 910 sentences respectively. For PD, we use the 46, 815 sentences in January 1998 as the training data, the first 2, 000 sentences in February as the development data, and the first 5, 000 sentences in June as the test data. Evaluation Metrics. We use the standard token- wise tagging accuracy for POS tagging. For joint WS&amp;POS tagging, besides character-wise tagging accuracy, we also use the standard precision (P), recall (R), and F-score of only words (WS) or POS- tagged words (WS&amp;POS).</p><p>Parameter settings. Stochastic gradient descent (SGD) is adopted to train the baseline single-side tagging models, the guide-feature models, and the coupled models. <ref type="bibr">4</ref> For the coupled models, we directly follow the simple corpus-weighting strategy proposed in <ref type="bibr" target="#b5">Li et al. (2015)</ref> to balance the contribution of the two datasets. We randomly sample 5, 000 CTB-train sentences and 5, 000 PD-train sentences, which are then merged and shuffled for one-iteration training. After each iteration, the coupled model is evaluated on both CTB-dev and PD-dev, providing us two single-side tag accuracies, one on CTB-side tags, and the other on PD-dev tags. Another advantage of using a subset of training data in one iteration is to monitor the training progress in smaller steps. For fair comparison, when building the baseline and guide-feature models, we also randomly sample 5, 000 training sentences from the whole training data for one-iteration training, and then report an tagging accuracy on development data. For all mod- els, the training terminates if peak accuracies stop improving within 30 consecutive iterations, and we use the model that performs the best on development data for final evaluation on test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on POS Tagging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parameter Tuning</head><p>For both online and offline pruning, we need to de- cide the maximum number of single-side tag candi- dates r and the accumulative probability threshold λ for further truncating the candidates. <ref type="table" target="#tab_2">Table 2</ref> shows <ref type="bibr">4</ref> We use the implementation of SGD in CRFsuite (http:// www.chokkan.org/software/crfsuite/), and set b = 30 as the batch-size and C = 0.1 as the regularization factor. r λ Accuracy (%) #Tags (pruned) CTB5-dev PD-dev CTB-side PD-side  the tagging accuracies and the averaged numbers of single-side tags for each token after pruning.</p><note type="other">Online Pruning 2 0.98 94.25 95.03 2.0 2.0 4 0.98 95.06 95.66 3.9 4.0 8 0.98 95.14 95.83 6.3 7.4 16 0.98 95.12 95.81 7.8 14.1 8 0.90 95.15 95.79 3.7 6.3 8 0.95 95.13 95.82 5.1 7.1 8 0.99 95.15 95.74 7.4 7.9 8 1.00 95.15 95.76 8.0 8.</note><p>The first major row tunes the two hyper- parameters for online pruning. We first fix λ = 0.98 and increase r from 2 to 8, leading to consistently improved accuracies on both CTB5-dev and PD- dev. No further improvement is gained with r = 16, indicating that tags below the top-8 are mostly very unlikely ones and thus insignificant for computing feature expectations. Then we fix r = 8 and try different λ. We find that λ has little effect on tagging accuracies but influences the numbers of remaining single-side tags. We choose r = 8 and λ = 0.98 for final evaluation.</p><p>The second major row tunes r and λ for offline pruning. Different from online pruning, λ has much greater effect on the number of remaining single-side tags. Under λ = 0.9999, increasing r from 8 to 16 leads to 0.20% accuracy improvement on CTB5-dev, but using r = 32 has no further gain. Then we fix r = 16 and vary λ from 0.99 to 0.99999. We choose r = 16 and λ = 0.9999 for offline pruning for final evaluation, which leaves each word with about 5.2 CTB-tags and 7.6 PD-tags on average. 94.60 - - <ref type="table">Table 3</ref>: POS tagging performance of difference approaches on CTB5 and PD. <ref type="table">Table 3</ref> summarizes the accuracies on the test data and the tagging speed during the test phase. "Cou- pled (No Prune)" refers to the coupled model with complete mapping in <ref type="bibr" target="#b5">Li et al. (2015)</ref>, which maps each one-side tag to all the-other-side tags. "Coupled (Relaxed)" refers the coupled model with relaxed mapping in <ref type="bibr" target="#b5">Li et al. (2015)</ref>, which maps a one-side tag to a manually-designed small set of the-other- side tags. <ref type="bibr" target="#b4">Li et al. (2012b)</ref> report the state-of-the- art accuracy on this CTB data, with a joint model of Chinese POS tagging and dependency parsing. It is clear that both online and offline pruning greatly improve the efficiency of the coupled model by about two magnitudes, without the need of a carefully predefined set of tag-to-tag mapping rules. <ref type="bibr">5</ref> Moreover, the coupled model with offline pruning achieves 0.76% accuracy improvement on CTB5- test over the baseline model, and 0.48% over our reimplemented guide-feature approach of <ref type="bibr">Jiang et al. (2009)</ref>. The gains on PD-test are marginal, possibly due to the large size of PD-train, similar to the results in <ref type="bibr" target="#b5">Li et al. (2015)</ref>. <ref type="table" target="#tab_5">Table 4</ref> shows results for tuning r and λ. From the results in the first major row, we can see that in the online pruning method, λ seems useless and r becomes the only threshold for pruning unlikely single-side tags. The accuracies are much inferior to r λ Accuracy (%) #Tags (pruned) CTB5-dev PD-dev CTB-side PD-side  those from the offline pruning approach. We believe that the accuracies can be further improved with larger r, which would nevertheless lead to severe inefficiency issue. Based on the results, we choose r = 16 and λ = 1.00 for final evaluation. The second major row tries to decide r and λ for the offline pruning approach. Under λ = 0.995, increasing r from 8 to 16 improves accuracies both on CTB5-dev and PD-dev, but further using r = 32 leads to little gain. Then we fix r = 16 and vary λ from 0.95 to 0.999. Using λ = 0.95 leaves only 1.6 CTB tags and 2.1 PD tags for each character, but has a large accuracy drop. We choose r = 16 and λ = 0.995 for offline pruning for final evaluation, which leaves each character with 3.2 CTB-tags and 4.3 PD-tags on average. <ref type="table" target="#tab_7">Table 5</ref> summarizes the accuracies on the test data and the tagging speed (characters per second) during the test phase. "Coupled (No Prune)" is not tried due to the prohibitive tag set size in joint WS&amp;POS tag- ging, and "Coupled (Relaxed)" is also skipped since it seems impossible to manually design reasonable tag-to-tag mapping rules in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments on Joint WS&amp;POS Tagging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Parameter Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Main Results</head><p>In terms of efficiency, the coupled model with offline pruning is on par with the baseline single-side tagging model. <ref type="bibr">6</ref>    In terms of F-score, the coupled model with offline pruning achieves 0.67% (WS) and 1.09% (WS&amp;POS) gains on CTB5-test over the baseline model, and 0.48% (WS) and 0.79% (WS&amp;POS) over our reimplemented guide-feature approach of <ref type="bibr">Jiang et al. (2009)</ref>. Similar to the case of POS tagging, the baseline model is very competitive on PD-test due to the large scale of PD-train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P/R/F (%) on CTB5-test P/R/F (%) on PD-test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>Online vs. offline pruning. The averaged numbers of single-side tags after pruning in <ref type="table" target="#tab_5">Table 4</ref> and 2), suggest that the online pruning approach works badly in assigning proper marginal probabilities to different tags. Our first guess is that in online prun- ing, the weights of separate features are optimized as a part of the coupled model, and thus producing somewhat flawed probabilities. However, our fur- ther analysis gives a more convincing explanation. <ref type="figure" target="#fig_2">Fig. 3</ref> compares the distribution of averaged probabilities of k th -best CTB-side tags after online and offline pruning. The statistics are gathered on CTB5-test. Under online pruning, the averaged probability of the best tag is only about 0.4, which is surprisingly low and cannot be explained with the equal to the time of two baseline models.</p><p>aforementioned improper optimization issue. Please note that both the online and offline models uses the best choices of r and λ based on <ref type="table" target="#tab_5">Table 4</ref>, and are trained until convergence.</p><p>After a few trials of reducing the size of PD-train for training the coupled model, we realize that the underlying reason is that ambiguous labeling makes the probability mass more uniformly distributed, since for a PD-train sentence, the characters only have the gold-standard PD-side tags, and the model basically uses all CTB-side tags as gold-standard answers. Thanks to the CTB-train sentences, the model may be able to choose the correct tag, but inevitably becomes more indecisive at the same time due to the PD-train sentences.</p><p>In contrast, the offline pruning approach directly uses two baseline models for pruning, which is a job perfectly suitable for the baseline models. The entropy of the probability distribution for online pruning is about 1.524 while that for offline pruning is only 0.355.</p><p>Error distributions. To better understand the gains from the coupled approach, we show the F- score of specific POS tags for both the baseline and coupled models in <ref type="figure" target="#fig_3">Fig. 4</ref>, in the descending order of absolute F-score improvements. The largest improvement is from words tagged as "LB" (mostly for the word "", marking a certain type of passive construction), and the F-score increases by 65.22 − 54.55 = 10.67%. Nearly all POS tags have more or less F-score improvement. Due to the space limit, we only show the tags with more than 2.0% improvement. The most noticeable exception is that F-score drops by 84.80 − 86.49 = −1.69% for words tagged as "OD" (ordinal numbers, as opposed to cardinal numbers).</p><p>In terms of words, we find the largest gain is from "/NR" (Luxemburgo, place name), which appears 11 times in CTB5-test, with an absolute   improvement of 90.00 − 16.67 = 73.33% in recall ratio. The reason is that PD-train contains a lot of related words such as "" (Luxembourg, place name) and " " (Krayzelburg, person name) while CTB5-train has none.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison with Previous Work</head><p>In order to compare with previous work, we also run our models on CTB5X and PD, where CTB5X adopts a different data split of CTB5 and is widely used in previous research on joint WS&amp;POS tagging ( <ref type="bibr">Jiang et al., 2009;</ref><ref type="bibr" target="#b12">Sun and Wan, 2012</ref>). CTB5X-dev/test only contain 352/348 sentences respectively. <ref type="table" target="#tab_9">Table 6</ref> presents the F scores on CTB5X-test. We can see that the coupled model with offline pruning achieves 0.64% (WS) and 1.16% (WS&amp;POS) F-score improvements over the baseline model, and 0.05% (WS) and 0.33% (WS&amp;POS) over the guide-feature approach. The original guide-feature method in <ref type="bibr">Jiang et al. (2009)</ref> achieves 98.23% and 94.03% F-score, which is very close to the results of our reimplemented model. The sub-word stacking approach of Sun and <ref type="bibr" target="#b12">Wan (2012)</ref> can be understood as a more complex variant of the basic guide-feature method. <ref type="bibr">7</ref> The results on both the larger CTB5-test (in <ref type="table" target="#tab_7">Ta- ble 5</ref>) and CTB5X-test suggest that the coupled approach is more consistent and robust than the guide-feature method. The reason may be two- fold. First, in the coupled approach, the model is able to actively learn the implicit mappings between two sets of annotations, whereas the guide-feature model can only passively learn when to trust the automatically produced tags. Second, the coupled approach can directly learn from both heterogeneous training datasets, thus covering more phenomena of language usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>A lot of research has been devoted to design an effec- tive way to exploit non-overlapping heterogeneous labeled data, especially in Chinese language process- ing, where such heterogeneous resources are ubiqui- tous due to historical reasons. <ref type="bibr">Jiang et al. (2009)</ref> first propose the guide-feature approach, which is similar to stacked learning <ref type="bibr" target="#b7">(Nivre and McDonald, 2008)</ref>, for joint WS&amp;POS tagging on CTB and PD. <ref type="bibr" target="#b12">Sun and Wan (2012)</ref> further extend the guide-feature method and propose a more complex sub-word stacking ap- proach. <ref type="bibr" target="#b9">Qiu et al. (2013)</ref> propose a linear coupled model similar to that of <ref type="bibr" target="#b5">Li et al. (2015)</ref>. The key difference is that the model of <ref type="bibr" target="#b9">Qiu et al. (2013)</ref> only uses separate features, while <ref type="bibr" target="#b5">Li et al. (2015)</ref> and this work explore joint features as well. <ref type="bibr" target="#b3">Li et al. (2012a)</ref> apply the guide-feature idea to dependency parsing on CTB and PD. <ref type="bibr" target="#b21">Zhang et al. (2014a)</ref> extend a shift-reduce dependency parsing model in order to simultaneously learn and produce two heterogeneous parse trees, which however as- sumes the existence of training data with both-side annotations.</p><p>Our context-aware pruning approach is similar to coarse-to-fine pruning in parsing community ( <ref type="bibr" target="#b1">Koo and Collins, 2010;</ref><ref type="bibr" target="#b11">Rush and Petrov, 2012)</ref>, which is a useful technique that allows us to use very complex parsing models without too much efficiency cost. The idea is first to use a simple and basic off-shelf model to prune the search space and only keep highly likely dependency links, and then let the complex model infer in the remaining search space. <ref type="bibr" target="#b15">Weiss and Taskar (2010)</ref> propose structured prediction cas- cades: a sequence of increasingly complex models that progressively filter the space of possible outputs, and provide theoretical generalization bounds on a novel convex loss function that balances pruning error with pruning efficiency.</p><p>This work is also closely related with multi-task learning, which aims to jointly learn multiple related tasks with the benefit of using interactive features under a share representation <ref type="bibr">(Ben-David and Schuller, 2003;</ref><ref type="bibr">Ando and Zhang, 2005;</ref><ref type="bibr" target="#b8">Parameswaran and Weinberger, 2010)</ref>. However, as far as we know, multi-task learning usually assumes the existence of data with labels for multiple tasks at the same time, which is unavailable in our scenario, making our problem more particularly difficult.</p><p>Our coupled CRF model is similar to a factorial CRF ( <ref type="bibr" target="#b13">Sutton et al., 2004</ref>), in the sense that the bundled tags can be factorized into two connected latent variables. Initially, factorial CRFs are de- signed to jointly model two related (and typically hierarchical) sequential labeling tasks, such as POS tagging and chunking. In this work, our coupled CRF model jointly handles two same tasks with different annotation schemes. Moreover, this work provides a natural way to learn from incomplete annotations where one sentence only contains one- side labels.</p><p>Learning with ambiguous labeling is previously explored for classification <ref type="bibr" target="#b0">(Jin and Ghahramani, 2002</ref>), sequence labeling ( <ref type="bibr">Dredze et al., 2009</ref>), parsing ( <ref type="bibr" target="#b10">Riezler et al., 2002;</ref><ref type="bibr" target="#b14">Täckström et al., 2013)</ref>. Recently, researchers propose to derive natural annotations from web data to supervise Chinese word segmentation models in the form of ambiguous labeling <ref type="bibr">(Jiang et al., 2013;</ref><ref type="bibr" target="#b6">Liu et al., 2014;</ref><ref type="bibr" target="#b18">Yang and Vozila, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper proposes a context-aware pruning ap- proach for the coupled sequence labeling model of <ref type="bibr" target="#b5">Li et al. (2015)</ref>. The basic idea is to more accurately constrain the bundled tag space of a token according to its contexts in the sentence, instead of using heuristic context-free tag-to-tag mapping rules in the original work. We propose and compare two different ways of realizing pruning, i.e., online and offline pruning. In summary, extensive experiments leads to the following findings.</p><p>(1) Offline pruning works well on both POS tag- ging and joint WS&amp;POS tagging, whereas on- line pruning only works well on POS tagging but fails on joint WS&amp;POS tagging due to the much larger tag set. Further analysis shows that the reason is that under online pruning, ambiguous labeling during training makes the probabilities of single-side tags more evenly distributed.</p><p>(2) In terms of tagging accuracy and F-score, the coupled approach with offline pruning outper- forms the baseline single-side tagging model by large margin, and is also consistently better than the mainstream guide-feature method on both POS tagging and joint WS&amp;POS tagging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Workflow of offline pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Probability distribution with online/offline pruning for the task of joint WS&amp;POS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: F-score comparison between the baseline and coupled WS&amp;POS tagging models on different CTB POS tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>An example of heterogeneous annotations. 

al., 2009). For instance, in Chinese language pro-
cessing, Penn Chinese Treebank version 5 (CTB5) is 
a widely used benchmark data and contains about 20 
thousand sentences annotated with word boundaries, 
part-of-speech (POS) tags, and syntactic structures 
(Xue et al., 2005; Xia, 2000), whereas People's 
Daily corpus (PD) 1 is a large-scale corpus annotated 
with words and POS tags, containing about 300 thou-
sand sentences from the first half of 1998 of People's 
Daily newspaper (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>0</head><label>0</label><figDesc></figDesc><table>Offline Pruning 
8 0.9999 
94.95 
96.05 
4.1 
5.1 
16 0.9999 
95.15 
96.09 
5.2 
7.6 
32 0.9999 
95.13 
96.09 
5.5 
9.3 
16 0.99 
94.42 
95.77 
1.6 
2.2 
16 0.999 
95.02 
96.10 
2.6 
4.0 
16 0.99999 
95.10 
96.09 
6.8 
8.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>POS tagging performance of online and offline pruning with different r and λ on CTB5 and PD.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>WS&amp;POS tagging performance of online and offline pruning with different r and λ on CTB5 and PD.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>WS&amp;POS tagging performance of difference approaches on CTB5 and PD. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>WS&amp;POS tagging performance of difference ap-
proaches on CTB5X and PD. 

</table></figure>

			<note place="foot" n="1"> http://icl.pku.edu.cn/icl_groups/ corpustagging.asp</note>

			<note place="foot" n="3"> Context-aware Pruning Using manually designed context-free tag-to-tag mapping rules to constrain the search space has two major drawbacks. On the one hand, for more complex problems such as joint WS&amp;POS tagging, it becomes very difficult to design proper mapping rules due to the much larger tag set. On the other hand, the experimental results in Li et al. (2015) 2 In contrast, computing E t|x,S;θ [f(x, t)] is not the bottleneck, since |Ti| = |T b | for CTB or |Ti| = |T a | for PD. 3 Please refer to http://hlt.suda.edu.cn/~zhli/ resources/pos-mapping-CTB-PD.html for their detailed mapping rules.</note>

			<note place="foot" n="5"> Due to the model complexity of &quot;Coupled (No Prune)&quot;, we discard all low-frequency (&lt; 3) features in the training data to speed up training. This explains why &quot;Coupled (No Prune)&quot; has slightly lower accuracies than &quot;Coupled (Relaxed)&quot;.</note>

			<note place="foot" n="6"> The time estimation does not include the two separate processes of pruning single-side tags, which is approximately</note>

			<note place="foot" n="7"> Sun and Wan (2012) achieve 94.68% F-score on CTB5Xtest by further employing a re-training strategy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for the helpful comments. We are very grateful to Meishan Zhang for inspiring us to use online pruning to improve the efficiency of the cou-pled approach. We also thank Wenliang Chen for the helpful discussions. This work was supported by National Natural Science Foundation of China (Grant No. 61525205, 61502325, 61432013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Rie Kubota Ando and Tong <ref type="bibr">Zhang. 2005</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning with multiple labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2001</title>
		<meeting>ICML 2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting multiple treebanks for parsing with quasisynchronous grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1681" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coupled sequence labeling on heterogeneous annotations: POS tagging as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1783" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation for CRF-based Chinese word segmentation using free annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="864" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating graph-based and transition-based dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large margin multi-task metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1867" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint Chinese word segmentation and POS tagging on heterogeneous annotated corpora with multiple task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="658" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tracy</forename><forename type="middle">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-2012</title>
		<meeting>NAACL-2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing approximation and estimation errors for Chinese lexical processing with heterogeneous annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1061" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured prediction cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The part-of-speech tagging guidelines for the penn Chinese treebank 3.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical Report, Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Penn Chinese Treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="207" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Specification for corpus processing at Peking University: Word segmentation, POS tagging and phonetic notation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiming</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Swen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese). Journal of Chinese Language and Computing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="121" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint word segmentation and POS tagging using a single perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="888" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Jointly or separately: Which is better for parsing heterogeneous dependencies?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqiu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Character-level Chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1326" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
