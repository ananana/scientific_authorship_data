<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topic Memory Networks for Short Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichuan</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">HKSAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiyun</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">HKSAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">HKSAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">HKSAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Topic Memory Networks for Short Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3120" to="3131"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3120</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many classification models work poorly on short texts due to data sparsity. To address this issue, we propose topic memory networks for short text classification with a novel topic memory mechanism to encode latent topic representations indicative of class labels. Different from most prior work that focuses on extending features with external knowledge or pre-trained topics, our model jointly explores topic inference and text classification with memory networks in an end-to-end manner. Experimental results on four benchmark datasets show that our model outperforms state-of-the-art models on short text classification , meanwhile generates coherent topics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Short texts have become an important form for individuals to voice opinions and share informa- tion on online platforms. A large body of daily- generated contents, such as tweets, web search snippets, news feeds, and forum messages, have far outpaced the reading and understanding capac- ity of individuals. As a consequence, there is a pressing need for automatic language understand- ing techniques for processing and analyzing such texts ( <ref type="bibr" target="#b47">Zhang et al., 2018</ref>). Among those tech- niques, text classification is a critical and funda- mental one proven to be useful in various down- stream applications, such as text summarization ( <ref type="bibr">Hu et al., 2015</ref>), recommendation ( <ref type="bibr" target="#b45">Zhang et al., 2012)</ref>, and sentiment analysis <ref type="bibr" target="#b6">(Chen et al., 2017)</ref>.</p><p>Although many classification models like sup- port vector machines (SVMs) ( <ref type="bibr" target="#b37">Wang and Manning, 2012</ref>) and neural networks <ref type="bibr" target="#b11">(Kim, 2014;</ref><ref type="bibr" target="#b40">Xiao and Cho, 2016;</ref><ref type="bibr">Joulin et al., 2017</ref>) have demon- strated their success in processing formal and well-edited texts, such as news articles (Zhang <ref type="table">Table 1</ref>: Tweet examples for classification. R i denotes the i-th training instance; S denotes a test instance.</p><p>[class] is the ground-truth label. Bold words are in- dicative of an instance's class label.</p><p>et al., 2015b), their performance is inevitably com- promised when directly applied to short and infor- mal online texts. This inferior performance is at- tributed to the severe data sparsity nature of short texts, which results in the limited features avail- able for classifiers ( <ref type="bibr" target="#b25">Phan et al., 2008)</ref>. To alle- viate the data sparsity problem, some approaches exploit knowledge from external resources like <ref type="bibr">Wikipedia (Jin et al., 2011</ref>) and knowledge bases ( <ref type="bibr" target="#b18">Lucia and Ferrari, 2014;</ref><ref type="bibr" target="#b35">Wang et al., 2017a</ref>). These approaches, however, rely on a large vol- ume of high-quality external data, which may be unavailable to some specific domains or languages ( <ref type="bibr" target="#b14">Li et al., 2016a)</ref>.</p><p>To illustrate the difficulties in classifying short texts, we take the tweet classification in <ref type="table">Table 1</ref> as an example. In the test instance S, only given the 11 words it contains, it is difficult to understand why its label is New.Music.Live. Without richer context, classifiers are likely to classify S into the same category as the training instance R 1 , which happens to share many words with S, in spite of the different categories they belong to, 1 rather than R 2 , which only shares the word "wristbands" with S. Under this circumstance, how might we en- rich the context of these short texts? If looking at R 2 , we can observe that the semantic mean- ing of "wristbands" can be extended from its co-occurrence with "Bieber", which is highly indica- tive of New.Music.Live. <ref type="bibr">2</ref> Such relation can further help in recognizing the word "wristbands" to be important when classifying the test instance S.</p><p>Motivated by the above-mentioned observa- tions, we present a novel neural framework, named as topic memory networks (TMN), for short text classification that does not rely on external knowledge. Our model can identify the indica- tive words for classification, e.g., "wristbands" in S, via jointly exploiting the document-level word co-occurrence patterns, e.g., "wristbands" and "Bieber" in R 2 . To be more specific, built upon the success of neural topic models <ref type="bibr" target="#b32">(Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b20">Miao et al., 2017)</ref>, our model is capable of discovering latent topics 3 , which can capture the co-occurrence of words in document level. To employ the latent topics for short text classification, we propose a novel topic memory mechanism, which is inspired by mem- ory networks <ref type="bibr" target="#b38">(Weston et al., 2014;</ref><ref type="bibr" target="#b10">Graves et al., 2014)</ref>, that allows the model to put attention upon the indicative latent topics useful to classifica- tion. With such corpus-level latent topic represen- tations, each short text instance is enriched, which thus helps alleviate the data sparsity issues.</p><p>In prior research, though the effects of topic models for short text classification have been ex- plored ( <ref type="bibr" target="#b25">Phan et al., 2008;</ref><ref type="bibr" target="#b28">Ren et al., 2016)</ref>, exist- ing methods tend to use pre-trained topics as fea- tures. To the best of our knowledge, our model is the first to encode latent topic representations via memory networks for short text classification, which allows joint inference of latent topics.</p><p>To evaluate our model, we experiment and com- pare it with existing methods on four benchmark datasets. Experimental results indicate that our model outperforms state-of-the-art counterparts on short text classification. The quantitative and qualitative analysis illustrate the capability of our model in generating topic representations that are meaningful and indicative of different categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Topic Memory Networks</head><p>In this section, we describe our topic memory net- works (TMN), whose overall architecture is shown 2 Justine Bieber was on New.Music.Live in 2011. There was a business activity for this event that gave free wristbands to fans if they supported Bieber on Twitter.</p><p>3 Latent topics are the distributional clusters of words that frequently co-occur in some of the instances instead of widely appearing throughout the corpus ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>. in <ref type="figure" target="#fig_0">Figure 1</ref>. There are three major components:</p><p>(1) a neural topic model (NTM) to induce la- tent topics (described in Section 2.1), (2) a topic memory mechanism that maps the inferred latent topics to classification features (described in Sec- tion 2.2), and (3) a text classifier, which produces the final classification labels for instances. These three components can be updated simultaneously via a joint learning process, which is introduced in Section 2.3. In particular, for the classifier, our TMN framework allows the combination of mul- tiple options, e.g., CNN and RNN, which can be determined by the specific application scenario. Formally, given X = {x 1 , x 2 , . . . , x M } as the input with M short text instances, each instance x is processed into two representations: bag-of- words (BoW) term vector x BoW ∈ R V and word index sequence vector x Seq ∈ R L , where V is the vocabulary size and L is the sequence length. x BoW is fed into the neural topic model to induce latent topics. Such topics are further matched with the embedded x Seq to learn classification features in the topic memory mechanism. Then, the classi- fier concatenates the representations produced by the topic memory mechanism and the embedded x Seq to predict the classification label y for x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Topic Model</head><p>Our topic model is inspired by neural topic model (NTM) ( <ref type="bibr" target="#b20">Miao et al., 2017;</ref><ref type="bibr" target="#b32">Srivastava and Sutton, 2017</ref>) that induces latent topics in neural net- works. NTM is based on variational auto-encoder (VAE) ( <ref type="bibr" target="#b12">Kingma and Welling, 2013)</ref>  z ∈ R K , where K denotes the number of topics.</p><p>In the following, we describe the generation and the inference of the model in turn.</p><p>NTM Generation. Similar to LDA-style topic models, we assume x having a topic mixture θ represented as a K-dimensional distribution, which is generated via Gaussian softmax construc- tion ( <ref type="bibr" target="#b20">Miao et al., 2017)</ref>. Each topic k is repre- sented by a word distribution φ k over the vocabu- lary. Specifically, the generation story for x is:</p><formula xml:id="formula_0">• Draw latent variable z ∼ N (µ, σ 2 ) • θ = softmax(f θ (z))</formula><p>• For the n-th word in x: -Draw word w n ∼ softmax(f φ (θ)) where f * (·) is a neural perceptron that linearly transforms inputs, activated by a non-linear trans- formation. Here we use rectified linear units (Re- LUs) <ref type="bibr" target="#b22">(Nair and Hinton, 2010)</ref> as activate func- tions. The prior parameters of z, µ and σ, are estimated from the input data and defined as:</p><formula xml:id="formula_1">µ = f µ (f e (x BoW )), log σ = f σ (f e (x BoW ))</formula><p>(1) Note that NTM is based on VAE, where an en- coder estimates the prior parameters and a decoder describes the generation story. Compared with the basic VAE, NTM includes the additional distribu- tional vectors θ and φ, which can yield latent topic representations and thus ensuring their better inter- pretability in learning process ( <ref type="bibr" target="#b20">Miao et al., 2017)</ref>.</p><p>NTM Inference. In NTM, we use variational in- ference ( <ref type="bibr" target="#b1">Blei et al., 2016</ref>) to approximate a poste- rior distribution over z given all the instances. The loss function of NTM is defined as</p><formula xml:id="formula_2">L N T M = D KL (q(z) || p(z | x)) − E q(z) [p(x | z)]</formula><p>(2) the negative of variational lower bound, where q(z) is a standard Normal prior N (0, I). p(z | x) and p(x | z) are probabilities to describe encoding and decoding processes, respectively. <ref type="bibr">4</ref> Due to the space limitation, we leave out the derivation de- tails and refer the readers to <ref type="bibr" target="#b20">Miao et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Memory Mechanism</head><p>We exploit a topic memory mechanism to map the latent topics produced by NTM (described in Section 2.1) to the features for classification. In- spired by memory networks <ref type="bibr" target="#b38">(Weston et al., 2014;</ref><ref type="bibr" target="#b33">Sukhbaatar et al., 2015)</ref>, we design two memory matrices, a source memory S and a target mem- ory T , both of which are in K × E size (K for the number of topics and E for the pre-defined size of word embeddings). S and T are produced by two ReLU-actived neural perceptrons, both taking the topic-word weight matrix W φ ∈ R K×V as in- puts. Recall that in NTM, we use f φ (·) to compute the word distributions given θ. W φ is the kernel weight matrix of f φ (·), where W φ k,v represents the importance of the v-th word in reflecting the k-th topic. Assuming U as the embedded x Seq (word sequence form of x), in source memory, we com- pute the match between the k-th topic and the em- bedding of the l-th word in x Seq by</p><formula xml:id="formula_3">P k,l = sigmoid(W s [S k ; U l ] + b s ) (3)</formula><p>where [x; y] denotes the merge of x and y, and we use concatenation operation here <ref type="bibr" target="#b9">(Dou, 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2017)</ref>. W s and b s are parameters to be learned. To further combine the instance-topic mixture θ with P , we define the integrated mem- ory weights as</p><formula xml:id="formula_4">ξ k = θ k + γ l P k,l (4)</formula><p>where γ is the pre-defined coefficient. Then, in target memory, via weighting target memory ma- trix T with ξ, we obtain the output representation R of the topic memory mechanism:</p><formula xml:id="formula_5">R k = ξ k T k (5)</formula><p>The concatenation of R and U (embedded x Seq ) further serves as the features for classification.</p><p>In particular, similar to the memory networks in prior research ( <ref type="bibr" target="#b33">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b6">Chen et al., 2017)</ref>, our model can be extended to handle mul- tiple computation layers (hops). As shown in <ref type="figure">Fig</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Joint Learning</head><p>The entire TMN model integrates the three mod- ules in <ref type="figure" target="#fig_0">Figure 1</ref>, i.e., the neural topic model, the topic memory mechanism, and the classi- fier, which can be updated simultaneously in one framework. In doing so, we jointly tackle topic modeling and classification, and define the loss function of the overall framework to combine the two effects as following:</p><formula xml:id="formula_6">L = L N T M + λL CLS (6)</formula><p>where L N T M represents the loss of NTM and L CLS is the cross entropy reflecting classification loss. λ is the trade-off parameter controlling the balance between topic model and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We conduct experiments on four short text datasets, namely, Snippets, TagMyNews, Twitter, and Weibo. Their details are described as follows.</p><p>Snippets. This dataset contains Google search snippets released by <ref type="bibr" target="#b25">Phan et al. (2008)</ref>. There are eight ground-truth labels, e.g., health and sport.</p><p>TagMyNews. We use the news titles as in- stances from the benchmark classification dataset released by <ref type="bibr" target="#b34">Vitale et al. (2012)</ref>. <ref type="bibr">5</ref> This dataset con- tains English news from really simple syndication (RSS) feeds. Each news feed (with its title) is an- notated with one from seven labels, e.g., sci-tech.</p><p>Twitter. This dataset is used to evaluate tweet topic classification, which is built on the dataset released by TREC2011 microblog track. <ref type="bibr">6</ref> Follow- ing previous settings <ref type="bibr" target="#b41">(Yan et al., 2013;</ref><ref type="bibr" target="#b14">Li et al., 2016a</ref>), hashtags, i.e., user-annotated topic la- bels in each tweet such as "#Trump" and "#Su- perBowl", serve as our ground-truth class labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Settings</head><p>We use pre-trained embeddings to initialize all word embeddings. For Snippets and TagMyNews <ref type="bibr">7</ref> The original dataset contains conversations to enrich the context of Weibo posts, which are not considered here.   <ref type="bibr" target="#b35">Wang et al., 2017a</ref>). The hidden size of CNN is set as 500. The dimension of word em- bedding E = 200. γ = 0.8 for trading off θ and P , and λ = 1.0 for controlling the effects of topic model and classification. In the learning process, we run our model for 800 epochs with early-stop strategy applied ( <ref type="bibr" target="#b5">Caruana et al., 2000</ref>).</p><note type="other">(Zhang and Wang, 2015) 0.943 0.943 0.838 0.828 0.375 0.348 0.547 0.547 CNN (Kim, 2014) 0.944 0.944 0.843 0.843 0.381 0.362 0.553 0.550 CNN+TEWE (Ren et al., 2016) 0.944 0.944 0.846 0.846 0.385 0.368 0.537 0.532 CNN+NTM 0.945 0.945 0.844 0.844 0.382 0.365 0.556 0.556 Our models TMN (Separate TM Inference) 0.961 0.961 0.848 0.847 0.394 0.386 0.568 0.569 TMN (Joint TM Inference) 0.964 0.964 0.851 0.851 0.397 0.375 0.591 0.589</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Models</head><p>For comparison, we consider a weak baseline of majority vote, which assigns the major class labels in training set to all test instances. We further com- pare with the widely-used baseline SVM+BOW, SVM with unigram features ( <ref type="bibr" target="#b37">Wang and Manning, 2012</ref>  <ref type="bibr" target="#b11">(Kim, 2014)</ref>. No topic representation is encoded in these two classifiers. We also compare with the state-of-the-art short- text classifier CNN+TEWE ( <ref type="bibr" target="#b28">Ren et al., 2016)</ref>, i.e., CNN classifier with topic-enriched word embed- dings (TEWE), where the word embeddings are enriched by pre-trained NTM-inferred topic mod- els. Moreover, to investigate the effectiveness of our proposed topic memory mechanism, we com- pare with CNN+NTM, which concatenates the representations learned by CNN and topics in- duced by NTM as classification features. In ad- dition, we compare with our variant, TMN (Sepa- rate TM Inference), where topics are induced sep- arately before classification, and only used for ini- tializing the topic memory. To be consistent, our model with a joint learning process for topic mod- eling and classification, described in Section 2.3, is named as TMN (Joint TM Inference). Note that the comparison CNN-based models share the same settings as our model, and the hidden size for each direction of BiLSTM is set to 100. <ref type="table" target="#tab_5">Table 3</ref> shows the comparison on classification re- sults, where the accuracy and average F1 scores on different classes labels are reported. We have the following observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification Comparison</head><p>• Topic representations are indicative features. On all four datasets, simply by combining topic representations into features, SVM models pro- duce better results than the models without ex-  ploiting topic features (i.e., SVM+BOW). This ob- servation indicates that latent topic representations captured at corpus level are helpful to alleviate the data sparsity problem in short text classification.</p><p>• Neural network models are effective. It is seen that neural models based on either CNN or At- tBiLSTM yield better results than SVM. This ob- servation shows the effectiveness of representation learning in neural networks for short texts.</p><p>• CNN serves as a better classifier for short texts than AttBiLSTM. In comparison of CNN and AttBiLSTM without taking topic features, we ob- serve that CNN yields generally better results on all the four datasets. This is consistent with the discovery in <ref type="bibr" target="#b35">Wang et al. (2017a)</ref>, where CNN can better encode short texts than sequential models.</p><p>• Topic memory is useful to classification. By exploring topic representations in memory mech- anisms, our TMN model, inferring topic mod- els either separately or jointly with classifica- tion, significantly outperform the best compari- son models on each of the four datasets. Par- ticularly, when compared with CNN+TEWE and CNN+NTM, both concatenating topics as part of the features, the results yielded by TMN are bet- ter. This demonstrates the effectiveness of topic memory to learn indicative topic representations for short text classification.</p><p>• Jointly inferring latent topics is effective to text classification. In comparison between two TMN variants, TMN (Joint TM Inference) produces bet- ter classification results, though large margin im- provements are not observed on the three English datasets, i.e., TagMyNews, Snippets, and Twitter. This may be because the classifiers do not rely too much on high-quality latent topics, since other fea- tures may be sufficient to indicate the labels, e.g., word positions in the instance. As a result, bet- ter topic models, learned via jointly induced with classification, may not provide richer information for classification. Nevertheless, we notice that on LDA mubarak :::: bring :: run obama democracy speech   Chinese Weibo dataset, the jointly trained topic model improves the accuracy and average F1 by 2.3% and 2.0%, respectively. It may result from the prevalence of word order misuse in informal Weibo messages. This mis-order phenomenon is common in Chinese and generally does not affect understanding. The rich information conveyed by Chinese characters are capable of indicating se- mantic meanings of words even without correct or- ders ( <ref type="bibr" target="#b26">Qin et al., 2016;</ref><ref type="bibr" target="#b36">Wang et al., 2017b)</ref>. As a result, the CNN classifier, which encodes orders of words, may also bring such mis-order noise to classification. For these instances with mis- ordered words, a better topic model that learns text instances as unordered words, provides useful rep- resentations that compensate the loss of informa- tion in word orders and in turn improves the per- formance of text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Coherence Comparison</head><p>In Section 4.1, we find that TMN can significantly outperform comparison models on short text clas- sification. In this section, we study whether jointly learning topic models and classification can be helpful in producing coherent and meaningful top- ics. We use the C V metric <ref type="bibr">(Röder et al., 2015)</ref> computed by Palmetto toolkit 12 to evaluate the topic coherence, which has been shown to give the closest scores to human evaluation compared to other widely-used topic coherence metrics like NPMI <ref type="bibr" target="#b3">(Bouma, 2009)</ref>. <ref type="table" target="#tab_8">Table 4</ref> shows the compar- ison results of LDA, BTM, NTM, and TMN on the three English datasets. <ref type="bibr">13</ref> Note that we do not re- port C V scores for Chinese Weibo dataset as the Palmetto toolkit cannot process Chinese topics.</p><p>12 https://github.com/dice-group/ Palmetto 13 In the rest of this paper, without otherwise indicated, TMN is used as a short form for TMN (Joint TM Inference). <ref type="table">TagMyNews Twitter Weibo  TMN-</ref>  <ref type="table">Table 6</ref>: The impact of the # of hops on accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of Hops Snippets</head><p>As can be seen, TMN yields higher C V scores by large margins than all others in comparison. This indicates that jointly exploring classification would be effective in producing coherent topics. The reason is that the supervision from classifica- tion labels can guide unsupervised topic models in discovering meaningful and interpretable topics. We also observe that NTM produces better results than LDA and BTM, which implies the effective- ness of inducing topic models by neural networks.</p><p>To further analyze the quality of yielded top- ics, <ref type="table">Table 5</ref> shows the top 10 words of the sam- ple latent topics reflecting "Egyptian revolution of 2011" discovered by various models. We find that LDA yields off-topic word "bowl". For the results of BTM and NTM, though we do not find off-topic words, non-topic words like "need" and "stay" are included. <ref type="bibr">14</ref> The topic generated by TMN appears to be the best, which presents indicative words like "tahrir" and "cairo", for the event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results with Varying Hyperparameters</head><p>We further study the impact of two important hy- perparameters in TMN, i.e., the hop number and the topic number, which will be discussed in turn.</p><p>Impact of Hop Numbers. Recall that <ref type="figure" target="#fig_1">Figure 2</ref> shows the capacity of TMN in combining multi- ple hops. Here we analyze the effects of hop num- bers on the accuracy of TMN. <ref type="table">Table 6</ref> reports the results, where N H refers to using N hops (N = 1, 2, ..., 6). As can be seen, generally, TMN with 5 hops achieves the best accuracy on most datasets except for Snippets dataset. We also observe that, although within a particular range, more hops can produce better accuracy, the increasing trends are not always monotonic. For example, TMN-6H al- ways exhibits lower accuracy than TMN-5H. This observation implies that the overall representation ability of TMN is enhanced as the increasing com- plexity of the model via combining more hops. However, this enhancement will reach saturation when the hop number exceeds a threshold, which is 5 hops for most datasets in our experiment.</p><p>Impact of Topic Numbers. <ref type="figure" target="#fig_4">Figure 3</ref> shows the accuracy of TMN and CNN+TEWE (the best comparison model in <ref type="table" target="#tab_5">Table 3</ref>) given varying K, the number of topics on TagMyNews and Twit- ter datasets. <ref type="bibr">15</ref> As we can see, the curves of all the models are not monotonic and the best accu- racy is achieved given a particular number of top- ics, e.g., K=50 for TMN on TagMyNews dataset. When comparing different curves, we observe that TMN yields consistently better accuracy than CNN+TEWE, a comparison model shown in Ta- ble 3, which demonstrates the robust performance of TMN over varying number of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A Case Study on Topic Memory</head><p>Section 4.1 demonstrates the effectiveness of us- ing topic memory on short text classification. To further understand why, in this section, we use the test instance S in <ref type="table">Table 1</ref> to analyze what the information captured by topic memory is in- dicative of class labels. Recall that the label of S, which should be New.Music.Live, can be indi- cated by containing word "wristbands" and the collocation of "wristbands" and "Bieber" in train- ing instance R 2 labeled New.Music.Live. <ref type="figure">Figure 4</ref> shows the heatmaps of the weight matrix P in topic memory and the topic mixture θ captured by NTM for instance S. As can be seen, the top 3 words for the latent topic with the largest value in θ are "bieber", "justine", and "tuesday", which can effectively indicate the class label of S to be New.Music.Live because Justine Bieber was there on Tuesday. Interestingly, S contains none of the top three words. The latent semantic relations of S and these words are purely uncovered by the co- occurrence of words in S with other instances in   <ref type="figure">Figure 4</ref>: Topic memory visualization for test instance S shown in <ref type="table">Table 1</ref>. (a) Heatmaps of topic mixture θ (the upper one) and topic memory weight matrix P (the lower one) illustrating the relevance between the words of S (left) and the learned topics (bottom, with top-2 words displayed). The red dotted rectangle indicates the representation for "wristband", the topical word in S. The red rectangles with solid frames indicates the 3 most relevant topics ordered by θ. (b) Top-10 words of these topics indicated by φ.</p><p>the corpus, which further shows the benefit of us- ing latent topics for alleviating the sparsity in short texts. We also observe that topic memory learns different representations for topical word "wrist- band", highly indicating instance label, and back- ground words, such as "i" and "for". This explains why topic memory is effective to classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis</head><p>In this section, we take our classification results on TagMyNews dataset as an example to analyze our errors. We observe that one major type of incorrect prediction should be ascribed to the polysemy phe- nomenon. For example, the instance "NBC gives 'the voice' post super bowl slot" should be catego- rized as entertainment. However, failing to under- stand the particular meaning of "the voice" here as the name of a television singing competition, our model mistakenly categorizes this instance as sport because of the occurrence "super bowl". In future work, we would exploit context-sensitive topical word embeddings <ref type="bibr" target="#b39">(Witt et al., 2016)</ref>, which is able to distinguish the meanings of the same word in different contexts. Another main error type comes from the failure to capture phrase-level semantics. Taking "On the merits of face time and living small" as an example, without understand- ing "face time" as a phrase, our model wrongly predicts its category as business instead of its cor- rect label as sci tech. Such errors can be reduced by enhancing our NTM to phrase discovery topic models ( <ref type="bibr" target="#b16">Lindsey et al., 2012;</ref><ref type="bibr">He, 2016)</ref>, which is worthy exploring in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our work mainly builds on two streams of prior work: short text classification and topic models.</p><p>Short Text Classification. In the line of short text classification, most work focuses on alleviat- ing the severe sparsity issues in short texts ( <ref type="bibr" target="#b41">Yan et al., 2013)</ref>. Some previous efforts encode knowl- edge from external resource <ref type="bibr">(Jin et al., 2011;</ref><ref type="bibr" target="#b18">Lucia and Ferrari, 2014;</ref><ref type="bibr" target="#b35">Wang et al., 2017a;</ref><ref type="bibr" target="#b19">Ma et al., 2018)</ref>. Instead, our work learns effective repre- sentations only from internal data. For some spe- cific classification tasks, such as sentiment analy- sis, manually-crafted features are designed to fit the target task <ref type="bibr" target="#b23">(Pak and Paroubek, 2010;</ref><ref type="bibr">Jiang et al., 2011)</ref>. Distinguished from them, we employ deep learning framework for representation learn- ing, which requires no feature engineering process and thus ensures its general applicability to di- verse classification scenarios. In comparison with the established classifiers applying deep learning methods (dos <ref type="bibr" target="#b31">Santos and Gatti, 2014;</ref><ref type="bibr" target="#b13">Lee and Dernoncourt, 2016)</ref>, our work differs from them in the leverage of corpus-level latent topic repre- sentations for alleviating data sparsity issues. In existing classification models using topic features, pre-trained topic mixtures are leveraged as part of features ( <ref type="bibr" target="#b25">Phan et al., 2008;</ref><ref type="bibr" target="#b28">Ren et al., 2016;</ref><ref type="bibr" target="#b6">Chen et al., 2017)</ref>. Differently, our model encodes topic representations in a memory mechanism where topics are induced jointly with text classification in an end-to-end manner.</p><p>Topic Models. Well-known topic models, e.g., probabilistic latent semantic analysis (pLSA) <ref type="bibr">(Hofmann, 1999</ref>) and latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, have shown advantages in capturing effective semantic rep- resentations, and proven beneficial to varying downstream applications, such as summariza- tion ( <ref type="bibr">Haghighi and Vanderwende, 2009</ref>) and recommendation ( <ref type="bibr" target="#b42">Zeng et al., 2018;</ref><ref type="bibr" target="#b0">Bai et al., 2018)</ref>. For short text data, topic model variants have been proposed to reduce the effects of spar- sity issues on topic modeling, such as biterm topic model (BTM) ( <ref type="bibr" target="#b41">Yan et al., 2013</ref>) and LeadLDA ( <ref type="bibr" target="#b15">Li et al., 2016b)</ref>. Recently, owing to the popularity of variational auto-encoder (VAE) <ref type="bibr" target="#b12">(Kingma and Welling, 2013)</ref>, it is able to induce latent topics in neural networks, namely, neural topic models (NTM) ( <ref type="bibr" target="#b20">Miao et al., 2017;</ref><ref type="bibr" target="#b32">Srivastava and Sutton, 2017)</ref>. Although the concept of NTM has been mentioned earlier in <ref type="bibr" target="#b4">Cao et al. (2015)</ref>, their model is based on matrix factorization. Differently, VAE-style NTM ( <ref type="bibr" target="#b32">Srivastava and Sutton, 2017;</ref><ref type="bibr" target="#b20">Miao et al., 2017</ref>) follows the LDA fashion as probabilistic generative models, which is easy to interpret and extend. The NTM in our framework is in VAE-style, whose effects on short text classification serve as the key focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented topic memory networks that exploit corpus-level topic representations with a topic memory mechanism for short text classifica- tion. The model alleviates data sparsity issues via jointly learning latent topics and text categories. Empirical comparisons with state-of-the-art mod- els on four benchmark datasets have demonstrated the validity and effectiveness of our model, where better results have been achieved on both short text classification and topic coherence evaluation.</p><p>Aria <ref type="bibr">Haghighi and Lucy Vanderwende. 2009</ref>. Explor- ing content models for multi-document summariza- tion. <ref type="table" target="#tab_2">In Proceedings of Human Language Tech- nologies: The 2009 Annual Conference of the North  American Chapter of the Association for Computa- tional Linguistics, HLT-NAACL 2009, pages 362- 370</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of our topic memory networks. The dotted boxes from left to right show the neural topic model, the topic memory mechanism, and the classifier. Here the classifier allows multiple options and the details are left out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Topic memory network with three hops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>- ure 2, each hop contains a source matrix and a tar- get matrix, and different hops are stacked follow- ing the way presented in Sukhbaatar et al. (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 :</head><label>5</label><figDesc>Top 10 representative terms of the sample la- tent topics discovered by various topic models from Twitter dataset. We interpret the topics as "Egyptian revolution of 2011" according to their word distribu- tions. :::::::: Non-topic :::::: words are wave-underlined and in blue, and off-topic words are underlined and in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The impact of topic numbers, where the horizontal axis shows the number of topics and the vertical axis shows the accuracy.</figDesc><graphic url="image-1.png" coords="7,307.56,62.81,217.70,76.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Statistics of the experimental datasets.</head><label>2</label><figDesc></figDesc><table>Labels 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the statistic information of the 
four datasets. Each dataset is randomly split 
into 80% for training and 20% for test. 20% of 
randomly selected training instances are used to 
form development set. We preprocess our English 
datasets, i.e., Snippets, TagMyNews, and Twit-
ter, with gensim tokenizer 8 for tokenization. As 
to the Chinese Weibo dataset, we use FudanNLP 
toolkit (Qiu et al., 2013) 9 for word segmentation. 
In addition, for each dataset, we maintain a vocab-
ulary built based on the training set with removal 
of stop words 10 and words occurring less than 3 
times. The inputs of topic models x BoW are con-
structed based on this vocabulary following com-
mon topic model settings (Blei et al., 2003; Miao 
et al., 2016). Differently, we use the raw word se-
quence (without words removal) for the inputs of 
classification x Seq as is done in previous work of 
text classification (Kim, 2014; Liu et al., 2017). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparisons of accuracy (Acc) and average F1 (Avg F1) on four benchmark datasets. Our TMN, either 
with separate or joint TM inference, performs significantly better than all the comparisons (p &lt; 0.05, paired t-test). 

datasets, we use pre-trained GloVe embed-
dings (Pennington et al., 2014) 11 . For Twitter and 
Weibo datasets, we pre-train embeddings on large-
scale external data with 99M tweets and 467M 
Weibo messages, respectively. For the number 
of topics, we follow previous settings (Yan et al., 
2013; Das et al., 2015; Dieng et al., 2016) to 
set K = 50. For all the other hyperparame-
ters, we tune them on the development set by grid 
search. For our classifier, we employ CNN in 
experiment because of its better performance in 
short text classification than its counterparts such 
as RNN (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc>C V coherence scores for topics generated by various models. Higher is better. The best result in each column is in bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>::::: believe regime power bowl BTM mubarak egypt push internet people govern- ment :::: phone hosni :::: need son NTM mubarak people egyptian egypt :: stay ::::::: tomorrow protest news ::::: phone protester TMN mubarak protest protester tahrir square egyptian al jazeera repo cairo</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>, Boulder, Colorado, USA.</head><label></label><figDesc></figDesc><table>Yulan He. 2016. Extracting Topical Phrases from Clin-
ical Documents. In Proceedings of the Thirtieth 
AAAI Conference on Artificial Intelligence, Phoenix, 
Arizona, USA, pages 2957-2963. 

Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual 
International ACM SIGIR Conference on Research 
and Development in Information Retrieval, SIGIR 
'99, Berkeley, CA, USA, pages 50-57. 

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC-
STS: A Large Scale Chinese Short Text Summa-
rization Dataset. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language 
Processing, EMNLP 2015, Lisbon, Portugal, pages 
1967-1972. 

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and 
Tiejun Zhao. 2011. Target-dependent Twitter Sen-
timent Classification. In The 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, Proceedings 
of the Conference, Portland, Oregon, USA, pages 
151-160. </table></figure>

			<note place="foot" n="1"> R1 is about SuperBowl, the annual championship game of the National Football League. R2 and S are both about New.Music.Live, the flagship live music show.</note>

			<note place="foot" n="4"> In implementation, to smooth the gradients, we apply reparameterization on z following previous work (Kingma and Welling, 2013; Rezende et al., 2014).</note>

			<note place="foot" n="5"> http://acube.di.unipi.it/tmn-dataset/ 6 http://trec.nist.gov/data/tweets Specifically, we construct the dataset with the following steps. First, we remove the tweets without hashtags. Second, we rank hashtags by their frequencies. Third, we manually remove the hashtags that cannot mark topics, such as &quot;#fb&quot; for indicating the source of tweets from Facebook, and combine the hashtags referring to the same topic, such as &quot;#DonaldTrump&quot; and &quot;#Trump&quot;. Finally, we select the top 50 frequent hashtags, and all tweets containing these hashtags. Weibo. To evaluate our model on a different language other than English, we employ a Chinese dataset with short segments of text for topic classification. This dataset is released by Li et al. (2016b) with a collection of messages posted in June 2014 on Weibo, a popular Twitter alike platform in China. 7 Similar to Twitter, Weibo allows up to 140 Chinese characters in its messages. In this Weibo dataset, each Weibo message is labeled with a hashtag as its category, and there are 50 distinct hashtag labels in total, following the same procedure performed for the Twitter dataset.</note>

			<note place="foot" n="11"> http://nlp.stanford.edu/data/glove. 6B.zip (200d)</note>

			<note place="foot" n="14"> Off-topic words are more likely to be interpreted to reflect other topics. Non-topic words cannot clearly indicate the corresponding topic.</note>

			<note place="foot" n="15"> We observe similar distributions on Snippets and Weibo.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Relational Topic Model for Scientific Article Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoli</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangbin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenglin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Variational Inference: A Review for Statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
		<idno>abs/1601.00670</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalized (Pointwise) Mutual Information in Collocation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GSCL</title>
		<meeting>GSCL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Novel Neural Topic Model and Its Supervised Extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence</title>
		<meeting>the TwentyNinth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2210" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, an&quot;d Early Stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="402" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent Attention Network on Memory for Aspect Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gaussian LDA for Topic Models with Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Dieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">William</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
		<idno>abs/1611.01702</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>abs/1410.5401</idno>
		<title level="m">Neural Turing Machines. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">AutoEncoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="515" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Topic Modeling for Short Texts with Auxiliary Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Topic Extraction from Microblog Posts Using Conversation Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamfai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">V</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stipicevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="214" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial Multi-task Learning for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">EgoCentric: Ego Networks for Knowledge-based Short Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lucia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1079" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discovering Discrete Latent Topics with Neural Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural Variational Inference for Text Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous Adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala University; Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="436" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Largescale Data Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Hieu Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Le</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susumu</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web</title>
		<meeting>the 17th International Conference on World Wide Web<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Topic Modeling of Chinese Language beyond a Bagof-Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="60" to="78" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FudanNLP: A Toolkit for Chinese Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving Twitter Sentiment Classification Using Topic-Enriched MultiPrototype Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3038" to="3044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the Space of Topic Coherence Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01488</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Autoencoding Variational Inference For Topic Models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, NIPS 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classification of Short Texts by Deploying Topical Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval-34th European Conference on IR Research</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="376" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploiting Word Internal Structures for Generic Chinese Sentence Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="298" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Baselines and Bigrams: Simple, Good Sentiment and Topic Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Explaining Topical Distances Using Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Workshop on Database and Expert Systems Applications, DEXA 2016 Workshops</title>
		<meeting><address><addrLine>Porto, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1602.00367</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Biterm Topic Model for Short Sexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International World Wide Web Conference</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1445" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Microblog conversation recommendation via joint modeling of topics and discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingshan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Beauchamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Shugars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="375" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Relation Classification via Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1508.01006</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>the 29th Pacific Asia Conference on Language, Information and Computation<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Advertising Keywords Recommendation for Short-Text Web Pages Using Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM TIST</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Character-level Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><forename type="middle">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Encoding conversation context for neural keyphrase extraction from microblog posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1676" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
