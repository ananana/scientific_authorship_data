<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guided Open Vocabulary Image Captioning with Constrained Beam Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<email>mark.johnson@mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Guided Open Vocabulary Image Captioning with Constrained Beam Search</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="936" to="945"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning ar-chitectures to take advantage of image tag-gers at test time, without retraining. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain caption-ing on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly , our results significantly outper-form approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic image captioning is a fundamental task that couples visual and linguistic learning. Re- cently, models incorporating recurrent neural net- works (RNNs) have demonstrated promising re- sults on this challenging task ( <ref type="bibr" target="#b29">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Devlin et al., 2015)</ref>, leverag- ing new benchmark datasets such as the MSCOCO dataset ( <ref type="bibr" target="#b18">Lin et al., 2014</ref>). However, these datasets are generally only concerned with a relatively small number of objects and interactions. Unsur- prisingly, models trained on these datasets do not generalize well to out-of-domain images contain- ing novel scenes or objects ( <ref type="bibr" target="#b26">Tran et al., 2016)</ref>. This limitation severely hinders the use of these models in real world applications dealing with im- ages in the wild. Although available image-caption training data is limited, many image collections are augmented with ground-truth text fragments such as semantic attributes (i.e., image tags) or object annotations. Even if these annotations do not exist, they can be generated using (potentially task specific) image taggers ( <ref type="bibr" target="#b2">Chen et al., 2013;</ref>) or object detectors <ref type="bibr" target="#b21">(Ren et al., 2015;</ref><ref type="bibr" target="#b17">Krause et al., 2016)</ref>, which are easier to scale to new concepts. In this paper our goal is to incorporate text frag- ments such as these during caption generation, to improve the quality of resulting captions. This goal poses two key challenges. First, RNNs are generally opaque, and difficult to influence at test time. Second, text fragments may include words that are not present in the RNN vocabulary.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, we address the first challenge (guidance) by using constrained beam search to guarantee the inclusion of selected words or phrases in the output of an RNN, while leaving the model free to determine the syntax and addi- tional details. Constrained beam search is an ap- proximate search algorithm capable of enforcing any constraints over resulting output sequences that can be expressed in a finite-state machine. With regard to the second challenge (vocabulary), empirically we demonstrate that an RNN can suc- cessfully generalize from similar words if both the input and output layers are fixed with pretrained word embeddings and then expanded as required.</p><p>To evaluate our approach, we use a held-out version of the MSCOCO dataset. Leveraging im- age tag predictions from an existing model  as constraints, we demonstrate state of the art performance for out-of-domain image captioning, while simultaneously improv- ing the performance of the base model on in- domain data. Perhaps surprisingly, our results significantly outperform approaches that incorpo- rate the same tag predictions into the learning algorithm ( ). Furthermore, we attempt the ex- tremely challenging task of captioning the Ima- geNet classification dataset ( <ref type="bibr">Russakovsky et al., 2015)</ref>. Human evaluations indicate that by lever- aging ground truth image labels as constraints, the proportion of captions meeting or exceeding hu- man quality increases from 11% to 22%. To facil- itate future research we release our code and data from the project page 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>While various approaches to image caption gen- eration have been considered, a large body of recent work is dedicated to neural network ap- proaches ( <ref type="bibr" target="#b6">Donahue et al., 2015;</ref><ref type="bibr" target="#b19">Mao et al., 2015;</ref><ref type="bibr" target="#b15">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b29">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Devlin et al., 2015</ref>). These approaches typically use a pretrained Convolutional Neural Network (CNN) image encoder, combined with a Recurrent Neural Network (RNN) decoder trained to pre- dict the next output word, conditioned on previ- ous words and the image. In each case the decod- ing process remains the same-captions are gener- ated by searching over output sequences greedily or with beam search.</p><p>Recently, several works have proposed mod- els intended to describe images containing ob- jects for which no caption training data exists (out- of-domain captioning). The Deep Compositional Captioner (DCC) ( ) uses a CNN image tagger to predict words that are rel- evant to an image, combined with an RNN lan- guage model to estimate probabilities over word sequences. The tagger and language models are pretrained separately, then fine-tuned jointly using the available image-caption data.</p><p>Building on the DCC approach, the Novel Ob- ject Captioner (NOC) ( ) is contemporary work with ours that also uses pre- trained word embeddings in both the input and output layers of the language model. Another re- cent work <ref type="bibr" target="#b26">(Tran et al., 2016</ref>) combines specialized celebrity and landmark detectors into a captioning system. More generally, the effectiveness of in- corporating semantic attributes (i.e., image tags) into caption model training for in-domain data has been established by several works <ref type="bibr" target="#b30">Wu et al., 2016;</ref><ref type="bibr" target="#b7">Elliot and de Vries, 2015)</ref>.</p><p>Overall, our work differs fundamentally from these approaches as we do not attempt to intro- duce semantic attributes, image tags or other text fragments into the learning algorithm. Instead, we incorporate text fragments during model decod- ing. To the best of our knowledge we are the first to consider this more loosely-coupled approach to out-of-domain image captioning, which allows the model to take advantage of information not avail- able at training time, and avoids the need to retrain the captioning model if the source of text frag- ments is changed.</p><p>More broadly, the problem of generating high probability output sequences using finite- state machinery has been previously explored in the context of poetry generation using RNNs ( <ref type="bibr" target="#b10">Ghazvininejad et al., 2016</ref>) and machine translation using n-gram language models <ref type="bibr" target="#b0">(Allauzen et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constrained Beam Search</head><p>Beam search <ref type="bibr" target="#b16">(Koehn, 2010)</ref> is an approximate search algorithm that is widely used to decode out- put sequences from Recurrent Neural Networks (RNNs). We briefly describe the RNN decod- ing problem, before introducing constrained beam search, a multiple-beam search algorithm that en- forces constraints in the sequence generation pro- cess.</p><p>Let y t = (y 1 , ..., y t ) denote an output sequence of length t containing words or other tokens from vocabulary V . Given an RNN modeling a proba- bility distribution over such sequences, the RNN decoding problem is to find the output sequence with the maximum log-probability, where the log probability of any partial sequence y t is typically given by t j=1 log p(y j | y 1 , ..., y j−1 ). As it is computationally infeasible to solve this problem, beam search finds an approximate solu- tion by maintaining a beam B t containing only the b most likely partial sequences at each decoding time step t, where b is known as the beam size. At each time step t, the beam B t is updated by retain- ing the b most likely sequences in the candidate set E t generated by considering all possible next word extensions:</p><formula xml:id="formula_0">E t = (y t−1 , w) | y t−1 ∈ B t−1 , w ∈ V<label>(1)</label></formula><p>To decode output sequences under constraints, a naive approach might impose the constraints on sequences produced at the end of beam search. However, if the constraints are non-trivial (i.e. only satisfied by relatively low probability output sequences) it is likely that an infeasibly large beam would be required in order to produce sequences that satisfy the constraints. Alternatively, impos- ing the constraints on partial sequences generated by Equation 1 is also unacceptable, as this would require that constraints be satisfied at every step during decoding-which may be impossible.</p><p>To fix ideas, suppose that we wish to generate sequences containing at least one word from each constraint set C1 = {'chair', 'chairs'} and C2 = {'desk', 'table'}. Note that it is possible to rec- ognize sequences satisfying these constraints us- ing the finite-state machine (FSM) illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, with start state s 0 and accepting state s 3 . More generally, any set of constraints that can be represented with a regular expression can also be expressed as an FSM (either deterministic or non-deterministic) that recognizes sequences sat- isfying those constraints <ref type="bibr" target="#b25">(Sipser, 2012)</ref>.</p><p>Since RNN output sequences are generated from left-to-right, to generate constrained se- quences, we take an FSM that recognizes se- quences satisfying the required constraints, and use the following multiple-beam decoding algo- rithm. For each state s ∈ S in the FSM, a cor- responding search beam B s is maintained. As in beam search, each B s is a set containing at most b output sequences, where b is the beam size. At each time step, each beam B s t is updated by retain- ing the b most likely sequences in its candidate set E s t given by:</p><formula xml:id="formula_1">E s t = s ∈S (y t−1 , w) | y t−1 ∈ B s t−1 , w ∈ V, δ(s , w) = s (2)</formula><p>where δ : S × V → S is the FSM state-transition function that maps states and words to states. As specified by Equation 2, the FSM state-transition function determines the appropriate candidate set for each possible extension of a partial sequence. This ensures that sequences in accepting states must satisfy all constraints as they have been rec- ognized by the FSM during the decoding process. Initialization is performed by inserting an empty sequence into the beam associated with the start state s 0 , so B 0 0 := {} and B i =0 0 := ∅. The algorithm terminates when an accepting state con- tains a completed sequence (e.g., containing an end marker) with higher log probability than all in- complete sequences. In the example contained in <ref type="figure" target="#fig_0">Figure 2</ref>, on termination captions in Beam 0 will not contain any words from C1 or C2, captions in Beam 1 will contain a word from C1 but not C2, captions in Beam 2 will contain a word from C2 but not C1, and captions in Beam 3 will contain a word from both C1 and C2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Implementation Details</head><p>In our experiments we use two types of con- straints. The first type of constraint consists of a conjunction of disjunctions</p><formula xml:id="formula_2">C = D 1 , ..., D m , where each D i = w i,1 , ..., w i,n i and w i,j ∈ V .</formula><p>Similarly to the example in <ref type="figure" target="#fig_0">Figure 2</ref>, a partial cap- tion y t satisfies constraint C iff for each D i ∈ C, there exists a w i,j ∈ D i such that w i,j ∈ y t . This type of constraint is used for the experiments in Section 4.2, in order to allow the captioning model freedom to choose word forms. For each image tag, disjunctive sets are formed by using Word- Net <ref type="bibr" target="#b9">(Fellbaum, 1998)</ref> to map the tag to the set of words in V that share the same lemma.</p><p>The use of WordNet lemmas adds minimal complexity to the algorithm, as the number of FSM states, and hence the number of search beams, is not increased by adding disjunctions. Nevertheless, we note that the algorithm maintains one beam for each of the 2 m subsets of disjunctive constraints D i . In practice m ≤ 4 is sufficient for the captioning task, and with these values our GPU constrained beam search implementation based on Caffe ( <ref type="bibr" target="#b14">Jia et al., 2014</ref>) generates 40k captions for MSCOCO in well under an hour.</p><p>The second type of constraint consists of a sub- sequence that must appear in the generated cap- tion. This type of constraint is necessary for the experiments in Section 4.3, because WordNet synsets often contain phrases containing multiple words. In this case, the number of FSM states, and the number of search beams, is linear in the length of the subsequence (the number of states is equal to number of words in a phrase plus one).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Captioning Model</head><p>Our approach to out-of-domain image captioning could be applied to any existing CNN-RNN cap- tioning model that can be decoding using beam search, e.g., <ref type="bibr" target="#b6">(Donahue et al., 2015;</ref><ref type="bibr" target="#b19">Mao et al., 2015;</ref><ref type="bibr" target="#b15">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b29">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Devlin et al., 2015)</ref>. However, for empir- ical evaluation we use the Long-term Recurrent Convolutional Network ( <ref type="bibr" target="#b6">Donahue et al., 2015</ref>) (LRCN) as our base model. The LRCN consists of a CNN visual feature extractor followed by two LSTM layers <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref>, each with 1,000 hidden units. The model is factored such that the bottom LSTM layer receives only language input, consisting of the embedded previous word. At test time the previous word is the predicted model output, but during train- ing the ground-truth preceding word is used. The top LSTM layer receives the output of the bottom LSTM layer, as well as a per-timestep static copy of the CNN features extracted from the input im- age.</p><p>The feed-forward operation and hidden state update of each LSTM layer in this model can be summarized as follows. Assuming N hidden units within each LSTM layer, the N -dimensional input gate i t , forget gate f t , output gate o t , and input modulation gate g t at timestep t are updated as:</p><formula xml:id="formula_3">i t = sigm (W xi x t + W hi h t−1 + b i )<label>(3)</label></formula><formula xml:id="formula_4">f t = sigm (W xf x t + W hf h t−1 + b f ) (4) o t = sigm (W xo x t + W ho h t−1 + b o )<label>(5)</label></formula><formula xml:id="formula_5">g t = tanh (W xc x t + W hc h t−1 + b c )<label>(6)</label></formula><p>where x t ∈ R K is the input vector, h t ∈ R N is the LSTM output, W 's and b's are learned weights and biases, and sigm (·) and tanh(·) are the sig- moid and hyperbolic tangent functions, respec- tively, applied element-wise. The above gates con- trol the memory cell activation vector c t ∈ R N and output h t ∈ R N of the LSTM as follows:</p><formula xml:id="formula_6">c t = f t c t−1 + i t g t<label>(7)</label></formula><formula xml:id="formula_7">h t = o t tanh (c t )<label>(8)</label></formula><p>where represents element-wise multiplication. Using superscripts to represent the LSTM layer index, the input vector for the bottom LSTM is an encoding of the previous word, given by:</p><formula xml:id="formula_8">x 1 t = W e Π t<label>(9)</label></formula><p>where W e is a word embedding matrix, and Π t is a one-hot column vector identifying the input word at timestep t. The top LSTM input vector comprises the concatenated output of the bottom LSTM and the CNN feature descriptor of the im- age I, given by: </p><formula xml:id="formula_9">x 2 t = (h 1 t , CNN θ (I))<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Vocabulary Expansion</head><p>In the out-of-domain scenario, text fragments used as constraints may contain words that are not ac- tually present in the captioning model's vocabu- lary. To tackle this issue, we leverage pretrained word embeddings, specifically the 300 dimen- sion GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>) embeddings trained on 42B tokens of external text corpora. These embeddings are introduced at both the word input and word output layers of the captioning model and fixed throughout training. Concretely, the ith column of the W e input embedding matrix is initialized with the GloVe vector associated with vocabulary word i. This entails reducing the di- mension of the original LRCN input embedding from 1,000 to 300. The model output is then:</p><formula xml:id="formula_10">v t = tanh (W v h 2 t + b v ) (11) p(y t | y t−1 , ..., y 1 , I) = softmax (W T e v t )<label>(12)</label></formula><p>where v t represents the top LSTM output pro- jected to 300 dimensions, W T e contains GloVe em- beddings as row vectors, and p(y t | y t−1 , ..., y 1 , I) represents the normalized probability distribution over the predicted output word y t at timestep t, given the previous output words and the image. The model is trained with the conventional soft- max cross-entropy loss function, and learns to pre- dict v t vectors that have a high dot-product sim- ilarity with the GloVe embedding of the correct output word.</p><p>Given these modifications -which could be applied to other similar captioning models -the process of expanding the model's vocabulary at test time is straightforward. To introduce an addi- tional vocabulary word, the GloVe embedding for the new word is simply concatenated with W e as an additional column, increasing the dimension of both Π t and p t by one. In total there are 1.9M words in our selected GloVe embedding, which for practical purposes represents an open vocab- ulary. Since GloVe embeddings capture seman- tic and syntactic similarities ( <ref type="bibr" target="#b20">Pennington et al., 2014)</ref>, intuitively the captioning model will gen- eralize from similar words in order to understand how the new word can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Microsoft COCO Dataset</head><p>The MSCOCO 2014 captions dataset ( <ref type="bibr" target="#b18">Lin et al., 2014</ref>) contains 123,293 images, split into a 82,783 image training set and a 40,504 image valida- tion set. Each image is labeled with five human- annotated captions.</p><p>In our experiments we follow standard prac- tice and perform only minimal text pre-processing, converting all sentences to lower case and tokeniz- ing on white space. It is common practice to filter vocabulary words that occur less than five times in the training set. However, since our model does not learn word embeddings, vocabulary filtering is not necessary. Avoiding filtering increases our vo- cabulary from around 8,800 words to 21,689, al- lowing the model to potentially extract a useful training signal even from rare words and spelling mistakes (which are generally close to the cor- rectly spelled word in embedding space). In all experiments we use a beam size of 5, and we also enforce the constraint that a single word cannot be predicted twice in a row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Out-of-Domain Image Captioning</head><p>To evaluate the ability of our approach to per- form out-of-domain image captioning, we repli- cate an existing experimental design (   For validation and testing on this task, we use the same splits as in prior work ( , with half of the original MSCOCO validation set used for vali- dation, and half for testing. We use the vali- dation set to determine hyperparameters and for early-stopping, and report all results on the test set. For evaluation the test set is split into in- domain and out-of-domain subsets, with the out- of-domain designation given to any test image that contains a mention of an excluded object in at least one reference caption.</p><p>To evaluate generated caption quality, we use the SPICE ( <ref type="bibr" target="#b1">Anderson et al., 2016</ref>) metric, which has been shown to correlate well with human judgment on the MSCOCO dataset, as well as the METEOR <ref type="bibr" target="#b4">(Denkowski and Lavie, 2014</ref>) and CIDEr ( ) metrics. For con- sistency with previously reported results, scores on out-of-domain test data are macro-averaged across the eight excluded object classes. To im- prove the comparability of CIDEr scores, the in- verse document frequency statistics used by this metric are determined across the entire test set,</p><note type="other">Base: A woman is playing tennis on a tennis court. Tags: tennis, player, ball, racket. Base+T4: A tennis player swinging a racket at a ball. Base: A man standing next to a yellow train. Tags: bus, yel- low, next, street. Base+T4: A man standing next to a yellow bus on the street. Base: A close up of a cow on a dirt ground. Tags: zebra, zoo, enclosure, stand- ing.</note><p>Base+T4: A zebra standing in front of a zoo enclosure.</p><p>Base: A dog is sitting in front of a tv.</p><p>Tags: dog, head, television, cat. Base+T4: A dog with a cat on its head watching televi- sion.</p><p>Base: A group of people playing a game of tennis. Tags: pink, tennis, crowd, ball. Base+T4: A crowd of people standing around a pink tennis ball. rather than within subsets. On out-of-domain test data, we also report the F1 metric for mentions of excluded objects. To calculate the F1 metric, the model is considered to have predicted condi- tion positive if the generated caption contains at least one mention of the excluded object, and neg- ative otherwise. The ground truth is considered to be positive for an image if the excluded object in question is mentioned in any of the reference cap- tions, and negative otherwise.</p><p>As illustrated in <ref type="table">Table 1</ref>, on the out-of-domain test data, our base model trained only with image captions (Base) receives an F1 score of 0, as it is incapable of mentioned objects that do not ap- pear in the training captions. In terms of SPICE, METEOR and CIDEr scores, our base model per- forms slightly worse than the DCC model on out-of-domain data, but significantly better on in- domain data. This may suggest that the DCC model achieves improvements in out-of-domain performance at the expense of in-domain scores (in-domain scores for the NOC model were not available at the time of submission).</p><p>Results marked with '+' in <ref type="table">Table 1</ref> indicate that our base model has been decoded with constraints in the form of predicted image tags. However, for the fairest comparison, and because re-using existing image taggers at test time is one of the motivations for this work, we did not train an im- age tagger from scratch. Instead, in results T1- 4 we use the top 1-4 tag predictions respectively from the VGG-16 CNN-based image tagger used in the DCC model. This model was trained by the authors to predict 471 MSCOCO visual concepts including adjectives, verbs and nouns. Examples of generated captions, including failure cases, are presented in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>As indicated in <ref type="table">Table 1</ref>, using similar model capacity, the constrained beam search approach with predicted tags significantly outperforms prior work in terms SPICE, METEOR and CIDEr scores, across both out-of-domain and in-domain test data, utilizing varying numbers of tag pre- dictions. Overall these results suggest that, per- haps surprisingly, it may be better to incorporate image tags into captioning models during decod- ing rather than during training. It also appears that, while introducing image tags improves per- formance on both out-of-domain and in-domain evaluations, it is beneficial to introduce more tag constraints when the test data is likely to con- tain previously unseen objects. This reflects the trading-off of influence between the image tags and the captioning model. For example, we noted that when using two tag constraints, 36% of gen- erated captions were identical to the base model, but when using four tags this proportion dropped to only 3%.</p><p>To establish performance upper bounds, we train the base model on the complete MSCOCO training set (Base All Data). We also evaluate captions generated using our approach combined with an 'oracle' image tagger consisting of the top 3 ground-truth image tags (T3*). These were de- termined by selecting the 3 most frequently men- tioned words in the reference captions for each test image (after eliminating stop words). The very high scores recorded for this approach may mo- tivate the use of more powerful image taggers in future work. Finally, replacing VGG-16 with the more powerful ResNet-50 ( ) CNN leads to modest improvements as indicated in the lower half of <ref type="table">Table 1</ref>.</p><p>Evaluating F1 scores for object mentions (see <ref type="table" target="#tab_3">Table 2</ref>), we note that while our approach outper- forms prior work when four image tags are used, a significant increase in this score should not be ex- pected as the underlying image tagger is the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Captioning ImageNet</head><p>Consistent with our observation that many image collections contain useful annotations, and that we should seek to use this information, in this section we caption a 5,000 image subset of the ImageNet ( <ref type="bibr">Russakovsky et al., 2015</ref>) ILSVRC 2012 classi- fication dataset for assessment. The dataset con- tains 1.2M images classified into 1,000 object cat- egories, from which we randomly select five im- ages from each category.</p><p>For this task we use the ResNet-50 ( ) CNN, and train the base model on a com- bined training set containing 155k images com- prised of the MSCOCO ( <ref type="bibr" target="#b3">Chen et al., 2015</ref>) train- ing and validation datasets, and the full Flickr 30k ( <ref type="bibr" target="#b31">Young et al., 2014</ref>) captions dataset. We use constrained beam search and vocabulary ex- pansion to ensure that each generated caption in- cludes a phrase from the WordNet <ref type="bibr" target="#b9">(Fellbaum, 1998</ref>) synset representing the ground-truth image category. For synsets that contain multiple en- tries, we run constrained beam search separately for each phrase and select the predicted caption with the highest log probability overall.</p><p>Note that even with the use of ground-truth object labels, the ImageNet captioning task re- mains extremely challenging as ImageNet con- tains a wide variety of classes, many of which are not evenly remotely represented in the available image-caption training datasets. Nevertheless, the injection of the ground-truth label frequently im- proves the overall structure of the caption over the base model in multiple ways. Examples of gen- erated captions, including failure cases, are pre- sented in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>As the ImageNet dataset contains no exist- ing caption annotations, following the human- evaluation protocol established for the <ref type="bibr">MSCOCO 2015</ref><ref type="bibr">Captioning Challenge (Chen et al., 2015</ref>, we used Amazon Mechanical Turk (AMT) to collect a human-generated caption for each sample image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base:</head><p>A bird standing on top of a grass covered field.</p><p>Synset: cricket. Base+Synset: A bird stand- ing on top of a cricket field. For each of the 5,000 samples images, three hu- man evaluators were then asked to compare the caption generated using our approach with the human-generated caption (Base+Syn v. Human). Using a smaller sample of 1,000 images, we also collected evaluations comparing our approach to the base model (Base+Syn v. Base), and compar- ing the base model with human-generated captions (Base v. Human). We used only US-based AMT  <ref type="table">Table 3</ref>: In human evaluations our approach lever- aging ground-truth synset labels (Base+Syn) im- proves significantly over the base model (Base) in both direct comparison and in comparison to human-generated captions. workers, screened according to their performance on previous tasks. For both tasks, the user in- terface and question phrasing was identical to the MSCOCO collection process. The results of these evaluations are summarized in <ref type="table">Table 3</ref>.</p><p>Overall, Base+Syn captions were judged to be equally good or better than human-generated cap- tions in 22% of pairwise evaluations (12% 'bet- ter', 10% 'equally good'), and equally poor or worse than human-generated captions in the re- maining 78% of evaluations. Although still a long way from human performance, this is a signifi- cant improvement over the base model with only 11% of captions judged to be equally good or bet- ter than human. For context, using the identical evaluation protocol, the top scoring model in the MSCOCO Captioning Challenge (evaluating on in-domain data) received 11% 'better', and 17% 'equally good' evaluations.</p><p>To better understand performance across synsets, in <ref type="figure" target="#fig_3">Figure 5</ref> we cluster some class labels into super-categories using the WordNet hierar- chy, noting particularly strong performances in super-categories that have some representation in the caption training data -such as birds, mammals and dogs. These promising results suggest that fine-grained object labels can be successfully integrated with a general purpose captioning model using our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Research</head><p>We investigate constrained beam search, an ap- proximate search algorithm capable of enforcing any constraints over resulting output sequences that can be expressed in a finite-state machine. Applying this approach to out-of-domain image captioning on a held-out MSCOCO dataset, we leverage image tag predictions to achieve state of the art results. We also show that we can signifi- cantly improve the quality of generated ImageNet captions by using the ground-truth labels.</p><p>In future work we hope to use more power- ful image taggers, and to consider the use of constrained beam search within an expectation- maximization (EM) algorithm for learning better captioning models from weakly supervised data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of constrained beam search decoding. Each output sequence must include the words 'chair' or 'chairs', and 'desk' or 'table' from vocabulary V. A finite-state machine (FSM) that recognizes valid sequences is illustrated at top. Each state in the FSM corresponds to a beam in the search algorithm (bottom). FSM state transitions determine the destination beam for each possible sequence extension. Valid sequences are found in Beam 3, corresponding to FSM accepting state s 3 .</figDesc><graphic url="image-3.png" coords="3,307.28,138.63,218.27,163.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of out-of-domain captions generated on MSCOCO using the base model (Base), and the base model constrained to include four predicted image tags (Base+T4). Words never seen in training captions are underlined. The bottom row contains some failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of ImageNet captions generated by the base model (Base), and by the base model constrained to include the groundtruth synset (Base+Synset). Words never seen in the MSCOCO / Flickr 30k caption training set are underlined. The bottom row contains some failure cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: AMT evaluations of generated (Base+Syn) ImageNet captions versus human captions, by super-category.</figDesc><graphic url="image-15.png" coords="9,72.00,227.24,218.28,119.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) using MSCOCO. Following this ap-Out-of-Domain Test Data In-Domain Test Data</head><label></label><figDesc></figDesc><table>Model 
CNN 
SPICE METEOR CIDEr 
F1 
SPICE METEOR CIDEr 

DCC (Hendricks et al., 2016) 
VGG-16 
13.4 
21.0 
59.1 
39.8 
15.9 
23.0 
77.2 
NOC (Venugopalan et al., 2016) VGG-16 
-
21.4 
-
49.1 
-
-
-
Base 
VGG-16 
12.4 
20.4 
57.7 
0 
17.6 
24.9 
93.0 
Base+T1 
VGG-16 
13.6 
21.7 
68.9 
27.2 
17.9 
25.0 
93.4 
Base+T2 
VGG-16 
14.8 
22.6 
75.4 
38.7 
18.2 
25.0 
92.8 
Base+T3 
VGG-16 
15.5 
23.0 
77.5 
48.4 
18.2 
24.8 
90.4 
Base+T4 
VGG-16 
15.9 
23.3 
77.9 
54.0 
18.0 
24.5 
86.3 

Base+T3* 
VGG-16 
18.7 
27.1 
119.6 54.5 
22.0 
29.4 
135.5 
Base All Data 
VGG-16 
17.8 
25.2 
93.8 
59.4 
17.4 
24.5 
91.7 

Base 
ResNet-50 
12.6 
20.5 
56.8 
0 
18.2 
24.9 
93.2 
Base+T1 
ResNet-50 
14.2 
21.7 
68.1 
27.3 
18.5 
25.2 
94.6 
Base+T2 
ResNet-50 
15.3 
22.7 
74.7 
38.5 
18.7 
25.3 
94.1 
Base+T3 
ResNet-50 
16.0 
23.3 
77.8 
48.2 
18.7 
25.2 
92.3 
Base+T4 
ResNet-50 
16.4 
23.6 
77.6 
53.3 
18.4 
24.9 
88.0 

Base+T3* 
ResNet-50 
19.2 
27.3 
117.9 54.5 
22.3 
29.4 
133.7 
Base All Data 
ResNet-50 
18.6 
26.0 
96.9 
60.0 
18.0 
25.0 
93.8 

Table 1: Evaluation of captions generated using constrained beam search with 1 -4 predicted image 
tags used as constraints (Base+T1 -4). Our approach significantly outperforms both the DCC and NOC 
models, despite reusing the image tag predictions of the DCC model. Importantly, performance on in-
domain data is not degraded but can also improve. 

Model 
bottle 
bus 
couch 
microwave 
pizza 
racket 
suitcase 
zebra 
Avg 

DCC (Hendricks et al., 2016) 
4.6 
29.8 
45.9 
28.1 
64.6 
52.2 
13.2 
79.9 
39.8 
NOC (Venugopalan et al., 2016) 
17.8 
68.8 
25.6 
24.7 
69.3 
68.1 
39.9 
89.0 
49.1 
Base+T4 
16.3 
67.8 
48.2 
29.7 
77.2 
57.1 
49.9 
85.7 
54.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 scores for mentions of objects not seen during caption training. Our approach (Base+T4) 
reuses the top 4 image tag predictions from the DCC model but generates higher F1 scores by interpreting 
tag predictions as constraints. All results based on use of the VGG-16 CNN. 

proach, all images with captions that mention one 
of eight selected objects (or their synonyms) are 
excluded from the image caption training set. This 
reduces the size of the caption training set from 
82,783 images to 70,194 images. However, the 
complete caption training set is tokenized as a bag 
of words per image, and made available as image 
tag training data. As such, the selected objects 
are unseen in the image caption training data, but 
not the image tag training data. The excluded ob-
jects, selected by Hendricks et. al. (2016) from the 
80 main object categories in MSCOCO, are: 'bot-
tle', 'bus', 'couch', 'microwave', 'pizza', 'racket', 
'suitcase' and 'zebra'. 

</table></figure>

			<note place="foot" n="1"> www.panderson.me/constrained-beam-search</note>

			<note place="foot" n="3"> Approach In this section we describe the constrained beam search algorithm, the base captioning model used in experiments, and our approach to expanding the model vocabulary with pretrained word embeddings.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for providing insightful comments and for helping to identify relevant prior literature. This research is supported by an Australian Government Re-search Training Program (RTP) Scholarship and by the Aus-tralian Research Council Centre of Excellence for Robotic Vision (project number CE140100016).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adrì a de Gispert, Gonzalo Iglesias, and Michael Riley</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="687" to="723" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Pushdown automata in statistical machine translation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SPICE: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zitnick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi Lin Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
	</analytic>
	<monogr>
		<title level="m">Microsoft COCO captions: Data collection and evaluation server</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing images using inferred visual dependency representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating topical poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of noisy data for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-RNN). In ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to the Theory of Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cengage Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Carapcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sienkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07770</idno>
		<title level="m">Captioning images with diverse objects</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What Value Do Explicit High Level Concepts Have in Vision to Language Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast zero-shot image tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
