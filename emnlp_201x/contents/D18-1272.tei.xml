<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WECA: A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Diao</surname></persName>
							<email>diaoyufeng@mail.dlut.edu.cn, hflin@dlut.edu.cn wudi@dlut.edu.cn, liang@dlut.edu.cn xukan@dlut.edu.cn, yangzh@dlut.edu.cn wangjian@dlut.edu.cn, zhangsw@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inner Mongolia University for Nationalities</orgName>
								<address>
									<settlement>Tong Liao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dongyuzhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">DaLian University of Technology</orgName>
								<address>
									<settlement>Da Lian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WECA: A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2507" to="2516"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2507</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Homographic puns have a long history in human writing, widely used in written and spoken literature, which usually occur in a certain syntactic or stylistic structure. How to recognize homographic puns is an important research. However, homographic pun recognition does not solve very well in existing work. In this work, we first use WordNet to understand and expand word embedding for settling the polysemy of homographic puns, and then propose a WordNet-Encoded Collocation-Attention network model (WECA) which combined with the context weights for recognizing the puns. Our experiments on the Se-mEval2017 Task7 and Pun of the Day demonstrate that the proposed model is able to distinguish between homographic pun and non-homographic pun texts. We show the effectiveness of the model to present the capability of choosing qualitatively informative words. The results show that our model achieves the state-of-the-art performance on homographic puns recognition.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A pun is a writers use of a word in an ambiguous and inconsistent way in language, often to play on the different meanings of the word or utilize simi- larly pronounced sounds for a common humorous effect. Puns are widely used in written and spo- ken literature, which intended as jokes. For ex- ample, Tom Swifty by <ref type="bibr" target="#b11">(Lippman and Dunn, 2000</ref>), in which puns usually occur in a certain syntac- tic or stylistic structure. From literature, speeches and oral storytelling, puns are also a standard rhetorical device, which also can be applied non- humorously. For instance, Shakespeare is well known for his puns, which continually appeared in his non-comedic works by <ref type="bibr" target="#b25">(Tanaka, 1992)</ref>. Both * Corresponding author humorous and non-humorous puns have been the theme of extensive and attractive works that has led to discernment for the nature of puns with dou- ble meaning.</p><p>There are many relevant studies on pun recogni- tion in natural language processing. Many schol- ars attempted to classify puns according to the similar relationship between the pronunciations and double meanings of the words. For exam- ple, <ref type="bibr" target="#b19">(Pafford, 1987)</ref> categorizes pun into homo- phonic puns and homographic puns, which used homonyms and polysemy of words respectively. The research on pun recognition has carried out according to this classification system of Redfern. Our work also considers that puns consist of ho- mophonic and homographic puns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type of Pun</head><p>Example Pun Word Homographic I used to be a banker interest but I lost interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Homophonic</head><p>When the church bought propane gas for their annual barbecue, proceeds went from sacred to the propane. Both homographic puns and homophonic puns have double meanings to increase deep impression in a certain environment. However, two types of puns have their own features, respectively. Homo- graphic puns, as an important class of puns, which the words for two senses of puns share the same orthographic form. While homophonic puns have the similarity in pronunciations with double senses that distinguished from homographic puns. The former one mainly settles synonyms, while the lat- ter one solves homonyms. Because of the differ- ence, we can not use the unified model to distin- guish. <ref type="table" target="#tab_0">Table 1</ref> illustrates the examples of homo- graphic pun and homophonic pun.</p><p>In this study, we mainly focus on homographic puns since they widely used everywhere <ref type="bibr" target="#b17">(Miller, Tristan and Turkovi´cTurkovi´c, Mladen, 2016</ref>) and easily obtain in existing corpus. However, homographic puns recognition in the current works does not solve very well because of their confused double meanings.</p><p>To solve the mentioned problem, we propose a computational WordNet-Encoded Collocation- Attention network model (WECA) to recognize homographic puns. Our model takes semantic word embedding and collocation into account for homographic puns recognition. Based on the ex- periments, the results show that our work will improve the performance of homographic puns recognition. This work is the first to recognize ho- mographic puns with improved word representa- tion and attention mechanism to the best of knowl- edge. Here, our contributions are as follows.</p><p>• The paper applies the lexical ontology Word- Net to understand and extend the word em- bedding for solving the polysemy of homo- graphic puns.</p><p>• The paper proposes a neural attention mech- anism to extract the collocation for homo- graphic puns classification, which combined with Bi-LSTM to obtain the context weights.</p><p>• Experimental results on the datasets of Se- meval2017 Task7 and Pun of the Day demon- strate our method outperforms several base- lines for recognition homographic puns. Fur- thermore, visualization of selected examples show the reasons that this model works well.</p><p>The rest of this paper is structured in the follow- ing. Section 2 mainly reviews the related work on word representation and puns classification. Sec- tion 3 presents our proposed word embedding and collocation attention-based network model. Sec- tion 4 shows our experiments and discusses eval- uation results. Finally, Section 5 concludes our research contributions and offers the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will review related works on word representation and homographic pun recog- nition for homographic puns classification briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Representation</head><p>In recent years, word representation has the great improvement because it solves data sparsity prob- lem and obtain more semantic relations between words compared with one-hot representation.</p><p>( <ref type="bibr" target="#b21">Rumelhart et al., 1986)</ref> proposed the idea of word distributed representation, which converts all the words into a low-dimensional continu- ous semantic space. This space took each word as a vector. These distributed low-dimensional word representation have been widely applied in many NLP tasks, including machine transla- tion( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), text classification ( <ref type="bibr" target="#b18">Niu et al., 2017;</ref><ref type="bibr" target="#b3">Du et al., 2017)</ref>, neural language models ( <ref type="bibr" target="#b13">Mikolov et al., 2010</ref><ref type="bibr" target="#b12">Mikolov et al., , 2013</ref>) and parsing <ref type="bibr" target="#b1">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b2">Chen et al., 2015)</ref>. Word embedding is taken as the essential and available inputs for NLP tasks, which enables encoding semantic representation in meaningful vector space.</p><p>The studies show that word representations are useful to achieve a good balance between effectiveness and efficiency, such as Word2Vec ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) and GloVe( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>). Therefore, the semantic meanings of words can reflect in the contexts according to these distributed representation models.</p><p>However, homographic puns always have multi- ple meanings. The word representation, consider- ing as only one vector for each word, which puz- zled by the understanding for polysemy of puns. This paper combines the representations of lem- mas, synsets and words from WordNet 1 <ref type="bibr" target="#b14">(Miller, 2002</ref>) to understand multiple meanings of homo- graphic puns. The lemma and synset annotation in WordNet provide helpful semantic information for detecting homographic puns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Homographic Pun Recognition</head><p>In recent years, homographic puns have increas- ingly become a respectable research topic, which widely appears in rhetoric and literary criticism. However, there were little related works in the fields of computational linguistics and natural lan- guage processing by <ref type="bibr" target="#b17">(Miller, Tristan and Turkovi´cTurkovi´c, Mladen, 2016)</ref>. In this subsection, we mainly in- troduce some puns detecting methods.</p><p>There are many useful methods to classify the puns in NLP. For example, ( <ref type="bibr" target="#b9">Kao et al., 2016;</ref><ref type="bibr" target="#b6">Huang et al., 2017</ref>) used a probability statisti- cal model to capture the latent semantic infor- mation between words for detecting homographic puns. ( <ref type="bibr" target="#b8">Jaech et al., 2016</ref>) proposed a new prob-ability model to learn phoneme edit probabilities for classifying the homophonic puns. The sys- tem ECNU( <ref type="bibr" target="#b27">Xiu et al., 2017</ref>) applied a supervised training classifier, which helpful features derived from WordNet and Word2Vec embeddings to dis- tinguish between homographic puns. The sys- tem Fermi (Indurthi and Oota, 2017) employed a supervised approach for the detection of ho- mographic puns. It used a bi-directional RNN for a classification model and adopted the dis- tributed semantic word embeddings as input fea- tures. These methods do not consider the colloca- tion between words in homographic puns.</p><p>The attention mechanism proposed by <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>) to settle machine translation problem, which was used to select the reference words for words before translation. ( <ref type="bibr" target="#b28">Xu et al., 2015</ref>) used attention model for image generation to select the similar image regions. For text clas- sification, ( <ref type="bibr" target="#b30">Yang et al., 2016</ref>) applied attention mechanism into solving document-level classifica- tion. Many other tasks in NLP used this mecha- nism, including natural language question answer- ing ( <ref type="bibr" target="#b10">Kumar et al., 2015</ref>), parsing ( ), image question answering( <ref type="bibr" target="#b29">Yang et al., 2015)</ref>, and classification <ref type="bibr" target="#b22">(Shen et al., 2018;</ref><ref type="bibr" target="#b24">Tan et al., 2018)</ref>. Therefore, this model is capable of discovering the important and semantic informa- tion. Meanwhile, attention mechanism can also improve the performance of classification tasks.</p><p>Hence, we explore an attention mechanism for collocation to mine the latent semantic informa- tion between the part of speech words to achieve good result for homographic puns recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Homographic puns recognition could influence by considering both the semantic word embed- ding and collocation with the context weights for homographic puns. In this section, we pro- pose our model as WordNet-Encoded Collocation- Attention network (WECA). <ref type="figure" target="#fig_0">Figure 1</ref> demon- strates the overall structure of our model.</p><p>It consists of three main components: an im- proved word embedding with WordNet-Encoded as inputs, a Bidirectional Long Short-Term Mem- ory (Bi-LSTM) as context weights in a sentence for homographic puns and a fully-connected net- work as the collocation-attention mechanism. The attention networks combined by a concatenate op- eration to discover the collocation. Then the con- text weights and attention networks combined by an element-wise multiplication operation in the classification layer. We describe the details of three components as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">WordNet-Encoded Word Embedding</head><p>The homographic pun is a clever trick to let one word relate to two aspects or multiple meanings. For example, "Before he sold Christmas trees, he got himself spruced up" The pun word spruced has two meaning: one meaning is spruce tree, while the other is making oneself or something look neater and tidier. We find this word spruced means the last meaning in this situation. There- fore, the polysemy of the ambiguity from homo- graphic puns need additional large lexical ontol- ogy. Thus, we apply WordNet for computational linguistics and natural language processing.</p><p>Polysemy is critical factor for recognizing ho- mographic puns. To combine the information of multiple meanings, we propose giving a WordNet- Encoded model (WE) to obtain the word embed- ding for each word. WordNet is a lexical ontology of words. Each word has multiple semantics corre- sponding with respect to different senses and each sense corresponds to multiple words.</p><p>We introduce lemmas, synsets (senses) and words in WordNet. For example, the word is "interest". The word "interest" has three main synsets: sake (a reason for wanting something done), pastime (a diversion that occupies one's time and thoughts) and interest (a sense of con- cern with and curiosity about someone or some- thing). The lemmas eliminate the ambiguity of each sense. For instance, the synset pastime rep- resents a diversion that occupies one's time and thoughts, which contains lemmas pastime, inter- est and pursuit. Then we propose two strategies to generate the Word-Net-Encoded embedding based on the information of lemmas and synsets in- formation, Average Lemma Aggregation Model (ALA) and Weighted Lemma Aggregation Model (WLA).</p><p>Average Lemma Aggregation Model (ALA) adopts a strategy of equal weight according to meanings of homographic puns. ALA model mixes all the lemmas of all the senses of a word to- gether for each word. Hence, it represents the tar- get word by using the average of its whole lemma embedding and puts this together on the original vector of target word. The formula is as follows:</p><formula xml:id="formula_0">w = 1 m s i (w) ∈S (w) l i (s j ) ∈L i (s j ) w l j s j<label>(1)</label></formula><p>which means the new embedding vector of w is determined by the average of all its lemma embed- ding. Here, m represents the number of lemmas with overlapping senses with respect to the word w, s i is the sense i, l j is the lemma j. Finally, word embedding of w is the concatenation of the original vector and above new vector. ALA model can apply lemmas to encoding la- tent semantic relationship because lemmas share the information by multiple words and senses. Therefore, words sharing the same lemmas are likely to obtain the similar representations.</p><p>Weighted Lemma Aggregation Model (WLA) The ALA Model takes the lemma embedding to encode lemma information for word representa- tion. Although ALA model represents the aver- age of all the lemma, which does not consider the importance of certain lemmas. Hence, we con- struct embedding for a target word with the help of word senses and lemmas in WordNet that we called WLA model. The formula is as follows:</p><formula xml:id="formula_1">w = s i (w) ∈S (w) |L i (s i ) | m l i (s j ) ∈L i (s j ) w l j s j<label>(2)</label></formula><p>where m represents for all the number of lemmas with overlapping senses with respect to the word w, s i is the sense i, l j is the lemma j, l i (s j ) is the number of lemmas in each sense with target word, w is the new embedding considering the weighted lemma information. Then, the target word embed- ding concatenates new vector to original vector.</p><p>The weighted lemma strategy assumes one sense of word obtains more attention if this sense of word has more lemmas. We can show each word as a special distribution on the sense. From the results, WLA model is the best representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional Long Short Term</head><p>Memory(Bi-LSTM) for Recognizing Homographic Puns Long Short Term Memory (LSTM) was proposed by <ref type="bibr">Hochreiter and Schmiduber (1997)</ref>, which has been widely adopted for text processing. There are three gates and one cell in LSTM: an input gate i t , a forget gate f t , an output gate o t and a memory cell c t . They are all vector in R d . The equations of transition are:</p><formula xml:id="formula_2">i t = σ(W i x t + U i h t−1 + V i c t−1 )<label>(3)</label></formula><formula xml:id="formula_3">f t = σ(W f x t + U f h t−1 + V f c t−1 )<label>(4)</label></formula><formula xml:id="formula_4">o t = σ(W o x t + U o h t−1 + V o c t−1 )<label>(5)</label></formula><formula xml:id="formula_5">˜ c t = tanh(W c x t + U c h t−1 )<label>(6)</label></formula><formula xml:id="formula_6">c t = f t c t−1 + i t ˜ c t<label>(7)</label></formula><formula xml:id="formula_7">h t = o t tanh(c t )<label>(8)</label></formula><p>where x t is an input vector at the current time step, σ is the sigmoid function and is the element- wise multiplication operation, W {i,f,o,c} ,U {i,f,o,c} ,V {i,f,o,c} are learned weight parameters, h t is the hidden state vector. In LSTM, the hidden state h t only encodes the front context in a forward direc- tion but not consider the backward context. In this study, we apply Bi-LSTM model <ref type="bibr" target="#b4">(Graves, 2012)</ref> to capture the latent semantic information of homographic puns for obtaining the context weights. For each sentence, it has a forward LSTM − → h and a backward LSTM ← − h to con- catenate the hidden states of two LSTMs as the representation of corresponding word. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the architecture of Bi-LSTM model. {w 1 , w 2 , · · · , w n } represent the word vector in a sentence whose length is N . Then, the forward and backward contexts can take into account si- multaneously. The equations of transition are:</p><formula xml:id="formula_8">− → h t = H(W x − → h x t + W− → h − → h − → h t−1 + b− → h ) (9) ← − h t = H(W x ← − h x t + W← − h ← − h ← − h t−1 + b← − h )<label>(10)</label></formula><formula xml:id="formula_9">h out = W− → h y − → h t + W← − h y ← − h t + b y<label>(11)</label></formula><p>where − → h t is a forward LSTM, ← − h t is a backward LSTM, h out is the output of Bi-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Collocation-Attention Mechansim</head><p>It proposes that not all the words provide the same contribution of word representation for the sen- tence. Especially for homographic puns recogni- tion, the collocation between candidate pun words in a sentence offers more clues for getting the col- locational word weights. Miller points out that the candidate pun words mainly consist of nouns, verbs, adjectives and adverbs in each pun. For ex- ample, "The money doesn't grow on the tree, but it can grow on the branch." The word "branch" is the pun word. From this example, we know that the collocation of candidate pun words {money, grow, tree, branch}, which should be more important for recognizing homographic puns.</p><p>Therefore, it is necessary to learn about the la- tent relationship in collocation of words. We de- sign an attention mechanism to obtain the colloca- tional weights by extracting such words of collo- cation from nouns, verbs, adjectives and adverbs, respectively. Then we concentrate on the four parts in sentences with pun to aggregate the in- formative words for classifying the homographic puns. This model uses an attention network taking word embedding with WordNet-Encoded as input then to extract polysemy attention signal, which made use of polysemy to understand ambiguity of homographic puns. The formula is as follows:</p><formula xml:id="formula_10">u ijt = V · tanh(W u h ijt + b w )<label>(12)</label></formula><formula xml:id="formula_11">α ijt = exp(u ijt ) Tx t=1 exp(u ijt )<label>(13)</label></formula><formula xml:id="formula_12">c ij = Tx t=1 α ijt h ijt (14)</formula><p>where h ijt is a hidden state at each time step for each part of speech, j ∈ {nouns, verbs, adjectives, adverbs}, u ijt is a hidden representation of h ijt through a one-layer MLP, α ijt is a normalized importance weight through a softmax function with each part of speech, c ij is a context vector as a high level representation over the words from attention- based model by the weighted mean of the hidden state sequence h ijt for each part of speech.</p><p>After combining the attention networks with the context weights in a sentence, we merge all the c ij vectors from collocation attention model and take the uniformed context weights in a sentence.</p><p>Then we mix the two parts results with element wise multiplication operation to recognize the ho- mographic puns. The formula is as follows:</p><formula xml:id="formula_13">c i = [c inouns ; c iverbs ; c iadjectives ; c iadverbs ] (15) l out = Sof tmax(h out )<label>(16)</label></formula><formula xml:id="formula_14">s i = c i · l out (17)</formula><p>where c i is merged by c ij , j ∈ {nouns, verbs, adjectives, adverbs}, l out is the softmax function of h out , s i is the result with the multiplication operation of c i and l out .</p><p>The model can be trained in an end-to-end way by backpropagation, where objective function is the cross-entropy loss. Let y be the target distribu- tion andˆyandˆ andˆy be the predicted distribution. The goal of training is to minimize the cross-entropy error between y andˆyandˆ andˆy for all sentences.</p><formula xml:id="formula_15">loss = − i j y j i logˆylogˆ logˆy i j + λθ 2<label>(18)</label></formula><p>where i is the index of sentence, j is the index of class. Our classification is two way. λ is the L2 regularization term. θ is the parameter set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head><p>In this section, we first evaluate the effective- ness of our WordNet-Encode model (WE) on two tasks to detect the polysemy of homographic puns. Then, we examine the performance of our WECA model compared with existing methods. Finally, we show the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>In this section, we introduce datasets, evaluation metrics, baseline methods, and present the details of the training process of our model. Datasets To verify the effectiveness of our pro- posed model, we use two datasets: SemEval2017 Task7 <ref type="bibr">2</ref> and Pun of the Day 3 .</p><p>SemEval2017 Task7. This dataset is composed of homographic and heterographic puns for rec- ognizing and interpreting puns. We focus on ho- mographic puns detection in semantic rather than phonology. The homographic pun word will have at least two words sense in the WordNet <ref type="bibr" target="#b15">(Miller and Gurevych, 2015)</ref>. <ref type="table" target="#tab_0">Table 1</ref> shows a detailed statistical distribution of our datasets.</p><p>Pun of the Day. This dataset only includes pun content in the beginning. Then it collects the neg- ative samples from Yahoo! Answer 4 , AP News 5 , Proverb, and New York Times in order to balance the distribution of positive and negative examples, which adapt to decrease the domain discrepancy. <ref type="table" target="#tab_2">Table 2</ref> provides a complete statistical description of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Positive  Metrics We apply the standard measures pre- cision, recall, accuracy and F1-score to evaluate the effectiveness for homographic puns recogni- tion, which also adopted as metrics in SemEval 2017 Task7 evaluation.</p><p>Baselines We compare several strong baselines as follows.</p><p>LSTM: LSTM without WordNet-Encoded em- bedding and Collocation-Attention mechanism.</p><p>Bi-LSTM: Bi-LSTM without WordNet- Encoded embedding and Collocation-Attention.</p><p>Bi-LSTM E : Bi-LSTM with WordNet-Encoded embedding used the WLA model.</p><p>Bi-LSTM-Attention: Bi-LSTM with single at- tention mechanism.</p><p>Fermi and N-Hance are the good performing model in the SemEval2017 task7.</p><p>Top1 Fermi: Fermi took a supervised ap- proach for homographic puns detection. It did not construct own train data set, but rather split the shared task data set into train sets and test sets <ref type="bibr" target="#b16">(Miller, Tristan and Hempelmann, Christian and Gurevych, Iryna, 2017)</ref>. It used a Bi-RNN to learn a classification model and treat the word embedding as the input features.</p><p>Top2 N-Hance: It assumed every pun had a particularly strong association with exactly one other word in context <ref type="bibr" target="#b16">(Miller, Tristan and Hempelmann, Christian and Gurevych, Iryna, 2017)</ref>. Then it calculated PMI between words in context to detect and locate puns. If the score exceeded a certain threshold, the text assumed to contain a pun. Otherwise, the text assumed to have no pun.</p><p>WECA: Here, we use Bi-LSTM with WordNet- Encoded embedding with WLA model and Collocation-Attention mechanism.</p><p>Training Details In experiments, our model is tuned with 5-fold cross validation. All word vec- tors are initialized by GloVe. We use 50, 100, 200 and 300 dimension to verify the performance, re- spectively. Here, 200 dimension is the best perfor- mance. Therefore, we set the dimensions of word, synset and lemma embedding to be 200. The size of units in LSTM is 800. RMSprop is used for our optimization method. We use learning rate decay and early stop in the training process. All models are trained by mini-batch of 64 instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Effectiveness of WordNet-Encoded Word Embedding</head><p>Comparing the GloVe model with our ALA model and WLA model, we evaluate the quality of our improved word representations to detect the ho- mographic puns. In this experiment, we use the same classifier Bi-LSTM and parameters to verify the effectiveness of our word embedding. <ref type="figure">Figure 3</ref> and <ref type="figure">Figure 4</ref> show the results of dif- ferent word embedding for detecting homographic puns. From the results we can observe that:</p><p>(1) Our models ALA and WLA, which out- perform the original vector GloVe on both two datasets. It indicates that our model can better cap- ture the semantic relations of words by utilizing lemma annotation properly based on the WordNet.</p><p>(2) The ALA model represents each word with the average of its lemma embedding. In gen- eral, the ALA model performs better than GloVe, showed that lemma and synset of WordNet is very useful. The reason is the words sharing mutual lemma representation are helpful with each other.</p><p>(3) The WLA model mostly performs better than GloVe and ALA model. This model can obtain a weighted distribution according to the senses and lemmas. The results show that their different senses are commonly different from oth- ers, but share certain representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pun Recognition with WordNet-Encoded</head><p>Collocation-Attention Network (WECA)</p><p>Our model WECA, combination of WordNet- Encoded word embedding and Collocation- Attention network with context weight, which performs compared with the suggested baselines. Here, we use Pun of the Day as the training set to obtain all the parameters, and test the results of homographic pun recognition in SemEval2017 task7. The results are shown in   .50%). N-Hance is the second place in Se- mEval2017 task7. It shows the WordNet-Encoded word embedding can capture more semantic in- formation between words with the help of lemma and synsets in WordNet. Meanwhile, it presents that the attention network mechanism combined with collocation of the specific part of speech of puns, which capture the characteristic information to recognize the homographic puns.</p><p>The best perform of SemEval2017 task7 is Fermi. However, Fermi only evaluates on 675 of 2250 homographic contexts <ref type="bibr" target="#b16">(Miller, Tristan and Hempelmann, Christian and Gurevych, Iryna, 2017</ref>) in SemEval2017 task7. Thus, our model uses 675 as a test set and rest of data as a train- ing set. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Experi- ment results present our model outperforms Fermi under the same data distribution. It shows the ef- fectiveness of our model again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization of Model</head><p>In order to verify our model is enable to select the valuable information of words that reflected the collocation, we visualize the attention layers for several sentences in Pun of the Day and Se- mEval2017 Task7 data sets whose labels are cor- rectly predicted by our model in <ref type="figure" target="#fig_3">Figure 5</ref>. We choose two examples. One presents collocation between nouns, the other presents collocation be- tween verbs.</p><p>Each line is a sentence. Blue denotes word weight. If color of word is darker, the word is more important. <ref type="figure" target="#fig_3">Figure 5</ref> shows our WECA model can select words carrying ambiguous meanings from the collocation of homographic puns. For exam- ple, in the first sentence, it highlights "interest", which is worthy attracting more attention because of multiple meanings for the pun word. In the second sentence, "sleep" is selected word by our attention model as related to homographic puns. Therefore, attention networks of collocation is ef- fective for recognizing homographic puns.</p><p>We apply the Bi-LSTM to capture latent seman- tic context for weighting part of speech which in- cluded nouns, verbs, adjectives and adverbs from forward and backward direction. <ref type="figure">Figure 6</ref> shows Bi-LSTM model distributes weights to the four part of speeches. The weights of verbs occupy the first place, then second one is nouns, adjec- tives and adverbs are lower. Meanwhile, it demon- strates the importance of part of speech.</p><p>Thus, we choose two examples to illustrate weights with the four part of speeches according to context information by Bi-LSTM in Figure7. For the first example, the word "cured", as a verb, which is a pun word. It shows weights of verbs are highest allocation by Bi-LSTM. For the second ex- ample, the word "cinch", as a noun, which is a pun word. It illustrates that Bi-LSTM distributes the higher weights to nouns, which presents the im- portance of nouns from the context semantic in- formation. Hence, context weights providing by Bi-LSTM are helpful of the collocation for recog- nizing the homographic puns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this study, we propose a computational model WECA combined with WordNet-Encoded word embedding and Collocation-Attention network. We extend the semantic information of word em- bedding by lemma and synset according to Word- Net. We also apply a neural attention network, combined with Bi-LSTM, which captures the col- location of homographic puns. Experimental re- sults show our model achieves the best perfor- mance and outperforms several baselines.</p><p>In future work, we would like to find an appro- priate way in incorporating the external linguistic knowledge to improve the performance of homo- graphic puns recognition. We also focus on au- tomatically generating homographic puns. Those are all promising jobs we can pursue in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: WordNet-Encoded Collocation-Attention network model(WECA)</figDesc><graphic url="image-1.png" coords="4,128.69,62.81,340.16,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Structure of Bi-LSTM</figDesc><graphic url="image-2.png" coords="5,72.00,245.07,226.77,136.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Comparison of Different Word Embedding on SemEval2017 Task7</figDesc><graphic url="image-4.png" coords="7,307.28,252.63,226.77,141.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of attention layers</figDesc><graphic url="image-5.png" coords="8,310.11,62.81,212.59,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of Bi-LSTM</figDesc><graphic url="image-7.png" coords="9,72.00,62.81,212.60,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Pun Examples</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Statistics of Datasets</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Precision 
Recall 
F1 
LSTM 
81.80 
83.7 
82.43 
Bi-LSTM 
85.40 
83.64 
84.51 
Bi-LSTME 
85.87 
85.07 
85.46 
Bi-LSTM-Attention 
84.92 
85.62 
85.26 
N-Hance 
75.53 
93.34 
83.50 
WECA 
89.19 
90.64 
89.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of Different Models of Homo-
graphic Puns Recognition 

(1) Bi-LSTM has the better performance for ho-
mographic puns detection compared with LSTM 
(84.51% vs.82.43%). It shows that Bi-LSTM ex-
ploits two parallels to discover more context in-
formation. At the same time, Bi-LSTM E out-
performs Bi-LSTM (85.46% vs.84.51%), which 
demonstrating the effectiveness of the WordNet-
Encoded word embedding. 
(2) Bi-LSTM-Attention performs slightly better </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Comparison of WECA and Fermi of Homo- graphic Puns Recognition</head><label>4</label><figDesc></figDesc><table>than Bi-LSTM and LSTM (85.26% vs. 84.51%, 
82.43%). The reason is that the attention mech-
anism can assign the weight to the whole words 
according to the context information. 
(3) Our model WECA has a better performance 
compared with Bi-LSTM E , Bi-LSTM-Attention 
and N-Hance (87.45% vs. 85.46%, 85.26%, 
83</table></figure>

			<note place="foot" n="1"> WordNet: http://wordnet.princeton.edu/</note>

			<note place="foot" n="2"> SemEval2017 Task7: htto://alt.qcri.org/semeval2017/task7/ 3 Pun of the Day: htto://www.punoftheday.com/</note>

			<note place="foot" n="4"> htto://answers.yahoo.com/ 5 htto://hosted.ap.org/dynamic/fronts/HOME?SITE=AP</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by grant from the Natural Science Foundation of <ref type="bibr">China (No.61632011, 61702080, 61572102, 61602079, 61602078)</ref>, the Fundamental Research Funds for the Central Universities (No.DUT18ZD102, DUT17RC(3)016).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distributed feature representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stance classification with target-specific neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3988" to="3994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidelberg</forename><surname>Springer Berlin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identification of homographic pun location for pun understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hsiang Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hen Hsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin Hsi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on World Wide Web Companion</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="797" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fermi at semeval-2017 task 7: Detection and interpretation of homographic puns in english language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayasaradhi</forename><surname>Indurthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subba Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="457" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Phonological pun-derstanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A computational model of linguistic humor in puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><forename type="middle">T</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1270" to="1285" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ask me anything: dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Contextual connections within puns: effects on perceived humor and memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Lippman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of General Psychology</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="197" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>INTERSPEECH 2010</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for the english language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Review</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="206" to="208" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic disambiguation of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="719" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hempelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iryna</surname></persName>
		</author>
		<title level="m">Semeval-2017 task 7: Detection and interpretation of english puns</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
	<note>International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards the automatic detection and identification of english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mladen</forename><surname>Turkovi´cturkovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Journal of Humour Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="75" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved word representation learning with sememes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2049" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H P</forename><surname>Pafford</surname></persName>
		</author>
		<title level="m">Redfern, w., puns. Notes Queries</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning representations by backpropagating errors. Parallel Distributed Processing Explorations in the Microstructure of Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="399" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Disan: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The pun in advertising: A pragmatic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiko</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lingua</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="91" to="102" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<biblScope unit="page" from="2773" to="2781" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ecnu at semeval-2017 task 7: Using supervised and unsupervised methods to detect and locate english puns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuan</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="453" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2048" to="2057" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
