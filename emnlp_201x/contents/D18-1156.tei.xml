<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">‡ Information Research Center of Military Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">PLA Academy of Military Science</orgName>
								<address>
									<postCode>100142</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
							<email>zhunchenluo@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">‡ Information Research Center of Military Science</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<postCode>100081</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">PLA Academy of Military Science</orgName>
								<address>
									<postCode>100142</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1247" to="1256"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1247</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential mod-eling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting events from natural language text is an essential yet challenging task for natural lan- guage understanding. When given a document, event extraction systems need to recognize event triggers with their specific types and their corre- sponding arguments with the roles. Technically speaking, as defined by the ACE 2005 dataset 1 , a benchmark for event extraction ( <ref type="bibr" target="#b4">Grishman et al., 2005</ref>), the event extraction task can be divided into two subtasks, i.e., event detection (identifying and classifying event triggers) and argument extraction (identifying arguments of event triggers and label- ing their roles).</p><p>In event extraction, it is a common phenomenon that multiple events exist in the same sentence. Extracting the correct multiple events from those * * Corresponding author.</p><p>1 https://catalog.ldc.upenn.edu/ ldc2006t06</p><p>sentences is much more difficult than in the one- event-one-sentence cases because those various types of events are often associated with each other. For example, in the sentence "He left the company, and planned to go home directly.", the trigger word left may trigger a Transport (a person left a place) event or an End-Position (a person re- tired from a company) event. However, if we take the following event triggered by go into consider- ation, we are more confident to judge it as a Trans- port event rather than an End-Position event. This phenomenon is quite common in our real world, as Injure and Die events are more likely to co-occur with Attack events than others, whereas Marry and Born events are less likely to co-occur with Attack events. As we investigated in ACE 2005 dataset, there are around 26.2% (1042/3978) sentences be- long to this category.</p><p>Significant efforts have been dedicated to solv- ing this problem. Most of them exploiting vari- ous features ( <ref type="bibr">Liu et al., 2016b;</ref><ref type="bibr">Yang and Mitchell, 2016;</ref><ref type="bibr" target="#b11">Li et al., 2013;</ref><ref type="bibr" target="#b8">Keith et al., 2017;</ref><ref type="bibr" target="#b15">Liu et al., 2016a;</ref><ref type="bibr" target="#b12">Li et al., 2015)</ref>, introducing memory vec- tors and matrices <ref type="bibr">(Nguyen et al., 2016)</ref>, introduc- ing more transition arcs ( <ref type="bibr">Sha et al., 2018)</ref>, keeping more contextual information <ref type="bibr" target="#b0">(Chen et al., 2015)</ref> into sentence-level sequential modeling methods like RNNs and CRFs. Some also seek features in document-level methods ( <ref type="bibr" target="#b13">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b7">Ji and Grishman, 2008</ref>). However, sentence- level sequential modeling methods suffer a lot from the low efficiency in capturing very long- range dependencies while the feature-based meth- ods require extensive human engineering, which also largely affects model performance. Besides, these methods do not adequately model the asso- ciations between events.</p><p>An intuitive way to alleviate this phenomenon is to introduce shortcut arcs represented by lin- guistic resources like dependency parsing trees to drain the information flow from a point to its target through fewer transitions. Comparing to sequen- tial order, modeling with these arcs often success- fully reduce the needed hops from one event trig- ger to another in the same sentences. In <ref type="figure" target="#fig_0">Figure  1</ref>, for example, there are two events: a Die event triggered by the word killed with four arguments in red and an Attack event triggered by the word barrage with three arguments in blue. We need six hops from killed to barrage according to se- quential order, but only three hops according to the arcs in dependency parsing tree (along the nmod- arc from killed to witnesses, along the acl-arc from witnesses to called, and along the xcomp-arc from called to barrage). These three arcs consist of a shortcut path 2 , draining the dependency syntactic information flow from killed to barrage with fewer hops <ref type="bibr">3</ref> . In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework by introducing syntactic shortcut arcs to enhance in- formation flow and attention-based graphic convo- lution networks to model the graph information. To implement modeling with the shortcut arcs, we adopt the graph convolutional networks (GCNs) ( <ref type="bibr" target="#b9">Kipf and Welling, 2016;</ref><ref type="bibr">Marcheggiani and Titov, 2017;</ref><ref type="bibr">Nguyen and Grishman, 2018)</ref> to learn syn- tactic contextual representations of each node by the representative vectors of its immediate neigh- bors in the graph. And then we utilize the syn- tactic contextual representations to extract triggers and arguments jointly by a self-attention mecha- nism to aggregate information especially keeping the associations between multiple events.</p><p>We extensively evaluate the proposed JMEE framework with the widely-used ACE 2005 dataset to demonstrate its benefits in the experi- ments especially in capturing the associations be- tween events. To summary, our contribution in this work is as follows:</p><p>• We propose a novel joint event extraction framework JMEE based on syntactic struc- tures which enhance information flow and alleviate the phenomenon where multiple events are in the same sentence.</p><p>• We propose a self-attention mechanism to ag- gregate information especially keeping the associations between multiple events and prove it is useful in event extraction.</p><p>• We achieve the state-of-the-art performance on the widely used datasets for event extrac- tion using the proposed model with GCNs and self-attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Generally, event extraction can be cast as a multi- class classification problem deciding whether each word in the sentence forms a part of event trig- ger candidate and whether each entity in the sen- tence plays a particular role in the event triggered by the candidate triggers. There are two main approaches to event extraction: (i) the joint ap- proach that extracts event triggers and arguments simultaneously as a structured prediction problem, and (ii) the pipelined approach that first performs trigger prediction and then identifies arguments in separate stages. We follow the joint approach that can effectively avoid the propagated errors in the pipeline.</p><p>Additionally, we extract events in sentence- level mainly for three reasons. Firstly, in our in- • The positional embedding vector of w i : If w c is the current word, we encode the rela- tive distance i − c from w i to w c as a real- valued vector by looking up the randomly initialized position embedding table <ref type="bibr">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b16">Liu et al., 2017;</ref><ref type="bibr">Nguyen and Grishman, 2018</ref>).</p><p>• The entity type label embedding vector of w i : Similarly to the POS-tagging label em- bedding vector of w i , we annotate the entity mentions in a sentence using BIO annotation schema and transform the entity type labels to real-valued vectors by looking up the embed- ding table. It should be noticed that we use the whole entity extent in ACE 2005 dataset which contains overlapping entity mentions and we sum all the possible entity type label embedding vectors for each token.</p><p>The transformation from the token w i to the vector x i essentially converts the input sentence W into a sequence of real-valued vectors X = (x 1 , x 2 , ..., x n ), which will be feed into later mod- ules to learn more effective representations for event extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Syntactic Graph Convolution Network</head><p>Considering an undirected graph G = (V, E) as the syntactic parsing tree for sentence W , where V = v 1 , v 2 , ..., v n (|V| = n) and E are sets of nodes and edges, respectively. In V, each v i is the node representing token w i in W . Each edge (v i , v j ) ∈ E is a directed syn- tactic arc from token w i to token w j , with the type label K(w i , w j ). Additionally, to allow in- formation to flow against the direction, we also add reversed edge (v j , v i ) with the type label K ′ (w i , w j ). Following <ref type="bibr" target="#b9">Kipf and Welling (2016)</ref>, we also add all the self-loops, i.e., (v i , v i ) for any v i ∈ V. For example, in the dependency parsing tree shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there are four arcs in the subgraph with only two nodes "killed" and "witnesses": the dependency arc with the type label K("killed", "witnesses") = nmod, the revresed dependency arc with the additional type label K("witnesses", "killed") = nmod ′ , and the two self-loops of "killed" and "wit- nesses" with type label K("killed", "killed") = K("witnesses", "witnesses") = loop.</p><p>Therefore, in the k-th layer of syntactic graph convolution network module, we can calculate the graph convolution vector h (k+1) v for node v ∈ V by:</p><formula xml:id="formula_0">h (k+1) v = f ( u∈N (v) (W (k) K(u,v) h (k) u + b (k) K(u,v) )) (1)</formula><p>where K(u, v) indicates the type label of the edge</p><formula xml:id="formula_1">(u, v); W (k) K(u,v) and b (k) K(u,v)</formula><p>are the weight matrix and the bias for the certain type label K(u, v), re- spectively; N (v) is the set of neighbors of v in- cluding v (because of the self-loop); f is the ac- tivation function. Moreover, we use the output of the word representation module x i to initialize the node representation h 0 v i of the first layer of GCNs. After applying the above two changes, the num- ber of predefined directed arc type label (let us say, N ) will be doubled (to <ref type="bibr">2N + 1</ref>  <ref type="bibr" target="#b10">Klein and Manning, 2003)</ref> to gen- erate the arcs in dependency parsing trees for sen- tences as the shortcut arcs. The current representa-tion contains approximately 50 different grammat- ical relations, which is too high for the parameter number of a single layer of GCN and not compat- ible with the existing training data scale. To re- duce the parameter numbers, following Marcheg- giani and Titov (2017), we modify the definition of type label K(w i , w j ) to:</p><formula xml:id="formula_2">K(w i , w j ) =    along, (v i , v j ) ∈ E rev, i! = j&amp;(v j , v i ) ∈ E loop, i == j<label>(2)</label></formula><p>where the new K(w i , w j ) only have three type la- bels.</p><p>As not all types of edges are equally informative for the downstream task, moreover, there are also noises in the generated syntactic parsing struc- tures; we apply gates on the edges to weight their individual importances. Inspired by <ref type="bibr" target="#b1">Dauphin et al. (2017)</ref>; <ref type="bibr">Marcheggiani and Titov (2017)</ref>, we calcu- late a weight g</p><formula xml:id="formula_3">(k)</formula><p>u,v for each edge (u, v) indicating the importance for event extraction by:</p><formula xml:id="formula_4">g (k) u,v = σ(h (k) u V (k) K(u,v) + d (k) K(u,v) )<label>(3)</label></formula><p>where σ is the logistic sigmoid function, V</p><formula xml:id="formula_5">(k) K(u,v)</formula><p>and d</p><p>K(u,v) are the weight matrix and the bias of the gate. With this additional gating mechanism, the final syntactic GCN computation is formulated as</p><formula xml:id="formula_7">h (k+1) v = f ( u∈N (v) g (k) u,v (W (k) K(u,v) h (k) u + b (k) K(u,v) ))<label>(4)</label></formula><p>As stacking k layers of GCNs can model in- formation in k hops, and sometimes the length of shortcut path between two triggers is less than k, to avoid information over-propagating, we adapt highway units ( <ref type="bibr">Srivastava et al., 2015)</ref>, which al- low unimpeded information flowing across stack- ing GCN layers. Typically, highway layers con- duct nonlinear transformation as:</p><formula xml:id="formula_8">t = σ(W T h k v + b T )<label>(5)</label></formula><formula xml:id="formula_9">h (k+1) v = h (k+1) v +t⊙g(W H h k v +b H )+(1−t)⊙h k v<label>(6)</label></formula><p>where σ is the sigmoid function; ⊙ is the element- wise product operation; g is a nonlinear activation function; t is called transform gate and (1 − t) is called carry gate. Therefore, the input of the k-th GCN layers should be h (k) instead of h (k) .</p><p>The GCNs are designed to capture the depen- dencies between shortcut arcs, while the layer number of GCNs limits the ability to capture lo- cal graph information. However, in this cases, we find that leveraging local sequential context will help to expand the information flow without in- creasing the layer number of GCNs, which means LSTMs and GCNs maybe complementary. There- fore, instead of feeding the word representation X = (x 1 , x 2 , ..., x n ) into the first GCN layer, we follow <ref type="bibr">Marcheggiani and Titov (2017)</ref>, apply Bidirectional LSTM (Bi-LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) to encode the the word repre- sentation X as:</p><formula xml:id="formula_10">− → p t = −−−−→ LST M ( − → p t−1 , x t ) (7) ← − p t = ←−−−− LST M ( ← − p t−1 , x t )<label>(8)</label></formula><p>and the input of t-th token to GCNs is</p><formula xml:id="formula_11">x t = [ − → p t , ← − p t ], where [, ]</formula><p>is the concatenation opera- tion. The Bi-LSTM adaptively accumulates and abstracts the context for each token in the sen- tence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-Attention Trigger Classification</head><p>When taking each token as the current word, we get the representation D from all tokens calcu- lated by GCNs. Traditional event extraction sys- tems often use max-pooling or its amelioration to aggregate information to each position. However, the max-pooling aggregation mechanisms tend to produce similar results after GCN modules in our framework. For example, if we get the aggregated vector Ag i at each position i by this max-pooling mechanism Ag i = max pooling n j=1 (H j ) with the GCNs output {H j |j = 1, ..., n} in which n is the sentence length, and the vector Ag i is all the same at each position. Besides, predicting a trigger la- bel for a token should take other possible trigger candidates into consideration. To capture the asso- ciations between triggers in a sentence, we design a self-attention mechanism to aggregate informa- tion especially keeping the associations between multiple events. Given the current token w i , the self-attention score vector and the context vector at position i are calculated as:</p><formula xml:id="formula_12">score = norm(exp(W 2 f (W 1 D +b 1 )+b 2 )) (9) C i = [ n j=1,j!=i score j * D j , D i ]<label>(10)</label></formula><p>where norm means the normalization operation. Then we feed the context vector C i into a fully- connected network to predict the trigger label in BIO annotation schema as:</p><formula xml:id="formula_13">C i = f (W c C i + b c )<label>(11)</label></formula><formula xml:id="formula_14">y t i = sof tmax(W t C i + b t )<label>(12)</label></formula><p>where f is a non-linear activation and y t i is the final output of the i-th trigger label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Argument Classification</head><p>When we have extracted an entire trigger candi- date, which is meeting an O label after an I-Type label or a B-Type label, we use the aggregated con- text vector C to perform argument classification on the entity list in the sentence. For each entity-trigger pair, as both the entity and the trigger candidate are likely to be a subse- quence of tokens, we aggregate the context vectors of subsequences to trigger candidate vector T i and entity vector E j by average pooling along the se- quence length dimension. Then we concatenate them together and feed into a fully-connected net- work to predict the argument role as:</p><formula xml:id="formula_15">y a ij = sof tmax(W a [T i , E j ] + b a )<label>(13)</label></formula><p>where y a ij is the final output of which role the j- th entity plays in the event triggered by the i-th trigger candidate. When training our framework, if the trigger can- didate that we focus on is not a correct trigger, we set all the golden argument labels concerning the trigger candidate to OTHER (not any roles). With this setting, the labels of the trigger candidate will be further adjusted to reach a reasonable probabil- ity distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Biased Loss Function</head><p>In order to train the networks, we minimize the joint negative log-likelihood loss function. Due to the data sparsity in the ACE 2005 dataset, we adapt our joint negative log-likelihood loss func- tion by adding a bias item as:</p><formula xml:id="formula_16">J(θ) = − N p=1 ( np i=1 I(y t i )log(p(y t i |θ)) +β tp i=1 ep j=1 log(p(y a ij |θ))) (14)</formula><p>where N is the number of sentences in training corpus; n p , t p and e p are the number of tokens, extracted trigger candidates and entities of the p-th sentence; I(y t i ) is an indicating function, if y t i is not O, it outputs a fixed positive floating number α bigger than one, otherwise one; β is also a floating number as a hyper-parameter like α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset, Resources and Evaluation Metric</head><p>We evaluate our JMEE framework on the ACE 2005 dataset. The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event de- tection and 37 categories in argument extraction. To comply with previous work, we use the same data split as the previous work <ref type="bibr" target="#b7">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b13">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b11">Li et al., 2013;</ref><ref type="bibr" target="#b0">Chen et al., 2015;</ref><ref type="bibr">Liu et al., 2016b;</ref><ref type="bibr">Yang and Mitchell, 2016;</ref><ref type="bibr">Nguyen et al., 2016;</ref><ref type="bibr">Sha et al., 2018)</ref>. This data split includes 40 newswire arti- cles (881 sentences) for the test set, 30 other doc- uments (1087 sentences) for the development set and 529 remaining documents (21,090 sentences) for the training set.</p><p>We deploy the Stanford CoreNLP toolkit 5 to preprocess the data, including tokenizing, sen- tence splitting, pos-tagging and generating depen- dency parsing trees.</p><p>Also, we follow the criteria of the previous work ( <ref type="bibr" target="#b7">Ji and Grishman, 2008;</ref><ref type="bibr" target="#b13">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b11">Li et al., 2013;</ref><ref type="bibr" target="#b0">Chen et al., 2015;</ref><ref type="bibr">Liu et al., 2016b;</ref><ref type="bibr">Yang and Mitchell, 2016;</ref><ref type="bibr">Nguyen et al., 2016;</ref><ref type="bibr">Sha et al., 2018)</ref> to judge the correctness of the pre- dicted event mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Setting</head><p>For all the experiments below, in the word rep- resentation module, we use 300 dimensions for the embeddings and 50 dimensions for the rest</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trigger</head><p>Trigger three embeddings including pos-tagging embed- ding, positional embedding and entity type em- bedding. In the syntactic GCN module, we use a three-layer GCN, a one-layer Bi-LSTM with 220 hidden units, self-attention with 300 hidden units and 200 hidden units for the rest transformation. We also set dropout rate to 0.5 and L2-norm to 1e- 8. The batch size in our experiments is 32, and we utilize a maximum length n = 50 of sentences in the experiments by padding shorter sentences and cutting off longer ones. These hyperparam- eters are either randomly searched or chosen by experiences when tuning in the development set. We use ReLU (Glorot et al., 2011) as our non- linear activate function. We apply the stochastic gradient descent algorithm with mini-batches and the AdaDelta update rule <ref type="bibr">(Zeiler, 2012)</ref>. The gra- dients are computed using back-propagation. Dur- ing training, besides the weight matrices, we also fine-tune all the embedding tables.</p><formula xml:id="formula_17">Argument Argument Identification (%) Classification (%) Identification (%) Role (%) P R F 1 P R F 1 P R F 1 P R F 1 Cross-Event N/A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Performance</head><p>We compare our performance with the following state-of-the-art methods: <ref type="table">Table 1</ref> shows the overall performance com- paring to the above state-of-the-art methods with golden-standard entities. From the table, we can see that our JMEE framework achieves the best F 1 scores for both trigger classification and argument- related subtasks among all the compared methods. There is a significant gain with the trigger classi- fication and argument role labeling performances, which is 2% higher over the best-reported mod- els. These results demonstrate the effectivenesses of our method to incorporate with the graph con- volution and syntactic shortcut arcs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effect on Extracting Multiple Events</head><p>To evaluate the effect of our framework for allevi- ating the multiple events phenomenon, we divide the test data into two parts (1/1 and 1/N) follow- ing <ref type="bibr">Nguyen et al. (2016)</ref>; <ref type="bibr" target="#b0">Chen et al. (2015)</ref> and perform evaluations separately. 1/1 means that one sentence only has one trigger or one argu- ment plays a role in one sentence; otherwise, 1/N is used. <ref type="table">Table 2</ref> illustrates the performance (F 1 scores) of JRNN ( <ref type="bibr">Nguyen et al., 2016)</ref>, <ref type="bibr">DMCNN (Chen et al., 2015)</ref>, the two baseline model <ref type="bibr">Embedding+T and CNN in Chen et al. (2015)</ref> and our framework in trigger classification subtask and argument role labeling subatsk. Embedding+T uses word embedding vectors and the traditional sentence-level features in <ref type="bibr" target="#b11">Li et al. (2013)</ref>, while ing the multiple-event phenomenon. In our frame- work, we introduce syntactic shortcut arcs to en- hance information flow and adapt the graph convo- lution network to capture the enhanced representa- tion. Then a self-attention aggregation mechanism is applied to aggregate the associations between events. Besides, we jointly extract event triggers and arguments by optimizing a biased loss func- tion due to the imbalances in the dataset. The ex- periment results demonstrate the effectiveness of our proposed framework. In the future, we plan to exploit the information of one argument which plays different roles in various events to do better in event extraction task.</p><p>Proceedings of the 55th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 1789- 1798. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of dependency parsing result produced by Stanford CoreNLP. There are two events in the sentence: a Die event triggered by the word killed with four arguments in red and an Attack event triggered by the word barrage with three arguments in blue. The red dotted arc is the shortcut path consisting of three directed arcs from trigger killed to another trigger barrage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: The architecture of our jointly multiple events extraction framework.</figDesc></figure>

			<note place="foot" n="2"> In a shortcut path which consists of existing arcs, some arcs may reverse their directions. 3 The length of the longest path in a tree is always no more than the sequential length consisting of the same number of nodes, which means even in the worst cases, the shortcut path will not perform worse than sequential modeling.</note>

			<note place="foot" n="5"> http://stanfordnlp.github.io/CoreNLP/</note>

			<note place="foot" n="1"> Cross-Event is proposed by Liao and Grishman (2010), which uses document level information to improve the performance of event extraction; 2 JointBeam is the method proposed by Li et al. (2013), which extracts events based on structure prediction by manually designed features; 3 DMCNN is proposed by Chen et al. (2015), which uses dynamic multi-pooling to keep multiple events&apos; information; 4 PSL is proposed by Liu et al. (2016b), which uses a probabilistic reasoning model to classify events by using latent and global information to encode the associations between events; 5 JRNN is proposed by Nguyen et al. (2016), which uses a bidirectional RNN and manually designed features to jointly extract event triggers and arguments. 6 dbRNN is proposed by Sha et al. (2018), which adds dependency bridges over Bi-LSTM for event extraction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yansong Feng, Ying Zeng, Xiaochi Wei, Qian Liu and Changsen Yuan for their insightful comments and suggestions. We also very appreciate the comments from anony-mous reviewers which will help further improve our work. This work is supported by National Nat-ural Science Foundation of China <ref type="bibr">(No. 61751201 and No. 61602490)</ref> and National Key R&amp;D Plan (No. 2017YFB0803302).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A languageindependent neural network for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nyu&apos;s english ace 2005 system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Satisfiability</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1927" to="1938" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Min</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">roceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identifying civilians killed by police with distantly supervised entity-event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><surname>Handler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pinkham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cara</forename><surname>Magliozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Mcduffie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1547" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving event detection with abstract meaning representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Computing News Storylines</title>
		<meeting>the 1st Workshop on Computing News Storylines</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="11" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event detection via gated multilingual attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4865" to="4872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Leveraging framenet to improve automatic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2134" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
