<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Content Selection in Deep Learning Models of Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College Park Microsoft Research</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College Park Microsoft Research</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College Park Microsoft Research</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College Park Microsoft Research</orgName>
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>New York City</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Content Selection in Deep Learning Models of Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Content selection is a central component in many natural language generation tasks, where, given a generation goal, the system must determine which information should be expressed in the output text <ref type="bibr">(Gatt and Krahmer, 2018)</ref>. In summarization, content selection is usually accomplished through sentence (and, occasionally, phrase) extraction. Despite being a key component of both extrac- tive and abstractive summarization systems, it is is not well understood how deep learning models perform content selection with only word and sen- tence embedding based features as input. Non- neural network approaches often use frequency and information theoretic measures as proxies for content salience <ref type="bibr">(Hong and Nenkova, 2014</ref>), but these are not explicitly used in most neural net- work summarization systems.</p><p>In this paper, we seek to better understand how deep learning models of summarization perform content selection across multiple domains ( § 4): news, personal stories, meetings, and medical articles (for which we collect a new corpus). <ref type="bibr">1</ref> We analyze several recent sentence extractive neural network architectures, specifically considering the design choices for sentence encoders ( § 3.1) and sentence extractors <ref type="bibr">( § 3.2)</ref>. We compare Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN) based sentence representations to the simpler approach of word embedding aver- aging to understand the gains derived from more sophisticated architectures. We also question the necessity of auto-regressive sentence extraction (i.e. using previous predictions to inform future predictions), which previous approaches have used ( § 2), and propose two alternative models that extract sentences independently.</p><p>Our main results ( § 5) reveal:</p><p>1. Sentence position bias dominates the learn- ing signal for news summarization, though not for other domains. <ref type="bibr">2</ref> Summary quality for news is only slightly degraded when con- tent words are omitted from sentence embed- dings. 2. Word embedding averaging is as good or bet- ter than either RNNs or CNNs for sentence embedding across all domains. 3. Pre-trained word embeddings are as good, or better than, learned embeddings in five of six datasets. 4. Non auto-regressive sentence extraction per- forms as good or better than auto-regressive extraction in all domains.</p><p>Taken together, these and other results in the pa- per suggest that we are over-estimating the abil-ity of deep learning models to learn robust and meaningful content features for summarization. In one sense, this might lessen the burden of apply- ing neural network models of content to other do- mains; one really just needs in-domain word em- beddings. However, if we want to learn something other than where the start of the article is, we will need to design other means of sentence represen- tation, and possibly external knowledge represen- tations, better suited to the summarization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The introduction of the CNN-DailyMail corpus by <ref type="bibr">Hermann et al. (2015)</ref> allowed for the applica- tion of large-scale training of deep learning mod- els for summarization. <ref type="bibr">Cheng and Lapata (2016)</ref> developed a sentence extractive model that uses a word level CNN to encode sentences and a sen- tence level sequence-to-sequence model to predict which sentences to include in the summary. Sub- sequently, <ref type="bibr">Nallapati et al. (2017)</ref> proposed a dif- ferent model using word-level bidirectional RNNs along with a sentence level bidirectional RNN for predicting which sentences should be extracted. Their sentence extractor creates representations of the whole document and computes separate scores for salience, novelty, and location. These works represent the state-of-the-art for deep learning- based extractive summarization and we analyze them further in this paper.</p><p>Other recent neural network approaches in- clude, <ref type="bibr">Yasunaga et al. (2017)</ref>, who learn a graph- convolutional network (GCN) for multi-document summarization. They do not closely examine the choice of sentence encoder, which is one of the focuses of the present paper; rather, they study the best choice of graph structure for the GCN, which is orthogonal to this work.</p><p>Non-neural network learning-based approaches have also been applied to summarization. Typi- cally they involve learning n-gram feature weights in linear models along with other non-lexical word or structural features <ref type="bibr" target="#b2">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr">Sipos et al., 2012;</ref><ref type="bibr">Durrett et al., 2016)</ref>. In this paper, we study representation learning in neural networks that can capture more complex word level feature interactions and whose dense representations are more compatible with current practices in NLP.</p><p>The previously mentioned works have focused on news summarization. To further understand the content selection process, we also explore other domains of summarization. In particular, we ex- plore personal narrative summarization based on stories shared on Reddit ( <ref type="bibr">Ouyang et al., 2017)</ref>, workplace meeting summarization ( <ref type="bibr">Carletta et al., 2005)</ref>, and medical journal article summarization ( <ref type="bibr">Mishra et al., 2014</ref>).</p><p>While most work on these summarization tasks often exploit domain-specific features (e.g. speaker identification in meeting summarization <ref type="bibr">(Galley, 2006;</ref><ref type="bibr">Gillick et al., 2009)</ref>), we purpose- fully avoid such features in this work in order to understand the extent to which deep learning mod- els can perform content selection using only sur- face lexical features. Summarization of academic literature (including medical journals), has long been a research topic in NLP ( <ref type="bibr">Kupiec et al., 1995;</ref><ref type="bibr">Elhadad et al., 2005</ref>), but most approaches have explored facet-based summarization ( <ref type="bibr">Jaidka et al., 2017)</ref>, which is not the focus of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>The goal of extractive text summarization is to se- lect a subset of a document's text to use as a sum- mary, i.e. a short gist or excerpt of the central con- tent. Typically, we impose a budget on the length of the summary in either words or bytes. In this work, we focus on sentence extractive summariza- tion, where the basic unit of extraction is a sen- tence and impose a word limit as the budget.</p><p>We model the sentence extraction task as a se- quence tagging problem, following <ref type="bibr">(Conroy and O'Leary, 2001)</ref>. Specifically, given a document containing n sentences s 1 , . . . , s n we generate a summary by predicting a corresponding label se- quence y 1 , . . . , y n ∈ {0, 1} n , where y i = 1 in- dicates the i-th sentence is to be included in the summary. Each sentence is itself a sequence of word embeddings s i = w</p><formula xml:id="formula_0">(i) 1 , . . . , w (i)</formula><p>|s i | where |s i | is the length of the sentence in words. The word budget c ∈ N enforces a constraint that the total summary word length n i=1 y i · |s i | ≤ c. For a typical deep learning model of extractive summarization there are two main design deci- sions: a) the choice of sentence encoder which maps each sentence s i to an embedding h i , and b) the choice of sentence extractor which maps a se- quence of sentence embeddings h = h 1 , . . . , h n to a sequence of extraction decisions y = y 1 , . . . , y n . </p><formula xml:id="formula_1">a) h1 h2 h3 y1 y2 y3 b) h1 h2 h3 − → h h1 h2 h3 ← − h y1 y2 y3 c) h1 h2 h3 h * h1 h2 y1 y2 y3 d) h1 h2 h3 y1 y2 y3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Encoders</head><p>We experiment with three architectures for map- ping sequences of word embeddings to a fixed length vector: averaging, RNNs, and CNNs. Hy- perparameter settings and implementation details can be found in Appendix A.</p><p>Averaging Encoder Under the averaging en- coder, a sentence embedding h is simply the aver- age of its word embeddings, i.e. h = 1 |s| |s| i=1 w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN Encoder</head><p>When using the RNN sentence encoder, a sentence embedding is the concatena- tion of the final output states of a forward and backward RNN over the sentence's word embed- dings. We use a Gated Recurrent Unit (GRU) for the RNN cell ( <ref type="bibr">Chung et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Encoder</head><p>The CNN sentence encoder uses a series of convolutional feature maps to encode each sentence. This encoder is similar to the con- volutional architecture of <ref type="bibr">Kim (2014)</ref> used for text classification tasks and performs a series of "one-dimensional" convolutions over word em- beddings. The final sentence embedding h is a concatenation of all the convolutional filter out- puts after max pooling over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Extractors</head><p>Sentence extractors take sentence embeddings h 1:n and produce an extract y 1:n . The sentence extractor is essentially a discriminative classi- fier p(y 1:n |h 1:n ). Previous neural network ap- proaches to sentence extraction have assumed an auto-regressive model, leading to a semi- Markovian factorization of the extractor probabil- ities p(y 1:n |h) = n i=1 p(y i |y &lt;i , h), where each prediction y i is dependent on all previous y j for all j &lt; i. We compare two such models pro- posed by <ref type="bibr">Cheng and Lapata (2016)</ref> and <ref type="bibr">Nallapati et al. (2017)</ref>. A simpler approach that does not allow interaction among the y 1:n is to model p(y 1:n |h) = n i=1 p(y i |h), which we explore in two proposed extractor models that we refer to as the RNN and Seq2Seq extractors. Implementation details for all extractors are in Appendix B.</p><p>Previously Proposed Sentence Extractors We consider two recent state-of-the-art extractors.</p><p>The first, proposed by <ref type="bibr">Cheng and Lapata (2016)</ref>, is built around a sequence-to-sequence model. First, each sentence embedding 3 is fed into an encoder side RNN, with the final encoder state passed to the first step of the decoder RNN. On the decoder side, the same sentence embeddings are fed as input to the decoder and decoder out- puts are used to predict each y i . The decoder input is weighted by the previous extraction probability, inducing the dependence of y i on y &lt;i . <ref type="figure" target="#fig_0">See Fig- ure 1</ref>.c for a graphical layout of the extractor. <ref type="bibr">Nallapati et al. (2017)</ref> proposed a sentence ex- tractor, which we refer to as the SummaRunner Extractor, that factorizes the extraction probabil- ity into contributions from different sources. First, a bidirectional RNN is run over the sentence em-beddings <ref type="bibr">4</ref> and the output is concatenated. A repre- sentation of the whole document is made by aver- aging the RNN output. A summary representation is also constructed by taking the sum of the pre- vious RNN outputs weighted by their extraction probabilities. Extraction predictions are made us- ing the RNN output at the i-th step, the document representation, and i-th version of the summary representation, along with factors for sentence lo- cation in the document. The use of the iteratively constructed summary representation creates a de- pendence of y i on all y &lt;i . See <ref type="figure" target="#fig_0">Figure 1</ref>.d for a graphical layout.</p><p>Proposed Sentence Extractors We propose two sentence extractor models that make a stronger conditional independence assumption p(y|h) = n i=1 p(y i |h), essentially making inde- pendent predictions conditioned on h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN Extractor</head><p>Our first proposed model is a very simple bidirectional RNN based tagging model. As in the RNN sentence encoder we use a GRU cell. The forward and backward outputs of each sentence are passed through a multi-layer perceptron with a logsitic sigmoid output to pre- dict the probability of extracting each sentence. See <ref type="figure" target="#fig_0">Figure 1</ref>.a for a graphical layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Seq</head><p>Extractor One shortcoming of the RNN extractor is that long range information from one end of the document may not easily be able to affect extraction probabilities of sentences at the other end. Our second proposed model, the Seq2Seq extractor mitigates this problem with an attention mechanism commonly used for neural machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) and abstractive summarization ( <ref type="bibr">See et al., 2017</ref>). The sentence embeddings are first encoded by a bidi- rectional GRU. A separate decoder GRU trans- forms each sentence into a query vector which attends to the encoder output. The attention weighted encoder output and the decoder GRU output are concatenated and fed into a multi-layer perceptron to compute the extraction probability. See <ref type="figure" target="#fig_0">Figure 1</ref>.b for a graphical layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Train <ref type="table" target="#tab_0">Valid  Test Refs   CNN/DM 287,113 13,368 11,490  1  NYT  44,382  5,523  6,495 1.93  DUC  516  91  657  2  Reddit  404  24  48  2  AMI  98  19  20  1  PubMed  21,250</ref> 1,250 2,500 1  PubMed We created a corpus of 25,000 ran- domly sampled medical journal articles from the PubMed Open Access Subset <ref type="bibr">6</ref> . We only included articles if they were at least 1000 words long and had an abstract of at least 50 words in length. We used the article abstracts as the ground truth hu- man summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ground Truth Extract Summaries</head><p>Since we do not typically have ground truth ex- tract summaries from which to create the labels y i , we construct gold label sequences by greedily optimizing ROUGE-1, using the algorithm in Ap- pendix C. We choose to optimize for ROUGE-1 rather than ROUGE-2 similarly to other optimiza- tion based approaches to summarization <ref type="bibr">(Sipos et al., 2012;</ref><ref type="bibr">Durrett et al., 2016)</ref> which found this to be the easier target to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate summary quality using ROUGE-2 recall (Lin, 2004); ROUGE-1 and ROUGE-LCS trend similarity in our experiments. We use tar- get word lengths of 100 words for news, and 75, 290, and 200 for Reddit, AMI, and PubMed respectively. We also evaluate using METEOR <ref type="bibr">(Denkowski and Lavie, 2014</ref>). 7 Summaries are generated by extracting the top ranked sentences by model probability p(y i = 1|y &lt;i , h), stopping when the word budget is met or exceeded. We estimate statistical significance by averaging each document level score over the five random initial- izations. We then test the difference between the best system on each dataset and all other systems using the approximate randomization test <ref type="bibr">(Riezler and Maxwell, 2005</ref>) with the Bonferroni correc- tion for multiple comparisons, testing for signifi- cance at the 0.05 level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training</head><p>We train all models to minimize the weighted neg- ative log-likelihood</p><formula xml:id="formula_2">L = − s,y∈D h=enc(s) n i=1 ω(y i ) log p (y i |y &lt;i , h) Ext.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emb. CNN/DM NYT DUC Reddit AMI PubMed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Seq</head><p>Fixed 25.6 35.7 22.8 13.6 5.5 17.7 Learn 25.3 (0.3) 35.7 (0.0) 22.9 (-0.1) 13.8 (-0.2) 5.8 <ref type="bibr">(-0.3</ref>   <ref type="table">Table 4</ref>: ROUGE-2 recall after removing nouns, verbs, adjectives/adverbs, and function words. Ablations are performed using the averaging sentence encoder and the RNN extractor. Bold indicates best performing system. † indicates significant difference with the non-ablated system. Difference in score from all words shown in paren- thesis.</p><p>over the training data D using stochastic gradient descent with the ADAM optimizer ( <ref type="bibr">Kingma and Ba, 2014)</ref>. ω(0) = 1 and ω(1) = N 0 /N 1 where N y is the number of training examples with label y. We trained for a maximum of 50 epochs and the best model was selected with early stopping on the validation set according to ROUGE-2. Each epoch constitutes a full pass through the dataset. The av- erage stopping epoch was: CNN-DailyMail, 16.2; NYT, <ref type="bibr">21.36; DUC, 37.11; Reddit, 36.59; AMI, 19.58; PubMed, 19.84</ref>. All experiments were re- peated with five random initializations. Unless specified, word embeddings were initialized using pretrained GloVe embeddings ( <ref type="bibr">Pennington et al., 2014</ref>) and we did not update them during training. Unknown words were mapped to a zero embed- ding. See Appendix D for more optimization and training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>Lead As a baseline we include the lead sum- mary, i.e. taking the first x words of the docu- ment as summary, where x is the target summary length for each dataset (see the first paragraph of § 5). While incredibly simple, this method is still a competitive baseline for single document summa- rization, especially on newswire.</p><p>Oracle To measure the performance ceiling, we show the ROUGE/METEOR scores using the ex- tractive summary which results from greedily op- timizing ROUGE-1. I.e., if we had clairvoyant knowledge of the human reference summary, the oracle system achieves the (approximate) maxi- mum possible ROUGE scores. See Appendix C for a detailed description of the oracle algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The results of our main experiment comparing the different extractors/encoders are shown in <ref type="table" target="#tab_1">Table 2</ref>. Overall, we find no major advantage when us- ing the CNN and RNN sentence encoders over the averaging encoder. The best performing en- coder/extractor pair either uses the averaging en- coder (five out of six datasets) or the differences are not statistically significant. When looking at extractors, the Seq2Seq extrac- tor is either part of the best performing system (three out of six datasets) or is not statistically dis- tinguishable from the best extractor.</p><p>Overall, on the news and medical journal do- mains, the differences are quite small with the dif- Ext.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Order</head><p>CNN/DM NYT DUC Reddit AMI PubMed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq2Seq</head><p>In-Order 25.6 35.7 22.8 13.6 5.5 17.7 Shuffled 21.7 (3.9) 25.6 (10.1) 21.2 (1.6) 13.5 (0.1) 6.0 (-0.5) 14.9 (2.8) <ref type="table">Table 5</ref>: ROUGE-2 recall using models trained on in-order and shuffled documents. Extractor uses the averag- ing sentence encoder. When both in-order and shuffled settings are bolded, there is no signifcant performance difference. Difference in scores shown in parenthesis.</p><p>Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil Defense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas. The storm was approaching from the southeast with sustained winds of 75 mph gusting to 92 mph. An estimated 100,000 people live in the province, including 70,000 in the city of Barahona, about 125 miles west of Santo Domingo. On Saturday, Hurricane Florence was downgraded to a tropical storm and its remnants pushed inland from the U.S. Gulf Coast. Tropical Storm Gilbert formed in the east- ern Caribbean and strengthened into a hurricane Saturday night.</p><p>Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil Defense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas. The storm was approaching from the southeast with sustained winds of 75 mph gusting to 92 mph. An esti- mated 100,000 people live in the province, including 70,000 in the city of Barahona, about 125 miles west of Santo Domingo. Tropical Storm Gilbert formed in the eastern Caribbean and strengthened into a hurricane Saturday night. Strong winds associated with the Gilbert brought coastal flooding, strong southeast winds and up to 12 feet feet to Puerto Rico's south coast. <ref type="table">Table 6</ref>: Example output of Seq2Seq extractor (left) and <ref type="bibr">Cheng &amp; Lapata Extractor (right)</ref>. This is a typical example, where only one sentence is different between the two (shown in bold).</p><p>ferences between worst and best systems on the CNN/DM dataset spanning only .56 of a ROUGE point. While there is more performance variability in the Reddit and AMI data, there is less distinc- tion among systems: no differences are significant on Reddit and every extractor has at least one con- figuration that is indistinguishable from the best system on the AMI corpus. This is probably due to the small test size of these datasets.</p><p>Word Embedding Learning Given that learn- ing a sentence encoder (averaging has no learned parameters) does not yield significant improve- ment, it is natural to consider whether learning word embeddings is also necessary. In <ref type="table" target="#tab_3">Table 3</ref> we compare the performance of different extrac- tors using the averaging encoder, when the word embeddings are held fixed or learned during train- ing. In both cases, word embeddings are initial- ized with GloVe embeddings trained on a combi- nation of Gigaword and Wikipedia. When learn- ing embeddings, words occurring fewer than three times in the training data are mapped to an un- known token (with learned embedding).</p><p>In all but one case, fixed embeddings are as good or better than the learned embeddings. This is a somewhat surprising finding on the CNN/DM data since it is reasonably large, and learning em- beddings should give the models more flexibility to identify important word features. 8 This sug- <ref type="bibr">8</ref> The AMI corpus is an exception here where learning gests that we cannot extract much generalizable learning signal from the content other than what is already present from initialization. Even on PubMed, where the language is quite different from the news/Wikipedia articles the GloVe em- beddings were trained on, learning leads to signif- icantly worse results.</p><p>POS Tag Ablation It is also not well explored what word features are being used by the encoders. To understand which classes of words were most important we ran an ablation study, selectively removing nouns, verbs (including participles and auxiliaries), adjectives &amp; adverbs, and function words (adpositions, determiners, conjunctions). All datasets were automatically tagged using the spaCy part-of-speech (POS) tagger <ref type="bibr">9</ref> . The em- beddings of removed words were replaced with a zero vector, preserving the order and position of the non-ablated words in the sentence. Abla- tions were performed on training, validation, and test partitions, using the RNN extractor with av- eraging encoder. <ref type="table">Table 4</ref> shows the results of the POS tag ablation experiments. While removing any word class from the representation generally hurts performance (with statistical significance), on the news domains, the absolute values of the does lead to small performance boosts, however, only in the Seq2Seq extractor is this diference significant; it is quite pos- sible that this is an artifact of the very small test set size.differences are quite small (.18 on CNN/DM, .41 on NYT, .3 on DUC) suggesting that the model's predictions are not overly dependent on any par- ticular word types. On the non-news datasets, the ablations have a larger effect (max differences are 1.89 on Reddit, 2.56 on AMI, and 1.3 on PubMed). Removing nouns leads to the largest drop on AMI and PubMed. Removing adjectives and adverbs leads to the largest drop on Reddit, suggesting the intensifiers and descriptive words are useful for identifying important content in personal narra- tives. Curiously, removing the function word POS class yields a significant improvement on DUC 2002 and AMI.</p><p>Document Shuffling Sentence position is a well known and powerful feature for news sum- marization <ref type="bibr">(Hong and Nenkova, 2014</ref>), owing to the intentional lead bias in the news article writ- ing <ref type="bibr">10</ref> ; it also explains the difficulty in beating the lead baseline for single-document summarization <ref type="bibr">(Nenkova, 2005;</ref><ref type="bibr" target="#b3">Brandow et al., 1999</ref>). In exam- ining the generated summaries, we found most of the selected sentences in the news domain came from the lead paragraph of the document. This is despite the fact that there is a long tail of sen- tence extractions from later in the document in the ground truth extract summaries (31%, 28.3%, and 11.4% of DUC, CNN/DM, and NYT training ex- tract labels come from the second half of the doc- ument). Because this lead bias is so strong, it is questionable whether the models are learning to identify important content or just find the start of the document. We conduct a sentence order exper- iment where each document's sentences are ran- domly shuffled during training. We then evalu- ate each model performance on the unshuffled test data, comparing to the model trained on unshuf- fled data; if the models trained on shuffled data drop in performance, then this indicates the lead bias is the relevant factor. <ref type="table">Table 5</ref> shows the results of the shuffling ex- periments. The news domains and PubMed suffer a significant drop in performance when the docu- ment order is shuffled. By comparison, there is no significant difference between the shuffled and in- order models on the Reddit domain, and shuffling actually improves performance on AMI. This sug- gest that position is being learned by the models in the news/journal article domain even when the model has no explicit position features, and that this feature is more important than either content or function words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Learning content selection for summarization in the news domain is severely inhibited by the lead bias. The summaries generated by all systems de- scribed here-the prior work and our proposed sim- plified models-are highly similar to each other and to the lead baseline. The Cheng &amp; Lapata and Seq2Seq extractors (using the averaging encoder) share 87.8% of output sentences on average on the CNN/DM data, with similar numbers for the other news domains (see <ref type="table">Table 6</ref> for a typical example). Also on CNN/DM, 58% of the Seq2Seq selected sentences also occur in the lead summary, with similar numbers for DUC, NYT, and Reddit. Shuf- fling reduces lead overlap to 35.2% but the overall system performance drops significantly; the mod- els are not able to identify important information without position.</p><p>The relative robustness of the news domain to part of speech ablation also suggests that models are mostly learning to recognize the stylistic fea- tures unique to the beginning of the article, and not the content. Additionally, the drop in performance when learning word embeddings on the news do- main suggests that word embeddings alone do not provide very generalizable content features com- pared to recognizing the lead.</p><p>The picture is rosier for non-news summariza- tion where part of speech ablation leads to larger performance differences and shuffling either does not inhibit content selection significantly or leads to modest gains. Learning better word-level rep- resentations on these domains will likely require much larger corpora, something which might re- main unlikely for personal stories and meetings.</p><p>The lack of distinction among sentence en- coders is interesting because it echoes findings in the generic sentence embedding literature where word embedding averaging is frustratingly diffi- cult to outperform <ref type="bibr">(Iyyer et al., 2015;</ref><ref type="bibr">Wieting et al., 2015;</ref><ref type="bibr" target="#b0">Arora et al., 2016;</ref><ref type="bibr">Wieting and Gimpel, 2017)</ref>. The inability to learn useful sen- tence representations is also borne out in the Sum- maRunner model, where there are explicit similar- ity computations between document or summary representations and sentence embeddings; these computations do not seem to add much to the per- formance as the Cheng &amp; Lapata and Seq2Seq models which lack these features generally per- form as well or better. Furthermore, the Cheng &amp; Lapata and SummaRunner extractors both con- struct a history of previous selection decisions to inform future choices but this does not seem to sig- nificantly improve performance over the Seq2Seq extractor (which does not). This suggests that we need to rethink or find novel forms of sentence representation for the summarization task.</p><p>A manual examination of the outputs revealed some interesting failure modes, although in gen- eral it was hard to discern clear patterns of be- haviour other than lead bias. On the news domain, the models consistently learned to ignore quoted material in the lead, as often the quotes provide color to the story but are unlikely to be included in the summary (e.g. "It was like somebody slugging a punching bag."). This behavior was most likely triggered by the presence of quotes, as the quote attributions, which were often tokenized as sep- arate sentences, would subsequently be included in the summary despite also not containing much information (e.g. Gil Clark of the National Hurri- cane Center said Thursday).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented an empirical study of deep learning based content selection algorithms for summarization. Our findings suggest such mod- els face stark limitations on their ability to learn robust features for this task and that more work is needed on sentence representation for summariza- tion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sentence extractor architectures: a) RNN, b) Seq2Seq, c) Cheng &amp; Lapata, and d) SummaRunner. The indicates attention. Green blocks repesent sentence encoder output and red blocks indicates learned "begin decoding" embeddings. Vertically stacked yellow and orange boxes indicate extractor encoder and decoder hidden states respectively. Horizontal orange and yellow blocks indicate multi-layer perceptrons. The purple blocks represent the document and summary state in the SummaRunner extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Sizes of the training, validation, test splits for 
each dataset and the average number of test set human 
reference summaries per document. 

ent biases within each domain can affect content 
selection. The corpora come from the news do-
main (CNN-DailyMail, New York Times, DUC), 
personal narratives domain (Reddit), workplace 
meetings (AMI), and medical journal articles 
(PubMed). See Table 1 for dataset statistics. 

CNN-DailyMail We use the preprocessing and 
training, validation, and test splits of See et al. 
(2017). This corpus is a mix of news on differ-
ent topics including politics, sports, and entertain-
ment. 

New York Times The New York Times (NYT) 
corpus (Sandhaus, 2008) contains two types of ab-
stracts for a subset of its articles. The first sum-
mary is an archival abstract and the second is a 
shorter online teaser meant to entice a viewer of 
the webpage to click to read more. From this col-
lection, we take all articles that have a concate-
nated summary length of at least 100 words. We 
create training, validation, and test splits by parti-
tioning on dates; we use the year 2005 as the val-
idation data, with training and test partitions in-
cluding documents before and after 2005 respec-
tively. 

DUC We use the single document summariza-
tion data from the 2001 and 2002 Document 
Understanding Conferences (DUC) (Over and 
Liggett, 2002). We split the 2001 data into train-
ing and validation splits and reserve the 2002 data 
for testing. 

AMI The AMI corpus (Carletta et al., 2005) is a 
collection of real and staged office meetings anno-
tated with text transcriptions, along with abstrac-
tive summaries. We use the prescribed splits. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>METEOR (M) and ROUGE-2 recall (R-2) results across all extractor/encoder pairs. Results that are 
statistically indistinguishable from the best system are shown in bold face. 

Reddit Ouyang et al. (2017) collected a corpus 
of personal stories shared on Reddit 5 along with 
multiple extractive and abstractive summaries. We 
randomly split this data using roughly three and 
five percent of the data validation and test respec-
tively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>ROUGE-2 recall across sentence extractors when using fixed pretrained embeddings or when embeddings 
are updated during training. In both cases embeddings are initialized with pretrained GloVe embeddings. All ex-
tractors use the averaging sentence encoder. When both learned and fixed settings are bolded, there is no signifcant 
performance difference. RNN extractor is omitted for space but is similar to Seq2Seq. Difference in scores shown 
in parenthesis. 

Ablation CNN/DM 
NYT 
DUC 
Reddit 
AMI 
PubMed 
all words 25.4 
34.7 
22.7 
11.4 
5.5 
17.0 
-nouns 
25.3  † (0.1) 34.3  † (0.4) 22.3  † (0.4) 10.3  † (1.1) 3.8  † (1.7) 15.7  † (1.3) 
-verbs 
25.3  † (0.1) 34.4  † (0.3) 22.4  † (0.3) 10.8 (0.6) 5.8 (-0.3) 16.6  † (0.4) 
-adj/adv 25.3  † (0.1) 34.4  † (0.3) 22.5 (0.2) 
9.5  † (1.9) 5.4 (0.1) 16.8  † (0.2) 
-function 25.2  † (0.2) 34.5  † (0.2) 22.9  † (-0.2) 10.3  † (1.1) 6.3  † (-0.8) 16.6  † (0.4) 

</table></figure>

			<note place="foot" n="1"> Data preprocessing and implementation code can be found here: https://github.com/kedz/nnsum/ tree/emnlp18-release 2 This is a known bias in news summarization (Nenkova, 2005).</note>

			<note place="foot" n="3"> Cheng and Lapata (2016) used an CNN sentence encoder with this extractor architecture; in this work we pair the Cheng &amp; Lapata extractor with several different encoders.</note>

			<note place="foot" n="4"> Datasets We perform our experiments across six corpora from varying domains to understand how differ4 Nallapati et al. (2017) use an RNN sentence encoder with this extractor architecture; in this work we pair the SummaRunner extractor with different encoders.</note>

			<note place="foot" n="5"> www.reddit.com 6 https://www.ncbi.nlm.nih.gov/pmc/ tools/openftlist/</note>

			<note place="foot" n="7"> We use the default settings for METEOR and use remove stopwords and no stemming options for ROUGE, keeping defaults for all other parameters.</note>

			<note place="foot" n="9"> https://github.com/explosion/spaCy</note>

			<note place="foot" n="10"> https://en.wikipedia.org/wiki/ Inverted_pyramid_(journalism)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>The authors would like to thank the anonymous re-viewers for their valuable feedback. Thanks goes out as well to Chris Hidey for his helpful com-ments. We would also like to thank Wen Xiao for identifying an error in the oracle results for the AMI corpus, which as since been corrected.</p><p>This research is based upon work supported in part by the Office of the Director of National Intel-ligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Gov-ernment is authorized to reproduce and distribute reprints for governmental purposes notwithstand-ing any copyright annotation therein.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material For: Content Selection in Deep Learning Models of Summarization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on Sentence Encoders</head><p>We use 200 dimenional word embeddings w i in all models. Dropout is applied to the embeddings during training. Wherever dropout is applied, the drop probability is .25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Details on RNN Encoder</head><p>Under the RNN encoder, a sentence embedding is defined as</p><p>where</p><p>and − −− → GRU amd ← −− − GRU indicate the forward and backward GRUs respectively, each with separate parame- ters. We use 300 dimensional hidden layers for each GRU. Dropout is applied to GRU during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details on CNN Encoder</head><p>The CNN encoder has hyperparameters associated with the window sizes K ⊂ N of the convolutional filters (i.e. the number of words associated with each convolution) and the number of feature maps M k ∈ N associated with each filter (i.e. the output dimension of each convolution). The CNN sentence embedding h is computed as follows:</p><p>where b (m,k) ∈ R and W (m,k) ∈ R k×n are learned bias and filter weight parameters respectively, and ReLU(x) = max(0, x) is the rectified linear unit activation. We use window sizes K = {1, 2, 3, 4, 5, 6} with corresponding feature maps sizes M 1 = 25, M 2 = 25, M 3 = 50, M 4 = 50, M 5 = 50, M 6 = 50, giving h a dimensionality of 250. Dropout is applied to the CNN output during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details on Sentence Extractors</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Details on RNN Extractor</head><p>where − −− → GRU and ← −− − GRU indicate the forward and backward GRUs respectively, and each have separate learned parameters; U, V and u, v are learned weight and bias parameters. The hidden layer size of the GRU is 300 for each direction and the MLP hidden layer size is 100. Dropout is applied to the GRUs and to a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Details on Seq2Seq Extrator</head><p>The final outputs of each encoder direction are passed to the first decoder steps; additionally, the first step of the decoder GRUs are learned "begin decoding" vectors − → q 0 and ← − q 0 (see <ref type="figure">Figure 1</ref>.b). Each GRU has separate learned parameters; U, V and u, v are learned weight and bias parameters. The hidden layer size of the GRU is 300 for each direction and MLP hidden layer size is 100. Dropout with drop probability .25 is applied to the GRU outputs and to a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Details on Cheng &amp; Lapata Extractor.</head><p>The basic architecture is a unidirectional sequence-to-sequence model defined as follows:</p><p>where h * is a learned "begin decoding" sentence embedding (see <ref type="figure">Figure 1</ref>.c). Each GRU has separate learned parameters; U, V and u, v are learned weight and bias parameters. Note in Equation 20 that the decoder side GRU input is the sentence embedding from the previous time step weighted by its probabilitiy of extraction (p i−1 ) from the previous step, inducing dependence of each output y i on all previous outputs y &lt;i . The hidden layer size of the GRU is 300 and the MLP hidden layer size is 100. Dropout with drop probability .25 is applied to the GRU outputs and to a i . Note that in the original paper, the Cheng &amp; Lapata extractor was paired with a CNN sentence encoder, but in this work we experiment with a variety of sentence encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Details on SummaRunner Extractor.</head><p>Like the RNN extractor it starts with a bidrectional GRU over the sentence embeddings</p><p>It then creates a representation of the whole document q by passing the averaged GRU output states through a fully connected layer:</p><p>A concatentation of the GRU outputs at each step are passed through a separate fully connected layer to create a sentence representation z i , where</p><p>The extraction probability is then determined by contributions from five sources:</p><p>where l i and r i are embeddings associated with the i-th sentence position and the quarter of the document containing sentence i respectively. In Equation 29, g i is an iterative summary representation computed as the sum of the previous z &lt;i weighted by their extraction probabilities,</p><p>Note that the presence of this term induces dependence of each y i to all y &lt;i similarly to the Cheng &amp; Lapata extractor.</p><p>The final extraction probability is the logistic sigmoid of the sum of these terms plus a bias,</p><p>The weight matrices W q , W z , W (con) , W (sal) , W (nov) , W (pos) , W (qrt) and bias terms b q , b z , and b are learned parameters; The GRUs have separate learned parameters. The hidden layer size of the GRU is 300 for each direction z i , q, and g i have 100 dimensions. The position and quartile embeddings are 16 dimensional each. Dropout with drop probability .25 is applied to the GRU outputs and to z i .</p><p>Note that in the original paper, the SummaRunner extractor was paired with an RNN sentence encoder, but in this work we experiment with a variety of sentence encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ground Truth Extract Summary Algorithm</head><p>Algorithm 1: ORACLEEXTRACTSUMMARYLABELS Data: input document sentences s 1 , s 2 , . . . , s n , human reference summary R, summary word budget c. 1 y i := 0 ∀i ∈ 1, . . . , n // Initialize extract labels to be 0. We use a learning rate of .0001 and a dropout rate of .25 for all dropout layers. We also employ gradient clipping (−5 &lt; θ &lt; 5). Weight matrix parameters are initialized using Xavier initialization with the normal distribution <ref type="bibr">(Glorot and Bengio, 2010)</ref> and bias terms are set to 0. We use a batch size of 32 for all datasets except AMI and PubMed, which are often longer and consume more memory, for which we use sizes two and four respectively. For the Cheng &amp; Lapata model, we train for half of the maximum epochs with teacher forcing, i.e. we set p i = 1 if y i = 1 in the gold data and 0 otherwise when computing the decoder input p i · h i ; we revert to the predicted model probability during the second half training.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic condensation of electronic publications by sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Brandow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Mitze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Rau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999-01" />
			<pubPlace>David C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">R</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nelson</surname></persName>
		</author>
		<title level="m">Advances in Automatic Text Summarization, chapter 19</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
