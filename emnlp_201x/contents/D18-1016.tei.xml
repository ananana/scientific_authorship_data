<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Preschool Lab</orgName>
								<orgName type="institution">Yitu Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Preschool Lab</orgName>
								<orgName type="institution">Yitu Tech</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shandong Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Preschool Lab</orgName>
								<orgName type="institution">Yitu Tech</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
							<email>ayuille1@jhu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Cognitive Science and Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Rong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Preschool Lab</orgName>
								<orgName type="institution">Yitu Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="172" to="181"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>172</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce PreCo, a large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38K documents and 12.5M words which are mostly from the vocabulary of English-speaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at https:// preschool-lab.github.io/PreCo/.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution, identifying mentions that refer to the same entities, is an important NLP problem. Resolving coreference is critical for many downstream applications, such as reading comprehension, translation, and text summariza- tion. Identifying a mention depends not only on its lexicons but also its contexts, and requires rep- resentations of all the entities before the mention. This is still a challenging task for the approaches based on the cutting-edge word2vec-like lexical representation. For example, it is hard to identify the mention "he" between two entities "Tom" and "Jerry" because they have almost the same word embeddings.</p><p>A number of datasets have been proposed to study the coreference resolution problem, such as MUC <ref type="bibr" target="#b8">(Hirschman and Chinchor, 1997</ref>), ACE <ref type="bibr" target="#b4">(Doddington et al., 2004</ref>), and OntoNotes <ref type="bibr" target="#b15">(Pradhan et al., 2012</ref>). The most popular one is OntoNotes, and recent work on coreference res- olution <ref type="bibr">(Clark and Manning, 2016a,b;</ref><ref type="bibr" target="#b11">Lee et al., 2017;</ref><ref type="bibr" target="#b14">Peters et al., 2018</ref>) evaluated their models on it. Other datasets were rarely studied after OntoNotes was published.</p><p>Previous work <ref type="bibr" target="#b17">(Sadat Moosavi and Strube, 2017)</ref> suggests that the overlap between train- ing and test sets makes significant impact on the performance of current coreference resolvers. In OntoNotes, which has relatively low training-test overlap, this impact is mixed together with the core challenges of coreference resolution. For ex- ample, consider the failure of referencing "them" to "the wounded" in "..., the wounded were carried off so fast and it was difficult to count them". It is hard to tell whether the algorithm can succeed if the currently low-frequency phrase "the wounded" has not been seen enough times in the training set. From a machine learning perspective, high over- lap is needed to ensure that the training and test datasets have similar statistics.</p><p>Another limitation of OntoNotes is that it only has annotations for non-singleton mentions, while singleton mentions are not annotated. Most of the algorithms for coreference resolution have two steps: mention detection and mention cluster- ing ( <ref type="bibr" target="#b21">Wiseman et al., 2016;</ref><ref type="bibr">Clark and Manning, 2016a,b)</ref>. The lack of singleton mention anno- tations makes training and evaluation of mention detectors more difficult.</p><p>To address both limitations of OntoNotes, we build a new dataset, PreCo. To alleviate the nega- tive impact of low training-test overlap, we restrict the data domain and collect a sufficient amount of data to achieve a relatively high training-test over- lap. Restricting the data domain is a common way to enable better studies of unsolved NLP tasks, such as language modeling <ref type="bibr" target="#b7">(Hill et al., 2015</ref>) and visual question answering <ref type="bibr" target="#b9">(Johnson et al., 2017)</ref>.</p><p>We select our data from English reading com- prehension tests for middle and high school Chi- nese students, which has several advantages. On one hand, the vocabulary size is appropriate. The English vocabulary of a typical Chinese high school student contains about 3000 commonly used words. This is similar to the vocabulary of a preschool English-speaking child <ref type="bibr" target="#b20">(Wikipedia, 2018)</ref>. Most words from the English tests are in this limited vocabulary. On the other hand, it is practical to collect enough data of this type from the Internet. With 12.5M words, PreCo is about 10 times larger than OntoNotes. Large scale datasets, e.g. ImageNet <ref type="bibr" target="#b3">(Deng et al., 2009</ref>), SQuAD <ref type="bibr" target="#b16">(Rajpurkar et al., 2016)</ref>, have played an important role for driving computer vision and NLP forward.</p><p>We use the rate of out-of-vocabulary (OOV) words between training and test sets to measure their overlap. PreCo shows much higher training- test overlap than OntoNotes by having an OOV rate of 0.8%, which is about 1/3 of OntoNotes's 2.1%. At the same time, PreCo presents a good challenge for coreference resolution research since its documents are in the open domain and have various writing styles. We test a state-of-the-art system ( <ref type="bibr" target="#b14">Peters et al., 2018</ref>) on PreCo and get an F1 score of 81.5. However, a modest human per- formance (87.9, which will be described in 4.1 ) is much higher, verifying there remain challenges.</p><p>To help training and evaluation of mention de- tection, we annotate singleton mentions in PreCo. Besides singleton mentions, we follow most other annotation rules of OntoNotes to label the new dataset. We show that in a state-of-the-art corefer- ence resolution system ( <ref type="bibr" target="#b14">Peters et al., 2018)</ref>, we can improve the model performance from 77.3 to 81.6 F1 on a training set of 2.5K PreCo documents by using an oracle mention detector, and the remain- ing gap of 18.4 F1 to the perfect 100 F1 can only be reduced by improving mention clustering. This indicates that future work should concern more about mention clustering than mention detection.</p><p>The advantages of our proposed dataset over ex- isting ones in coreference resolution can be sum- marized as follows:</p><p>• Its OOV rate is about 1/3 of OntoNotes.</p><p>• It has about 10 times larger corpus size than OntoNotes.</p><p>• It has annotated singleton mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing Datasets. The first two resources for coreference resolution study were MUC-6 and MUC-7 <ref type="bibr" target="#b8">(Hirschman and Chinchor, 1997</ref>). The MUC datasets are too small for training and test- ing, containing a total of 127 documents with 65K words. The next standard dataset was ACE <ref type="bibr" target="#b4">(Doddington et al., 2004</ref>) which has a much larger cor- pus of 1M words. But its annotations are restricted to a small subset of entities and are less consistent. OntoNotes ( <ref type="bibr" target="#b15">Pradhan et al., 2012</ref>) was presented to overcome those limitations. Machine learning based approaches, especially deep learning based, benefitted from this well annotated and large-scale (1.3M words) dataset. Continuous research on OntoNotes over the past 6 years improved perfor- mance by 10 F1 score ( <ref type="bibr" target="#b5">Durrett and Klein, 2013;</ref><ref type="bibr" target="#b14">Peters et al., 2018)</ref>. Datasets after OntoNotes, such as WikiCoref ( <ref type="bibr" target="#b6">Ghaddar and Langlais, 2016)</ref>, are seldom studied. Therefore, we mainly com- pare PreCo with OntoNotes in this paper. With a much larger scale, PreCo builds on the advantages of OntoNotes. Some of these existing datasets also have corpus in other languages, but we just focus on coreference resolution in English.</p><p>Out-of-domain Evaluation. <ref type="bibr" target="#b17">(Sadat Moosavi and Strube, 2017)</ref> show that if coreference re- solvers mainly rely on lexical representation, as it is the case in state-of-the-art ones, they are weak at generalizing to unseen domains. Even in the seen domains, the low degree of overlap for non-pronominal mentions between the training and test sets cause serious deterioration of coref- erence resolution performance. As a conclusion, <ref type="bibr" target="#b17">(Sadat Moosavi and Strube, 2017)</ref> suggested that out-of-domain evaluation is a must in the litera- ture. But we think the problem can be relieved by expanding the training data for the target do- mains to increase overlap, so that the field can pay more attention to the other challenges of corefer- ence resolution.</p><p>Data Simplification. Many simplified datasets were built to enable better study on unsolved tasks. Such simplifications can guide researchers to the core problems and make data collection easier. For example, ( <ref type="bibr" target="#b7">Hill et al., 2015</ref>) introduced the Chil- dren's Book Test to distinguish the task of pre- dicting syntactic function words from that of pre- dicting low-frequency words for language model. The dataset helped them to develop a generaliz- able model with explicit memory representations.</p><p>The reading comprehension dataset SQuAD <ref type="bibr" target="#b16">(Rajpurkar et al., 2016</ref>) imposes the constraint that ev- ery answer is always a segment of the input text. This constraint benefits both labeling and evalu- ation of the dataset, which has significant influ- ences in terms of benchmarks. Similarly, the rein- forcement learning literature develops algorithms by studying games instead of the real world envi- ronment ( <ref type="bibr" target="#b12">Mnih et al., 2013</ref>). We hope that, with high training-test overlap, PreCo can serve as a valuable resource for research on coreference res- olution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Creation</head><p>We discuss the data collection and annotation in this section. The overview of the process is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpus Collection</head><p>We crawl English tests from several web sites. The web pages often contain the full English tests in a lot of formats. We build an annotation website and hire annotators to manually extract the rele- vant contents. We have a total of 80 part-time Chinese annotators, most of whom are university students. They are required to have a minimum score in standard English tests. During annotation training, the annotators read the annotation rules, and take several practice tasks, in which they an- notate sample articles, and their results are com- pared with ground truth side by side for them to study. Before formal annotation, the annotators will need to pass an assessment.</p><p>Some data cleaning is done during annotation, such as unifying paragraph separators, etc. The questions with answers in these tests are also extracted for future research. Finally, we use NLTK's sentence and word tokenizer ( <ref type="bibr" target="#b0">Bird et al., 2009</ref>) to tokenize the crawled text.</p><p>In addition to having annotators manually clean the data, we also use heuristic rules to further clean the data. For example, in some cases the whites- paces between two words are missing. We use a spell checker to identify and correct most of these cases. We also use heuristic rules to fix some sentence partition boundaries, e.g., to make sure opening quotes are placed at the beginning of a sentence, instead of being wrongly placed at the end of a previous sentence (closing quotes are han- dled similarly).</p><p>In addition to the crawled data, we include the documents from the RACE dataset ( <ref type="bibr" target="#b10">Lai et al., 2017)</ref>. RACE is a reading comprehension dataset from English tests for middle and high school Chinese students, which has similar types of data sources as PreCo. About 2/3 of PreCo documents are from the RACE dataset. Since documents are from several data sources, we want to remove duplicated documents, and documents that are not exactly the same but have a high rate of repetitions. The similarity of two documents D 1 and D 2 is estimated using the bag- of-words model. Assume S 1 and S 2 are bag-of- words multisets to represent the two documents. The similarity between D 1 and D 2 is defined as</p><formula xml:id="formula_0">max( |S 1 ∩S 2 | |S 1 | , |S 1 ∩S 2 | |S 2 | ).</formula><p>If the similarity between two documents are larger than 0.9, we remove the shorter one. This process is referred as dedupli- cate in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Partition</head><p>The dataset has a total of 38K documents. We use 500 documents for the development set, 500 docu- ments for the test set, and the rest 37K documents for the training set. The development and test doc- uments were randomly selected from RACE's de- velopment and test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coreference Annotation and Refinement</head><p>We manually annotate coreferences on these doc- uments. The annotation rules are slightly differ- ent from OntoNotes ( <ref type="bibr" target="#b15">Pradhan et al., 2012</ref>). We modify some of the rules to make the definition of coreference more consistent and easier to be un- derstood by the annotators. The major differences are listed in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example document in PreCo with annotations. Good quality control of annotation is essential, since the rules are complicated and coreference resolution depends on meticulous reading of the whole document over and over. We found that annotators get low recall and insufficient preci- sion mainly because of negligence, as opposed to the lack of annotation rules or other ambigui- ties. For example, two co-referred mentions could be far apart and require careful searches, and an annotator may miss it. Therefore we further re- fine annotations as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Annotators can think about the complicated inconsistent cases when merging annotations, and the voting process will fix some errors while preserving the mentions and coreferences that are found only once by indi- vidual annotators.</p><p>The quality of different annotation processes is shown in [Parents] should get involved.</p><p>Generic mentions can only be coreferred by pronouns.</p><p>Generic mentions can be coreferred directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>non-proper modifiers</head><p>[Wheat] is important. <ref type="bibr">[Wheat]</ref> fields are everywhere.</p><p>Non-proper modifiers cannot be coreferred.</p><p>Non-proper modifiers can be coreferred as generic mentions. Nationality acronyms and all job titles can be coreferred. Labeling the whole dataset is costly because each annotation from scratch or comparison takes an average of about 10 minutes. Prompts from an algorithm do not help since they do not speed up the annotation much but instead introduce biases. We observed some biases when using an algorithm to help annotation. We have two models, M 1 and M 2 , and we have a test set T which is annotated manually, and a test set T which uses prompts from model M 1 to help annotation. While M 1 and M 2 have similar performance on T , M 1 's per- formance is much higher than M 2 's on T , which shows the biases.</p><p>Because of limited annotation resources, we have only finished the refinements on the devel-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process</head><p>Avg  opment and test sets with the process shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We refine the training set annotations as follows: for each document, two annotators an- notate it separately, and a third annotator com- pares and merges the two annotations. We use a training set of 2.5K documents to quantify the im- pact of this annotation refinement to model per- formance. <ref type="table">Table 3</ref> shows the model performances of the training set that is annotated once, and the training set of the merged annotation. The per- formance difference is quite significant. Further- more, the difference is consistent with <ref type="table" target="#tab_0">Table 2</ref>: the "AB-merge" model has a similar precision as the "Once" model, but it has a much higher re- call. It indicates that a further refinement of the training set such as DEF-voting could be essen- tial. A more interesting question is: how to make the definition of coreference more consistent and executable? We leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Avg. Prec Avg. Rec Avg. F1</head><p>Once 79.3 69.1 73.9 AB-merge 78.1 76.5 77.3 <ref type="table">Table 3</ref>: The annotation quality's impact on model performance. Each row shows the development set performance of the EE2E-Coref model (train- ing details in Section 4.1) trained by data of dif- ferent annotation quality. Each training set con- tains 2.5K documents. In the training set "Once", each document is annotated by one annotator. In the training set "AB-merge", each document is an- notated by two annotators independently, and the annotations are compared and merged by a third annotator. <ref type="table" target="#tab_5">Table 4</ref> shows some properties of OntoNotes and PreCo. As intended, PreCo has a lower OOV rate than OntoNotes. For a training set with vocabulary V and a test set with n tokens [t 1 , t 2 , ..., t n ], ignor- ing the tokens with non-alphabetic characters, the OOV rate is defined by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset Properties</head><formula xml:id="formula_1">i o(t i ) n , where o(t i ) = 0 if t i ∈ V 1 if t i / ∈ V</formula><p>The OOV rate can be extended to the rate of low- frequency words which also indicates the training- test overlap, by simply replacing V in the defini- tion above with the non-low-frequency vocabulary of the training set. We find that the OOV rate is consistent to the rates of low-frequency words in different levels. So we use the OOV rate for con- venience.</p><p>In PreCo, about 50.8% of the mentions are sin- gleton mentions. <ref type="figure">Figure 4</ref> shows the distribu- tion of cluster sizes within non-singleton clusters. The distribution is similar between OntoNotes and PreCo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>To verify our assumption that PreCo embodies the core challenges of coreference, we evaluate a strong baseline coreference resolver on it. Specif- ically, we (i) estimate the room for improvement of the baseline system to show that the dataset is challenging, (ii) study the impact of training-test overlap to model performance and error analysis to show the advantages of PreCo, and (iii) quan-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property</head><p>OntoNotes PreCo  titatively evaluate the mention detector to under- stand the bottlenecks of the coreference resolution system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Performance</head><p>We use the end-to-end neural coreference resolver, E2E-Coref ( <ref type="bibr" target="#b11">Lee et al., 2017)</ref>, enhanced by the deep contextualized word representations ( <ref type="bibr" target="#b14">Peters et al., 2018</ref>) as the baseline system, and we refer to this system as EE2E-Coref. This is the state-of- the-art model on OntoNotes, achieving a test aver- age F1 score of 70.4, which is the main evaluation metric for coreference resolution. The metric is computed by averaging the F1 of MUC, B 3 , and CEAF φ4 , which are three metrics of coreference resolution that have different focuses. Our implementation EE2E-Coref 2 gets 81.5 Avg. F1 score on PreCo. We follow the setting of most hyperparameters on OntoNotes and do grid- search for the decay parameter of the learning rate and the size of the hidden layers on the develop- ment set, since these two hyperparameters are rel- atively sensitive to the scale of the training data. The F1 score increment from OntoNotes to PreCo is probably due to the higher overlap between the training and test sets in PreCo. Figure 4: Distribution of cluster sizes within non- singleton clusters. We ignore singleton clusters in this figure so that it is easier to compare between OntoNotes and PreCo.</p><p>[&lt;His father&gt; and he] get off the car.</p><p>[They] find the old man lying near the taxi. The banana skin is near him. The old man looks at <ref type="bibr">[them]</ref> and says, "Teach [your] child to throw the banana skin to the right place!"</p><p>He gave his last few coins to [a beggar], but then he saw &lt;another one&gt;, and forgot that he did not have any money. He asked &lt;the man&gt; if &lt;he&gt; would like to have lunch with him, and [the beggar] accepted, so they went into a small restaurant and had a good meal.</p><p>[Holmes] and &lt;Dr. Watson&gt; went on a camping trip. After a good meal and a bottle of wine, they lay down in a tent for the night and went to sleep. Some hours later, Holmes woke up and pushed [his friend]. We demonstrate three typical error cases made by EE2E-Coref on PreCo in <ref type="table" target="#tab_6">Table 5</ref>. Corefer- ence resolution in these cases requires good under- standing of multiple sentences, which is an open problem in NLP. A capable entity representation for "them", "another one" or "Dr. Watson" may help to resolve these error cases. We also compare the performance of EE2E-Coref with human per- formance to estimate the room for improvement on PreCo. As described in Section 3.4, human an- notators get low recall mostly due to negligence. So we use the AB-merge annotation to estimate human's ability on coreference resolution. The gap of performance between model and human is 6.4 F1 score, from 81.5 to 87.9. The actual gap is larger, since AB-merge still has some missed coreference annotations due to negligence. This shows that the dataset is challenging and encour- ages future research. The error cases show the challenges as well.</p><p>Note that PreCo is not a general purpose dataset. Our motivation of designing PreCo is to make it easier to improve coreference resolution algo- rithms, e.g., to make error analysis easier. It is not a goal of PreCo to generalize well on corpus from other domains. Furthermore, we find that there are a certain amount of annotation errors in the devel- opment and test sets. We suggest that researchers working on PreCo should be careful about these errors, especially after a model gets F1 score be- yond 90.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of Training-test Overlap</head><p>Training-test overlap makes significant impact on error analysis. Consider an error case of corefer- ence resolution, if there are low-frequency words in the related mentions, then it will be hard to tell whether the algorithm can succeed if the words has not been seen enough times in the training set. We call an error case LFW if there are low-frequency words 3 in its related mentions <ref type="bibr">4</ref> . Therefore, the lower LFW rate a training set contains, the more precisely it may expose the drawbacks of the algo- rithm.</p><p>To study the impact of training-test overlap, ac- tually, the training-dev overlap, we pick different subsets from the training data and evaluate the models trained on them. At first, we control over- lap by picking different sizes of the training data randomly. <ref type="figure" target="#fig_5">Figure 5(a)</ref> shows that, as the training data size grows, the OOV rate, which is the over- lap indicator, decreases and the F1 score of EE2E- Coref increases significantly. <ref type="figure" target="#fig_5">Figure 5(b)</ref> shows that when training set size increases, the OOV rate and the LFW rate drop together. Then, to remove the impact of data size, we pick training sets which have a fixed size but different overlaps with the development set vocabulary. The OOV rates and F1 scores of these subsets are shown in <ref type="figure" target="#fig_5">Figure  5</ref>(c). This experiment verifies the positive cor-  relation between training-dev overlap and coref- erence resolution performance suggested by <ref type="bibr" target="#b17">(Sadat Moosavi and Strube, 2017)</ref>. <ref type="figure" target="#fig_5">Figure 5(d)</ref> shows that for training sets with the same size, the OOV rate and the LFW rate also drop together.</p><p>We observe that the training set of 2.5K doc- uments in <ref type="figure" target="#fig_5">Figure 5</ref>(a) has a higher model perfor- mance than all the training sets in <ref type="figure" target="#fig_5">Figure 5</ref>(c). This is not expected. One hypothesis is that the lower performance in <ref type="figure" target="#fig_5">Figure 5</ref>(c) is due to the smaller diversity of these training sets, which are selected to have certain training-dev OOV rates.</p><p>The training-dev LFW rate of OntoNotes is 34.8%. As a comparison, the number for PreCo is 12.3%. A subset of PreCo with a similar to- ken number to OntoNotes has a LFW rate of 33.0%. This indicates that research of corefer- ence algorithms on PreCo will be much more ef- ficient than on OntoNotes. Even if we can ignore the LFW error cases, there are others related to low-frequency word senses, phrases and sentence structures, which are hard to filter out. They will also obscure the error analysis. It is reasonable to believe that training-dev overlap impacts the rate of these error cases in a similar way to impact LFW rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Mention Detection</head><p>Since most coreference systems consist of a men- tion detection module and a mention clustering module, an important question is: with a perfect mention detection module, what is the model per- formance on coreference resolution? The answer would help us understand the bottlenecks of the entire system, by quantifying the impact of the mention detection module on the final F1 score. ( <ref type="bibr" target="#b11">Lee et al., 2017</ref>) gave an answer by taking ground truth non-singleton mentions as the input of the coreference resolver for both training and evalu- ation, assuming that the perfect mention detector can also make perfect anaphoricity decisions, e.g., to decide whether a mention should be linked to an antecedent. But this assumption can be vio- lated since mention detectors usually take local in- formation but anaphoricity decisions usually need more context, nearly as much as entity identifica- tion. The anaphoricity decisions should be made in the mention clustering module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mention</head><p>OntoNotes PreCo detected 66.7 77.3 *all N/A 81.6 *non-singleton 85.2 89.2 <ref type="table">Table 6</ref>: Coreference resolution performances on development set under different mention detection qualities. A prefixed * denotes ground truth. The model trained on OntoNotes is E2E-Coref ( <ref type="bibr" target="#b11">Lee et al., 2017</ref>) while the one trained on PreCo is EE2E-Coref. The PreCo training set contains the same 2.5K documents as in <ref type="table">Table 3</ref>.</p><p>We argue that a better way to answer the ques- tion is to take all ground truth mentions (including singletons) for coreference. This operation is not feasible in OntoNotes since it does not have an- notations for singleton mentions. We do this on PreCo and the results are shown in <ref type="table">Table 6</ref>. There is an obvious difference between the F1 scores achieved with all gold mentions and non-singleton gold mentions. Therefore, the room for improve- ment by better mention detection is not as enor- mous as suggested in ( <ref type="bibr" target="#b11">Lee et al., 2017</ref>). The ma- jor challenge remained in coreference resolution is mention clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a large-scale coreference resolution dataset to overcome the limitations of existing ones. Our dataset, PreCo, features higher training-test overlap, about 10 times larger scale than previous datasets, and singleton mention an- notations. By evaluating a state-of-the-art corefer- ence resolver, we show that there is a wide gap be- tween the model and human performance, which demonstrated challenges of the dataset. We veri- fied the expectation that PreCo's higher training- test overlap helps research on coreference resolu- tion. For the first time, we quantified the impact of mention detector to the entire system, thanks to our singleton mention annotations. We make the dataset public, and hope it will stimulate further research on coreference resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An Example from PreCo. In the example, mentions are indicated by boxes, and mention clustering is indicated by the subscripted numbers. If two mentions have the same number, they refer to the same entity.</figDesc><graphic url="image-1.png" coords="2,86.17,62.81,425.19,278.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of dataset creation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Process of annotation refinement. A document is firstly annotated by 3 annotators A, B, and C, independently. Then another annotator D merges annotations from A and B. Similarly, annotator E merges annotations from A and C, and annotator F merges annotations from B and C. Finally, annotations from D, E and F are merged using an ensemble algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>copular structures [John] is [a good teacher]. The referent and the attribute cannot be coreferred. The referent and the attribute can be coreferred. appositives [[John]a, [a linguist I know] b ]c, ... Sub-spans are not coreferred with the whole-span. a and b are not coreferent with c. Sub-spans are coreferred with the whole-span. a and b are coreferred with c. misc. The [U.S.] policy ... [Secretary of State] [Colin Powell] ... Nationality acronyms and job titles in appositives cannot be coreferred.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of training-dev overlap. (a) and (b) show the impact of training set sizes. (c) and (d) show the impact of the training-dev OOV rate, when the training sets have the same size of 2.5K documents. The 8 subsets, s1-s8, consist of documents ranked by their overlaps with the development set vocabulary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>OntoNotes took 2 individual 
annotations for each document and got an adjudi-
cated version based on them. Taking the adjudi-
cated version as ground truth, the average MUC 
score (Vilain et al., 1995) 1 of individual annota-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Major differences of annotation rules between PreCo and OntoNotes. The annotation rules of 
OntoNotes are described in (OntoNotes Guidelines) 

tions is 89.6, and the inter-annotator MUC score 
is 83.0. The corresponding numbers for PreCo are 
85.3 and 77.5. The actual gap of individual anno-
tation quality between OntoNotes and PreCo is not 
as large as it looks like. Note that, OntoNotes's 
two individual coreference annotations of each 
document are based on the same syntactic anno-
tations of the document, so they could be more 
consistent than PreCo's which are annotated on 
raw text. Therefore, if we want to fairly compare 
PreCo with OntoNotes, we should take into ac-
count OntoNotes's inter-annotator consistency of 
syntactic parsing annotations. As it has a rough 
upper bound of 98.5 F1 score according to the re-
annotation of English Treebank on OntoNotes by 
the principal annotator a year after the original an-
notation (Weischedel et al., 2011), we could infer 
that the individual annotation quality of PreCo is 
quite close to OntoNotes. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Annotation quality. DEF-voting is taken 
as the ground truth to evaluate other annotation 
processes. The annotation "AB-merge" is merged 
by annotator G, who is different from D, E and F. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Properties of OntoNotes and PreCo. The 
mention (cluster) density is defined by: number of 
mentions (clusters) / number of tokens. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Error cases of EE2E-Coref on PreCo. 
Each bold mention is incorrectly referred to the 
entity in []s. The mentions of its gold entity are in 
&lt;&gt;s. 

</table></figure>

			<note place="foot" n="1"> MUC score is one of the metrics to evaluate the quality of coreference resolution.</note>

			<note place="foot" n="2"> It gets an F1 score of 70.0±0.3 on OntoNotes, slightly lower than the F1 score reported in the original paper.</note>

			<note place="foot" n="3"> In our experiments, a word is defined as low-frequency if it appears in the training set less than 10 times. 4 There are 3 kinds of error cases of coreference resolution: false-new, false-link and wrong-link. In our experiments, the related mentions include: the current mention in all 3 kinds of cases, the nearest gold antecedent in false-new and wrong-link and the false referred antecedent in false-link and wrong-link.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Lance A Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wikicoref: An english coreference-annotated corpus of wikipedia articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillippe</forename><surname>Langlais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Muc-7 coreference task definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MUC7</title>
		<meeting>MUC7</meeting>
		<imprint>
			<publisher>Applications International Corporation</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<title level="m">Race: Large-scale reading comprehension dataset from examinations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1707.07045</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ontonotes english coreference guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ontonotes</forename><surname>Guidelines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Sixteenth Conference on Computational Natural Language Learning<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lexical features in coreference resolution: To be used with caution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Message Understanding, MUC6 &apos;95</title>
		<meeting>the 6th Conference on Message Understanding, MUC6 &apos;95<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ontonotes: A large training corpus for enhanced processing. Handbook of Natural Language Processing and Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vocabulary development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="994" to="1004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
