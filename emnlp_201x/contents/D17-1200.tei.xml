<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Frermann</surname></persName>
							<email>l.frermann@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Development Center</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country>Germany GmbH</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Szarvas</surname></persName>
							<email>szarvasg@amazon.de</email>
							<affiliation key="aff0">
								<orgName type="department">Amazon Development Center</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country>Germany GmbH</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Inducing Semantic Micro-Clusters from Deep Multi-View Representations of Novels</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1873" to="1883"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatically understanding the plot of novels is important both for informing literary scholarship and applications such as summarization or recommendation. Various models have addressed this task, but their evaluation has remained largely intrinsic and qualitative. Here, we propose a principled and scalable framework leveraging expert-provided semantic tags (e.g., mystery, pirates) to evaluate plot representations in an extrinsic fashion, assessing their ability to produce locally coherent groupings of novels (micro-clusters) in model space. We present a deep recurrent autoencoder model that learns richly structured multi-view plot representations, and show that they i) yield better micro-clusters than less structured representations ; and ii) are interpretable, and thus useful for further literary analysis or labelling of the emerging micro-clusters.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the literature aficionado, the quest for the next novel to read can be daunting: the sheer number of novels of different styles, topics and genres is dif- ficult to navigate. It is intuitively clear that readers select novels based on specific but potentially di- verse and structured preferences (e.g., they might prefer novels of a particular theme (small-town ro- mance), mood (dark) or based on character types (grumpy boss), character relations (love, enmity) and their development). These preferences also manifest in the organization of online book stores or recommendation platforms. <ref type="bibr">1</ref> For example, the Amazon book catalog contains semantic tags pro- vided by experts (publishers), including labels of character types (pirates) or theme (secret baby ro- mance) to aid focused search for novels of interest.</p><p>Although these tags are already fairly granular, many cover large sets of novels (e.g., the tag secret baby romance covers almost 4, 000 novels), limit- ing their utility for exhaustive exploration and call for even finer grained micro-groupings. Can we instead automatically induce fine-grained novel clusters in an unsupervised, data-driven way?</p><p>We propose a framework to learn structured, in- terpretable book representations that capture dif- ferent aspects of the plot, and verify that such representations are rich enough to support down- stream tasks like generating interpretable book groupings. A real-world application of this work is content-based book recommendation based on diverse and interpretable book characteristics. Content-based recommendation has been criti- cized by the limited complexity of typically em- ployed features (limited content analysis; <ref type="bibr" target="#b17">Lops et al. (2011)</ref>; <ref type="bibr" target="#b0">Adomavicius and Tuzhilin (2005)</ref>). This work addresses this problem by inducing complex, structured and interpretable representa- tions. Our contributions are two-fold.</p><p>First, assuming that richly structured book tags call for rich content representations (which expert taggers arguably possess), we describe a deep un- supervised model for learning multi-view repre- sentations of novel plots. We use the term view to refer to specific types of plot characteristics (e.g., pertaining to events, characters or mood), and multi-view to refer to combinations of these views. We use multi-view book representations to construct meaningful and locally coherent neigh- bourhoods in model space, which we will refer to as micro-clusters. To this end, we extend a recent autoencoder model <ref type="bibr" target="#b14">(Iyyer et al., 2016</ref>) to learn multi-view representations of books. Our model encodes properties of characters (view v 1 ), rela- tions between characters (view v 2 ), and their re- spective trajectories over the plot. <ref type="bibr">2</ref> View-specific encodings are learnt in an unsupervised way from raw text as separate sets of word clusters which are jointly optimized to encode relevant and distinct information. These properties are crucial for applications such as book recommen- dation, because they allow to i) explain why par- ticular books are similar based on the inferred la- tent structure and ii) find similarities based on im- portant and distinct aspects of a novel (character types or interactions). Our framework of unsu- pervised multi-view learning is very flexible and can straightforwardly be applied to learn arbitrary kinds and numbers of views from raw text.</p><p>Secondly, we propose an empirical evaluation framework. Before we can use models to extend existing categories as discussed above, it must be shown that the representations capture existing as- sociations. To this end, we investigate whether micro-clusters derived from induced representa- tions resemble reference clusters defined as groups of novels sharing tags in the Amazon catalog. While automatic induction of plot representations has attracted considerable attention (see <ref type="bibr" target="#b15">Jockers (2013)</ref>), evaluation has remained largely qualita- tive and intrinsic. To the best of our knowledge, we are the first to investigate the utility of auto- matically induced plot representations on an ex- trinsic task at scale. We evaluate micro-clusters as local neighbourhoods in model space containing 10, 000 novels under 50 reference tags.</p><p>We show that rich multi-view representations produce better micro-clusters compared to com- petitive but simpler models, and that interpretabil- ity of the learnt representations is not compro- mised despite the more complex objective. We also qualitatively demonstrate that high-quality micro-clusters emerge from a smaller, more ho- mogeneous data set of Gutenberg 3 novels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatically learning representations of book plots, as structured summaries of their content, has attracted much attention (cf, Jockers (2013) for a review). Unsupervised models have been <ref type="bibr">2</ref> We argue that both characters, and their relations evolve throughout the plot: Heroes pick up new attitudes or skills, and utilize those to different extents; relations change and de- velop over time (hate to love, friendship to enmity and back).</p><p>3 https://www.gutenberg.org/ proposed which, given raw text, extract prototyp- ical event structure <ref type="bibr" target="#b19">(McIntyre and Lapata, 2010;</ref><ref type="bibr" target="#b4">Chambers and Jurafsky, 2009)</ref>, prototypical char- acters ( <ref type="bibr" target="#b1">Bamman et al., 2013</ref><ref type="bibr" target="#b2">Bamman et al., , 2014</ref><ref type="bibr" target="#b8">Elsner, 2012)</ref> and their social networks ( <ref type="bibr" target="#b9">Elson et al., 2010</ref>).</p><p>Other work focused on the dynamics of a plot, learning trajectories of relations between two char- acters ( <ref type="bibr" target="#b14">Iyyer et al., 2016;</ref><ref type="bibr" target="#b6">Chaturvedi et al., 2017)</ref>. <ref type="bibr" target="#b14">Iyyer et al. (2016)</ref> combine dictionary learn- ing <ref type="bibr" target="#b21">(Olshausen and Field, 1997</ref>) with deep recur- rent autoencoders to learn interpretable character relationship descriptors. They show that their deep model learns better representations than concep- tually similar topic models ( <ref type="bibr" target="#b12">Gruber et al., 2007;</ref><ref type="bibr" target="#b5">Chang et al., 2009</ref>). Here, we extend the model of <ref type="bibr" target="#b14">Iyyer et al. (2016)</ref> to simultaneously induce multiple views on the plot.</p><p>Methodologically, our work falls into the class of multi-view learning, and we propose a novel formulation of the model objective which encour- ages encoding of distinct information in the views. Our objective function is inspired by prior work in multi-task learning and deep domain adaptation for classification ( <ref type="bibr" target="#b10">Ganin and Lempitsky, 2015;</ref><ref type="bibr" target="#b11">Ganin et al., 2016)</ref>. They train neural networks to simultaneously learn classifiers which are ac- curate on their target task and are agnostic about feature fluctuation pertaining to domain shift. We adapt this idea to unsupervised models with a re- construction objective and learn multi-view repre- sentations which efficiently encode the input data and, at the same time, learn to only encode infor- mation relevant for the particular view.</p><p>Evaluating induced plot representations is no- toriously difficult. Most evaluation has resorted to manual inspection, or crowd-sourced human judgments of the coherence and interpretability of the representations <ref type="bibr" target="#b14">(Iyyer et al., 2016;</ref><ref type="bibr" target="#b6">Chaturvedi et al., 2017)</ref>. While such evaluations demonstrated that the induced representations are qualitatively valuable, it is not clear whether they are rich and general enough to be used for downstream tasks and applications. Others have used automatically created gold-standards of re-occurring character names across scripts ('gang member') ( <ref type="bibr" target="#b1">Bamman et al., 2013)</ref>, prototypical plot templates (tropes, e.g., 'corrupt corporate executive') or manually created gold-standards of character types (Vala et al., 2016) or their relations ( <ref type="bibr" target="#b18">Massey et al., 2015;</ref><ref type="bibr" target="#b6">Chaturvedi et al., 2017)</ref> to automatically measure the intrinsic value of learnt representations. Here, we investigate how these results extend to extrin- sic tasks, and use structured plot representations for the task of inducing micro-clusters of novels.</p><p>Elsner (2012) depart from the above pattern, suggesting an extrinsic, albeit artificial, evaluation paradigm. Approaching plot understanding from the angle of its utility for summarization, they use kernel methods to learn character-centric plot rep- resentations. They evaluate their trained models on their ability to differentiate between real and ar- tificially distorted novels (e.g., with shuffled chap- ters). While this evaluation is extrinsic and quanti- tative, it leverages artificial data and it is not clear how the results extend to real-world summaries.</p><p>Language features were previously used in content-based book recommendation e.g., as bags- of-words <ref type="bibr" target="#b20">(Mooney and Roy, 1999</ref>) or semantic frames ( <ref type="bibr" target="#b7">Clercq et al., 2014</ref>). Both works use struc- tured databases and plot summaries rather than the raw book text. Other work used topic mod- els to augment a recommender system of scien- tific articles ( <ref type="bibr" target="#b24">Wang and Blei, 2011</ref>). Similar to our work, these works emphasize the added value of interpretable representations and recommenda- tions, however, they do not leverage the raw con- tent of entire novels and the richness of informa- tion encoded in those.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-View Novel Representations</head><p>We first provide an intuitive description of Rela- tionship Modeling Networks (RMN; <ref type="bibr" target="#b14">Iyyer et al. 2016)</ref>, and our extension (henceforth MVPlot), which jointly induces temporally aware multi-view representations of novel plots. Afterwards we de- scribe the MVPlot model in technical detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intuition</head><p>Iyyer et al. <ref type="formula" target="#formula_2">(2016)</ref> introduce the RMN, an un- supervised model which learns interpretable plot representations in terms of types of relations be- tween pairs of book characters, and their devel- opment over time. Given a book and a charac- ter pair, the model learns relation types as word clusters (not unlike topics in a topic model ( <ref type="bibr" target="#b3">Blei et al., 2003)</ref>) from local contexts mentioning both characters. In addition the RMN learns for each character pair how these relations vary over time as a trajectory of relations. Methodologically, the RMN combines a deep recurrent autoencoder with dictionary learning, where terms in the dictionary are relationship descriptors. The RMN learns to  laugh scream laughing yell joke cringe disgrace embarrassment hate cursing v1 snug fleece warm comfortable wet blanket flan- nel cozy comfort roomy v1 excellency mademoiselle monsieur majesty duchess empress madame countess madam v2 love loving lovely dear sweetest dearest thank darling congratulation hello v2 associate assistant senior chairman executive leadership vice director liaison vice-president <ref type="table" target="#tab_3">Table 1</ref>: Example property (v 1 ) and relation (v 2 ) descriptors induced by MVPlot on the Gutenberg corpus, as their nearest neighbours in GloVe space.</p><p>efficiently encode local text spans as a linear com- bination of these relation descriptors.</p><p>We extend RMNs to induce temporally aware multi-view representations of novel plots. Multi- ple interpretable views are induced jointly within one process in an unsupervised way. The core of our model closely corresponds to the structure of the RMN (as technically described in Section 3.2). However, we provide the model with distinct types of informative input for each view, and, reformu- late the objective in a way that jointly optimizes parameterizations of all views to encode distinct information (cf., Section 3.3).</p><p>Our MVPlot model learns two views: prop- erties associated with individual characters (v 1 ), relations between character pairs (v 2 , as in the RMN) and their respective development over the course of the plot (examples of descriptors learnt by MVPlot for both views are shown in <ref type="table" target="#tab_3">Table 1</ref>). Our modeling framework, however, is very gen- eral in the sense that any number and type of views can be learnt jointly as long as input with relevant signals can be provided for each view. For exam- ple, we could naturally extend the model described here with a 'plot' view to capture properties of the story which are not related to any character.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The MVPlot Model</head><p>We now formally describe the MVPlot model for learning multi-view plot representations encoding individual character properties (v 1 ), character pair relationships (v 2 ), and their respective trajectories. The full model is shown in <ref type="figure" target="#fig_2">Figure 1</ref>. Input to our model are two corpora of text spans, one for each view, S v1 and S v2 . The cor- pora consist of different sets of relevant view- specific local contexts as described in Section 5. Given a book b and a character c, S c,b v1 contains  </p><formula xml:id="formula_0">S c,b v1 ={s 1 , s 2 , ..., s T } s.th. ∀ t : c ∈ s t Similarly, S c 1 ,c 2 ,b v 2</formula><p>, given a book b and a pair of characters c 1 and c 2 , contains linearly ordered text spans which mention both c 1 and c 2 , but no other character,</p><formula xml:id="formula_1">S c 1 ,c 2 ,b 2 ={s 1 , s 2 , ..., s T } s.th. ∀t : c1 ∈ s t , c2 ∈ s t .</formula><p>The rest of the input preparation follows <ref type="bibr" target="#b14">Iyyer et al. (2016)</ref> as follows. We map text spans into word embedding space, by mapping each word w to its 300-dimensional GloVe embedding e w <ref type="bibr" target="#b22">(Pennington et al., 2014</ref>) pre-trained on Common- Crawl, and averaging the word embeddings,</p><formula xml:id="formula_2">e t = 1 |s t | w∈s t ew.<label>(1)</label></formula><p>We provide MVPlot with a trainable matrix B of dimensions b × n, where b is the num- ber of books in our data set, and each row e b is an n-dimensional book embedding, encoding background information (e.g, about its general set- ting or style) which is relevant to neither view of MVPlot. <ref type="bibr">5</ref> Finally the span embedding and the corresponding book embedding are concatenated, and passed through a ReLu non-linearity (cf., <ref type="figure">Fig- ure</ref> 1, bottom),</p><formula xml:id="formula_3">h t = ReLu(W h [e t ; e b t ]).<label>(2)</label></formula><p>Model architecture MVPlot uses the architec- ture of the RMN autoencoder, but replicates it for each input view, v 1 and v 2 (cf., <ref type="figure" target="#fig_2">Figure 1</ref>, center). Each part will induce an encoding of view-specific information. The feed-forward pass, described be- low, is identical for both parts, however, the loss and backpropagation will differ (cf., Section 3.3).</p><p>We describe the feed-forward pass for v 2 , not- ing that it works analogously for v 1 . The latent input representation h t (eqn <ref type="formula" target="#formula_3">(2)</ref>) is passed through a softmax layer which returns a weight vector over descriptors,</p><formula xml:id="formula_4">d t v2 = sof tmax(W d v2 [h t ])</formula><p>. Descrip- tors are rows in the k × d-dimensional descrip- tor matrix R v2 , with each row k corresponding to one d-dimensional descriptor (similar to a topic in a topic model). The input e t is reconstructed through the dot product of d t v2 and the descriptor matrix R v2 ,</p><formula xml:id="formula_5">r t = d t v2 Rv2.<label>(3)</label></formula><p>Like in the original RMN, we want to capture the temporal development of character relations or properties. Intuitively, we assume that the rela- tions between (or properties of) characters at time t depend on their relations (or properties) at time t − 1. As in the RMN, we factor the descriptor weights of the previous time step d t−1 into the rep- resentation at time t, such that</p><formula xml:id="formula_6">d t v2 = α sof tmax W d v2 [ht; d t−1 v2 ] +(1 − α)d t−1 v2 (4)</formula><p>Output First, the model induces property de- scriptors (rows in R v1 ) and the relationship de- scriptors (rows in R v2 ). Both sets of descriptors are optimized to reconstruct model input in GloVe embedding space (cf., Section 3.3 for details). They consequently themselves live in GloVe word embedding space, and can be visualized through their nearest neighbours in this space. In addi- tion, for each book b, character c b and character pair {c 1 , c 2 }, sequences of weight vectors over re- lations</p><formula xml:id="formula_7">T c 1 ,c 2 ,b v 2 = d 1 v2 ...d T v2</formula><p>, and over properties</p><formula xml:id="formula_8">T c,b v 1 = d 1 v1 ...d T v1</formula><p>are induced, which encode their trajectory of re- lations and properties, respectively. We will uti- lize these trajectories for inducing micro-clusters of novels (Section 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Multi-View Loss</head><p>We formulate our loss as a Hinge loss within the contrastive max-margin framework. Our objective is to learn parameters for each view ∈ {v 1 , v 2 } which efficiently encode view-specific input in a low-dimensional space from which the original in- put can be re-constructed with high accuracy. In addition, we want to learn view-specific parame- ters which encode distinct information such that when utilized together, they provide an improved embedding of the data. Intuitively, we achieve this by discouraging parameters of view v 1 from ac- curately reconstructing input spans from view v 2 , and vice versa. Our loss combines these two objectives as fol- lows. The first part of the loss corresponds to the loss of the RMN. We use negative sam- pling to induce parameters for each view which reconstruct their respective view-specific input well. Formally, assuming model input from view v 1 , e t v1 , we construct a set of 10 'negative inputs'{e n 1 v1 , ...e n I v1 } which are sampled at random from the v1 input corpus. We want to learn pa- rameters encoding view v 1 to reconstruct the input such that the inner product between the true in- put e t v 1 and its reconstruction r t v1 is higher than the inner product between r t v1 and any of the neg- ative samples e n i v1 by a margin of at least 1,</p><formula xml:id="formula_9">J(θ) = t i max(0, 1 − r t v1 e t v1 + r t v1 e n i v1 ),<label>(5)</label></formula><p>where θ refers to the set of all model parameters. We add an orthogonality-encouraging regulariz- ing term to this objective in order to obtain view- specific descriptors which are distant from each other <ref type="bibr" target="#b13">(Hyvärinen and Oja, 2000</ref>),</p><formula xml:id="formula_10">J(θ) = t i max(0, 1 − r t v1 e t v1 + r t v1 e n i v1 ) + λ||Rv1R T v1 − I||.<label>(6)</label></formula><p>The loss is defined analogously for input of view v 2 . Note that so far, the loss is defined in an entirely view-specific way, independent of the v2 parameters (e.g., the v1 loss in equation <ref type="formula" target="#formula_10">(6)</ref> is independent of the v2 parameters). We break this independence by adding a sec- ond term to our loss function, which ensures that view-specific parameters encode only relevant in- formation. That is, we want v 2 -specific parameters to only encode v 2 -specific information, and vice versa. Assuming model input from view v 1 , e t v 1 ,  we learn parameters for to view v 2 that reconstruct the input poorly. Again, we use the max-margin framework, maximizing the margin between the (high) quality reconstruction of e t v 1 from v 1 pa- rameters, r t v1 , and the (poor) quality of the recon- struction from v 2 parameters, r t v2 ,</p><formula xml:id="formula_11">K(θ) = max(0, 1 − e t v 1 r t v1 + e t v 1 r t v2 ).<label>(7)</label></formula><p>The update is defined analogously, swapping v1 and v2 subscripts, when the true input stems from v2. The full loss is defined as a weighted linear combination of its terms (eqns (6) and <ref type="formula" target="#formula_11">(7)</ref>),</p><formula xml:id="formula_12">L(θ) =βJ(θ) + (1 − β)K(θ).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic Micro-Cluster Evaluation</head><p>MVPlot induces structured representations of a novel b as relation trajectories between charac- ters pairs in b, and property trajectories of indi- vidual characters in b. Are those representations rich and informative enough to produce mean- ingful and interpretable micro-clusters of novels?</p><p>In Section 6.1 we evaluate the quality of such micro-clusters, i.e., local novel neighbourhoods in model space. We propose an objective and empir- ical evaluation employing expert-provided seman- tic novel tags in the Amazon catalog. Novels listed in the Amazon catalog are tagged with respect to their genre (e.g., mystery, ro- mance).</p><p>They are further labelled with re- finements pertaining to diverse information like character types or mood, which take dif- ferent sets of values, depending on the genre, and are as such predestined as an objective reference for evaluating the diverse information captured by our model. <ref type="table" target="#tab_2">Table 2</ref> lists example tags for the re- finement character type.</p><p>All tags are provided by publishers and can con- sequently be taken as a reliable source of infor- mation. In our evaluation we assume that novels which share a tag are related to each other. We use this tag-overlap metric to evaluate local neigh- bourhoods of book representations in model space.</p><p># novels # v1 sequences # v2 sequences Gutenberg 3,500 45,182 60,493 Amazon 10,000 91,511 70,156 <ref type="table">Table 3</ref>: The number of novels and property (v 1 ) relation (v 2 ) input sequences for the Gutenberg and the Amazon corpus.</p><p>We selected a set of 50 representative tags from the Amazon catalog and did not tune this set for our evaluation. The full tag set is included in the supplementary material. Note that while this scheme provides an em- pirical way of evaluating plot representations, it may not capture their full potential: our models are not explicitly tuned towards producing micro- clusters which are coherent with respect to our gold-standard tags, and may encode additional structure which is not probed in this evaluation. That said, we consider this evaluation as a good procedure to evaluate the relative quality of differ- ent models in the sense that a better model should produce micro-clusters that better correspond to reference clusters derived from gold-standard tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data</head><p>We evaluate our model on two data sets. First, we create a diverse data set by sampling 10,000 digital novels under our 50 gold-standard tags (cf., Section 4) of the Amazon catalog (Ama- zon). Our second data set consists of 3,500 nov- els from Project Gutenberg, a large digital collec- tion of freely available novels consisting primar- ily of classic literature (Gutenberg). The Ama- zon novels are already labelled with genre and re- finement tags, such that evaluation using our gold- standard is straightforward. While Gutenberg nov- els come with the advantage of being freely avail- able, they are unlabelled, and not fully covered by our 50 gold-standard tags. We therefore restrict our quantitative analysis to the Amazon data set. However, we also report qualitative results on the Gutenberg corpus, demonstrating that our model induces meaningful novel representations for cor- pora of varying size and diversity.</p><p>Both data sets were pre-processed with the BookNLP pipeline ( <ref type="bibr" target="#b2">Bamman et al., 2014</ref>) for coreference resolution of character mentions. We filtered stop-words and low-frequency words by discarding the 500 most frequent words and those which occur in less than 100 novels, and discarded novels less than 100 sentences long or containing fewer than 5 characters from our data set.</p><p>We created view-specific input corpora as fol- lows: (1) a relation corpus of chronologically or- dered sequences of text spans of 20 words for char- acter pairs {c 1 , c 2 } in a book b, S c 1 ,c 2 ,b v2 , which mention only c 1 and c 2 with a margin of 10 words for the Amazon corpus (1 word for the smaller Gutenberg corpus) but no other character; and (2) a property corpus of chronologically ordered sequences of 20 word text spans for individual characters c in book b, S c 1 ,c 2 ,b v2 , which mention only c, using the same margins as above.</p><p>We keep only sequences of length n time steps s.th., 5 ≤ n ≤ 250. We only keep pair sequences if we also obtain sequences for each individual character confirming to the above criteria. <ref type="table">Table 3</ref> summarizes statistics on our input corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>Section 6.1 quantitatively evaluates the quality of local neighbourhoods in model space induced from the Amazon corpus against our proposed gold-standard. Section 6.2 evaluates the quality of the induced descriptors from both the Amazon and Gutenberg corpus both through crowd sourc- ing and illustrative examples.</p><p>Models We set the MVPlot performance into perspective comparing it against the RMN. 6 MV- Plot induces both character properties and rela- tions, and is trained on both the relation-view and property-view input, while the RMN only induces pair relationships and can only utilize relation- view input. In addition, we report a frequency baseline, which is trained on both property and relation-view input. We concatenate all input spans of a given view for a particular novel; con- struct its term frequency vector and use cosine similarity to compute the nearest neighbours to each novel.</p><p>Parameter settings Across all experiments and corpus-specific models, we set β=0.99 for MV- Plot, and for both MVPlot and RMN we set α=0.5, λ=10 −5 , k=50. <ref type="bibr">7</ref> We train both RMN and MVPlot for 15 epochs using SGD and ADAM <ref type="bibr" target="#b16">(Kingma and Ba, 2014</ref>). <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Nearest Neighbours Evaluation</head><p>We evaluate local neighbourhoods in model space using the 500 most popular novels by their num- ber of Amazon reviews as reference novels from the Amazon corpus. For each reference novel we compute the 10 nearest neighbours as described below. We measure neighbourhood quality us- ing the gold-standard tags from Section 4, regard- ing neighbours as relevant if at least one tag is shared with the reference novel. We report pre- cision at rank 10 (P @10) and mean average preci- sion (M AP ).</p><p>Method MVPlot represents a book b in terms of trajectories of weight vectors over relation de- scriptors T b v2 and property descriptors T b v1 . RMN only learns relation descriptors and their tra- jectories. For both models, we map each in- duced trajectory for book b to a fixed-sized k- dimensional vector representation by averaging the time-specific weight vectors, for example for a v 2 trajectory,</p><formula xml:id="formula_13">T c 1 ,c 2 ,b v 2 = 1 T c 1 ,c 2 ,b v2 t∈ T c 1 ,c 2 ,b v2 d t v2 ,<label>(9)</label></formula><p>and equivalently for v 1 trajectories, T c,b v1 . We compute the similarity between two books {b r , b c } as follows. We align the v 2 tra- jectory for each character pair {c 1 , c 2 } in b r , T c 1 ,c 2 ,br , to its closest neighbouring character pair vector in b c , T c 1 ,c 2 ,bc , by Euclidean distance, and compute the overall book similarity in terms of character relations between b r and b c as the av- erage over all distances.</p><formula xml:id="formula_14">sim br ,bc v2 = 1 |T br v 2 | T ∈T br v 2 arg min T ∈T bc v 2 dist(T , T ).<label>(10)</label></formula><p>We obtain sim br,bc v1 in an analogous process. For cosine and MVPlot we obtain a final, multi-view similarity by averaging similarity scores obtained in each view's space, </p><p>For RMN we compute similarity only in character relation space.</p><p>Results <ref type="table">Table 4</ref> presents micro-cluster quality in terms of precision@10 and M AP . The full MVPlot model statistically significantly outper- foms all other models. <ref type="bibr">9</ref> The same pattern emerges <ref type="bibr">9</ref> Also, intra-view comparisons except for MVPlot v1 and cosine v1 are statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>View P @10 M AP</p><formula xml:id="formula_16">cosine v 1 0.516 ‡ 0.392 † v 2 0.468 ‡ 0.339 ‡ both 0.512 ‡ 0.390 ‡ RMN v 2 0.479 ‡ 0.347 ‡ MVPlot v 1 0.529 † 0.401 † v 2</formula><p>0.496 ‡ 0.367 ‡ both 0.546 0.421 <ref type="table">Table 4</ref>: Micro-cluster quality results (Amazon corpus). Differences of cosine and RMN com- pared to the best MVPlot result are significant with p &lt; 0.05 ( †) or p &lt; 0.01 ( ‡) (paired t-test).</p><p>when comparing models with the same underly- ing views: MVPlot v2 outperforms both cosine v2 and RMN v2 (similarly for MVPlot v1 and cosine v1), indicating that the MVPlot character relation representations are most informative for micro-cluster induction.</p><p>In order to shed light on the contribution of in- dividual model components, we compare the full MVPlot model (both) to model versions with ac- cess to only v1 or v2 <ref type="table">(Table 4</ref> bottom). Combining information from both views boosts performance compared to the single-view versions. This con- firms that MVPlot indeed encodes distinct and rel- evant information in the respective views.</p><p>While cosine is a strong baseline, its representa- tions are not structured or interpretable. It conse- quently does not provide sufficient information for applications like book tagging or recommendation with respect to specific aspects or criteria. Simi- larly, RMN cannot learn representations of multi- ple, distinct views of the plot.</p><p>Advancing our understanding of the informa- tion encoded in the individual views of MVPlot, we took a closer look at the refinement tags for which the single view MVPlot model (v1) has the clearest advantage over the pair view MVPlot model (v2), and vice-versa. We computed tag- wise F1-scores for the two MVPlot variants. Ta- ble 5 lists the book tags for which the scores of the two views diverge the most.</p><p>In terms of types of refinements, view v2 suffers most for predicting book categories, or topical tags ('sports', 'second changes'), while view v1 is par- ticularly deficient for predicting character types. While this seems counterintuitive we hypothesize that character types are to a large extent defined by their interactions with, or relations to, other char- <ref type="table">Tag  RefType  Tag  RefType  Robots &amp; Androids Character  Hard SciFi  Category  Corporations  Character  Sports  Category  International  Theme  Horror  Theme  Aliens  Character Second Chances Theme  Cowboys  Character  Crime  Category   Table 5</ref>: The tags (Tag) and their refinement types (RefType) for which MVPlot v1 most clearly out- performs MVPlot v2 (left) and vice versa (right) in terms of tag-specific F1-measure.</p><formula xml:id="formula_17">F 1 v2 &gt;&gt; F 1 v1 F 1 v1 &gt;&gt; F 1 v2</formula><p>acters. Topical information, however, is encoded robustly in the properties of individual characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluating Induced Descriptors</head><p>This evaluation investigates whether induced re- lation descriptors indeed capture relational infor- mation. We evaluate the interpretability of the in- duced descriptors, comparing the v 2 (relation) de- scriptors induced by RMN and MVPlot. We apply both models to both the Amazon and the Guten- berg corpus, and report results on both data sets.</p><p>Method We collect crowdsourced judgments on Amazon Mechanical Turkto qualitatively evaluate the learnt descriptors, following <ref type="bibr" target="#b6">Chaturvedi et al. (2017)</ref>. In each task a worker is shown one in- duced descriptor as a set of its 10 closest words in GloVe space (like in <ref type="table" target="#tab_3">Table 1</ref>), and is asked to indicate whether "the words in the group describe a relation, event or interaction between people". We compare the proportion of positive answers, i.e., the number of descriptors considered relevant, for RMN descriptors and MVPlot pair descriptors. We collect 30 judgments for each of k=50 de- scriptors induced by the respective models.</p><p>Results <ref type="figure" target="#fig_4">Figure 2</ref> displays our results. We ob- serve a similar pattern of ratings across models and corpora, e.g., around 50% of the descriptors are labelled as relevant by at least 50% of the annota- tors. None of the differences are statistically sig- nificant which lets us conclude that interpretability of induced descriptors is comparable for the RMN and MVPlot. This is encouraging because we con- firm that representation interpretability is not com- promised by MVPlot's more complex objective.  v 2 descriptor refers to a love relation). Despite its smaller size and more homogeneous nature, we show that MVPlot learns meaningful representa- tions from the Gutenberg corpus, demonstrating the flexibility of our model. <ref type="figure" target="#fig_5">Figure 3</ref> further illustrates this, displaying ex- ample local neighbourhoods of four reference nov- els (left) with their eight nearest neighbours or- dered by proximity (left to right). The neighbour- hoods are intuitively meaningful, and particularly impressive bearing in mind that the full model space contains 3, 500 novels. While most neigh- bourhoods are dominated by novels of the same author, some exceptions emerge. Row two, for example, contains novels by Thomas Hardy and Charles Dickens who both are known for bio- graphical 17th century novels focusing on class and social changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Content-based micro-clustering of novels is a complex but interesting task. In order to even- tually augment the diverse associations humans have, models must be able to pick up rich and structured signals from raw text. This paper pre- sented a deep recurrent autoencoder which learns multi-view representations of plots, and intro- duced a principled evaluation framework using clusters based on expert-provided book tags.</p><p>Our evaluation showed that rich multi-view rep- resentations are better suited to recover such refer- ence clusters compared to each individual view, as well as compared to simpler, but competitive mod- els which induce less structured representations. Our view-specific representations are interpretable which allows to analyse and explain the emerging micro-clusters, and might reveal previously unno- ticed parallels between novels and may be useful for literary analysis or content-based recommen- dation. This is an exciting avenue for future work.</p><p>Our method is general and scalable both in terms of its input, utilizing raw text with only au- tomatic pre-processing, and in terms of the num- ber of distinct views it can learn. We described an objective function which triggers views to encode distinct information. In future work we plan to ex- plore joint learning of more and different views.</p><p>Our approach relies strongly on the assumption that text spans mentioning two characters contain information about character relations, and that text spans mentioning one character contain informa- tion about the character's properties. While our re- sults suggest that these assumptions are valid, they are arguably crude. In the future we plan to define more targeted input, e.g., by using semantic and syntactic information from dependency parses.</p><p>In this work we induced dual-view representa- tions of book content, however, we emphasize that the proposed method is very general. The number and kinds of views, as well as underlying data are in no way constrained, as long as relevant view- specific input can be defined. In the context of novel representation it would be interesting to in- duce additional views, for example one that cap- tures the mood of a novel. Another interesting av- enue for future work would be to apply the frame- work to questions arising in the digital humanities, e.g., to extract different views from news articles.</p><p>The presented model and evaluation are de- signed with the objective to detect a different kinds of similarity between novels, with the ultimate goal to enrich human-provided genres and tags. We described a first step in this direction, verifying that the learnt information is meaningful and can reproduce human-created semantic book tags. Ex- pert book tags exist for a wide variety of informa- tion (mood, theme, characters), and provide a rich evaluation environment for learnt representations. We invite the community to join us in exploring the full space of opportunities and evaluating in- duced representations holistically in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of the MVPlot model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results of descriptor interpretability. (% of descriptors marked as 'relevant descriptors of relations' by various proportions of annotators).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Nearest neighbours for four classic stories from the Gutenberg Corpus. Target novels on the left (with red border), and NNs are presented in the same row, ordered by their distance to the target novel.</figDesc><graphic url="image-1.png" coords="9,72.00,62.81,453.54,255.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example tags from the Amazon book cat-
alog for the refinement character type. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 displays</head><label>1</label><figDesc></figDesc><table>examples of property and re-
lation descriptors induced by MVPlot from the 
Gutenberg corpus. We can see that the different 
views indeed capture differing information (e.g., a 
v 1 descriptor refers to individuals' titles, while a 

</table></figure>

			<note place="foot" n="4"> with respect to their occurrence in the novel 5 The RMN learns background encodings for characters in addition to the book embeddings. We omit this for MVPlot as this information is explicitly learned in the views.</note>

			<note place="foot" n="6"> We do not compare against topic model baselines because they were outperformed by RMN (Iyyer et al., 2016). 7 Parameters were tuned on a small subset of books used in the nearest neighbourhood evaluation (Section 6.1). 8 Our implementation builds on the available RMN code https://github.com/miyyer/rmn.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Alex Klementiev, Kevin Small, Joon Hao Chuah and Mohammad Kanso for their valuable insights, feedback and technical help on the work presented in this paper. We also thank the anonymous reviewers for their valuable feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning latent personas of film characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bayesian mixed effects model of literary character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative schemas and their participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connections between the lines: augmenting social networks with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of evolving relationships between literary characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting framenet for content-based book recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Orphée De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Schuhmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on New Trends in Content-based Recommender Systems co-located with the 8th ACM Conference on Recommender Systems</title>
		<meeting>the 1st Workshop on New Trends in Content-based Recommender Systems co-located with the 8th ACM Conference on Recommender Systems<address><addrLine>CBRecSys@RecSys; Foster City, Silicon Valley, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Character-based kernels for novelistic plot structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="634" to="644" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting social networks from literary fiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Dames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hidden topic markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rosen-Zvi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
<note type="report_type">JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Independent component analysis: Algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feuding families and former friends: Unsupervised learning for dynamic fictional relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Jockers</surname></persName>
		</author>
		<title level="m">Macroanalysis: Digital Methods and Literary History</title>
		<imprint>
			<publisher>University of Illinois Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Content-based Recommender Systems: State of the Art and Trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Marco De Gemmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semeraro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer US</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Annotating character relationships in literary texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Massey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1512.00728</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Plot induction and evolutionary search for story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1562" to="1572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contentbased book recommending using learning for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loriene</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR-99 Workshop on Recommender Systems: Algorithms and Evaluation</title>
		<meeting>the SIGIR-99 Workshop on Recommender Systems: Algorithms and Evaluation<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Annotating Characters in Literary Corpora: A Scheme, the CHARLES Tool, and an Annotated Novel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hardik</forename><surname>Vala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Piper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Ruths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
