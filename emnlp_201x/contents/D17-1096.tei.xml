<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sound-Word2Vec: Learning Word Representations Grounded in Sounds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Vijayakumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Virgina Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sound-Word2Vec: Learning Word Representations Grounded in Sounds</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="920" to="925"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To be able to interact better with humans, it is crucial for machines to understand sound-a primary modality of human perception. Previous works have used sound to learn embeddings for improved generic semantic similarity assessment. In this work, we treat sound as a first-class citizen , studying downstream 6textual tasks which require aural grounding. To this end, we propose sound-word2vec-a new embedding scheme that learns specialized word embeddings grounded in sounds. For example, we learn that two seemingly (se-mantically) unrelated concepts, like leaves and paper are similar due to the similar rustling sounds they make. Our embed-dings prove useful in textual tasks requiring aural reasoning like text-based sound retrieval and discovering Foley sound effects (used in movies). Moreover, our embedding space captures interesting dependencies between words and onomatopoeia and outperforms prior work on aurally-relevant word relatedness datasets such as AMEN and ASLex.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sound and vision are the dominant perceptual sig- nals, while language helps us communicate com- plex experiences via rich abstractions. For exam- ple, a novel can stimulate us to mentally construct the image of the scene despite having never phys- ically perceived it. Indeed, language has evolved to contain numerous constructs that help depict vi- sual concepts. For example, we can easily form the picture of a white, furry cat with blue eyes via. a description of the cat in terms of its visual at- tributes ( <ref type="bibr" target="#b11">Lampert et al., 2009;</ref><ref type="bibr" target="#b17">Parikh and Grauman, 2011</ref>).</p><p>Need for Onomatopoeia. However, how would one describe the auditory instantiation of cats? While a first thought might be to use audio de- scriptors like loud, shrill, husky etc. as mid-level constructs or "attributes", arguably, it is difficult to precisely convey and comprehend sound through such language. Indeed, <ref type="bibr" target="#b23">Wake and Asahi (1998)</ref> find that humans first communicate sounds using "onomatopoeia" -words that are suggestive of the phonetics of sounds while having no explicit meaning e.g. meow, tic-toc. When asked for further explanation of sounds, humans provide descriptions of potential sound sources or impres- sions created by the sound (pleasant, annoying, etc.)</p><p>Need for Grounding in Sound. While ono- matopoeic words exist for commonly found concepts, a vast majority of concepts are not as perceptually striking or sufficiently frequent for us to come up with dedicated words describing their sounds. Even worse, some sounds, say, musical instruments, might be difficult to mimic using speech. Thus, for a large number of concepts there seems to be a gap between sound and its counterpart in language <ref type="bibr" target="#b22">(Sundaram and Narayanan, 2006</ref>). This becomes problematic in specific situations where we want to talk about the heavy tail of concepts and their sounds, or while describing a particular sound we want to create as an effect (say in movies). To alleviate this, a common literary strategy is to provide metaphors to more relatable exemplars. For example, when we say, "He thundered angrily", we compare the person's angry speech to the sound of thunder to convey the seriousness of the situation. However, without this grounding in sound, thunder and anger both appear to be seemingly unrelated concepts in terms of semantics.</p><p>Contributions. In this work, we learn em- beddings to bridge the gap between sound and its counterpart in language. We follow a retrofitting strategy, capturing similarity in sounds associated with words, while using distributional semantics (from word2vec) to provide smoothness to the embeddings. Note that we are not interested in capturing phonetic similarity, but the grounding in sound of the concept associated with the word (say "rustling" of leaves and paper.) We demonstrate the effectiveness of our embeddings on three downstream tasks that require reasoning about related aural cues: 1. Text-based sound retrieval -Given a textual query describing the sound and a database con- taining sounds and associated textual tags, we retrieve sound samples by matching text (Sec. 5.1) 2. Foley Sound Discovery -Given a short phrase that outlines the technique of producing Foley sounds 1 , we discover other relevant words (ob- jects or actions) which can produce similar sound effects (Sec. 5.2) 3. Aurally-relevant word relatedness assessment on AMEN and ASLex ( <ref type="bibr" target="#b9">Kiela and Clark, 2015)</ref> (Sec. 5.3)</p><p>We also qualitatively compare with word2vec to highlight the unique notions of word relatedness captured by imposing auditory grounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Audio and Word Embeddings. Multiple works in the recent past ( <ref type="bibr" target="#b1">Bruni et al., 2014;</ref><ref type="bibr" target="#b12">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b13">Lopopolo and van Miltenburg, 2015;</ref><ref type="bibr" target="#b9">Kiela and Clark, 2015;</ref><ref type="bibr" target="#b10">Kottur et al., 2016)</ref> have explored using perceptual modalities like vision and sound to learn language embeddings. While <ref type="bibr" target="#b13">Lopopolo and van Miltenburg (2015)</ref> show preliminary results on using sound to learn distributional representations, <ref type="bibr" target="#b9">Kiela and Clark (2015)</ref> build on ideas from <ref type="bibr" target="#b1">Bruni et al. (2014)</ref> to learn word embeddings that respect both linguistic and auditory relationships by optimizing a joint objective. Further, they propose various fusion strategies to combine knowledge from both the modalities. Instead, we "specialize" embeddings to exclusively respect relationships defined by sounds, while initializing with word2vec embed- dings for smoothness. Similar to previous findings <ref type="bibr" target="#b14">(Melamud et al., 2016)</ref>, we observe that our spe- cialized embeddings outperform language-only as well as other multi-modal embeddings in the downstream tasks of interest. In an orthogonal and interesting direction, other recent works ( <ref type="bibr" target="#b2">Chung et al., 2016;</ref><ref type="bibr" target="#b7">He et al., 2016;</ref><ref type="bibr" target="#b20">Settle and Livescu, 2016</ref>) learn word representa- tions based on similarity in their pronunciation and not the sounds associated with them. In other words, phonetically similar words that have near identical pronunciations are brought closer in the embedding space (e.g., flower and flour). <ref type="bibr" target="#b22">Sundaram and Narayanan (2006</ref>) study the appli- cability of onomatopoeia to obtain semantically meaningful representations of audio.</p><p>Using a novel word-similarity metric and principal component analysis, they find representations for sounds and cluster them in this derived space to reason about similarities. In contrast, we are inter- ested in learning word representations that respect aural-similarity. More importantly, our approach learns word representations for in a data-driven manner without having to first map the sound or its tags to corresponding onomatopoeic words.</p><p>Multimodal Learning with Surrogate Su- pervision. <ref type="bibr" target="#b10">Kottur et al. (2016)</ref> and <ref type="bibr" target="#b16">Owens et al. (2016)</ref> use a surrogate modality to induce supervision to learn representations for a desired modality. While the former learns word embed- dings grounded in cartoon images, the latter learns visual features grounded in sound. In contrast, we use sound as the surrogate modality to supervise representation learning for words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>Freesound. We use the freesound database <ref type="bibr" target="#b5">(Font et al., 2013)</ref>, also used in prior work ( <ref type="bibr" target="#b9">Kiela and Clark, 2015;</ref><ref type="bibr" target="#b13">Lopopolo and van Miltenburg, 2015)</ref> to learn the proposed sound-word2vec embeddings. Freesound is a freely available, collaborative dataset consisting of user uploaded sounds permitting reuse. All uploaded sounds have human descriptions in the form of tags and captions in natural language. The tags contain a broad set of relevant topics for a sound (e.g., ambience, electronic, birds, city, reverb) and captions describing the content of the sound, in addition to details pertaining to audio quality. For the text-based sound retrieval task, we use a subset of 234,120 sounds from this database and divide it into training (80%), validation (10%) and testing splits (10%). Further, for foley sound discovery, we aggregate descriptions of foley sound production provided by sound engineers (epicsound, accessed 23-Jan-2017; Singer, ac- cessed 23-Jan-2017) to create a list of 30 foley sound pairs, which forms our ground truth for the task. For example, the description to produce a foley "driving on gravel" sound is to record the "crunching sound of plastic or polyethene bags". <ref type="bibr">AMEN and ASLex. AMEN and ASLex (Kiela and Clark, 2015</ref>) are subsets of the standard MEN ( <ref type="bibr" target="#b1">Bruni et al., 2014</ref>) and <ref type="bibr">SimLex (Hill et al., 2015</ref>) word similarity datasets consisting of word-pairs that "can be associated with a distinctive associated sound". We evaluate on this dataset for completeness to benchmark our approach against previous work. However, we are primarily interested in the slightly different problem of relating words with similar auditory instantions that may or may not be semantically related as opposed to relating semantically similar words that can be associated with some common auditory signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We use the Freesound database to construct a dataset of tuples {s, T }, where s is a sound and T is the set of associated user-provided tags. We then aim to learn an embedding space for the tags that respects auditory grounding using sound information as cross-modal context -similar to word2vec <ref type="bibr" target="#b15">(Mikolov et al., 2013</ref>) that uses neighboring words as context / supervision. We now explain our approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio Features and Clustering.</head><p>We repre- sent each sound s by a feature vector consisting of the mean and variance of the following audio descriptors that are readily available as part of Freesound database:</p><p>• Mel-Frequency Cepstral Co-efficients: This feature represents the short-term power spec- trum of an audio and closely approximates the response of the human auditory system -com- puted as given in ( <ref type="bibr" target="#b6">Ganchev et al., 2005</ref>).</p><p>• Spectral Contrast: It is the magnitude difference  <ref type="bibr" target="#b19">(Ricard, 2004</ref>). We then use K-Means algorithm to cluster the sounds in this feature space to assign each sound to a cluster C(s) ∈ {1, . . . K}. We set K to 30 by evaluating the performance of the embeddings on text-based audio-retrieval on the held out validation set. Note that the clustering is only performed once, prior to representation learning described below.</p><p>Representation Learning. We represent each tag t ∈ T using a |V| dimensional one-hot encoding denoted by v t , where V is the set of all unique tags in the training set (the size of our dictionary). This one-hot vector v t is projected into a D- dimensional vector space via W P ∈ R |V|×D , the projection matrix. This projection matrix com- putes the representation for each word in V. The idea of our approach is to use W P to accurately predict cluster assignments (for sounds associated with words), which enforces grounding in sound. For each data-point, we obtain the summary of the tags T , by averaging the projections of all tags in the set as 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|T |</head><p>t∈T W P v t . We then transform the so obtained summary representation via a linear layer (with parameters W O ) and pass the output through the softmax function to obtain a distribution, p(c|T ) over the K sound clusters. We perform maximum-likelihood training for the correct cluster assignment C(s) 2 , optimizing for parameters W P and W O :</p><formula xml:id="formula_0">max W P ,W O log P (c = C(s)|T )<label>(1)</label></formula><p>We use SGD with momentum to optimize this objective which essentially is the cross-entropy between cluster assignments and p(c|T ). We set D to 300 to be consistent with the publicly available word2vec embeddings.</p><p>Initialization. We initialize W P with word2vec embeddings ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) trained on the Google news corpus dataset with ∼3M words. We fine-tune on a subset of 9578 tags which are present in both Freesound as well as Google news corpus datasets, which is 55.68% of the original tags in the Freesound dataset. This helps us remove noisy tags unrelated to the content of the sound.</p><p>In addition to enlarging the vocabulary, the pre- training helps induce smoothness in the sound- word2vec embeddings -allowing us to transfer semantics learnt from sounds to words that were not present as tags in the Freesound database. In- deed, we find that word2vec pre-training helps im- prove performance (Sec. 5.3). Our use of language embeddings as an initialization to fine-tune (spe- cialize) from, as opposed to formulating a joint objective with language and audio context <ref type="bibr" target="#b9">(Kiela and Clark, 2015</ref>) is driven by the fact that we are interested in embeddings for words grounded in sounds, and not better generic word similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Ablations. In addition to the language-only base- line word2vec <ref type="bibr" target="#b15">(Mikolov et al., 2013)</ref>, we com- pare against tag-word2vec -that predicts a tag us- ing other tags of the sound as context, inspired by <ref type="bibr" target="#b4">(Font et al., 2014</ref>). We also report results with a randomly initialized projection matrix (sound- word2vec(r) to evaluate the effectiveness of pre- training with word2vec. Prior work. We compare against previous works <ref type="bibr" target="#b13">Lopopolo and van Miltenburg (2015)</ref> and <ref type="bibr" target="#b9">Kiela and Clark (2015)</ref>. While the former uses a stan- dard bag of words and SVD pipeline to arrive at distributional representations for words, the latter trains under a joint objective that respects both lin- guistic and auditory similarity. We use the openly available implementation for <ref type="bibr" target="#b13">Lopopolo and van Miltenburg (2015)</ref> and re-implement <ref type="bibr" target="#b9">Kiela and Clark (2015)</ref> and train them on our dataset for a fair comparison of the methods. In addition, we show a comparison to word-vectors released by <ref type="bibr" target="#b9">(Kiela and Clark, 2015</ref>) in the supplementary material. All approaches use an embedding size of 300 for consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Text-based Sound Retrieval</head><p>Given a textual description of a sound as query, we compare it with tags associated with sounds in the database to retrieve the sound with the closest matching tags. Note that this is a purely textual task, albeit one that needs awareness of sound. In a sense, this task exactly captures what we want our model to be able to do -bridge the semantic gap between language and sound. We use the training split (Sec. 3) to learn the sound-word2vec vectors, validation to pick the number of clusters (K), and report results on the test split. For retrieval, we represent sounds by averaging the learnt embeddings for the associated tags. We embed the caption provided for the sound (in the Freesound database) in a similar manner, and use it as the query. We then rank sounds based on the cosine similarity between the tag and query representations for retrieval. We evaluate using standard retrieval metrics -Recall@{1,10,50,100}. Note that the entire testing set (≈10k sounds) is present in the retrieval pool. So, recall@100 corresponds to obtaining the correct result in the top 1% of the search results, which is a relatively stringent evaluation criterion.</p><p>Results. Table. 1 shows that our sound-word2vec embeddings outperform the baselines. We see that specializing the embeddings for sound using our two-stage training outperforms prior work( <ref type="bibr" target="#b9">Kiela and Clark (2015)</ref> and <ref type="bibr" target="#b13">Lopopolo and van Miltenburg (2015)</ref>), which did not do specialization. Among our approaches, tag-word2vec performs second best -this is intuitive since the tag dis- tributions implicitly capture auditory relatedness (a sound may have tags cat and meow), while word2vec and sound-word2vec(r) have the lowest performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recall</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@1</head><p>@10 @50 @100 word2vec 6.47±0.00 14.25±0.05 21.72±0.12 26.03±0.22 tag-word2vec</p><p>6.95±0.02 15.10±0.03 22.43±0.09 27.21±0.24 sound-word2vec(r) 6.49±0.00 14.98±0.03 21.96±0.11 <ref type="bibr">26.43±0.20 (Lopopolo and van Miltenburg, 2015)</ref> 6.48±0.02 15.09±0.05 21.82±0.13 26.89±0. <ref type="bibr">23</ref> ( <ref type="bibr" target="#b9">Kiela and Clark, 2015)</ref> 6.52±0.01 15.21±0.03 21.92±0.08 27.74±0.21 sound-word2vec</p><p>7.11±0.02 15.88±0.04 23.14±0.09 28.67±0.17   <ref type="bibr" target="#b9">Kiela and Clark, 2015</ref>) (higher is better). Our ap- proach performs better than <ref type="bibr" target="#b9">Kiela and Clark (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Foley Sound Discovery</head><p>In this task, we evaluate how well embeddings identify matching pairs of target sounds (flapping bird wings) and descriptions of Foley sound production techniques (rubbing a pair of gloves).</p><p>Intuitively, one expects sound-aware word embed- dings to do better at this task than sound-agnostic ones. We setup a ranking task by constructing a set of original Foley sound pairs and decoy pairs formed by pairing the target description with every word from the vocabulary. We rank using cosine similarity between the average word-vectors in each member of the pair. A good embedding is one in which the original Foley sound pair has the lowest rank. We use the mean rank of the Foley sound in the dataset for evaluation. We transfer the embeddings from Sec. 5.1 to this task, without additional training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results.</head><p>We find that Sound-word2vec per- forms the best with a mean rank of 34.6 compared to other baselines tag-word2vec (38.9), sound- word2vec(r) (114.3) and word2vec (189.45). As observed previously, the second best performing approach is tag-word2vec. <ref type="bibr" target="#b13">Lopopolo and van Miltenburg (2015)</ref> and <ref type="bibr" target="#b9">Kiela and Clark (2015)</ref> perform worse than tag-word2vec with a mean rank of 48.4 and 42.1 respectively. Note that ran- dom chance gets a rank of (|V| + 1)/2 = 4789.5. <ref type="bibr">AMEN and ASLex (Kiela and Clark, 2015</ref>  concepts related by sound; however we observe that relatedness is often confounded. For exam- ple, (river, water), (automobile, car) are marked as aurally related however they do not stand out as aurally-related examples as they are already se- mantically related. In contrast, we are interested in how onomatopoeic words relate to regular words <ref type="table" target="#tab_3">(Table 3)</ref>, which we study by explicit grounding in sound. Thus while we show competitive perfor- mance on this dataset, it might not be best suited for studying the benefits of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on AMEN and ASLex</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Conclusion</head><p>We show nearest neighbors in both sound- word2vec and word2vec space <ref type="table" target="#tab_3">(Table 3)</ref> to quali- tatively demonstrate the unique dependencies cap- tured due to auditory grounding. While word2vec maps a word (say, apple) to other semantically similar words (other fruits), similar 'sounding' words (chips) or onomatopoeia (munch) are closer in our embedding space. Moreover, onomatopoeic words (say, boom and slam) are mapped to rele- vant objects (explosion and door). Interestingly, parts (e.g., lock, latch) and actions (closing) are also closer to the onomatopoeic query -exhibit- ing an understanding of the auditory scene.</p><p>Conclusion. In this work we introduce a novel word embedding scheme that respects auditory grounding. We show that our embeddings provide strong performance on text-based sound retrieval, Foley sound discovery along with intuitive nearest neighbors for onomatopoeia that are tasks in text requiting auditory reasoning. We hope our work motivates further efforts on understanding and re- lating onomatopoeia words to "regular" words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The model used to learn the proposed soundword2vec embeddings. The projection matrix WP containing that is used as the sound-word2vec embedding is learned by training the model to accurately predict the cluster assignment of the sound. in the peaks and valleys of the spectrum-computed according to (Akkermans et al., 2009). • Dissonance: It measures the perceptual roughness of the sound (Plomp and Levelt, 1965). • Zero-crossing Rate: It is the percentage of sign changes between consecutive signal values and is indicative of noise content. • Spectral Spread: This feature is the concatenation of the k-order moments of the spectrum, where k ∈ {0, 1, 2, 3, 4}. • Pitch Salience: This feature helps discriminate between musical and non-musical tones. While, pure tones and unpitched sounds have values near 0, musical sounds containing harmonics have higher values (Ricard, 2004). We then use K-Means algorithm to cluster the sounds in this feature space to assign each sound to a cluster C(s) ∈ {1,. .. K}. We set K to 30 by evaluating the performance of the embeddings on text-based audio-retrieval on the held out validation set. Note that the clustering is only performed once, prior to representation learning described below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Text-based sound retrieval (higher is better).</head><label>1</label><figDesc></figDesc><table>We find 
that our sound-word2vec model outperforms all baselines. 

Embedding 
Spearman Correlation ρ s 
AMEN 
ASLex 

(Lopopolo and van Miltenburg, 2015) 0.410±0.09 0.237±0.04 
(Kiela and Clark, 2015) 
0.648±0.08 0.366±0.11 
sound-word2vec 
0.674±0.05 0.391±0.06 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison to state of the art AMEN and ASLex 
datasets (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>We show nearest neighbors in both word2vec and 
sound-word2vec spaces for eight words ('regular' words, top 
half and onomatopoeic words, bottom half). 

</table></figure>

			<note place="foot" n="1"> Foley sounds are sound effects (typically ambient sounds) that are added to movies in the post-production stage to make actions or situations appear more realistic. These sounds are generally created using easily available proxy objects that mimic the sound of the true situation being depicted. For example, sound of breaking celery sticks is used to create the effect of breaking bones.</note>

			<note place="foot" n="2"> We also tried to regress directly to sound features instead of clustering, but found that it had poor performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape-based spectral contrast descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Akkermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Serrà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Sound and Music Computing Conf.(SMC)</title>
		<meeting>of the Sound and Music Computing Conf.(SMC)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>and Perfecto Herrera</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Chung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The guide to sound effects</title>
		<imprint>
			<date type="published" when="2017-01" />
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extending tagging ontologies with domain specific knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">György</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Semantic Web Conference</title>
		<meeting>the International Semantic Web Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Freesound technical demo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia</title>
		<meeting>the 21st ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparative evaluation of various mfcc implementations on the speaker verification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Fakotakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kokkinakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPECOM</title>
		<meeting>the SPECOM</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-view recurrent neural acoustic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04496</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-and cross-modal semantics beyond vision: Grounding in auditory perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.02598</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sound-based distributional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lopopolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IWCS</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00893</idno>
		<title level="m">The role of context types and dimensionality in learning word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ambient Sound Provides Supervision for Visual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Relative Attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tonal consonance and critical bandwidth. The journal of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinier</forename><surname>Plomp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem Johannes Maria</forename><surname>Levelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>Acoustical Society of America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards computational morphological description of sound. DEA pre-thesis research work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Ricard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Barcelona</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Universitat Pompeu Fabra</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02550</idno>
		<title level="m">Discriminative acoustic word embeddings: Recurrent neural network-based approaches</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">R</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01" />
			<biblScope unit="volume">23</biblScope>
		</imprint>
		<respStmt>
			<orgName>Art of foley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vector-based representation and clustering of audio using onomatopoeia words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiva</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2006 Fall Symposia</title>
		<meeting>AAAI 2006 Fall Symposia</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sound retrieval with intuitive verbal expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanae</forename><surname>Wake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Asahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Auditory Display (ICAD)</title>
		<meeting>the 5th International Conference on Auditory Display (ICAD)</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
