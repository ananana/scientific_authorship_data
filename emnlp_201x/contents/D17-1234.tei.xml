<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affordable On-line Dialogue Policy Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhe</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Brain Science and Technology Research Center Shanghai</orgName>
								<orgName type="laboratory">Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Eng. SpeechLab</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affordable On-line Dialogue Policy Learning</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2200" to="2209"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The key to building an evolvable dialogue system in real-world scenarios is to ensure an affordable on-line dialogue policy learning, which requires the on-line learning process to be safe, efficient and economical. But in reality, due to the scarcity of real interaction data, the dialogue system usually grows slowly. Besides, the poor initial dialogue policy easily leads to bad user experience and incurs a failure of attracting users to contribute training data, so that the learning process is un-sustainable. To accurately depict this, two quantitative metrics are proposed to assess safety and efficiency issues. For solving the unsustainable learning problem, we proposed a complete companion teaching framework incorporating the guidance from the human teacher. Since the human teaching is expensive, we compared various teaching schemes answering the question how and when to teach, to economically utilize teaching budget, so that make the online learning process affordable.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A task-oriented dialogue system is designed for interacting with humans users to accomplish sev- eral predefined domains or tasks ( <ref type="bibr" target="#b30">Young et al., 2013;</ref><ref type="bibr" target="#b7">Daubigney et al., 2012)</ref>. Dialogue Man- ager is the core component in a typical dialogue system, which controls the flow of dialogue by a state tracker and a policy module ( <ref type="bibr" target="#b14">Levin et al., 1997</ref>). The state tracker tracks the internal s- tate of the system while the policy module de- cides the response to the user according to the s- tatus of states ( <ref type="bibr" target="#b18">Sun et al., 2014a</ref>; Thomson and * Both authors contributed equally to this work. . The approaches of constructing a policy module can be divided into two cate- gories: rule-based and statistical. Rule-based poli- cies are usually hand-crafted by domain experts which means they are inconvenient and difficult to be optimized <ref type="bibr" target="#b28">(Williams and Young, 2007;</ref><ref type="bibr" target="#b23">Wang and Lemon, 2013)</ref>. In recent mainstream statisti- cal studies, Partially Observable Markov Decision Process (POMDP) framework has been applied to model dialogue management with unobservable s- tates, where policy training can be formulated as a Reinforcement Learning (RL) problem, which enables the policy to be optimized automatically ( <ref type="bibr" target="#b13">Kaelbling et al., 1998;</ref><ref type="bibr" target="#b1">Arnold, 1998;</ref><ref type="bibr" target="#b30">Young et al., 2013)</ref>.</p><p>Though RL-based approaches have the poten- tial to improve themselves as they interact more with human users and achieve better performance than rule-based approaches, they are rarely used in real-world applications, especially in on-line sce- narios, since the training process is unsustainable. The main causes of unsustainable on-line dia- logue policy learning are two-fold:</p><p>• Safety issue: the initial policy trained from scratch may lead to terrible user experience at the early training period, thus fail to attract sufficient users for more dialogues to do fur- ther policy training.</p><p>• Efficiency issue: if the progress of policy learning is not so efficient, it will exhaust users' patience before the policy reaches a desirable performance level.</p><p>Prior works have mainly focused on improving efficiency, such as Gaussian Processes RL <ref type="bibr" target="#b9">(Gaši´Gaši´c et al., 2010)</ref>, deep RL ( <ref type="bibr" target="#b8">Fatemi et al., 2016)</ref>, etc. For deep RL approaches, recent researches on the student-teacher RL framework have shown promi- nent acceleration to policy learning process <ref type="bibr" target="#b22">(Torrey and Taylor, 2013;</ref><ref type="bibr" target="#b29">Williams and Zweig, 2016;</ref><ref type="bibr" target="#b0">Amir et al., 2016)</ref>. In such framework, the teach- er agent instructs the student agent by providing suggestions on what actions should be taken next <ref type="bibr" target="#b6">(Clouse, 1996)</ref>.</p><p>For the safety issue, <ref type="bibr" target="#b4">Chen et al. (2017)</ref> devel- oped several teaching strategies answering "how" the human teacher guide the learning process.</p><p>However, those previous teaching methods ex- clude "when" to teach from concern. They sim- ply exhaust all the budget continuously from the beginning, which is wasteful and causes a heavy workload of the human teacher. An affordable dia- logue policy learning with human teaching should require a lighter workload and economically uti- lize teaching budget.</p><p>Furthermore, as for safety and efficiency eval- uation, previous works have been observing the training curves and testing curves to tell which one is better, or evaluate policy performance after cer- tain dialogues of training, which are subjective and error prone <ref type="bibr" target="#b3">(Chen et al., 2015a;</ref><ref type="bibr" target="#b4">Chen et al., 2017</ref>).</p><p>Our contribution is to address the above prob- lems. We propose a complete framework of com- panion teaching, and develop various teaching schemes which combine different teaching strate- gies and teaching heuristics together, to answer the questions of "how" and "when" to teach to achieve affordable dialogue policy learning (sec- tion 2). Specifically, a novel failure prognosis based teaching heuristic is proposed, where Mul- tiTask Learning (MTL) is utilized to predict the dialogue success reward (section 3). To avoid the drawbacks of traditional subjective measure- ments, we propose two evaluation metrics, called Risk Index (RI) and Hitting Time (HT), to quantify the safety and efficiency of on-line policy learning respectively (section 4). Simulation experiments showed, with the proposed companion teaching schemes, sustainable and affordable on-line dia- logue policy learning has been achieved (section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Companion Teaching Framework</head><p>The companion teaching framework is an on-line policy training framework with three intelligen- t participants: machine dialogue manager, human user, and human teacher <ref type="bibr" target="#b4">(Chen et al., 2017)</ref>. Un- der this framework, the human teacher is able to accompany the dialogue manager to guide poli- cy learning with a limited teaching budget. By investigating the real work mode in a call cen- ter, this framework makes a reasonable assump- tion that human teacher has access to the extracted dialogue states from the dialogue state tracker as well as the system's dialogue act, and can also re- ply in the same format.</p><p>However, there are two major problems in the previous framework. First, the system will judge whether a dialogue session succeeds by several simple rules and then determine whether to feed a success reward signal to dialogue manager for re- inforcement learning. Actually, the success feed- back made by the system lacks flexibility and cred- ibility, and it could mislead the policy learning. A more suitable judge should be the user or the hu- man teacher. Second, the previous framework on- ly answers in which way the human teacher can guide the online dialogue policy learning, but an- other essential question, when should the human teacher give guidance, remains undiscussed.  In this paper, we proposed a complete frame- work of companion teaching, depicted as <ref type="figure" target="#fig_2">Figure  1</ref>. At each turn, the input module receives a speech input from the human user, then produces possible utterances u t of the speech in text. After that, the dialogue state tracker extracts the dialogue state s t from possible utterances. This dialogue state will be shared with policy model and human teacher if needed. When the final response a t , has been determined, the output module will translate this dialogue act to the natural language and reply to the human user. The success signal will be fed back by the user or the human teacher as an im- portant part of system reward, at the end of each session. The human teacher can take the initia- tive or be activated by student initiated heuristic to give the dialogue guidance with strategies cor- responding to different configurations of switches in the illustration. We call the combination of s- trategy and heuristic as teaching scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Teaching Strategies</head><p>The teacher can choose among three teaching s- trategies corresponding to different configurations of switches in a wiring diagram as <ref type="figure" target="#fig_2">Figure 1</ref> shows: The left switch is a Single-Pole, Double-Throw (SPDT) switch, which controls whether the an- swer is made by the system (connected to 1) or given by the teacher as an example (connected to 2). The right switch is a simple on-off switch, which represents whether there is an extra reward signal from the teacher (ON) or not (OFF). The s- trategy related to the right switch is called teaching via Critic Advice (CA), also known as turn-level reward shaping <ref type="bibr" target="#b20">(Thomaz and Breazeal, 2008;</ref><ref type="bibr" target="#b12">Judah et al., 2010)</ref>. When the switch at position 3 is turned on, the teacher will give the policy model extra turn-level reward to distinguish the student's actions between good and bad actions. Besides, the left switch corresponds to teaching via Exam- ple Action (EA), which means the teacher gives example action for the student to take according to the student's state.</p><p>The other strategy is proposed by <ref type="bibr" target="#b4">Chen et al. (2017)</ref>, which take the advantages of both EA and CA, named teaching via Example Action with Pre- dicted Critique (EAPC). With this strategy, the hu- man teacher gives example actions, meanwhile, a weak action predictor is trained using this teach- ing information to provide the extra reward even in teacher's absence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Teaching Heuristics</head><p>The strategies only answer how the human teacher can offer companion teaching to the system. How- ever, the timing of teaching should not be ignored for the sake of utilizing the limited teaching bud- get better. Exhausting all the budget at early train- ing stage, named Early teaching heuristic (Early), is simple and straightforward but wastes teaching opportunities on unnecessary cases. Thus, it is im- perative to design some effective heuristics to in- struct when the teacher should give a hand to the student.</p><p>In addition to early teaching, the teaching heuristics can be broadly divided into two cat- egories: teacher-initiated heuristics and student- initiated heuristics ( <ref type="bibr" target="#b0">Amir et al., 2016)</ref>. However, the teacher-initiated approaches require the con- stant long-term attention of the teacher to moni- tor the dialogue process <ref type="bibr" target="#b22">(Torrey and Taylor, 2013;</ref><ref type="bibr" target="#b0">Amir et al., 2016)</ref>, which is costly and impractical for real applications. Therefore, in this paper, we only discuss student-initiated heuristics, shown as the line with a stopwatch in <ref type="figure" target="#fig_2">Figure 1</ref>, which means that the student agent decides when to ask for the teacher's help.</p><p>Previous works have presented several effective heuristics based on state importance, I(s), which is determined by the Q-values of the RL agent:</p><formula xml:id="formula_0">I(s) = max a Q (s,a) − min a Q (s,a)</formula><p>Torrey and Taylor <ref type="formula" target="#formula_1">(2013)</ref> proposed State Impor- tance based Teaching heuristic (SIT) which make the student ask for advice only when the current state is important:</p><formula xml:id="formula_1">I(s) &gt; t si ,<label>(1)</label></formula><p>where t si is a fixed threshold for importance. And Clouse (1996) proposed an State Uncertain- ty based Teaching heuristic (SUT) which ask for advice when the student is uncertain about which action to take:</p><formula xml:id="formula_2">I(s) &lt; t su ,<label>(2)</label></formula><p>where t su is a given threshold for uncertainty. Though teaching effort can be conserved by on- ly applying to those important or uncertain states, it may end up wasting advice if the dialogue is likely to be successful without teaching. In this paper, we propose a novel Failure Prognosis based Teaching heuristic (FPT) for on-line policy learn- ing to reduce that unnecessary advice. The details are given in section 3. For comparison, we will al- so investigate Random teaching heuristic (Rand) which means the student seek for advice with a fixed probability p r .</p><p>to predict whether the ongoing dialogue will end in success and ask for advice only when the cur- rent prediction is a failure. The proposed approach utilizes MultiTask Learning (MTL) for the policy model to estimate future dialogue success reward and is compatible with various RL algorithms. In this paper, we implement the policy model with a Deep Q-Network (DQN), in which a neural net- work function approximator, named Q-network, is used to estimate the action-value function <ref type="bibr" target="#b15">(Mnih et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multitask Deep Q-Network</head><p>The goal of the policy model is to interact with hu- man user by choosing actions in each turn to max- imize future rewards. We define the dialogue state shared by dialogue state tracker in the t-th turn as s t , the action taken by policy model under current policy π θ with parameters θ in the t-th turn as a t , and a t ∼ π θ (·|s t ). In an ideal dialogue environ- ment, once the policy model emit an action a t , the human user will give an explicit feedback, like a normal response or a feedback of whether the di- alogue is successful, which will be converted to a reward signal r t delivering to the policy model im- mediately, and then the policy model will transit to next state s t+1 . The reward r t is composed of two parts:</p><formula xml:id="formula_3">r t = r turn t + r succ t ,</formula><p>where r turn t is the turn penalty reward and r succ t is the dialogue success reward. Typically, r turn t is fixed for each turn as a negative constant R turn , while r succ t equals to a predefined positive con- stant R succ only when the dialogue terminates and receives a successful user feedback otherwise ze- ro.</p><p>In DQN algorithm, all these transitions (s t , a t , r t , s t+1 ) will be stored in a replay mem- ory D. And the objective is to optimize MSE be- tween Q-network Q(s, a; θ) and Q-learning target Q e . The loss function L(θ) is defined as:</p><formula xml:id="formula_4">L(θ) = E s,a∼π θ (Q e − Q(s, a; θ)) 2 .<label>(3)</label></formula><p>During the training period, Q e is estimated with old fixed parameter θ − and sampled transitions e ∼ D:</p><formula xml:id="formula_5">Q e = r + γ max a t+1 Q(s t+1 , a t+1 ; θ − ), (4)</formula><p>where γ is the discount factor.</p><p>The reward Q(s, a) estimated by original Q- learning algorithm is essentially a combination of future turn penalty reward Q turn (s, a) and fu- ture dialogue success reward Q succ (s, a). For a task-oriented dialogue system, the prediction of Q succ (s, a) is much more important because it re- flects the possibility of the dialogue to be success- ful. If these two rewards are estimated separate- ly, the objective of Q succ (s, a) can be optimized explicitly, and we can get more insights into the estimated future. We found that in practice, opti- mizing these two objectives with MultiTask Learn- ing (MTL) converges faster and more stable com- pared with two separate models, the reason of which may lie in that MTL can learn different re- lated tasks in parallel using shared representation- s, which will be helpful for each task to be learned better <ref type="bibr" target="#b2">(Caruana, 1997</ref>). The structure of proposed MTL-DQN is depicted in <ref type="figure" target="#fig_3">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Failure Prognosis</head><p>In the proposed multitask DQN, we define on-line task success predictor T (s t ) as:</p><formula xml:id="formula_6">T (s t ) = Q succ (s t , a t ),</formula><p>where a t is the action taken under state s t . It is reasonable to assume that the dialogue is going to fail if T (s t ) is relatively small. Based on the task success predictor, we propose a novel student- initiated heuristic, named Failure Prognosis based Teaching heuristic (FPT).</p><p>The key to the proposed heuristic is to define failure prognosis quantitatively. A straight way is to set a ratio threshold α, and consider it to be fail- ure prognosis when T (s t ) &lt; αR succ . However, this assumes that the numerical scale of Q succ is consistent through the training period, which is not always the case. And the student's noisy estima- tion of Q succ at early training period will make the learning process unstable. To smooth the teaching, we consider using a turn-level sliding window n- ear the current state to calculate an average value as the replacement of the fixed R succ . So in the t- th turn, the failure prognosis for the student to be true is equivalent to:</p><formula xml:id="formula_7">T (s t ) &lt; α 1 w t−1 j=t−w T (s j ),<label>(5)</label></formula><p>where w is the size of the sliding window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Quantitative Measurements for Safety and Efficiency</head><p>The performance of different teaching strategies and heuristics should be measured in both the safe- ty and efficiency dimension. However, the mea- surements in these two dimensions are subjective and error prone in the previous work <ref type="bibr" target="#b4">(Chen et al., 2017)</ref>. Especially for assessing the degree of safe- ty of various teaching strategies and heuristics, we simply obverse the training curves so that we can- not tell of two interleaving curves which training process is safer. Thus, it is imperative to set up some quantitive measurements for both safety and efficiency evaluations. In this paper, we propose two scalar criteria: Risk Index (RI) and Hitting Time (HT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Risk Index</head><p>The Risk Index is a nonnegative index designed to indicate how risky the training process could be for evaluating the safety issue during the whole online dialogue policy learning process. Because we expect that the system satisfies the quality-of- service requirement in the early training period, specifically, we hope it can keep a relatively ac- ceptable success rate. It is straightforward to set a success rate threshold for the training process. In a real application scenario, this threshold can be obtained by an appropriate user study. If the success rate over a training process keeps above this threshold all the time, we will think this training process is absolutely safe. Therefore, its RI should equal to zero.</p><p>On the other hand, if the success rate over a training process rises and falls and sometimes is below the threshold, it is risky. The riskiness con- sists of two parts:</p><p>• Disruptiveness: Sometimes the success rate during a certain period will fall much lower than the threshold, which could be very dis- ruptive. To quantify the disruptiveness, we consider the function</p><formula xml:id="formula_8">dis(t) = threshold − %succ(t)</formula><p>over the training process. The higher the val- ue of dis(t ) is, the riskier the training pro- cess could be during the period of a certain length centered with time t .</p><p>• Persistence: Another thing we should take into account is the duration of the time at high risk. Let δ risk (t) be the indicator of whether threshold ≥ %succ(t). Then the persis- tence can be quantified as total risky time</p><formula xml:id="formula_9">per(T ) = T t=0 δ risk (t)dt</formula><p>The longer the danger persists over the train- ing process, the value of persistence of the training process will be, and the riskier it is.</p><p>Our Risk Index integrate these two contents of riskiness. That is, a nonnegative scalar</p><formula xml:id="formula_10">RI = T t=0 dis(t)δ risk (t)dt,</formula><p>which measures the integrated riskiness for the on- line training process of total length T . The RI also has an intuitive interpretation as the area of the region which is below the threshold line and above training curve. Straightforwardly, high RI indicates poor safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hitting Time</head><p>To measure the efficiency, we proposed the Hitting Time in order to show how fast the system learns and reaches the satisfactory performance. The difficulty of designing such a criterion lies in the dramatic and undamped fluctuation of the test curves, which is inherent in the instability dialogue task. Therefore, many popular criteria for the evaluating dynamic performance in con- trol theory, such as "settling time" and "rise time", cannot be applied to evaluate efficiency here.</p><p>We use Hitting Time to evaluate efficiency over the fluctuant testing curve first by fitting it to the empirical learning curve</p><formula xml:id="formula_11">f (t) = a − b · e −(t/c) 2 ,</formula><p>where the parameter a is the stationary perfor- mance which is predicted as the asymptotic goal of the system, b relates to the initial performance, and c relates to the climbing speed. This empirical model forces our fitted learning curve be an "S" shape curve satisfying constraints f (0) = 0 and f (c/2) = 0. Then we observe when this fitted learning curve hits the target performance τ and this time (measured in sessions) is Hitting Time. It can be calculated analytically as follow</p><formula xml:id="formula_12">HT = c ln b a − τ .</formula><p>Ideally, the ultimate success rate a should be very close under different settings because of the sufficient training. However, if the success rate keeps very poor during the given sessions, the fit- ted a will be very low, and even less than the target satisfactory performance τ . In this situation, a is meaningless, and HT becomes a complex number. And this indicates the real hitting time is far larg- er than given number of sessions T . We will note the HT in this case as ULT (Unacceptably Large Time).</p><p>In this way, we overcome the fluctuation and make the HT tell us how much time the system takes to hit and surpass the target success rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>Three objectives are set for our experiments: (1) Observing the effect of multitask DQN; (2) Con- trasting the performances of different teaching schemes (strategies and heuristics) under the com- panion teaching framework; (3) Observing the safety and efficiency issues under sparse user feed- back scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Our experiments are conducted with the Dialogue State Tracking Challenge 2 (DSTC2) dataset, which is on restaurant information domain <ref type="bibr" target="#b11">(Henderson and Thomson, 2014</ref>). The human user is emulated by an agenda-based user simulator with error model ( <ref type="bibr" target="#b16">Schatzmann et al., 2007)</ref>, while the human teacher is emulated by a pre-trained poli- cy model with success rate of about 0.78 through multitask DQN approach without teaching. A rule-based tracker is used for dialogue state track- ing ( <ref type="bibr" target="#b19">Sun et al., 2014b</ref>). The semantic parser is implemented according to an SVM-based method proposed by <ref type="bibr" target="#b10">Henderson et al. (2012)</ref>. The natu- ral language generator is implemented and modi- fied based on an RNNLG toolkit <ref type="bibr" target="#b24">(Wen et al., , 2015a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rand SIT SUT FPT</head><p>None pr = 0.6 tsi = 5 tsu = 10 α = 1.2 w = 25 <ref type="table">Table 1</ref>: Experimental configurations of teaching heuristics introduced in section 2.2 and 3.2.</p><p>In our experiments, all dialogues are limited to twenty turns. The "dialogue success" is judged by the user simulator according to whether all user goals are satisfied. And for policy learning, we set a small per-turn penalty of one to encourage short interactions, i.e. R turn = −1, and a large dialogue success reward of thirty to appeal to suc- cessful interactions, i.e. R succ = 30 , and the dis- count factor γ is set to one. <ref type="table">Table 1</ref> summarizes the heuristics studied in our experiments, together with corresponding configurations which are cho- sen empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Observing the Effect of MTL-DQN</head><p>The MTL-DQN described in section 3.1 can esti- mate the prediction of Q turn and Q succ respective- ly. In our experiments, it was implemented with one shared hidden layer and two dependent hidden layers for two different tasks using MXNet <ref type="bibr" target="#b5">(Chen et al., 2015b)</ref>. <ref type="figure">Figure 3</ref> shows a typical failure in dialogue pol- icy training. The policy showed in the example hasn't been trained well, and it tends to ask the user to repeat over and over again when the confi- dence score of the user utterance is not high, which causes the user to terminate the dialogue impa- tiently. <ref type="figure">Figure 3</ref>: An example of failed dialogue while training without teaching. The labels "Score" and "FP" represent for the confidence score of user ut- terance and the value of failure prognosis of the current turn respectively. This kind of failure can be predicted and cor- rected in advance. By equation 5, the third turn will be estimated to be failure prognosis, which can be a sign for the teacher to intervene and cor- rect the following actions to avoid dialogue fail- ure. Besides, the explicit separate estimation of Q turn and Q succ provides a better understanding of the state of the current turn. For example, al- though the first turn and second turn have similar Q-values (Q turn +Q succ ), the latter turn is predict- ed with less future turns and less possibility to lead to dialogue success. See appendix A for additional successful example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparing Different Teaching Schemes</head><p>Our proposed complete companion teaching framework allows us to teach dialogue system- s with different teaching schemes, which consist- s various strategies and heuristics. In our exper- iments, we compared 18 schemes consisting of three teaching strategies (CA, EA and EAPC), and six teaching heuristics (Early, Rand, SIT, SUT, FP- T and SUT&amp;FPT). The SUT&amp;FPT heuristic mean- s the student only ask for advice when equation 2 and 5 are both satisfied. For comparison, we use No Teaching (NoTeaching) as the baseline.</p><p>To verify the effects of different companion teaching schemes, we conduct a set of experiments to see their performances on safety and efficiency dimensions. During training, the teacher can on- ly teach for a limited budget of 1000 turns. All the training curves shown in this paper are moving av- erage curves with a window of size 250 dialogues and over eight runs with an endurable standard er- ror.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Safety Evaluation</head><p>To compare effects of different teaching schemes on safety dimension, we use the Risk Index (RI) in section 4.1 to quantitatively measure each training process. We set the empirical safety threshold as 65% here. The results are shown in <ref type="table">Table 2</ref>.</p><p>As RIs implies, schemes composed with EAPC strategy is much safer than those composed with other strategies. As for teaching heuristics, FP- T, SUT and SUT&amp;FPT are three relatively safer heuristic accompanying different strategies. One exception is that Early teaching looks more suit- able for CA. A possible explanation is that when the teacher gives critique earlier, the student will mind its behavior earlier so that increase safe- ty. <ref type="figure" target="#fig_4">Figure 4</ref> shows the training curves of on-line  <ref type="table">Table 2</ref>: RIs of learning processes under differ- ent teaching schemes. The least risky teaching scheme is annotated with * . For comparing differ- ent teaching heuristics with fixed teaching strate- gy, the smallest RIs in each column are bold and underlined, the 2 nd smallest ones are bold only, and the 3 rd smallest ones are underlined only. See abbreviations of schemes in section 2.1 and 2.2.</p><p>learning process under EAPC with various heuris- tics. Among all 18 teaching schemes, EAPC+SUT is the safest teaching scheme which reduces about 78% risk of no-teaching learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Efficiency Evaluation</head><p>We use Hitting Time (HT) in section 4.2 to mea- sure the efficiency of learning process under dif- ferent teaching schemes. The empirical satisfacto- ry target success rate for the student is 70% in our experimental settings.   <ref type="table" target="#tab_2">Table 3</ref> contains all HTs of learning process un- der 18 teaching schemes. Intuitively, The num- ber in the table reflect the number of sessions at which the model achieves target success rate. As it shows, not any teaching scheme will im- prove the learning efficiency. If the teacher in- tervenes at an improper time, it will distract sys- tem or confuse system even with a right guidance. But teaching when a potential failure exists (F- PT) is always good for improving learning effi- ciency. EAPC+SUT&amp;FPT is the teaching scheme that leads to the most efficient learning process in our experiments. <ref type="figure" target="#fig_6">Figure 6</ref> gives some example test curves and fitted empirical learning curves of learning process under EAPC with various heuris- tics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Teacher's Workload</head><p>We also observe teacher's workload of all the teaching schemes since economically utilizing teaching budget is one of our goals. The total teaching budget is 1000 for every teach- ing scheme. See abbreviations of schemes in sec- tion 2.1 and 2.2. <ref type="figure" target="#fig_5">Figure 5</ref> illustrates the cumulative usage of teaching budget of 18 teaching schemes. It shows that early teaching is the most costly teaching heuristic so that the teaching budget is soon used up. SIT looks a bit lazy at the beginning and con- sumes teaching budget slowly. When the teaching strategy is EA or EAPC, FPT-based schemes do not use up full teaching budget in our experiments. Combine SUT and FPT, the workload is relative- ly lighter than that of teaching in other heuristic- s. And through proper teaching schemes, we can make better use of the teaching budget and reduce teacher's workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Safety and Efficiency Issues under Sparse User Feedback Scenarios</head><p>In real application scenarios, the user rarely pro- vides feedback at the end of the dialogue, so that safety and efficiency issues are even more serious. To observe the effectiveness of different teaching schemes under sparse user feedback, we conduct- ed experiments with sparse user feedback. The user feedback rate is set to 30% empirical- ly and experiments are carried out under teaching schemes consisting of EAPC strategies and differ- ent heuristics, since EAPC is much safer and more efficient than other teaching strategies.   <ref type="table" target="#tab_4">Table 4</ref> records the RIs and HTs of those differ- ent learning process when user feedback is sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RIs</head><p>We can see that when the user feedback rate drop- s from 100% to 30%, the RIs and HTs increase dramatically. The NoTeaching baseline is very risky and inefficient (its hitting time is even unpre- dictable within 10000 sessions learning). Howev- er, with teaching scheme such as EAPC+FPT, both safety and efficiency can be improved a lot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>This paper addressed the safety and efficiency is- sues of sustainable on-line dialogue policy learn- ing with different teaching schemes, which answer both "how" and "when" to teach, within the com- plete companion teaching framework. To evaluate the policy learning process precisely, we proposed two measurements, Risk Index (RI) and Hitting Time (HT), to quantify the degree of safety and ef- ficiency. Particularly, through multitask learning, we managed to optimize the predicted remaining turns and dialogue success reward explicitly, based on which we developed a novel Failure Progno- sis based Teaching (FPT) heuristic to better utilize the fixed teaching budget and make the teaching affordable.</p><p>Experiments showed that different teaching schemes have different effects on safety and ef- ficiency dimension. And they also require differ- ent workload of the teacher. Among 18 compared teaching schemes, FPT-based heuristics combined with EAPC strategy achieved promising perfor- mance on RI and HT, and required relatively slight workload. This result indicates a proper teaching scheme under the companion teaching framework is able to guarantee a sustainable and affordable on-line dialogue policy learning process.</p><p>There are several directions for our future work. We expect to deploy our proposed framework in real-world scenarios collaborating with real hu- man teachers to verify the results presented in this paper and discover more potential challenges of on-line dialogue system development. Further- more, the current study is focused on dialogue suc- cess rate, which is a simplification of the human satisfaction evaluation. So future work is needed to take more qualities into consideration to achieve better user experience.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Vicious Cycle: Unsustainable On-line Learning Poor (initial) Policy Bad User Experience Insufficient Real User (Data) Unsafe Policy Behavior (Solvable) ✔ Individual Rationality (Unsolvable) ✘ Possible Solutions to break the vicious cycle Inefficient Learning Process (Solvable) ✔</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Companion Teaching Framework for On-line Policy Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: MTL-DQN structure</figDesc><graphic url="image-19.png" coords="4,336.25,331.72,159.22,65.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: On-line learning process under different teaching schemes (EAPC + different heuristics). The yellow dashed line indicates safe success rate threshold. The area in gray indicates how risky a training process is. See abbreviations of schemes in section 2.1 and 2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Cumulative usage of teaching budget. The total teaching budget is 1000 for every teaching scheme. See abbreviations of schemes in section 2.1 and 2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Test curves and fitted empirical learning curves of learning process with different teaching schemes (EAPC+different heuristic). See abbreviations of schemes in section 2.1 and 2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>HTs of test curves of different teaching 
schemes. The most efficient teaching scheme is 
annotated with  *  . For comparing different teach-
ing heuristics with fixed strategy on efficiency is-
sue, the smallest HTs in each column are bold and 
underlined, and the 2 nd smallest bold only. See 
abbreviations of schemes in section 2.1 and 2.2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>RIs &amp; HTs of learning processes un-
der EAPC strategy and different heuristics when 
user feedback rate is 30%. See abbreviations of 
schemes in section 2.1 and 2.2. 

</table></figure>

			<note place="foot" n="3"> Failure Prognosis Based Teaching Heuristic To make better use of teaching advice, we propose to use an on-line turn-level task success predictor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Shanghai Sail-ing Program No. 16YF1405300, the China NS-FC projects <ref type="bibr">(No. 61573241 and No. 61603252)</ref> and the Interdisciplinary Program (14JCZ03) of Shanghai Jiao Tong University in China. Exper-iments have been carried out on the PI supercom-puter at Shanghai Jiao Tong University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive teaching strategies for agent training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofra</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kolobov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Grosz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI. International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement learning: An introduction (adaptive computation and machine learning)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedict</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1054</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperparameter optimisation of gaussian process reinforcement learning for statistical dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<biblScope unit="page" from="407" to="411" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On-line dialogue policy learning with companion teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runzhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On integrating apprentice learning and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffery Allen Clouse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive reinforcement learning framework for dialogue management optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daubigney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="891" to="902" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Policy networks with two-stage training for dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv.org</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gaussian processes for fast policy optimisation of pomdp-based dialogue managers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">Young</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative spoken language understanding using word confusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reinforcement learning via practice and critique advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Judah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="481" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony R</forename><surname>Michael L Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning dialogue strategies within the markov decision process framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a pomdp dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On-line active reward learning for policy optimisation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Tsung Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="2431" to="2441" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The sjtu system for dialog state tracking challenge 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<biblScope unit="page" from="318" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The sjtu system for dialog state tracking challenge 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Teachable robots: Understanding human teaching behavior to build more effective robot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Breazeal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="716" to="737" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="562" to="588" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Teaching on a budget: Agents advising agents in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Torrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems</title>
		<meeting>the 2013 international conference on Autonomous agents and multi-agent systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1053" to="1060" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A simple and generic belief tracking mechanism for the dialog state tracking challenge: On the believability of observed information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sigdial Meeting on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). Association for Computational Linguistics</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Toward multi-domain language generation using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning for Spoken Language Understanding and Interaction</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-domain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2016 Conference on North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Partially observable markov decision processes for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-toend LSTM-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
