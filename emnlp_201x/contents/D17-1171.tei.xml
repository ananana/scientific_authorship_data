<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CRF Autoencoder for Unsupervised Dependency Parsing *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CRF Autoencoder for Unsupervised Dependency Parsing *</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1638" to="1643"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning gen-erative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discrim-inative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised dependency parsing, which aims to discover syntactic structures in sentences from un- labeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by <ref type="bibr" target="#b10">Klein and Manning (2004)</ref>. Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors <ref type="bibr" target="#b4">(Cohen et al., 2008)</ref>, representing dependencies with features <ref type="bibr" target="#b1">(Berg-Kirkpatrick et al., 2010)</ref>, and representing discrete tokens with continuous vec- tors ( <ref type="bibr" target="#b9">Jiang et al., 2016)</ref>.</p><p>Besides generative approaches, <ref type="bibr" target="#b8">Grave and Elhadad (2015)</ref> proposed an unsupervised discrim- * This work was supported by the National Natural Sci- ence Foundation of China (61503248). inative parser. They designed a convex quadratic objective function under the discriminative clus- tering framework. By utilizing global features and linguistic priors, their approach achieves state- of-the-art performance. However, their approach uses an approximate parsing algorithm, which has no theoretical guarantee. In addition, the perfor- mance of the approach depends on a set of manu- ally specified linguistic priors.</p><p>Conditional random field autoencoder ( <ref type="bibr" target="#b0">Ammar et al., 2014</ref>) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The en- coder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation of the struc- tured input from the latent structure. <ref type="bibr" target="#b0">Ammar et al. (2014)</ref> applied the model to two sequential structured prediction tasks, part-of-speech induc- tion and word alignment and showed that by uti- lizing context information the model can achieve better performance than previous generative mod- els and locally normalized models. However, to the best of our knowledge, there is no previous work applying the CRF autoencoder to tasks with more complicated outputs such as tree structures.</p><p>In this paper, we propose an unsupervised dis- criminative dependency parser based on the CRF autoencoder framework and provide tractable al- gorithms for learning and parsing. We performed experiments in eight languages and show that our approach achieves comparable results with previ- ous state-of-the-art models. Encoder Decoder <ref type="figure">Figure 1</ref>: The CRF Autoencoder for the input sen- tence "These stocks eventually reopened" and its corresponding parse tree (shown at the top). x andˆxandˆ andˆx are the original and reconstructed sentence. y is the dependency parse tree represented by a sequence where y i contains the token and index of the parent of token x i in the parse tree, e.g., y 1 = stocks, 2 and y 2 = reopened, 4. The encoder is represented by a factor graph (with a global factor specifying valid parse trees) and the decoder is represented by a Bayesian net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>2.1 Model <ref type="figure">Figure 1</ref> shows our model with an example in- put sentence. Given an input sentence x = (x 1 , x 2 , . . . , x n ), we regard its parse tree as the latent structure represented by a sequence y = (y 1 , y 2 , . . . , y n ) where y i is a pair t i , h i , t i is the head token of the dependency connecting to token x i in the parse tree, and h i is the index of this head token in the sentence. The model also contains a reconstruction output, which is a token sequencêsequencê</p><formula xml:id="formula_0">x = (ˆ x 1 , ˆ x 2 , . . . , ˆ x n ). Throughout this paper, we setˆxsetˆ setˆx = x.</formula><p>The encoder in our model is a log-linear model represented by a first-order dependency parser. The score of a dependency tree can be factorized as the sum of scores of its dependencies. For each dependency arc (x, i, j), where i and j are the in- dices of the head and child of the dependency, a feature vector f (x, i, j) is specified. The score of a dependency is defined as the inner product of the feature vector and a weight vector w,</p><formula xml:id="formula_1">φ(x, i, j) = w T f (x, i, j)</formula><p>The score of a dependency tree y of sentence x is</p><formula xml:id="formula_2">φ(x, y) = n i=1 φ(x, h i , i)</formula><p>We define the probability of parse tree y given sen- tence x as</p><formula xml:id="formula_3">P (y|x) = exp(φ(x, y)) Z(x)</formula><p>Z(x) is the partition function,</p><formula xml:id="formula_4">Z(x) = y ∈Y(x) exp(φ(x, y ))</formula><p>where Y(x) is the set of all valid parse trees of x.</p><p>The partition function can be efficiently computed in O(n 3 ) time using a variant of the inside-outside algorithm <ref type="bibr" target="#b15">(Paskin, 2001</ref>) for projective tree struc- tures, or using the Matrix-Tree Theorem for non- projective tree structures ( <ref type="bibr" target="#b11">Koo et al., 2007)</ref>.</p><p>The decoder of our model consists of a set of categorical conditional distributions θ x|t , which represents the probability of generating token x conditioned on token t. So the probability of the reconstruction outputˆxoutputˆ outputˆx given the parse tree y is</p><formula xml:id="formula_5">P (ˆ x|y) = n i=1 θ ˆ x i |t i</formula><p>The conditional distribution ofˆxofˆ ofˆx, y given x is P (y, ˆ x|x) = P (y|x)P (ˆ x|y)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Features</head><p>Following <ref type="bibr" target="#b13">McDonald et al. (2005)</ref> and <ref type="bibr" target="#b8">Grave et al. (2015)</ref>, we define the feature vector of a de- pendency based on the part-of-speech tags (POS) of the head, child and context words, the direc- tion, and the distance between the head and child of the dependency. The feature template used in our parser is shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Parsing</head><p>Given parameters w and θ, we can parse a sen- tence x by searching for a dependency tree y which has the highest probability P (ˆ x, y|x). <ref type="table">Table 1</ref>: Feature template of a dependency, where i is the index of the head, j is the index of the child, dis = |i − j|, and dir is the direction of the dependency.</p><formula xml:id="formula_6">y * = arg max y∈Y(x) log P (ˆ x, y|x) = arg max y∈Y(x) n i=1 φ(x, h i , i) + log θ ˆ x i |t i POS i × dis × dir POS j × dis × dir POS i × POS j × dis × dir POS i × POS i−1 × POS j × dis × dir POS i × POS i+1 × POS j × dis × dir POS i × POS j × POS j−1 × dis × dir POS i × POS j × POS j+1 × dis × dir</formula><p>For projective dependency parsing, we can use Eisners algorithm (1996) to find the best parse in O(n 3 ) time. For non-projective dependency parsing, we can use the Chu-Liu/Edmond algo- rithm ( <ref type="bibr" target="#b3">Chu and Liu, 1965;</ref><ref type="bibr" target="#b5">Edmonds, 1967;</ref><ref type="bibr" target="#b18">Tarjan, 1977)</ref> to find the best parse in O(n 2 ) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Objective Function</head><p>Spitkovsky et al. <ref type="formula">(2010)</ref> shows that Viterbi EM can improve the performance of unsupervised de- pendency parsing in comparison with EM. There- fore, instead of using negative conditional log like- lihood as our objective function, we choose to use negative conditional Viterbi log likelihood,</p><formula xml:id="formula_7">− N i=1 log max y∈Y(x i ) P (ˆ x i , y|x i ) + λΩ(w) (1)</formula><p>where Ω(w) is a L1 regularization term of the encoder parameter w and λ is a hyper-parameter controlling the strength of regularization. To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following <ref type="bibr" target="#b14">Naseem et al. (2010)</ref> and <ref type="bibr" target="#b8">Grave et al. (2015)</ref>. Hence our objec- tive function becomes</p><formula xml:id="formula_8">− N i=1 log max y∈Y(x i ) P (ˆ x i , y|x i )Q α (x i , y) +λΩ(w)</formula><p>where Q(x, y) is a soft constraint factor over the parse tree, and α is a hyper-parameter controlling the strength of the constraint factor. The factor Q is also decomposable by edges in the same way as the encoder and the decoder, and therefore our parsing algorithm can still be used with this factor <ref type="table">Table 2</ref>: Universal linguistic rules taken into account.</p><formula xml:id="formula_9">VERB → VERB NOUN → NOUN VERB → NOUN NOUN → ADJ VERB → PRON NOUN → DET VERB → ADV NOUN → NUM VERB → ADP NOUN → CONJ ADJ → ADV ADP → NOUN</formula><formula xml:id="formula_10">Q(x, y) = exp i 1[(t i → x i ) ∈ R] where 1[(t i → x i ) ∈ R]</formula><p>is an indicator function of whether dependency t i → x i satisfies one of the universal linguistic rules in R. The universal linguistic rules that we use are shown in <ref type="table">Table 2</ref> ( <ref type="bibr" target="#b14">Naseem et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Algorithm</head><p>We apply coordinate descent to minimize the ob- jective function, which alternately updates w and θ. In each optimization step of w, we run two epochs of stochastic gradient descent, and in each optimization step of θ, we run two iterations of the Viterbi EM algorithm. To update w using stochastic gradient de- scent, for each sentence x, we first run the pars- ing algorithm to find the best parse tree y * = arg max y∈Y(x) (P (ˆ x, y|x)Q α (x, y)); then we can calculate the gradient of the objective function based on the following derivation,</p><formula xml:id="formula_11">∂log P (ˆ x, y * |x) ∂w = ∂log P (y * |x) ∂w + ∂log P (ˆ x|y * ) ∂w = ∂log P (y * |x) ∂w = ∂ n i=1 w T f (x, h i , i) − log Z(x) ∂w = (i,j)∈D(x) f (x, i, j) 1[y * j = i, x i ] − µ(x, i, j)</formula><p>where D(x) is the set of all possible dependency arcs of sentence x, 1 <ref type="bibr">[·]</ref> is the indicator function, and µ(x, i, j) is the expected count defined as fol- lows,  <ref type="table">Table 4</ref>: Parsing accuracy on seven languages. Our model is compared with DMV ( <ref type="bibr" target="#b10">Klein and Manning, 2004</ref>), Neural DMV ( <ref type="bibr" target="#b9">Jiang et al., 2016)</ref>, and Convex-MST ( <ref type="bibr" target="#b8">Grave and Elhadad, 2015)</ref> Methods</p><formula xml:id="formula_12">µ(x, i, j) = y∈Y(x) 1[y j = i, x i ]P (y|x)</formula><note type="other">(No Prior) 49.0 33.9 28.8 39.3 47.6 34.7 51.3 40.6 CRFAE (With Prior) 49.9 48.1 53.4 43.9 68.0 52.5 64.7 54.3 Length: All DMV(EM) 31.2 28.1 40.3 44.2 23.5 25.2 32.0 32.0 DMV(Viterbi) 40.9 20.4 32.6 33.0 26.9 16.5 36.2 29.5 Neural DMV (EM) 38.5 29.3 46.1 46.2 16.2 36.6 32.8 35.1 Neural DMV (Viterbi) 41.8 23.8 34.2 33.6 29.4 30.8 40.2 33.4 Convex-MST (No Prior) 30.5 33.4 44.2 29.3 38.3 32.2 28.3 33.7 Convex-MST (With Prior) 30.6 40.0 45.8 35.6 46.3 51.8 40.5 41.5 CRFAE (No Prior) 39.8 25.4 24.2 35.2 52.2 26.4 40.0 34.7 CRFAE (With Prior) 41.4 36.8 40.5 38.6 58.9 43.3 48.5 44.0</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSJ10 WSJ Basic Setup</head><p>Feature DMV <ref type="bibr" target="#b1">(Berg-Kirkpatrick et al., 2010)</ref> 63.0 - UR-A E-DMV ( <ref type="bibr" target="#b19">Tu and Honavar, 2012)</ref> 71.4 57.0 Neural E-DMV <ref type="bibr" target="#b9">(Jiang et al., 2016)</ref> 69.7 52.5 Neural E-DMV (Good Init) ( <ref type="bibr" target="#b9">Jiang et al., 2016)</ref> 72.5 57.6 Basic Setup + Universal Linguistic Prior Convex-MST <ref type="bibr" target="#b8">(Grave and Elhadad, 2015)</ref> 60.8 48.6 HDP-DEP <ref type="bibr" target="#b14">(Naseem et al., 2010)</ref> 71.9 - CRFAE 71.7 55.7 Systems Using Extra Info LexTSG-DMV <ref type="bibr" target="#b2">(Blunsom and Cohn, 2010)</ref> 67.7 55.7 CS <ref type="bibr" target="#b16">(Spitkovsky et al., 2013)</ref> 72.0 64.4 MaxEnc ( <ref type="bibr" target="#b12">Le and Zuidema, 2015)</ref> 73.2 65.8 <ref type="table">Table 3</ref>: Comparison of recent unsupervised de- pendency parsing systems on English. Basic setup is the same as our setup except that linguistic prior is not used. Extra info includes lexicalization, longer training sentences, etc.</p><p>The expected count can be efficiently computed using the Matrix-Tree Theorem ( <ref type="bibr" target="#b11">Koo et al., 2007)</ref> for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures <ref type="bibr" target="#b15">(Paskin, 2001</ref>).</p><p>To update θ using Viterbi EM, in the E-step we again use the parsing algorithm to find the best parse tree y * for each sentence x; then in the M- step we update θ by maximum likelihood estima- tion.</p><formula xml:id="formula_13">θ c|t = x i 1[x i = c, y * i = ·, t] c x i 1[x i = c , y * i = ·, t]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We experimented with projective parsing and used the informed initialization method proposed by <ref type="bibr" target="#b10">Klein and Manning (2004)</ref> to initialize our model before learning. We tested our model both with and without using the universal linguistic rules. We used AdaGrad to optimize w. We used POS tags of the input sentence as the tokens in our model. We learned our model on training sen- tences of length ≤ 10 and reported the directed dependency accuracy on test sentences of length ≤ 10 and on all test sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on English</head><p>We evaluated our model on the Wall Street Jour- nal corpus. We trained our model on section 2- 21, tuned the hyperparameters on section 22, and tested our model on section 23. <ref type="table">Table 3</ref> shows the directed dependency accuracy of our model (CR- FAE) compared with recently published results. It can be seen that our method achieves a compara- ble performance with state-of-the-art systems.</p><p>We also compared the performances of CRF au- toencoder using an objective function with nega- tive log likelihood vs. using our Viterbi version of the objective function (Eq.1). We find that the Viterbi version leads to much better performance (55.7 vs. 41.8 in parsing accuracy of WSJ), which echoes Spitkovsky et al. 's findings on Viterbi EM (2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multilingual Results</head><p>We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction ( <ref type="bibr" target="#b7">Gelling et al., 2012</ref>). We did not use the Arabic corpus because the number of training sentences with length ≤ 10 is less than 1000. The result is shown in <ref type="table">Table 4</ref>. The accuracies of DMV and Neural DMV are from Jiang et.al <ref type="bibr">(2016)</ref>. Both our model (CRFAE) and Convex-MST were tuned on the validation set of each corpus. It can be seen that our method achieves the best results on av- erage. Besides, our method outperforms Convex- MST both with and without linguistic prior. From the results we can also see that utilizing universal linguistic prior greatly improves the performance of Convex-MST and our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a new discriminative model for unsupervised dependency parsing based on CRF autoencoder. Both learning and inference of our model are tractable. We tested our method on eight languages and show that our model is competitive to the state-of-the-art systems.</p><p>The code is available at https://github. com/caijiong/CRFAE-Dep-Parser</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional random field autoencoders for unsupervised structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised induction of tree substitution grammars for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1204" to="1213" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1396</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal challenge on grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Gelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>the NAACL-HLT Workshop on the Induction of Linguistic Structure</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="64" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convex and feature-rich discriminative approach to dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="763" to="771" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 478. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured prediction models via the matrix-tree theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><forename type="middle">Carreras</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing: Let&apos;s use supervised parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="651" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using universal linguistic knowledge to guide grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cubic-time parsing and learning algorithms for grammatical bigram models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark A Paskin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1983" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Finding optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Endre Tarjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Networks</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1324" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
