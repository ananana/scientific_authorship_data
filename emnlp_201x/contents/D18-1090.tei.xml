<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="791" to="797"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>791</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic essay scoring (AES) is the task of assigning grades to essays without human interference. Existing systems for AES are typically trained to predict the score of each single essay at a time without considering the rating schema. In order to address this issue, we propose a reinforcement learning framework for essay scoring that incorporates quadratic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our framework.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural networks have been widely used to grade student essays automatically and achieve state-of-the-art performance. In partic- ular, a distributed representation is learned for an essay with variant neural networks and a lin- ear layer is then used to produce the final score. Existing researches focus on learning better es- say representation using different neural networks, including long short-term memory (LSTM) net- work <ref type="bibr" target="#b16">(Taghipour and Ng, 2016</ref>), hierarchical con- volutional neural networks (CNN) <ref type="bibr" target="#b4">(Dong and Zhang, 2016)</ref>, hierarchical CNN-LSTM structure with attention mechanism ( <ref type="bibr" target="#b5">Dong et al., 2017)</ref>, and SKIPFLOW LSTM ( <ref type="bibr" target="#b18">Tay et al., 2017)</ref>.</p><p>The major evaluation metric for AES is quadratic weighted kappa (QWK), which is also the official metric of Automated Student Assess- ment Prize 1 (ASAP). It evaluates the scoring re- sults by taking rating schema into consideration. Because QWK is not differentiable, it is hard to train systems via optimizing this metric directly. Alternatively, existing AES systems are typically trained to predict the score for a single essay and optimized using mean square error <ref type="bibr">(MSE)</ref>. The gap between training and testing also limits the performance of state-of-the-art AES systems.</p><p>Recently, reinforcement learning (RL) has been introduced to optimize models in terms of non- differentiable quality metrics and studies have shown its effectiveness for various tasks including language generation ( <ref type="bibr" target="#b13">Ranzato et al., 2015;</ref><ref type="bibr" target="#b14">Rennie et al., 2016;</ref>, machine transla- tion ( <ref type="bibr" target="#b1">Bahdanau et al., 2016)</ref> and relation classifi- cation <ref type="bibr" target="#b6">(Feng et al., 2018)</ref>.</p><p>Inspired by these researches, we propose a novel reinforcement learning framework that in- corporates QWK as the guidance to optimize the essay scoring system. In our framework, we score a pack of essays at a time and the scoring of each single essay is treated as an action. The QWK value computed for the pack of essays is then de- livered as a reward to update the scoring system. Because the existing regression-based essay scorer is unable to generate a probability distribution in nature, it is non-trivial to be used within the rein- forcement learning framework. We therefore pro- pose to use a classification-based scoring system instead. The proposed framework is evaluated in the benchmark datasets from ASAP and experi- ment results confirm its effectiveness on two dif- ferent settings of essay representation structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Typically, an essay scorer contains two compo- nents, namely, essay representation and essay scoring. The component of essay representation transforms an input essay into a distributed vec- tor and the component of essay scoring assigns a score to the essay based on the vector. Both components are usually trained jointly. In order to incorporate QWK to guide the process of essay scoring, we introduce a novel essay scoring strat- egy named packed evaluation. At each time, essay scorer grades a pack of essays together with the target essay, and QWK is calculated for the pack. To avoid contingency, for each target essay, we re- peat the packed evaluation multiple times by ran- domly choosing other essays in a pack. And the average QWK it achieves is set to be the reward. The reward is then delivered to the essay scorer as a weak signal to supervise the scorer. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the training process of our model. We will introduce the different parts in detail in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Essay Representation</head><p>This component converts an input essay into a dense vector as its representation. Recurrent neu- ral networks <ref type="bibr" target="#b24">(Williams and Zipser, 1989)</ref> are widely used to learn a representation for a se- quence of words for essay scoring. Following ex- isting researches, we also use recurrent neural net- works (RNN) and test two different structures.</p><p>Bidirectional LSTM We first use a double- layer bi-directional LSTM network <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) to process the essay. LSTM is a variant of recurrent neural network which uses gates to control the information flow. Our LSTM processes one word at a timestamp. Given the word embedding sequence {x 1 , x 2 , ..., x n } for the essay, the hidden states of the LSTM are calculated as follows:</p><formula xml:id="formula_0">f t = σ(W f x t + U f h t−1 + b f ) i t = σ(W i x t + U i h t−1 + b i ) o t = σ(W o x t + U o h t−1 + b o ) ˜ c t = tanh(W c x t + U c h t−1 + b c ) c t = f t • c t−1 + i t • ˜ c t h t = o t • tanh(c t ) where W f , W i , W o , W c , U f , U i , U o</formula><p>and U c are weight matrices, b f , b i , b o and b c are bias vec- tors. σ denotes sigmoid function and • denotes element-wise multiplication.</p><p>In particular, the average value over all hidden states of each LSTM layer are computed, and we concatenate the mean states of the two layers to- gether as the embedding vector of the essay. Given h i,j as the j-th hidden state of the i-th layer, the layer outputs and the essay embedding vector E are defined as follows:</p><formula xml:id="formula_1">output i = 1 n n j=1 h i,j E = output 0 output 1</formula><p>Dilated LSTM Dilated recurrent neural net- works ( <ref type="bibr" target="#b2">Chang et al., 2017</ref>) are proved to be more effective than traditional RNNs in long sequence processing, by capturing multi-timescale informa- tion along the sequence, with the mechanism of dilated skip connections.</p><p>Denoting</p><formula xml:id="formula_2">s i t = f (x i t , s i t−1 )</formula><p>as the iteration of cell states in traditional RNNs, states in dilated RNNs are iterated as</p><formula xml:id="formula_3">s i t = f (x i t , s i t−k i )</formula><p>, where k i is the skip length in the i-th layer. In order to keep the most information active, we simply con- catenate the average hidden states of every layer to form the essay embedding.</p><formula xml:id="formula_4">E = concat(output i ), f or i in 1, 2, ..., L</formula><p>where L is the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Essay Scoring</head><p>Traditionally, a linear layer with sigmoid function is used to score an essay. Given an essay embed- ding vector E, the essay score is calculated as fol- lows:</p><formula xml:id="formula_5">y = sigmoid(W T l E + b l )</formula><p>where W l and b l are weight vector and bias for scoring. By running n examples together, mean square error is used to evaluate the predicted score.</p><formula xml:id="formula_6">loss M SE = 1 n n i=1 (y i − ˆ y i ) 2</formula><p>where y andˆyandˆ andˆy are score vectors representing pre- dicted scores and ground truth scores, respectively. As we can see, such objective function is unable to take rating schema into consideration.</p><p>The regression-based scorer only outputs a sin- gle value without probability distribution. It is thus non-trivial to use it for policy learning in RL framework directly. Therefore, we propose to use a classification-based scorer, in which differ- ent score categories and their probabilities consti- tute an action space.</p><p>Classification-based Scoring We first feed the essay vector into a fully connected layer, then soft- max function is used to transform the output into a probability distribution. Given an essay embed- ding vector E, the probability distribution vector c is calculated as follows:</p><formula xml:id="formula_7">c = sof tmax(W c E + b c )</formula><p>where W c and b c are weight matrix and bias vec- tor, respectively.</p><p>Given the ground truth category, cross entropy loss is applied to evaluate the agreement of the probabilities as follows:</p><formula xml:id="formula_8">loss CE = − N i=1 Y i log(c i )</formula><p>where N is the number of categories, which is equal to the number of possible ratings. Y is a one-hot vector with the element representing the ground truth category set as one.</p><p>Inter-class Penalty Cross entropy loss used in classification-based scorer does not imply the difference between categories, i.e. the rank infor- mation that is deemed to be important for essay scoring. Thus we enforce a penalty in addition to the cross entropy loss. Inspired by the definition of QWK, the penalty vector p is defined as follows:</p><formula xml:id="formula_9">p i = (i − score) 2 (N − 1) 2</formula><p>where score is the ground truth score of the essay. The penalty loss function is defined as:</p><formula xml:id="formula_10">loss P = N i=1 c i p i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed Scoring</head><p>In practice, we jointly train both a regression-based scoring layer and a classification-based scoring layer over the same document representation to help the classification- based scorer converge. By combining the two scorers together, the overall loss function can be written as:</p><formula xml:id="formula_11">loss pre = α 0 loss M SE + β 0 loss CE + γ 0 loss P</formula><note type="other">where α 0 , β 0 and γ 0 are hyper parameters. Mixed scoring is used</note><p>as a pre-train model for our essay scorer in the phase of reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reinforcement Learning</head><p>We define our loss function as the negative ex- pected reward:</p><formula xml:id="formula_12">loss RL = −E τ ∼p(τ ) r(τ )</formula><p>where τ is the set of actions, r denotes the reward, which is the average QWK an essay achieves in the packed evaluation.</p><p>By running n examples at a time, according to the REINFORCE algorithm <ref type="bibr" target="#b23">(Williams, 1992)</ref>, an approximated gradient can be calculated by:</p><formula xml:id="formula_13">∂loss RL ∂θ = n i=1 [∂log(p i,y |E i ; θ)R i ]</formula><p>where θ denotes all parameters relevant to score calculation, and ∂log(p i,y |E i ; θ) can be computed by standard back propagation. Note that only the classification-based scorer is involved in the process of reinforcement learning for essay scoring. The overall loss function for this phase can be written as:</p><formula xml:id="formula_14">loss overall = α 1 loss RL + β 1 loss CE + γ 1 loss P</formula><p>where α 1 , β 1 and γ 1 are hyper parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Quadratic Weighted Kappa(QWK)</head><p>QWK calculation emphasizes on the overall rating schema. By setting QWK as the reward, our model is trained at a macro aspect taking the grading spe- cialty of different sets of essays into consideration.</p><p>An N-by-N quadratic weight matrix W is first computed to encode the rating information.</p><formula xml:id="formula_15">W i,j = (i − j) 2 (N − 1) 2</formula><p>where N is the number of possible ratings. An N-by-N matrix A is calculated such that A i,j corre- sponds to the number of essays that receive a score i by the human rater, and a score j by the scoring system. Another N-by-N matrix B is constructed as the outer product of the histogram vectors of the two ratings. A and B are then normalized such that they have the same sum. Finally, from the three matrices, the quadratic weighted kappa is calcu- lated as follows:</p><formula xml:id="formula_16">κ = 1 − Σ i,j W i,j A i,j Σ i,j W i,j B i,j</formula><p>set # of essays avg. of len. rating <ref type="table" target="#tab_1">range  1  1783  350  2-12  2  1800  350  1-6  3  1726  150  0-3  4  1772  150  0-3  5  1805  150  0-4  6  1800  150  0-4  7  1569  250  0-30  8  723  650</ref> 0-60 <ref type="table">Table 1</ref>: Details of the ASAP dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup</head><p>The ASAP dataset is used for evaluation. It con- sists of essays written by middle-school English- speaking students ranging among eight different topics. More details are listed in <ref type="table">Table 1</ref>. As there are no released labels for the test data, we separate the validation set and test set from the original training data. Following <ref type="bibr" target="#b16">Taghipour and Ng (2016)</ref> and <ref type="bibr" target="#b5">Dong et al. (2017)</ref>, we use 5-fold cross-validation. In each fold, the split is 60%, 20%, 20% for training, validation and testing re- spectively.</p><p>All essays are parsed with the NLTK 2 tok- enizer. We pre-train the word embedding via word2vec ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>) on the whole dataset. The number of hidden states in LSTMs is 200. We use a four-layer double-directional di- lated LSTM with skip lengths 1,2,4,8 in each layer respectively. During the training and the scoring, scores are scaled to range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> for regression- based scorer. They are restored to integers when calculating QWK values. In the RL phase, the pack size is 64 essays, and packed evaluation is repeated 7 times per essay. The essay scorer for RL is pre-trained by mixed scoring.</p><p>We compare the performance of different ap- proaches:</p><p>• B0: This model uses a double-layer bi- directional LSTM to encode an essay and mean square error as objective function to train the essay scorer; • B1: This is a classification-based scorer and it is trained jointly with a regression-based scorer;  • P1: This model shares the same settings with P0, but uses dilated LSTM instead of a double-layer bi-directional LSTM for essay representation;</p><p>• RL0: This model uses P0 as the scorer under our reinforcement learning framework; • RL1: This model uses P1 as the scorer under our reinforcement learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The overall results of our models in terms of QWK are shown in <ref type="table" target="#tab_1">Table 2</ref>. We have the following find- ings:</p><p>• By incorporating a penalty loss to the classifi- cation scorer, the performance of P0 is equal to or better than B1 on all the eight sets. This indicates the effectiveness of combining rank information with cross-entropy loss for essay scoring.</p><p>• By replacing double-layer bi-directional LSTM with dilated LSTM, P1 improves the QWK values by a large margin compared with P0 on all the eight sets. This indicates the effectiveness of using dilated LSTM for document representation for the task of au- tomatic essay scoring. The performance im- provement brought by P1 compared to P0 is even greater when the length of essays are higher (set 1,2,7,8, see <ref type="table">Table 1</ref>), indicating that dilated networks are specifically better at long sequence processing.</p><p>• By incorporating QWK to guide the opti- mization of essay scorer, approaches (RL0 and RL1) with reinforcement learning strat- egy can improve the performance consis- tently on all the eight sets compared to their counterparts (P0 and P1). We also performed one-tailed t-test, showing that the improve- ments brought by reinforcement learning are significant with p &lt; 0.05 compared to their base scorer models (RL1 vs. P1 and RL2 vs. P2).</p><p>• The performance of classification-based scorer B1 can equate or improve the per- formance on four datasets (set 3,4,5,6) compared with regression-based scorer B0.</p><p>The rating ranges for set 1,2,7,8 are much greater than set 3,4,5,6 (see <ref type="table">Table 1</ref>). The performance difference between B1 and B0 decreases (from positive to negative) when the number of rating categories increases. This is because when the number of cat- egories get larger, it requires much more parameters for the classification-based scorer to be well trained. Given N categories, the classification layer should output N probabilities for each category per essay, costing N times more parameters than regression-based scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There are two lines of research related to our work including text quality evaluation and reinforce- ment learning for natural language processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Quality Evaluation</head><p>Traditionally, AES models are usually divided into three categories: classification, regression and ranking. Naive Bayes models are mostly used in classification tasks. Larkey (1998) use bag- of-word features. Following that, Rudner and Liang (2002) develop a system based on multi- nomial Bernoulli Naive Bayes, using content and style features. E-rater ( <ref type="bibr" target="#b0">Attali and Burstein, 2004</ref>) is one of the earliest systems to adopt regres- sion methods.  <ref type="bibr">Wang et al. (2017)</ref> make use of linguis- tic features to evaluate the persuasiveness of ar- guments in online forums. <ref type="bibr" target="#b22">Wei et al. (2016b);</ref><ref type="bibr" target="#b8">Ji et al. (2018)</ref> consider features from the perspec- tives of argumentation interaction between par- ticipants. <ref type="bibr" target="#b11">Persing and Ng (2017)</ref> construct their model based on error types for argumentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reinforcement Learning for Natural Language Processing</head><p>Being able to optimize non-differentiable quality metrics, reinforcement learning has been widely used in natural language processing tasks such as machine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2016)</ref>, im- age captioning <ref type="bibr" target="#b14">(Rennie et al., 2016;</ref>) and text summarization ( <ref type="bibr" target="#b13">Ranzato et al., 2015)</ref>. To the best of our knowledge, this paper is the first attempt to optimize the scorer by QWK that considers rating schema. Skip connections in RNNs are capable of capturing long-term dependencies in sequences. <ref type="bibr" target="#b19">Vezhnevets et al. (2017)</ref> introduces dilated LSTM to allow its manager to operate at a low temporal resolution.  propose a reinforce- ment learning method to let the network learn how long to skip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a reinforcement learning framework incorporating QWK metric as the re- ward to train the essay scoring system directly. A packed evaluation strategy is used for QWK com- putation and the scoring of each essay is treated as a single action. In particular, dilated LSTM is used to encode an essay, and a softmax layer is utilized for essay grading. Experiment results on benchmark datasets prove that training the grading system toward QWK is effective.</p><p>Further analysis on experiment results indicates the disadvantage of using a classification-based scorer for essays with complex grading schema. One of the future directions can be exploring other kinds of scoring actions than classification under the reinforcement learning framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The reinforcement learning framework for automatic essay scoring. Node in color stands for a target essay and nodes in grey are essays randomly chose to form a pack for QWK calculation.</figDesc><graphic url="image-1.png" coords="2,72.00,63.81,218.50,127.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) fo- cusing on domain-adaptation tasks. Ranking mod- els use linguistic features. Yannakoudakis et al. (2011) formulate AES as a pair-wise ranking prob- lem by ranking the order of pair essays. Chen and He (2013) formulate AES into a list-wise ranking problem by considering the order relation among the whole essays. Argument quality evaluation is a task closely related to AES, which involves evaluation of ar- gumentative texts with various grains (argument- level, post-level, etc.). Tan et al. (2016); Wei et al. (2016a);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Experiment results for different models in terms of QWK. Bolded number is the best performance in each row.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks for the constructive comments from anonymous reviewers. This work is partially supported by National Natural Science Founda-tion of China (Grant No. 61702106), Shang-hai Science and Technology Commission (Grant No. 17JC1420200, Grant No. 17YF1427600 and Grant No. 16JC1420401).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater R v. 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETS Research Report Series</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1741" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic features for essay scoring-an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1072" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attentionbased recurrent convolutional neural network for automatic essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating argument-level interactions for persuasion comments evaluation using co-attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangkun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3703" to="3714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic essay grading using text categorization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leah S Larkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why cant you convince me? modeling weaknesses in unpersuasive arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4082" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Steven J Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00563</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automated essay scoring using bayes&apos; theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahung</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Technology, Learning and Assessment</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural approach to automated essay scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1882" to="1891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescuniculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="624" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Skipflow: Incorporating neural coherence features for end-to-end automatic text scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04981</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01161</idno>
		<title level="m">Feudal networks for hierarchical reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Beauchamp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05040</idno>
		<title level="m">Sarah Shugars, and Kechen Qin. 2017. Winning on the merits: The joint effects of content and style on debate outcomes</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is this post persuasive? ranking argumentative comments in online forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A preliminary study of disputation behavior in online debating forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Stallbohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Argument Mining (ArgMining2016)</title>
		<meeting>the Third Workshop on Argument Mining (ArgMining2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="166" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongrae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06877</idno>
		<title level="m">Learning to skim text</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09601</idno>
		<title level="m">Shaogang Gong, Yongxin Yang, and Timothy M Hospedales. 2017. Actor-critic sequence training for image captioning</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
