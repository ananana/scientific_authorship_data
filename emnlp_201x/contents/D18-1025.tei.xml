<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLUSE: Cross-Lingual Unsupervised Sense Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ta-Chung</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CLUSE: Cross-Lingual Unsupervised Sense Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="271" to="281"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>271</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a modularized sense induction and representation learning model that jointly learns bilingual sense embeddings that align well in the vector space, where the cross-lingual signal in the English-Chinese parallel corpus is exploited to capture the collocation and distributed characteristics in the language pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS) dataset to ensure the quality of monolingual sense em-beddings. In addition, we introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality dataset for evaluating cross-lingual sense embeddings, which is the first attempt of measuring whether the learned em-beddings are indeed aligned well in the vector space. The proposed approach shows the superior quality of sense embeddings evaluated in both monolingual and bilingual spaces. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings have recently become the ba- sic component in most NLP tasks for its ability to capture semantic and distributed relationships learned in an unsupervised manner. The higher similarity between word vectors can indicate sim- ilar meanings of words. Therefore, embeddings that encode semantics have been shown to serve as the good initialization and benefit several NLP tasks. However, word embeddings do not allow a word to have different meanings in different contexts, which is a phenomenon known as pol- ysemy. For example, "apple" may have different meanings in fruit and technology contexts. Sev- eral attempts have been proposed to tackle this problem by inferring multi-sense word representa- tions <ref type="bibr" target="#b20">(Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b19">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b14">Li and Jurafsky, 2015;</ref><ref type="bibr" target="#b13">Lee and Chen, 2017)</ref>. <ref type="bibr">1</ref> The code and dataset are available at http:// github.com/MiuLab/CLUSE. These approaches relied on the "one-sense per collocation" heuristic <ref type="bibr" target="#b28">(Yarowsky, 1993)</ref>, which assumes that presence of nearby words correlates with the sense of the word of interest. However, this heuristic provides only a weak signal for dis- criminating sense identities, and it requires a large amount of training data to achieve competitive per- formance.</p><p>Considering that different senses of a word may be translated into different words in a foreign lan- guage, <ref type="bibr" target="#b6">Guo et al. (2014</ref><ref type="bibr">Guo et al. ( ) andŠusterandˇandŠuster et al. (2016</ref> proposed to learn multi-sense embeddings using this additional signal. For example, "bank" in English can be translated into banc or banque in French, depending on whether the sense is finan- cial or geographical. Such information allows the model to identify which sense a word belongs to. However, the drawback of these models is that the trained foreign language embeddings are not aligned well with the original embeddings in the vector space.</p><p>This paper addresses these limitations by proposing a bilingual modularized sense induction and representation learning system. Our learn- ing framework is the first pure sense representa- tion learning approach that allows us to utilize two different languages to disambiguate words in En- glish. To fully use the linguistic signals provided by bilingual language pairs, it is necessary to en- sure that the embeddings of each foreign language are related to each other (i.e., they align well in the vector space). We solve this by proposing an algorithm that jointly learns sense representations between languages. The contributions of this pa- per are four-fold:</p><p>• We propose the first system that maintains purely sense-level cross-lingual representa- tion learning with linear-time sense decod- ing.</p><p>• We are among the first to propose a single ob-jective for modularized bilingual sense em- bedding learning.</p><p>• We are the first to introduce a high-quality dataset for directly evaluating bilingual sense embeddings.</p><p>• Our experimental results show the state-of- the-art performance for both monolingual and bilingual contextual word similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There are a lot of prior works focusing on repre- sentation learning, while this work mainly focuses on bridging the work about sense embeddings and cross-lingual embeddings and introducing a newly collected bilingual data for better evaluation.</p><p>Sense Embeddings Reisinger and Mooney (2010) first proposed multi-prototype embeddings to address the lexical ambiguity when using a sin- gle embedding to represent multiple meanings of a word. <ref type="bibr" target="#b9">Huang et al. (2012)</ref>; <ref type="bibr" target="#b19">Neelakantan et al. (2014)</ref>; <ref type="bibr" target="#b14">Li and Jurafsky (2015)</ref>; <ref type="bibr" target="#b2">Bartunov et al. (2016)</ref> utilized neural networks as well as the Bayesian non-parametric method to learn sense embeddings. <ref type="bibr" target="#b13">Lee and Chen (2017)</ref> first utilized a reinforcement learning approach and proposed a modularized framework that separates learning of senses from that of words. However, none of them leverages the bilingual signal, which may be help- ful for disambiguating senses.</p><p>Cross-Lingual Word Embeddings <ref type="bibr" target="#b10">Klementiev et al. (2012)</ref> first pointed out the importance of learning cross-lingual word embeddings in the same space and proposed the cross-lingual docu- ment classification (CLDC) dataset for extrinsic evaluation. <ref type="bibr" target="#b5">Gouws et al. (2015)</ref> trained directly on monolingual data and extracted a bilingual sig- nal from a smaller set of parallel data. Kočisk` <ref type="bibr" target="#b11">Kočisk`y et al. (2014)</ref> used a probabilistic model that simul- taneously learns alignments and distributed repre- sentations for bilingual data by marginalizing over word alignments. <ref type="bibr" target="#b11">Hermann and Blunsom (2014)</ref> learned word embeddings by minimizing the dis- tances between compositional representations be- tween parallel sentence pairs. ˇ Suster et al. (2016) reconstructed the bag-of-words representation of semantic equivalent sentence pairs to learn word embeddings. <ref type="bibr" target="#b22">Shi et al. (2015)</ref> proposed a training algorithm in the form of matrix decomposition, and induced cross-lingual constraints for simul- taneously factorizing monolingual matrices. <ref type="bibr" target="#b15">Luong et al. (2015)</ref> extended the skip-gram model to bilingual corpora where contexts of bilingual word pairs were jointly predicted. <ref type="bibr" target="#b27">Wei and Deng (2017)</ref> proposed a variational autoencoding approach that explicitly models the underlying semantics of the parallel sentence pairs and guided the generation of the sentence pairs. Although the above ap- proaches aimed to learn cross-lingual embeddings jointly, they fused different meanings of a word in one embedding, leading to lexical ambiguity in the vector space model. <ref type="bibr" target="#b6">Guo et al. (2014)</ref> adopted the heuristics where different meanings of a polysemous word usually can be represented by different words in another language and clustered bilingual word embeddings to in- duce senses. ˇ <ref type="bibr" target="#b23">Suster et al. (2016)</ref> proposed an encoder, which uses parallel corpora to choose a sense for a given word, and a decoder that predicts context words based on the chosen sense. <ref type="bibr" target="#b1">Bansal et al. (2012)</ref> proposed an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common se- mantic sense. <ref type="bibr" target="#b26">Upadhyay et al. (2017)</ref> leveraged cross-lingual signals in more than two languages. However, they either used pretrained embeddings or learned only for the English side, which is un- desirable since cross-lingual embeddings shall be jointly learned such that they aligned well in the embedding space. Evaluation Datasets Several datasets can be used to justify the performance of learned sense embeddings. <ref type="bibr" target="#b9">Huang et al. (2012)</ref> presented SCWS, the first and only dataset that contains word pairs and their sentential contexts for mea- suring the quality of sense embeddings. However, it is a monolingual dataset constructed in English, so it cannot evaluate cross-lingual semantic word similarity. On the other hand, while <ref type="bibr">CamachoCollados et al. (2017)</ref> proposed a cross-lingual se- mantic similarity dataset, it ignored the contextual words but kept only word pairs, making it impos- sible to judge sense-level similarity. In this paper, we present an English-Chinese contextual word similarity dataset in order to benchmark the exper- iments about bilingual sense embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual Sense Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLUSE: Cross-Lingual Unsupervised Sense Embeddings</head><p>Our proposed model borrows the idea about mod- ularization from <ref type="bibr" target="#b13">Lee and Chen (2017)</ref>, which treats the sense induction and representation mod- </p><formula xml:id="formula_0">手機_2 (cellphone_2) company_1</formula><p>Apple company designs the best cellphone in the world <ref type="figure">Figure 1</ref>: Sense induction modules decide the senses of words, and two sense representation learning modules optimize the sense collocated likelihood for learning sense embeddings within a language and between two languages. Two languages are treated equally and optimized iteratively.</p><p>ules separately to avoid mixing word-level and sense-level embeddings together. Our model consists of four different modules il- lustrated in <ref type="figure">Figure 1</ref>, where sense induction mod- ules decide the senses of words, and two sense rep- resentation learning modules optimize the sense collocated likelihood for learning sense embed- dings within a language and between two lan- guages in a joint manner. All modules are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>We denote our parallel corpus without word align- ment C, where C en is for the English part and C zh is for the Chinese part. Our English vocabulary is W en and Chinese vocabulary is W zh . Moreover, C en t and C zh t are the t-th sentence-level parallel sentences in English and Chinese respectively. In the following sections, we treat English as the ma- jor language and Chinese as an additional bilin- gual signal, while their roles can be mutually ex- changed. Specifically, English and Chinese itera- tively become the major language during the train- ing procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bilingual Sense Induction Module</head><p>The bilingual sense induction module takes a par- allel sentence pair as input and determines which sense identity a target word belongs to given the bilingual contextual information. Formally, for the t-th English sentence C en t , we aim to decode the most probable sense z ik ∈ Z i for the i-th word w i ∈ W en in C en t , where Z i is the set of sense candidates for w i and 1 ≤ k ≤ |Z i |. We assume that the meaning of w i can be determined by its surrounding words, or the so-called local context,</p><formula xml:id="formula_1">c i = {w i−m , · · · , w i+m },</formula><p>where m is the size of context window. Aside from monolingual information, it is desir- able to exploit the parallel sentences as additional bilingual contexts to enable cross-lingual embed- ding learning. Note that word alignment is not re- quired in this work, so we consider the whole par- allel bilingual sentence during training. Consider- ing training efficiency, we sample M words in the parallel bilingual sentence with their original rel- ative order or pad it to M for those shorter than M . Formally, given the t-th parallel bilingual sen- tence C zh t , the bilingual context of w i is therefore</p><formula xml:id="formula_2">c i = {w 0 , · · · , w M −1 } and w ∈ W zh .</formula><p>To ensure efficiency, continuous bag-of-words (CBOW) model is applied, where it takes word- level input tokens and outputs sense-level identi- ties. Specifically, given an English word embed- ding matrix P en , the local context can be mod- eled as the average of word embeddings from its context, 1</p><formula xml:id="formula_3">|c i | w j ∈c i P en j .</formula><p>Similarly, we can model the bilingual contextual information given Chinese word embedding matrix P zh using the CBOW for- mulation and obtain 1 M w j ∈c i P zh j . We linearly combine the contextual information from different languages as:</p><formula xml:id="formula_4">¯ C = α · 1 |c i | w j ∈c i P en j + (1 − α) · 1 M w j ∈c i P zh j .<label>(1)</label></formula><p>The likelihood of selecting each sense iden- tity z ik for w i can be formulated in the form of Bernoulli distribution with a sigmoid function σ(·):</p><formula xml:id="formula_5">p(z ik | c i , c i ) = σ((Q en ik ) T ¯ C),<label>(2)</label></formula><p>where Q en is a 3-dimensional tensor with each dimension denotes W en , z ik for a specific word i in W en , and the corresponding latent variable, respectively. Therefore, Q en ik will retrieve the la- tent variable of k-th sense of i-th English word. Finally, we can induce the sense identity, z * ik , given the contexts of a word w i from different lan- guages, c i and c i .</p><formula xml:id="formula_6">z * ik = arg max z ik p(z ik | c i , c i )<label>(3)</label></formula><p>In order to allow the module to explore other po- tential sense identities, we apply an -greedy al- gorithm <ref type="bibr" target="#b17">(Mnih et al., 2013</ref>) for exploration in the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Monolingual Sense Induction Module</head><p>This module is the degraded version of bilingual sense induction module when α = 1, which oc- curs where no parallel bilingual signal exists. In other words, every bilingual sense induction mod- ule will experience the degradation during the training process presented in Algorithm 1. The only difference is that it cannot access the bilin- gual information. The purpose of this module is to maintain the stability of sense induction and to decode the sampled bilingual sense identity which will later be used in the bilingual sense representa- tion learning module. As shown in <ref type="figure">Figure 1</ref>, given the monolingual context of a word, this module selects its sense identity using <ref type="formula" target="#formula_5">(2)</ref> and <ref type="formula" target="#formula_6">(3)</ref> with α = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Monolingual Sense Representation Learning Module</head><p>Given the decoded sense identities from the sense induction module, the skip-gram architec- ture ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) is applied consider- ing that it only requires two decoded sense iden- tities for stochastic training. We first create an in- put English sense representation matrix U en and an English collocation estimation matrix V en as the learning targets. Given a target word w i and its collocated word w j in the t-th English sen- tence C en t , we map them to their sense identities as z * ik = s i and z * jl = s j by the sense induction module and maximize the sense collocation likeli- hood. The skip-gram objective can be formulated as p(s j | s i ):</p><formula xml:id="formula_7">p(s j | s i ) = exp((U en s i ) T V en s j ) s k exp((U en s i ) T V en s k ) ,<label>(4)</label></formula><p>where s k iterates over all possible English sense identities in the denominator. This formulation shares the same architecture as skip-gram but ex- tends to rely on senses. Note that the Chinese sense representation learning module is built sim- ilarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Bilingual Sense Representation Learning Module</head><p>To ensure sense embeddings of two different lan- guages align well, we hypothesize that the target sense identity s i not only predicts the sense iden- tity s j of w j in C en t but also one sampled sense identity s l of w l from the parallel sentence C zh t , where s l is decoded by the Chinese monolingual sense induction module. Specifically, the bilin- gual skip-gram objective can be formulated using the English sense embedding matrix U en and the bilingual collocation estimation matrix V zh as:</p><formula xml:id="formula_8">p(s l | s i ) = exp((U en s i ) T V zh s l ) s k exp((U en s i ) T V zh s k ) ,<label>(5)</label></formula><p>where s k iterates over all possible Chinese sense identities in the denominator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Joint Learning</head><p>In this learning framework, the gradient cannot be back-propagated from the representation mod- ule to the induction module due to the usage of arg max operator. It is therefore desirable to con- nect these two modules in a way such that they can improve each other by their own estimations. In one direction, forwarding the prediction of the sense induction module to the sense representation learning module is trivial, while in another direc- tion, we treat the estimated collocation likelihood as the reward for the induction module.</p><p>First note that calculating the partition func- tion in the denominator of (4) and (5) is in- tractable since it involves a computationally ex- pensive summation over all sense identities. In practice, we adopt the negative sampling strategy technique ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) and rewrite <ref type="formula" target="#formula_7">(4)</ref> and <ref type="formula" target="#formula_8">(5)</ref> as:</p><formula xml:id="formula_9">log p(s j | s i ) = log σ((U en s i ) T V en s j )+ N k=1 E s k ∼pneg(s) [σ(−(U en s i ) T V en s k )],<label>(6)</label></formula><formula xml:id="formula_10">log p(s l | s i ) = log σ((U en s i ) T V zh s l )+ N k=1 E s k ∼pneg(s ) [σ(−(U en s i ) T V zh s k )],<label>(7)</label></formula><p>where p neg (s) and p neg (s ) is the distribution over all English senses and all Chinese senses for nega- tive samples respectively, and N is the number of negative sample. The rewritten objective for op- timizing two sense representation learning mod- ules is the same as maximizing <ref type="formula" target="#formula_9">(6)</ref> and <ref type="formula" target="#formula_10">(7)</ref>. More- over, we can utilize the probability of correctly classifying the skip-gram sense pair as the reward signal. The intuition is that a correctly decoded sense identity is more likely to predict its neigh- boring sense identity compared to incorrectly de- coded ones. This learning framework can now be viewed as a reinforcement learning agent solving one- step Markov Decision Process ( <ref type="bibr" target="#b24">Sutton and Barto, 1998;</ref><ref type="bibr" target="#b13">Lee and Chen, 2017)</ref>. For bilingual mod- ules, the state, action, and reward correspond to bilingual context ¯ C, sense z ik , and σ((U en s i ) T V zh s l ) respectively. As for the monolingual modules, the state, action, and reward correspond to monolin- gual context c t , sense z ik , and σ((U en s i ) T V en s j )). Finally, we can optimize both bilingual and mono- lingual sense induction modules (P and Q from (2) by minimizing the cross entropy loss between decoded sense probability and reward. We also in- clude an entropy regularization term as suggested in <ref type="bibr" target="#b23">( ˇ Suster et al., 2016</ref>) to let the sense induction module converge faster and make more confident predictions. Formally,</p><formula xml:id="formula_11">min H(σ((U en s i ) T V zh s l ), p(z ik | c i , c i )) + λE(p(z ik | c i , c i ))<label>(8)</label></formula><formula xml:id="formula_12">min H(σ((U en s i ) T V en s j ), p(z ik | c i )) + λE(p(z ik | c i ))<label>(9)</label></formula><p>E is the entropy of selection probability weighted by λ. Note that the major language is switched</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Bilingual Sense Embedding Learn- ing Algorithm</head><p>Input: C en , C zh , W en , W zh Output: P en , P zh , Q en , Q zh , U en , U zh , V en , V zh 1: loop until converge 2: MAIN(en, zh, 0.4) 0.4 is just an example weight 3: MAIN(zh, en, 0.4) 4: end loop 5: function MAIN(maj, bi, α) 6:</p><p>t, i, j, k, l ← GETTRAINDATA(maj) 7:</p><p>si, predi ← INDUCESENSE(maj, bi, t, i, α) 8: sj, ← INDUCESENSE(maj, bi, t, j, α) 9:</p><formula xml:id="formula_13">s l , pred l ← INDUCESENSE(bi, bi, t, k, 1.0) 10: s k , ← INDUCESENSE(bi, bi, t, l, 1.0) 11:</formula><p>r ← TRAINSRL(maj, maj, si, sj) 12:</p><p>r ←TRAINSRL(maj, bi, si, s l ) 13:</p><formula xml:id="formula_14">r ←TRAINSRL(bi, bi, s l , s k ) 14:</formula><p>TRAINSI(maj, bi, r, predi) 15:</p><p>TRAINSI(maj, bi, r , predi) 16:</p><p>TRAINSI(bi, bi, r , pred l ) 17: end function 18: function INDUCESENSE(maj, bi, t, i, α) 19:</p><p>calculate α-weighted ¯ C by (1) 20: select z * ik by <ref type="formula" target="#formula_5">(2)</ref> and <ref type="formula" target="#formula_4">(3)  21</ref>:</p><formula xml:id="formula_15">return z * ik , p(z * ik | ¯ C) 22: end function 23: function TRAINSRL(maj, bi, si, sj) 24:</formula><p>if maj==bi then 25:</p><p>optimize U maj , V maj by (6) given si, sj 26:</p><formula xml:id="formula_16">else 27:</formula><p>optimize U maj , V bi by <ref type="formula" target="#formula_10">(7)</ref> given si, sj 28:</p><p>end if 29:</p><p>return collocation prob of (si, sj) 30: end function 31: function TRAINSI(maj, bi, r, pred) 32:</p><p>if maj==bi then 33:</p><p>optimize P maj , Q maj by (9) given r, pred 34: else 35:</p><p>optimize P maj , Q bi by (8) given r, pred 36:</p><p>end if 37: end function iteratively among two languages. Algorithm 1 presents the full learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">New Dataset-Bilingual Contextual</head><p>Word Similarity (BCWS)</p><p>We propose a new dataset to measure the bilingual contextual word similarity. English and Chinese are chosen as our language pair for three reasons: 1. They are the top widely used languages in the world. 2. English and Chinese belong to completely different language families, making it inter- esting to explore syntactic and semantic dif- ference among them. 3. Chinese is a language that requires segmen- tation, this dataset can also help researchers experiment on different segmentation levels and investigate how segmentation affects the English Sentence Chinese Sentence Score Judges must give both sides an equal &lt; &gt; 7.00 opportunity to &lt;state&gt; their cases.</p><p>(I like this story a lot, which &lt;tells&gt; us some important inspiration.) It was of negligible &lt;importance&gt; prior &lt; &gt; 6.94 to 1990, with antiquated weapons and (The prevention and early treatment of few members. macular lesions is very &lt;important&gt;.) Due to the San Andreas Fault bisecting &lt; &gt; 3.70 the hill, one side has &lt;cold&gt; water, the other has hot.</p><p>(The owner of the fruit stall seemed surprised that someone bought this &lt;unpopular&gt; product, talking me few words about "you are such a pro".) <ref type="table">Table 1</ref>: Sentence pair examples and average annotated scores in BCWS.</p><p>sense similarity. This dataset also provides a direct measure to determine whether the two language embeddings align well in the vector space. Note that we focus on word-level, and this is different from <ref type="bibr" target="#b10">(Klementiev et al., 2012)</ref>, which also measured the cross- lingual embedding similarity but rely on the am- biguous document-level classification.</p><p>Our dataset contains 2093 question pairs, where each pair consists of exactly one English and one Chinese sentence; note that they are not parallel but with their own sentential contexts shown in <ref type="table">Table 1</ref>. Eleven raters 2 were recruited to anno- tate this dataset. Each rater gives a score ranging from 1.0 (different) to 10.0 (same) for each ques- tion to indicate the semantic similarity of bilingual word pairs based on sentential clues. The anno- tated dataset shows very high intra-rater consis- tency; we leave one rater out and calculate Spear- man correlation between the rater and the average of the rest, and the average number is about 0.83, indicating the human-level performance (the aver- age number in SCWS is 0.52).</p><p>We describe the construction of BCWS below.</p><p>Chinese Multi-Sense Word Extraction We uti- lize the Chinese Wikipedia dump to extract the most frequent 10000 Chinese words that are nouns, adjective, and verb based on Chinese Wordnet ( <ref type="bibr" target="#b8">Huang et al., 2010)</ref>. In order to test the sense-level representations, we discard single- sense words to ensure that the selected words are polysemous. Also, the words with more than 20 senses are deleted, since those senses are too fine- 2 They are all Chinese native speaker whose scores are at least 29 in the TOEFL reading section or 157 in the GRE verbal section. grained and even hard for human to disambiguate. We denote the list of Chinese words l c .</p><p>English Candidate Word Extraction We have to find an English counterpart for each Chinese word in l c . We utilize BabelNet ( <ref type="bibr" target="#b18">Navigli and Ponzetto, 2010)</ref>, a free and open-sourced knowl- edge resource, to serve as our bilingual dictionary. To be more concrete, we first query the selected Chinese word using the free API call provided by Babelnet to retrieve all WordNet senses 3 . For ex- ample, the Chinese word "" has two major meanings:</p><p>• a type of clothing worn by members of an or- ganization • force to submit or subdue.</p><p>Hence, we can obtain two candidate English words "uniform" and "subjugate". Each word in l c retrieves its associated English candidate words and obtain the dictionary D.</p><p>Enriching Semantic Relationship Note that D is merely a simple translation mapping between Chinese and English words. It is desirable that we have a more complicated and interesting relation- ship between bilingual word pairs. Hence, we tra- verse D and for each English word we find its hy- ponyms, hypernyms, holonyms and attributes, and add the additional words into D. In our example, we may obtain {:[uniform, subjugate, livery, clothing, repress, dominate, enslave, dragoon...]}. We sample 2 English words if the number of En- glish candidate words is more than 5, 3 English words if more than 10, and 1 English word oth-erwise to form the final bilingual pair. For ex- ample, a bilingual word pair (, enslave) can be formed accordingly. After this step, we obtain 2093 bilingual word pairs P .</p><p>Adding Contextual Information Given the bilingual word pairs P , appropriate contexts should be found in order to form the full sentences for human judgment. For each Chinese word, we randomly sample one example sentence in Chi- nese WordNet that matches the PoS tag we se- lected in section 4. For each English word, we traverse the whole English Wikipedia dump to find the sentences that contain the target English word. We then sample one sentence where the target word is tagged as the matched PoS tag 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Two sets of parallel data are used in the ex- periments, one for English-Chinese (EN-ZH) and another for English-German (EN-DE). UM- corpus (Tian et al.) is used for EN-ZH train- ing, while Europarl corpus ( <ref type="bibr" target="#b12">Koehn, 2005</ref>) is used for EN-DE training. UM-corpus contains 15,764,200 parallel sentences with 381,921,583 English words and 572,277,658 unsegmented Chi- nese words. Europarl contains 1,920,209 par- allel sentences with 44,548,491 German words and 47,818,827 English words. We evaluate our proposed model on the benchmark monolingual dataset, SCWS, and on the bilingual dataset, our proposed BCWS, where the evaluation metrics are actually introduced in section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyperparameter Settings</head><p>In our experiments, we use a mini-batch size of 512, context window size for major language is set to m = 5 and we sample M = 20 words for bilingual context. For the exploration of sense in- duction module, we set = 0.05. The λ of en- tropy regularization is set to 1. 5 For negative sam- pling in (6) and <ref type="formula" target="#formula_10">(7)</ref>, we pick N = 25. The fixed learning rate is set to 0.025. The embedding di- mension is 300 and the sense number per word is set to 3 for both Chinese, German, and English (|Z i | = 3). This setting is for a fair comparison with prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline</head><p>The baselines for comparison can be categorized into three:</p><p>• Monolingual sense embeddings: <ref type="bibr" target="#b13">Lee and Chen (2017)</ref> is the current state-of-the-art model of monolingual sense embedding eval- uated on SCWS. We re-train the sense em- beddings using the same data but only in En- glish for fair comparison.</p><p>• Cross-lingual word embeddings: Luong et al.</p><p>(2015) treated words from different lan- guages the same and trained cross-lingual embeddings in the same space. <ref type="bibr" target="#b4">Conneau et al. (2017)</ref> utilized adversarial training to map pretrained word embeddings into an- other language space.</p><p>• Cross-lingual sense embeddings: Upadhyay et al. <ref type="formula" target="#formula_4">(2017)</ref> utilized more than two languages to learn multilingual embeddings. We report the number shown in the paper for compari- son.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Metric</head><p>Reisinger and Mooney (2010) introduced two contextual similarity estimations, AvgSimC and MaxSimC. AvgSimC is a soft measurement that addresses the contextual information with a prob- ability estimation:</p><formula xml:id="formula_17">AvgSimC(w i , ¯ C t , w j , ¯ C t ) = |Z i | k=1 |Z j | l=1 π(z ik | ¯ C t )π(z jl | ¯ C t )d(z ik , z jl ),</formula><p>AvgSimC weights the similarity measurement of each sense pair z ik and z jl by their probabil- ity estimations. On the other hand, MaxSimC is a hard measurement that only considers the most probable senses:</p><formula xml:id="formula_18">MaxSimC(w i , ¯ C t , w j , ¯ C t ) = d(z ik , z jl ), z ik = arg max z ik π(z ik | ¯ C t ), z jl = arg max z jl π(z jl | ¯ C t ).</formula><p>d(z ik , z jl ) refers to the cosine similarity between U maj z ik and U bi z jl in the bilingual case (BCWS) and U maj z ik and U maj z jl in the monolingual case (SCWS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Bilingual Embedding Evaluation</head><p>Cross-lingual sense embeddings are the main con- tribution of this paper. <ref type="table" target="#tab_2">Table 2</ref> shows that all re- sults from the proposed model are significantly <ref type="bibr" target="#b13">Lee and Chen (2017)</ref> 66.8 / 65.5 63.8 / 63.4 2) Cross-Lingual Word Embeddings <ref type="bibr" target="#b15">Luong et al. (2015)</ref> 49.2 61.1 62.1 <ref type="bibr" target="#b4">Conneau et al. (2017)</ref> 52.5 65.5  better than the baselines that learn cross-lingual word embeddings. It indicates that the sense-level information is critical for precise vector represen- tations. In addition, all results for AvgSimC and MaxSimC are the same in the proposed model, showing that the learned selection distribution is reliable for sense decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model α EN-ZH EN-DE Bilingual/BCWS Mono(EN)/SCWS Mono(EN)/SCWS 1) Monolingual Sense Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="64.0">3) Cross-Lingual Sense Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Monolingual Embedding Evaluation</head><p>Because our model considers multiple languages and learns the embeddings jointly, the multilin- gual objective makes learning more difficult due to more noises. In order to ensure the quality of the monolingual sense embeddings, we also eval- uate our learned English sense embeddings on the benchmark SCWS data. Comparing the results be- tween training on EN-ZH and training on EN-DE, all results using EN-ZH are better than ones us- ing EN-DE. The probable reason is that the lan- guage difference between English and Chinese is larger than English and German; parallel Chinese sentences therefore provide informative cues for learning better sense embeddings. Furthermore, our proposed model achieves comparable or supe- rior performance than the current state-of-the-art monolingual sense embeddings proposed by <ref type="bibr" target="#b13">Lee and Chen (2017)</ref> when trained on our monolingual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Sensitivity of Bilingual Contexts</head><p>To investigate how much the bilingual sense in- duction module relies on another language, the re-</p><formula xml:id="formula_19">Model EN2DE DE2EN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Sentence-Level Training</head><p>Hermann and Blunsom <ref type="formula" target="#formula_4">(2014)</ref>   To justify the usefulness of utilizing bilingual signal, we compare our model with <ref type="bibr" target="#b13">Lee and Chen (2017)</ref>, which used monolingual signal in a sim- ilar modular framework. Our method outper- forms theirs in terms of MaxSimC on both EN-ZH and EN-DE. However, this trend is not observed on AvgSimC. The reason may be that bilingual signal is indicative but noisy, which largely af- fects AvgSimC due to its weighted sum operation. MaxSimC only picks the most probable senses, which makes it robust to noises.</p><p>In addition, our performance slightly degrades as α increases for EN-DE, and the best perfor- mance is obtained when α is small, indicating that Target kNN Senses (EN) kNN Senses (ZH) apple 0 fruit, cake, sweet , , , iphone, , , (apple, spring, cake, iphone, egg, chocolate, purples) apple 1 iphone, cake, google, stores , iphone, , , , (apple, iphone, microsoft, competitor, spring, google) uniform 0 dressed, worn, tape, wearing, cloth ,,,,, (even, smooth, clothes, shoes, wearing, clothing) uniform 1 particle, <ref type="figure">computed, varying, gradient ,,,,,</ref> (phase, powder, longitudinal, plasma, cut, stiffness) bilingual signal does help. However, this trend is not observed on EN-ZH, because English is very different from Chinese, such that it can benefit lit- tle from Chinese than from German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Extrinsic Evaluation</head><p>We further evaluate our bilingual sense embed- dings using a downstream task, cross-lingual doc- ument classification (CLDC), with a standard setup ( <ref type="bibr" target="#b10">Klementiev et al., 2012)</ref>. To be more con- crete, a set of labeled documents in language A is available to train a classifier, and we are interested in classifying documents in another language B at test time, which tests semantic transfer of informa- tion across different languages. We use the aver- aged sense embeddings as word embeddings for a fair comparison. The result is shown in <ref type="table" target="#tab_4">Table 3</ref>. We can see that our proposed model achieves comparable perfor- mance or even superior performance to most prior work on the DE2EN direction; however, the same conclusion does not hold for the EN2DE direc- tion. The reason may be that we test the model that works best on BCWS and hence not able to tune hyperparameters on the development set of CLDC. In addition, we use the average of sense vectors as input word embeddings, which may in- duce some noises into the resulting vectors. In sum, the comparable performance of the down- stream task shows the practical usage and the po- tential extension of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Qualitative Analysis</head><p>Some examples of our learned sense embeddings are shown in <ref type="table" target="#tab_5">Table 4</ref>. It is obvious to see that the first sense of Apple is related to fruit and things to eat, while the second one means the tech company Apple Inc. Most English and Chinese nearest neighbors match the meanings of the in- duced senses, but there are still some noises that are underlined. For example, cake should be the neighbor of the first sense rather than the sec- ond one. The same observation applies to iphone and spring. In our second example for uniform, the first sense is related to outfit and clothes, while the second is related to engineering terms. How- ever, even appears in the outfit and clothes sense, which is incorrect. The reason may be that the size of the parallel corpus is not large enough for the model to accurately distinguish all senses via unsupervised learning. Hence, utilizing external resources such as bilingual dictionaries or design- ing a new model that can use existing large mono- lingual corpora like Wikipedia can be our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper is the first purely sense-level cross- lingual representation learning model with effi- cient sense induction, where several monolingual and bilingual modules are jointly optimized. The proposed model achieves superior performance on both bilingual and monolingual evluation datasets. A newly collected dataset for evaluating bilingual contextual word similarity is presented, which provides potential research directions for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Contextual similarity results evaluated on the SCWS/BCWS dataset, where the reported numbers 
indicate Spearman's rank correlation ρ × 100 on AvgSimC / MaxSimC. indicates that Upadhyay et al. 
(2017) trained the sense embeddings using a different parallel dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy on cross-lingual document clas-
sification (%). 

sults with different α are shown in the table. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Words with similar senses obtained by kNN. 

</table></figure>

			<note place="foot" n="3"> BabelNet contains sense definitions from various resources such as Wordnet, Wikitionary, Wikidata, etc</note>

			<note place="foot" n="4"> We use the NLTK PoS tagger to obtain the tags. 5 We tried different values of λ = 0.001, 0.5, and the model converges approximately 12, 5 times slower compared to λ = 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank reviewers for their insight-ful comments on the paper. This work was finan-cially supported from the Young Scholar Fellow-ship Program by Ministry of Science and Technol-ogy (MOST) in Taiwan, under Grant 107-2636-E-002-004.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised translation sense clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="773" to="782" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Breaking sticks and ambiguities with adaptive skip-gram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semeval2017 task 2: Multilingual and cross-lingual semantic word similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multilingual models for compositional distributed semantics</title>
		<idno type="arXiv">arXiv:1404.4641</idno>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chinese wordnet: Design, implementation, and application of an infrastructure for cross-lingual knowledge processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Kai</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Fei</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengwei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving Word Representations via Global Context and Multiple Word Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0947</idno>
		<title level="m">Learning bilingual word representations by marginalizing alignments</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MUSE: Modularizing unsupervised sense embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-He</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="327" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1722" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Babelnet: Building a very large multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning cross-lingual word embeddings via matrix co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilingual learning of multi-sense embeddings with discrete autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simoň</forename><surname>Suster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1346" to="1356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Um-corpus: A large english-chinese parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Liang Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Quaresma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond bilingual: Multi-sense word embeddings using multilingual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
		<meeting>the 2nd Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A variational autoencoding approach for inducing crosslingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4165" to="4171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">One sense per collocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="266" to="271" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
