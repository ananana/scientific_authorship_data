<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 1478</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Ailem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1478" to="1487"/>
							<date type="published">October 31-November 4, 2018. 2018. 1478</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Several recent studies have shown the benefits of combining language and perception to infer word embeddings. These multimodal approaches either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a text corpus. Our approach learns textual and visual representations jointly: latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed model. Concretely , on the tasks of assessing pairwise word similarity and image/caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous-valued vector representation of words has been one of the key components in neural archi- tectures for natural language processing ( <ref type="bibr" target="#b26">Mikolov et al., 2013;</ref><ref type="bibr" target="#b29">Pennington et al., 2014;</ref><ref type="bibr" target="#b24">Levy and Goldberg, 2014</ref>). The main idea is based on the distribu- tional hypothesis <ref type="bibr" target="#b14">(Harris, 1954)</ref>, which states that words used in similar contexts have similar seman- tic meanings. To this end, words are mapped to points in an Euclidean space such that the displace- ment between their coordinates (i.e., embeddings) reflects similarity and difference in semantics <ref type="bibr" target="#b29">(Pennington et al., 2014</ref>). As such, word embeddings * On leave from U of Southern California have been shown to be useful in determining se- mantic and syntactic similarity between individual words ( <ref type="bibr" target="#b26">Mikolov et al., 2013;</ref><ref type="bibr" target="#b25">Levy et al., 2015)</ref>, as well as in downstream NLP tasks, e.g., sentiment analysis, question answering, and coreference resolution, just to name a few.</p><p>Most existing approaches rely solely on text cor- pora to infer word representations. While success- ful, the embeddings produced by such models do not necessarily reflect all inherent aspects of human semantic knowledge, such as the perceptual aspect <ref type="bibr" target="#b11">(Feng and Lapata, 2010)</ref>. This has motivated many researchers to explore different ways to infuse vi- sual information, often represented in the form of pre-computed visual features, into word embed- dings ( <ref type="bibr" target="#b18">Kiela and Bottou, 2014;</ref><ref type="bibr" target="#b34">Silberer et al., 2017;</ref><ref type="bibr" target="#b7">Collell et al., 2017;</ref><ref type="bibr" target="#b23">Lazaridou et al., 2015)</ref>. The main theme is to take either the text embeddings, or the visual features or both as such to derive mul- timodal embeddings: through concatenation ( <ref type="bibr" target="#b18">Kiela and Bottou, 2014)</ref>, or by treating visual features as regression targets ( <ref type="bibr" target="#b23">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b7">Collell et al., 2017)</ref>.</p><p>Despite the success of these prior efforts in yield- ing multimodal embeddings and applying them to downstream NLP tasks, there are still several defi- ciencies. In particular, the visual features (as such) are not guaranteed to be suitable for the word em- bedding task since they are typically optimized independently for another objective (e.g., image classification). Hence, fusing pre-computed word representations and visual features may not be a good strategy.</p><p>To address the above issues, we explore a new way to integrate linguistic and perceptual infor- mation. We develop a new model which jointly learns word embeddings from text and extracts la- tent visual information, from pre-computed visual features, that could supplement the linguistic em- beddings in modeling the co-occurrence of words and their contexts in a corpus. Instead of using pre-trained visual features as it is or as regression targets, we posit that they contain latent perceptual information that could complement text in repre- senting words.</p><p>More specifically, the proposed model consists of two components. The visual component is an unsupervised probabilistic model for learning latent factors that generates the visual data. The linguistic component is a revised SKIP-GRAM model in which the text embeddings work in concert with the latent visual factors to explain the occurrence of word- context pairs in a corpus. One advantage of our joint modeling is that it allows two-way interaction. On one hand, the linguistic information can guide the extraction of latent visual factors. On the other hand, the extracted visual factors can improve the modeling of word-context co-occurrences in text data. Another appealing property of our model is its natural ability to propagate perceptual information to the embeddings of words lacking visual features (e.g., abstract words) during learning.</p><p>We conduct extensive quantitative and qualita- tive experiments to examine and understand the effectiveness of our approach, on the tasks of word similarity and image/caption retrieval. We show its matching or stronger performance when compared to other state-of-the-art approaches for learning multimodal embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>We start by introducing the problem setup and notations. We then describe our model, namely PIXIE (ProbabIlistic teXtual Image Embeddings), for joint learning of word representations from text and perceptual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Setup and Background</head><p>We are given a corpus of H tokens (words): w 1 , . . . , w i , . . . , w H . From the corpus, we form a collection of word-context pairs δ w,c = (w, c), such that w ∈ V w , c ∈ V c , with V w and V c denoting respectively the word and context vocabularies. As in most previous work, the contexts for word w i are the words that surround it in a L-sized window. We introduce the binary indicator variables y wc , such that y wc = 1 if δ w,c appears in our collection, and y wc = 0 otherwise.</p><p>For some words with visual grounding (we will refer to them as visual words), we have access to a visual representation x w . In practice, we use con- volutional net features (see Section 4 for details).</p><p>SKIP-GRAM WITH NEGATIVE SAMPLING (SGNS) The SGNS's objective is to learn word representations that are good at distinguishing the observed pairs (y wc = 1) from non-observed or "negative" pairs (y wc = 0), using logistic regression. Formally, SGNS maximizes the following log-likelihood:</p><formula xml:id="formula_0">w,c [y wc log σ(v T c e w )+(1−y wc ) log σ(−v T c e w )],<label>(1)</label></formula><p>where σ(·) is the sigmoid function, v c , e w denote respectively the vectors for the context c and target word w. The second term in (1) is intractable due to the large number of possible negative pairs, and is approximated by sampling N negative examples</p><formula xml:id="formula_1">{c i } N i=1</formula><p>for every observed pair of words and their contexts. This gives rise to the following objective function for each observed pair:</p><formula xml:id="formula_2">log σ(v T c e w ) + N i=1 log σ(−v T c i e w ),<label>(2)</label></formula><p>where c i is a (negative) context that does not ap- pear in the context of w ( <ref type="bibr" target="#b26">Mikolov et al., 2013</ref>). In practice, criterion (2) is optimized in an online fash- ion, by using Stochastic Gradient Descent (SGD) over the observed pairs δ wc in the corpus. Each observed pair δ wc typically occurs several times in the corpus, therefore performing SGD over the corpus amounts to weighting equation (2) by the number of occurrence of each pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint Visual and Text Modeling</head><p>We now describe our model, namely PIXIE (Proba- bIlistic teXtual Image Embeddings) illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, for joint learning of word representations from textual and perceptual information.</p><p>Formally, PIXIE is a probabilistic model of image features x and word-context pairs' labels y. Simi- lar to SKIP-GRAM, PIXIE represents each word w and context c with low dimensional embeddings noted respectively e w ∈ R K and v c ∈ R K . PIXIE further assumes latent visual factors, z w ∈ R K , for each word's visual representation x w . Next we de- scribe the two main components of PIXIE, namely textual and perceptual, in more details.</p><p>Perceptual Component Each visual vector x w is drawn conditional on its latent representation z w , i.e., x w ∼ p θ (x|z w ), with p θ (z) = N (0, I). Since x w is real valued, we let p θ (x w |z w ) be a Gaussian parameterized by a generative neural net- work (or decoder). That is,</p><formula xml:id="formula_3">p θ (x w |z w ) = N (x w |µ θ (z w ), Σ θ (z w )). (3)</formula><p>For tractability purposes, Σ θ is restricted to be diagonal. Moreover, both the co-variance Σ θ (z w ) (its diagonal) and the mean µ θ are the outputs of a decoder network with parameters θ and input z w .</p><p>Textual Component To model the occur- rence/absence of word-context pair δ wc in the lin- guistic corpus, we adopt a Bernoulli (Ber) model:</p><formula xml:id="formula_4">p(y wc | e w , v c , z w ) = Ber(σ[f (e w , v c , z w )]) (4)</formula><p>The function f (·) defines how multimodal em- beddings are fused. While many choices can be experimented, we use the simple additive model:</p><formula xml:id="formula_5">f (e w , v c , z w ) = v T c (e w + z w ).<label>(5)</label></formula><p>For words without visual representation, we sim- ply set the corresponding latent factors z w to the zero vector. Note that, without the visual factors z w , equation <ref type="formula">(4)</ref> reduces to the Skip-Gram with negative sampling objective (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Model</head><p>The perceptual and the textual infor- mation interact through the shared latent z w . The joint model of the above two sources of information takes the following form:</p><formula xml:id="formula_6">p(x w , y wc | e w , v c ) = p θ (z w )p θ (x w |z w )p(y wc | e w , v c , z w )dz w (6)</formula><p>The intuition behind our joint formulation is to let the textual information guide the extraction of latent visual factors z w . Through equations <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_5">(5)</ref>, the model will put high probability on fac- tors z w reflecting patterns that can supplement the linguistic embeddings e w in explaining the word- context co-occurrences. Thus, the extracted latent visual factors can contribute to improve the perfor- mance on predicting the occurrence of a word and its contexts in the linguistic corpus, which would encourage the model to leverage the perceptual in- formation. The underlying assumption here is to infer visual and textual embeddings that can work in concert to represent words.</p><p>Visual Information Propagation Equation <ref type="formula" target="#formula_5">(5)</ref> implies that the embeddings z, e and v will af- fect each other during the learning process. Inter- estingly, if a non-visual word w 1 shares a similar context c with a visual word w 2 , then the factor z w 2 will affect e w 1 via v c . In other words, our for- mulation makes it possible to implicitly propagate perceptual information from one word to another through shared contexts. We illustrate this aspect in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Approximate Inference and Learning</head><p>Training PIXIE amounts to inferring the posterior over the visual latent factors, p θ (z|x, y), as well as finding the decoder's parameters θ, the word and context embeddings, e and v, that maximize the likelihood (6). However, as in many complex probabilistic models, the likelihood (due to the integral over z) and the posterior are intractable.</p><p>We therefore resort to approximation techniques. More precisely, we rely on Variational Inference (VI) ( <ref type="bibr" target="#b3">Blei et al., 2017)</ref>. The idea of VI is to intro- duce a tractable approximate posterior distribution q φ (z|x) (the variational distribution) and optimize a lower bound on the likelihood, known as Evi- dence Lower BOund (ELBO). The latter can be written for each word w as follows:</p><formula xml:id="formula_7">L w = E q [log p θ (x w |z w ) + c log p(y wc |e w , v c , z w )] − KL(q φ (z w |x w )p θ (z w ))<label>(7)</label></formula><p>where KL(···) is the Kullback-Leibler divergence. The variational distribution is chosen to be a multi- variate Gaussian parameterized by an inference net- work (or encoder) which takes x as input, namely</p><formula xml:id="formula_8">q φ (z w |x w ) = N (z w |µ φ (x w ), Σ φ (x w )), (8)</formula><p>where we drop the dependency on all y wc variables to be computationally tractable. The pair of en- coder and decoder neural networks gives rise to the interpretation of PIXIE's visual component (formed by z and x) as a probabilistic autoencoder. In fact, if we drop the textual part in PIXIE, namely y, e and v, then we recover the Variational Auto-Encoder (VAE) ( <ref type="bibr" target="#b20">Kingma and Welling, 2013)</ref>. Lastly, we approximate the intractable (i) ex- pectation with respect to q φ (z|x) and the (ii) sum over the negative pairs in <ref type="formula" target="#formula_7">(7)</ref>, by relying on a Monte Carlo estimator of L. Concretely, for (ii) we use negative sampling as in (2). Con- cerning (i), for every observed x w , we sample {z</p><formula xml:id="formula_9">(j) w } J j=1</formula><p>from q φ (z w |x w ) using the reparame- terization trick <ref type="bibr" target="#b20">(Kingma and Welling, 2013)</ref></p><formula xml:id="formula_10">, i.e., z (j) w = µ φ (x w ) + Σ φ (x w ), with ∼ N (0, I).</formula><p>Then we approximate L with:</p><formula xml:id="formula_11">L w = 1 J j log p θ (x w |z (j) w ) − KL(q φ (z w |x w )p θ (z w )) + 1 J c,j y wc log σ[v T c (e w + z (j) w )] + 1 J c,j N i=1 y wc log(σ[−v T c i (e w + z (j) w )]).<label>(9)</label></formula><p>The last two summands correspond to the familiar conditional likelihood term in the SGNS model, augmented with latent visual factors. We optimize the objective (9) via SGD with re- spect to both the encoder/decoder networks param- eters (θ and φ) and the embeddings (e and v). We evaluate the gradients of L, with respect to θ and φ using backpropagation. Similarly, the gradient with respect to e and v can be easily carried out using automatic differentiation tools. Our learning procedure is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Variational PIXIE</head><p>Input: x, y, sample sizes B and J Steps: Randomly initialize θ, φ, e and v repeat</p><p>• Draw a minibatch W B of words:</p><formula xml:id="formula_12">w (1) , . . . , w (B)</formula><p>• For each xw with w ∈ W B , sample {z</p><formula xml:id="formula_13">(j)</formula><p>w } J j=1 from q φ (zw|xw) using the reparameterization trick. For each observed pair δwc draw N negative examples</p><formula xml:id="formula_14">{c i } N i=1 . • Compute the estimator L W B ← w∈W B Lw • Compute the gradient: G ← θ,φ,e,v</formula><p>L W B • Use G to update θ, φ, e and v (e.g., with ADAM) until convergence return θ, φ, e and v</p><p>Inference Once the parameters of the model are learned, for any given word with or without visual representation, we can compute its multimodal em- bedding. As a short hand, let the binary variable m w denote whether or not the word w has a visual representation. The multimodal embedding for w can be written as</p><formula xml:id="formula_15">s w = e w + m w µ(x w ),<label>(10)</label></formula><p>where µ(x w ) = E q (z w ) is the output of the encod- ing neural network, cf. Eq. (8).</p><p>In our experiments, we have also studied an al- ternative way to compose multimodal embeddings by concatenating the two vectors e w and µ(x w )</p><formula xml:id="formula_16">t w = [e w m w µ(x w )].<label>(11)</label></formula><p>Note that for non-visual words, only zeros are ap- pended to e w . One advantage of t over s is that if one uses distances to measure similarity, t can be seen as a simple summation of distances in two different spaces (in terms of e and µ respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Combining language and perception has been re- cently considered in various NLP tasks such as machine translation <ref type="bibr" target="#b6">(Calixto and Liu, 2017)</ref>, visual question generation ( <ref type="bibr" target="#b28">Mostafazadeh et al., 2016)</ref>, image captioning ( <ref type="bibr" target="#b22">Klein et al., 2015)</ref>, etc. In this work, we focus on learning word embeddings from images and texts. Multimodal embeddings have been studied in several recent research work. One strategy is to obtain word embeddings from linguistic data and visual data independently and then proceed with some kind of fusion steps. <ref type="bibr" target="#b18">Kiela and Bottou (2014)</ref> simply concatenates pre-trained linguistic word em- beddings and visual features computed by convo- lutional nets. <ref type="bibr" target="#b4">Bruni et al. (2014)</ref> performs an addi- tional step of dimensionality reduction via singular value decomposition. Silberer et al. (2017) extend on this work by feeding the linguistic embedding and visual features into a stacked auto-encoder for nonlinear dimensionality reduction. The above- mentioned approaches perform a two-stage process to derive multimodal representations (unimodal in- ference followed by fusion) and have been evalu- ated only on words for which both perceptual and textual representations are available.</p><p>A standing question is how to propagate visual information from words with visual features to words lacking them (for instance, abstract words). While the previous methods fall short on that, the recent work by <ref type="bibr" target="#b7">Collell et al. (2017)</ref> addresses this challenge by learning a mapping from language to vision, using a set of words with known linguistic embeddings and visual features. This mapping can then be used to infer visual representations for new words from their textual embeddings.</p><p>All the aforementioned methods rely on inde- pendently pre-trained linguistic embeddings and visual features. In this work, we propose a differ- ent strategy, which consists in adapting those rep- resentations so that the information can be fused in earlier stages. In this respect, the closest work to ours is ( <ref type="bibr" target="#b23">Lazaridou et al., 2015)</ref>, which proposes to augment the SKIP-GRAM objective function with a term mapping the textual embeddings to the vi- sual features. Crudely, the linguistic embeddings must therefore predict both the text co-occurrences and (pre-trained) visual features. We emphasize two key differences with our approach. First, in- stead of performing a regression or mapping from the textual embeddings to the visual features, our model learns to infer perceptual latent factors to retain only the portion of visual information that can supplement the linguistic embeddings in rep- resenting words. Second, while <ref type="bibr" target="#b23">Lazaridou et al. (2015)</ref> combines two objectives, we use a joint probabilistic model integrating both visual and text information in a principled way. Specifically, our model seeks latent factors that are good at explain- ing the word-context co-occurrences. For instance, a visual feature of (an image of) OCEAN often con- tains information about SKY and BLUE -such visual information could be beneficial to predict co- occurrence of tokens in the context of OCEAN. This desiderata further strengthens the learned embed- dings to be visually grounded. In our experiments, we show that our approach tends to group concrete visually similar concepts together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate our model and contrast it to other competing approaches on two different tasks: word similarity and image/caption retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Text corpus We use the Text8 WIKIPEDIA cor- pus 1 containing over 17 million tokens. Text8 was pre-processed to contain only letters and noncon- secutive spaces. After removing infrequent words, we obtain a vocabulary of 50, 000 unique words.</p><p>Image features We use the ImageNet dataset ( <ref type="bibr" target="#b33">Russakovsky et al., 2015)</ref>, including the fall Ima- 1 http://mattmahoney.net/dc/textdata geNet 2011 release <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>. It contains 14, 188, 125 images organized according to 21, 842 synsets of WordNet <ref type="bibr" target="#b10">(Fellbaum, 1998)</ref>. Each synset contains 600 images on average. To extract image features, we rely on the Caffe toolkit ( <ref type="bibr" target="#b16">Jia et al., 2014</ref>) and use the GoogLeNet convolutional neural nets ( <ref type="bibr" target="#b37">Szegedy et al., 2015)</ref> pre-trained on the 1000 synsets of ILSVRC 2012. The 1024-dimensional activation of the pooling units (before the softmax layer) are then taken as our image features.</p><p>Visual representation of words For each word in the vocabulary, we recover all the synsets that it belongs to using the WordNet interface of the NLTK module (Python) ( <ref type="bibr" target="#b2">Bird et al., 2009</ref>). We then remove the synsets not covered by our Im- ageNet dataset. This results in 9,713 words, out of the 50,000 words in the vocabulary. For each visual word, we randomly draw 1,000 distinct im- ages in ImageNet. If the number of images for a word is less than 1,000, we increase the coverage using images belonging to the hypernyms of the considered word's synsets, as in ( <ref type="bibr" target="#b18">Kiela and Bottou, 2014</ref>). We then take the average of these features as the word's visual representation x.</p><p>Hyper-parameter setting For all models, we set the dimension of linguistic and visual embeddings, e and z, to 100, following many previous works. In our model, the encoder/decoder neural networks are implemented as one-hidden-layer neural nets with 500 hidden units each. The dimensions of the inputs and the outputs of the decoder neural networks are 100 and 1, 024 respectively (1, 024 and 100 for the encoder). For the encoder, the hidden units are hyperbolic tangent, and the output units are linear. For the decoder, the hidden units are hyperbolic while the outputs are sigmoid. For SGNS, we set the window size L to 10 and the number of negative samples to 64. Our model is learned by Stochastic Gradient Descent using the ADAM optimizer <ref type="bibr" target="#b19">(Kingma and Ba, 2014</ref>) with a learning rate set to 0.001.  <ref type="table">Table 2</ref>: Results on word similarity task. Reported are the Spearman's rank order correlation between model prediction and human judgment (higher is better and bolds highlight the best methods). See text for details.</p><p>Semantic/taxonomic similarity General relatedness Visual similarity <ref type="table" target="#tab_0">REL+SIM   Models  SEMSIM SimLex  SIM  EN-RG  EN-MC  MEN  REL  MTurk  VISSIM  WORDSIM  100% 98% 100% 39% 100% 44% 100% 72% 100% 73% 100% 54% 100% 53% 100% 26% 100% 98% 100% 39%</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 1: Word Similarity</head><p>Datasets Word similarity is a common type of evaluation task for measuring the effectiveness of word embeddings. To this end, we retain 10 bench- mark datasets consisting of pairs of words associ- ated with similarity scores given by human judges. • VAE: Vatiational Auto-Encoder ( <ref type="bibr" target="#b20">Kingma and Welling, 2013)</ref>, which corresponds to the visual- specific component of PIXIE.</p><p>• CNN: Visual features extracted from a convolu- tional neural net as described in Section 4.1.</p><p>• CNN⊕SGNS ( <ref type="bibr" target="#b18">Kiela and Bottou, 2014)</ref>: Concate- nation of CNN and SKIP-GRAM embeddings.</p><p>• VAE⊕SGNS: Concatenation of VAE and SKIP- GRAM embeddings.</p><p>• V-SGNS ( <ref type="bibr" target="#b23">Lazaridou et al., 2015)</ref>: A multimodal approach which augments SGNS with a term that treats CNN visual features as regression targets.</p><p>Comparisons with V-SGNS will allow us to eval- uate the impact of our modeling assumptions.</p><p>• IV-SGNS ( <ref type="bibr" target="#b7">Collell et al., 2017)</ref>: Learns a mapping from SGNS embeddings to CNN visual features. Due to a large degree of discrepancies in experi- mental setups across previously published methods and results, 2 we re-implemented all the baselines and evaluate them under the same conditions. 3 For ( <ref type="bibr" target="#b23">Lazaridou et al., 2015)</ref>, we implemented its model "A" as model "B" is comparable according to the original authors. For ( <ref type="bibr" target="#b7">Collell et al., 2017)</ref>, we im- plemented both linear and nonlinear variants.</p><p>Evaluation metrics We use the cosine to mea- sure the similarity between word representations. To assess the coherence between human ratings and models' predictions, we use the Spearman correla- tion coefficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Main results</head><p>The results across different datasets are shown in <ref type="table">Table 2</ref>. We perform evaluations under two set- tings: by considering (i) word similarity between visual words only and (ii) between all words (col- umn 100% in <ref type="table">Table 2</ref>). For the models CNN, VAE and their concatenation with SGNS embeddings, the latter setting is not applicable. The two last rows correspond to the multimodal embeddings inferred from our model. In particular, PIXIE + (resp. PIXIE ⊕ ) represents the multimodal embed- dings built using Eq. (10) (resp. Eq. <ref type="formula" target="#formula_0">(11)</ref>).</p><p>Overall, we note that PIXIE ⊕ offers the best per- formance in almost all situations. This provides strong empirical support for the proposed model. Below, we discuss the above results in more depth to better understand them and characterize the cir- cumstances in which our model performs better.</p><p>How relevant is our formulation? Except PIXIE and V-SGNS, most of the multimodal compet- ing methods rely on independently pre-computed linguistic embeddings. As <ref type="table">Table 2</ref> shows, PIXIE and V-SGNS are often the best performing multi- modal models, which provides empirical evidence that accounting for perceptual information while learning word embeddings from text is beneficial. Moreover, the superior performance of PIXIE ⊕ over V-SGNS suggests that our model does a better job at combining perception and language to learn word representations.</p><p>Joint learning is beneficial PIXIE ⊕ outperforms VAE⊕SGNS in almost all cases, which demonstrates the importance of joint learning.</p><p>Where does our approach perform better? On datasets that focus on semantic/taxonomic simi- larity, our approach dominates all other methods.</p><p>On datasets focusing on general relatedness, our approach obtains mixed results. While dominat- ing other approaches on MEN, it tends to perform worst than SGNS on MTurk and REL (under the 100% setting). One possible explanation is that general relatedness tends to focus more on "ex- trapolating" from one word to another word (such as SWAN is related to LAKE), while our approach better models more concrete relationships (such as SWAN is related to GOOSE). The low performance of CNN and VAE confirms this hypothesis.</p><p>On the VISSIM dataset focusing on visual simi- larity, both CNN⊕SGNS and VAE⊕SGNS perform the best, strongly suggesting that visual and linguis- tic data are complementary. Our approach comes very close to these two methods. Note that our learning objective is to jointly explain visual fea- tures and word-context co-occurrences. Thus, two visually similar words, which never occur within the same context, could be mapped into slightly different directions in the latent space.</p><p>Visual Propagation Here we wish to evaluate the ability of our model to propagate perceptual information to words lacking visual features. To <ref type="table">Table 3</ref>: Spearman's score on subsets of visual words. The symbol ( * ) indicates the visual features for the sub- set of 2K words have been ignored when training. Bold highlights the best performing method. Blue color high- lights the best performing model under the ( * ) setting. <ref type="bibr">Semantic</ref>  this end, we randomly select a subset of 2, 000 words for which we have visual features, and we train our model under two different settings: the visual features of the selected 2K words (i) are taken into account (PIXIE ⊕ ), (ii) are ignored, i.e. set to zero (PIXIE ⊕ ( * ) ). We then perform evalu- ations, under the two settings, on the datasets of <ref type="table" target="#tab_0">Table 1</ref> considering only pairs composed of words in the above subset of 2K words. As baselines for this experiment, we consider SGNS and the multi- modal approaches which can propagate perceptual information, namely V-SGNS and IV-SGNS, as well as their outputs when the 2K visual features are ignored (denoted by V-SGNS <ref type="bibr">( * )</ref> , and IV-SGNS <ref type="bibr">( * )</ref> ).</p><p>The results are given in <ref type="table">Table 3</ref>. We observe that PIXIE ⊕ ( * ) outperforms SGNS in almost all cases. Recall that, if we ignore the visual features for all words, PIXIE reduces to SGNS. We can therefore at- tribute the performance improvement of PIXIE ⊕ ( * ) over SGNS to the propagation of visual informa- tion to the subset of 2K words. Compared to mul- timodal methods, PIXIE ⊕ ( * ) (resp. PIXIE ⊕ ) per- forms better than V-SGNS ( * ) (resp. V-SGNS) and IV-SGNS ( * ) (resp. IV-SGNS) in almost situations. This suggests that our formulation allows percep- tual information to propagate better.  <ref type="table">Table 5</ref>: 10 nearest neighboring words to the query words in different embedding spaces generated by different methods. Only "visual" words contain direct visual representations in our dataset. The concreteness score of each query word is reported between parenthesis (see text for details). In <ref type="table" target="#tab_3">Table 4</ref>, we report the cosine similarity be- tween 5 semantically/visually coherent word pairs (from our subset of 2K words). Although the vi- sual vectors of these words were removed during training, the PIXIE's word embeddings of each pair correlate better as compared to their SGNS counter- parts, which provides further support to the propa- gation of visual information under PIXIE. <ref type="table">Table 5</ref> displays several qualitative examples of word similarity. We have selected 4 words: goose, brave, birthstone and savagery. The first two have visual feature representations in our training dataset and the last two do not. Furthermore, for each case, we chose one concrete and one abstract word. <ref type="bibr">4</ref> For each word, we identify their nearest neighbors in the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative analysis</head><p>For the visual words, there is a noticeable differ- ence between our method and others. For instance, for word goose, SGNS expresses more "general" relatedness and returns other animals like pig or shark, while our approach is more specific and tends to give visually similar neighbors by focusing on goose looks-like birds. V-SGNS's result is some- what in between. On the abstract word brave, we observe that PIXIE ⊕ tends to select more explicit embodiments of the adjective brave than SGNS and V-SGNS.</p><p>Moving towards the non-visual words, we do not seem to find a consistent discrepancy pattern between V-SGNS and PIXIE ⊕ , though, as for visual words, both methods seem to select more explicit exemplars compared to SGNS. For instance, for the abstract word savagery, both multimodal ap- proaches suggest cannibals and zombies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Task 2: Image and Caption Retrieval</head><p>We now study the usefulness of the learned word embeddings for the tasks of image and caption retrieval. Our hypothesis is that multimodal word embeddings will perform better for downstream tasks involving multimodal information.</p><p>Experimental setup We use the Flickr30K dataset ( <ref type="bibr" target="#b38">Young et al., 2014</ref>) containing 31,000 im- ages and 155,000 sentences (5 captions per image). The sentences describe the images. The task is to identify the best sentence describing an image or to identify the best image depicting a sentence. We follow the data split setting provided by , in which 1,000 images are used for validation and 1,000 for testing. The rest is used for training.</p><p>The retrieval models compute the proximity be- tween the image features and the sentence em- beddings. For image features, we use the pre- computed features provided by <ref type="bibr" target="#b9">Faghri et al. (2017)</ref>, which are extracted from the FC7 layer of <ref type="bibr">VGG-19 (Simonyan and Zisserman, 2014</ref>). These 4,096- dimensional features are then linearly mapped to 1,024-dimensional features. For sentences, we use an one GRU-layer over the sequences of the word embeddings, resulting in 1,024-dimensional sen- tence embeddings.</p><p>We use a triplet loss to train the retrieval model such that the inner product between the correspond- ing image feature and the sentence is greater than the inner products with incorrect sentences (or im-ages) <ref type="bibr" target="#b21">(Kiros et al., 2014</ref>). The linear mapping and the GRU layer are then optimized to minimize the loss. We use the ADAM optimizer with the learn- ing rate of 0.0002 and divide it by 10 every 15 epochs and we train the model for 30 epochs. Note that we do not fine-tune either the original visual feature or the word embeddings.</p><p>Results <ref type="table" target="#tab_5">Table 6</ref> summarizes the results. The eval- uation metrics are accuracies at top-K (K=1, 5, or 10) retrieved sentences or images. Our model con- sistently outperforms SGNS and other competing multimodal methods, which provides additional support for the benefits of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose PIXIE, a novel probabilistic model join- ing textual and perceptual information to infer mul- timodal word embeddings. In our model, both linguistic and visual latent factors work in concert to explain the co-occurrences of words and their contexts in a corpus. Empirical results show that our model achieves equally competitive or stronger results when compared to state-of-the-art methods for multimodal embeddings.</p><p>Currently our model relies on unsupervised learning to infer visual factors. Explicit knowl- edge of similar and dissimilar visual categories could potentially disentangle latent factors better for alignment with linguistic data. How to incor- porate visual domain knowledge more explicitly into the model would be an interesting direction for future research. While we build on skip-gram, the idea of PIXIE could be extended to other word embedding models, e.g., <ref type="bibr">Glove (Pennington et al., 2014</ref>), ELMO ( <ref type="bibr" target="#b30">Peters et al., 2018)</ref>, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plate representation of our model PIXIE. The model consists of a generative model for visual data and a conditional model for text data. The latent visual factors z and text embedding e jointly predict the wordcontext pair's label y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>:(Kiela and Bottou, 2014), ‡ : (Lazaridou et al., 2015), § : (Collell et al., 2017)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Datasets for the task of word similarity.</head><label>1</label><figDesc></figDesc><table>Datasets 
#word pairs 
MEN (Bruni et al., 2014) 
3000 
EN-MC (Miller and Charles, 1991) 
31 
EN-RG (Rubenstein and Goodenough, 1965) 
65 
SimLex (Hill et al., 2015) 
999 
MTurk (Radinsky et al., 2011; Halawi et al., 2012) 
287 
WORDSIM (Finkelstein et al., 2001) 
350 
REL (Agirre et al., 2009) 
150 
SIM (Agirre et al., 2009) 
200 
SEMSIM (Silberer and Lapata, 2014) 
5494 
VISSIM (Silberer and Lapata, 2014) 
5494 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 summarizes</head><label>1</label><figDesc>their basic properties. There are different types of similarities being assessed. SEMSIM, SimLex, SIM, EN-RG and EN-MC fo- cus on semantic or taxonomic similarity -e.g. CAR is similar to AUTOMOBILE. MEN, REL and MTurk consider general relatedness -e.g. CAR is related to GARAGE. VISSIM is about visual simi- larity -e.g. GOOSE looks like SWAN. Note that SIM and REL are the similarity and relatedness subsets of the full WORDSIM dataset (Finkelstein et al., 2001) respectively. VISSIM contains the same word pairs as SEMSIM.</figDesc><table>Competing models We benchmark our model 
PIXIE against several strong uni-and multi-modal 
models listed below: 
• SGNS: Skip-Gram with Negative Sampling 
(Mikolov et al., 2013). Without the visual com-
ponent, PIXIE reduces to SGNS. We can thus 
assess the impact of the perceptual information 
by comparing PIXIE to SGNS. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Word pair cosine similarity computed based 
on SGNS and PIXIE ⊕ 
( * ) embeddings. 

Word pairs 

SGNS PIXIE⊕ 

( * ) 

(chicken, turkey) 
0.35 
0.55 
(helicopter, jet) 
0.63 
0.76 
(falcon, hawk) 
0.49 
0.70 
(cathedral, chapel) 0.69 
0.80 
(cup, mug) 
0.39 
0.46 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Results for image (I) ↔ sentence (S) retrieval. 

Models 
I → S 
S → I 
K=1 K=5 K=10 K=1 K=5 K=10 

SGNS 

23.1 49.0 61.6 16.6 41.0 53.8 

V-SGNS 

21.9 51.7 64.2 16.2 42.0 54.8 

IV-SGNS (LINEAR) 22.7 50.5 61.7 17.1 42.6 55.4 

PIXIE+ 

24.2 52.5 65.4 17.5 43.8 56.2 

PIXIE⊕ 

25.7 55.7 67.7 18.4 44.9 56.9 

</table></figure>

			<note place="foot" n="2"> For instance, Lazaridou et al. (2015) reports only 5,100 visual words, nearly half of what we have defined. Collell et al. (2017) used pre-trained GloVe word vectors obtained from a different corpus. 3 For each method we use the hyper-parameters recommended by the authors.</note>

			<note place="foot" n="4"> We rely on the concreteness ratings made available by Brysbaert et al. (2014), ranging from 1 to 5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the virtual lab Inria@SiliconValley, ANR Grant GRASP No. ANR-16-CE33-0011-01, and a grant from CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020, NSF IIS-1065243, 1451412, 1513966/ 1632803/1833137, 1208500, CCF-1139148, a Google Research Award, an Alfred P. Sloan Research Fellowship, gifts from Facebook and Netflix, and ARO# W911NF-12-1-0241 and W911NF-15-1-0484. We thank anonymous reviewers for their suggestions and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Marius Pa¸scaPa¸sca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Ressearch</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Concreteness ratings for 40 thousand generally known english word lemmas. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="904" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating global visual features into attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing, (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagined Visual Representations as Multimodal Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Collell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4378" to="4384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05612</idno>
		<title level="m">VSE++: Improving VisualSemantic Embeddings with Hard Negatives</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual information in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining Language and Vision with a Multimodal Skip-gram Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<title level="m">Visually grounded meaning representations. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2284" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Grounded Meaning Representations with Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
