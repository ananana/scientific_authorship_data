<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning the Structure of Variable-Order CRFs: a Finite-State Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIMSI</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Univ. Paris-Sud</orgName>
								<orgName type="institution" key="instit4">Université Paris Saclay Campus Universitaire</orgName>
								<address>
									<postCode>F-91 403</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIMSI</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Univ. Paris-Sud</orgName>
								<orgName type="institution" key="instit4">Université Paris Saclay Campus Universitaire</orgName>
								<address>
									<postCode>F-91 403</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning the Structure of Variable-Order CRFs: a Finite-State Perspective</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="433" to="439"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The computational complexity of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with morphologically rich languages or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies , we propose new ways to perform feature selection at large scale and report experimental results where we outper-form strong baselines on a tagging task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conditional Random Fields (CRFs) ( <ref type="bibr" target="#b9">Lafferty et al., 2001;</ref>) are a method of choice for many sequence labelling tasks such as Part of Speech (PoS) tagging, Text Chunking, or Named Entity Recognition. Linear- chain CRFs are easy to train by solving a convex optimization problem, can accomodate rich fea- ture patterns, and enjoy polynomial exact infer- ence procedures. They also deliver state-of-the-art performance for many tasks, sometimes surpass- ing seq2seq neural models ( <ref type="bibr" target="#b18">Schnober et al., 2016)</ref>.</p><p>A major issue with CRFs is the complexity of training and inference procedures, which are quadratic in the number of possible output la- bels for first order models and grow exponen- tially when higher order dependencies are consid- ered. This is problematic for tasks such as precise PoS tagging for Morphologically Rich Languages (MRLs), where the number of morphosyntactic la- bels is in the thousands <ref type="bibr" target="#b7">(Hajič, 2000;</ref><ref type="bibr" target="#b13">Müller et al., 2013)</ref>. Large label sets also naturally arise when joint labelling tasks (eg. simultaneous PoS tag- ging and text chunking) are considered, For such tasks, processing first-order models is demanding, and full size higher-order models are out of the question. Attempts to overcome this difficulty are based on a greedy approach which starts with first- order dependencies between labels and iteratively increases the scope of dependency patterns under the constraint that a high-order dependency is se- lected only if it extends an existing lower order feature <ref type="bibr">(Müller et al., 2013)</ref>. As a result, fea- ture selection may only choose only few higher- order features, motivating the need for an effec- tive variable-order CRF (voCRF) training proce- dure ( <ref type="bibr" target="#b28">Ye et al., 2009)</ref>. <ref type="bibr">1</ref> The latest implementation of this idea ( <ref type="bibr" target="#b27">Vieira et al., 2016</ref>) relies on (struc- tured) sparsity promoting regularization <ref type="bibr" target="#b11">(Martins et al., 2011</ref>) and on finite-state techniques, han- dling high-order features at a small extra cost (see § 2). In this approach, the sparse set of label de- pendency patterns is represented in a finite-state automaton, which arises as the result of the fea- ture selection process. In this paper, we somehow reverse the perspec- tive and consider VoCRF training mostly as an au- tomaton inference problem. This leads us to con- sider alternative techniques for learning the finite- state machine representing the dependency struc- ture of sparse VoCRFs (see § 3). Two lines of enquiries are explored: (a) to take into account the internal structure of large tag sets in order to learn better and/or leaner feature sets; (b) to de- tect unconditional structural dependencies in label sequences in order to speed-up the discovery of useful features. These ideas are implemented in 6 feature selection strategies, allowing us to explore a large set of dependency structures. Relying on lazy finite-state operations, we train VoCRFs up to order 5, and achieve PoS tagging performance that surpass strong baselines for two MRLs (see § 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variable order CRFs</head><p>In this section, we recall the basics of CRFs and VoCRFs and introduce some notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basics</head><p>First-order CRFs use the following model:</p><formula xml:id="formula_0">p θ (y|x) = Z θ (x) −1 exp(θ T F (x, y)) (1)</formula><p>where x = (x 1 , . . . , x T ) and y = (y 1 , . . . , y T ) are the input (in X T ) and output (in Y T ) sequences and Z θ (x) is a normalizer. Each component F j (x, y) of the global feature vector decomposes as a sum of local features T t=1 f j (y t−1 , y t , x t ) and is associated to parameter θ j . Local features typically use binary tests and take the form:</p><formula xml:id="formula_1">f u,g (y t−1 , y t , x, t) = I(y t = u ∧ g(x, t)) f uv,g (y t−1 , y t , x, t) = I(y t−1 y t = uv ∧ g(x, t))</formula><p>where I() is an indicator function and g() tests a local property of x around x t . In this setting, the number of parameters is |Y| 2 × |X | train , where |A| is the cardinality of A and |X | train is the number of values of g(x, t) observed in the training set. Even in moderate size applications, the parameter set can be very large and contain dozen of millions of features, due to the introduction of sequential dependencies in the model.</p><formula xml:id="formula_2">Given N i.i.d. sequences {x (i) , y (i) } N i=1</formula><p>, esti- mation is based on the minimization of the negated conditional log-likelihood l(θ). Optimizing this objective requires to compute its gradient and to repeatedly evaluate the conditional expectation of the feature vector. This can be done using a forward-backward algorithm having a complexity that grows quadratically with |Y|. l(θ) is usu- ally complemented with a regularization term so as to avoid overfitting and stabilize the optimiza- tion. Common regularizers use the 1 -or the 2 - norm of the parameter vector, the former having the benefit to promote sparsity, thereby perform- ing automatic feature selection <ref type="bibr" target="#b25">(Tibshirani, 1996)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variable order CRFs (VoCRFs)</head><p>When the label set is large, many pairs of labels never occur in the training data and the sparsity of label ngrams quickly increases with the order p of the model. In the variable order CRF model, it is assumed that only a small number of ngrams</p><formula xml:id="formula_3">Algorithm 1: Building A[W] W : list of patterns, A[W] initially empty U = Pref(W) foreach w ∈ W do TrieInsert(w, A[W]) // Add missing transitions foreach u = vy ∈ U do new FailureTrans(u, LgSuff(v, U))</formula><p>(out of |Y| p ) are associated with a non-zero param- eter value. Denoting W the set of such ngrams and w ∈ W, a generic feature function is then</p><formula xml:id="formula_4">f w,g (w, x, t) = I(y t−s . . . y t = w ∧ g(x, t)).</formula><p>In (order-p) VoCRFs, the computational cost of training and inference is proportional to the size of a finite-state automaton A[W] encoding the pat- terns in W, 2 which can be much less than |Y| p . Our procedure for building A <ref type="bibr">[W]</ref> is sketched in Algorithm 1, where TrieInsert inserts a string in a trie, Pref(W) computes the set of prefixes of the strings in W, 3 LgSuff(v, U) returns the longest suffix of v in U, and FailureTrans is a special ε-transition used only when no la- belled transition exists ( <ref type="bibr" target="#b0">Allauzen et al., 2003</ref>). <ref type="bibr">4</ref> Each state (or pattern prefix) v in A <ref type="bibr">[W]</ref> is asso- ciated with a set of feature functions {f u,g , ∀u ∈ Suff(v), g}. <ref type="bibr">5</ref> The forward step of the gradient computation maintains one value α(v, t) per state and time step, which is recursively accumulated over all paths ending in v at time t.</p><p>The next question is to identify W. The sim- plest method keeps all the ngrams viewed in train- ing, additionally filtering rare patterns ( <ref type="bibr" target="#b5">Cuong et al., 2014</ref>). However, frequency based feature se- lection does not take interactions into account and is not the best solution. Ideally, one would like to train a complete order-p model with a sparsity promoting penalty, a technique that only works for small label sets. <ref type="bibr">6</ref> The greedy algorithm of <ref type="bibr" target="#b17">Schmidt and Murphy (2010)</ref>; <ref type="bibr" target="#b27">Vieira et al. (2016)</ref> is more scalable: it starts with all unigram patterns and iteratively grows W by extending the ngrams that have been selected in the simpler model. At each round of training, feature selection is per- formed using a 1 penalty and identifies the pat- terns that will be further augmented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning patterns</head><p>We introduce now several alternatives for learning W. Our motivation for doing so is twofold: (a) to take the internal structure of large label sets into account; (b) to identify more abstract patterns in label sequences, possibly containing gaps or iter- ations, which could yield smaller A <ref type="bibr">[W]</ref>. As dis- cussed below, both motivations can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Greedy 1</head><p>The greedy strategy iteratively grows patterns up to order p. Considering all possible unigram and bigram patterns, we train a sparse model to select a first set of useful bigrams. In subsequent iter- ations, each pattern w selected at order k is ex- tended in all possible ways to specify the pattern set at order k + 1, which will be filtered during the next training round. This approach is close, yet simpler, than the group lasso approach of <ref type="bibr" target="#b27">Vieira et al. (2016)</ref> and experimentally yields slightly smaller pattern sets (see <ref type="table" target="#tab_2">Table 2</ref>). This is because we do not enforce closure under last-character re- placement: once pattern w is pruned, longer pat- terns ending in w are never considered. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Component-wise training</head><p>Large tag sets often occur in joint tasks, where multiple levels of information are encoded in one compound tag. For instance, the fine grain labels in the Tiger corpus ( <ref type="bibr">Brants et al., 2002</ref>) combine PoS and morphological information in tags such as NN.Dat.Sg.Fem for a feminine singular da- tive noun. In the sequel, we refer to each piece of information as a tag component. We assume that all tags contain the same components, using a "non-applicable" value whenever needed. Us- ing features that test arbitrary combinations of tag components would make feature selection much more difficult, as the number of possible patterns grows combinatorially with the number of compo- nents. We keep things simple by allowing features to only evaluate one single component at a time: <ref type="bibr">7</ref> cf. the discussion in ( <ref type="bibr">Vieira et al., 2016, § 4).</ref> this allows us to identify dependencies of different orders for each component.</p><p>Assuming that each tag y contains K compo- nents y = [z 1 , z 2 . . . , z K ], with z k ∈ Y k , W is then computed as in § 3.1, except that we now con- sider one distinct set of patterns W k for each com- ponent k. At each training round, each set W k is extended and pruned independently from the oth- ers. Note that all these automata are trained simul- taneously using a common set of features. This process results in K automata, which are inter- sected on the fly 8 using "lazy" composition. In our experiments, we also consider the case where we additionally combine the automaton represent- ing complete tag sequences: this has the benefi- cial effect to restrict the combinations of subtags to values that actually exist in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pruned language models</head><p>Another approach for computing W assumes that useful dependencies between tags can be iden- tified using an auxiliary language model (LM) trained without paying any attention to observa- tion sequences. A pattern w will then be deemed useful for the labelling task only if w is a useful history in a LM of tag sequences. This strategy was implemented by first training a compact p- gram LM with entropy pruning 9 <ref type="bibr" target="#b22">(Stolcke, 1998)</ref> and including all the surviving histories in W. In a second step, we train the complete CRF as usual, with all observation features and 1 penalty to fur- ther prune the parameter set. and is labelled with completely specified tags. <ref type="bibr">9</ref> Starting with a full back-off n-gram language model, this approach discards n-grams if their removal causes a suffi- ciently small drop in cross-entropy. We used the implemen- tation of <ref type="bibr" target="#b23">Stolcke (2002).</ref> (MELMs) <ref type="bibr" target="#b16">(Rosenfeld, 1996)</ref>. MELMs decom- pose the probabililty of a sequence y 1 . . . y T using the chain rule, where each term p λ (y t |y &lt;t ) is a lo- cally normalized exponential model including all possible ngram features up to order p:</p><formula xml:id="formula_5">p(y t |y &lt;t ; λ) = Z(λ) −1 exp λ T G(y 1 . . . y t )</formula><p>In contrast to globally normalized models, the complexity of training remains linear wrt. |Y|, ir- respective of p. It it also straightforward both to (a) use a 1 penalty to perform feature selection; (b) include features that only test specific compo- nents of a complex tag. For an order p model, our feature functions evaluate all n-grams (for n ≤ p) of complete tags or of one specific component:</p><formula xml:id="formula_6">G w (y 1 , . . . , y t ) =I(y t−n+1 . . . y t = w) G u (y 1 , . . . , y t ) =I(z k,t−n+1 . . . z k,t = u)</formula><p>Once a first round of feature selection has been performed, <ref type="bibr">10</ref> we compute A[W] as explained above. The last step of training reintroduces the observations and estimates the CRF paramaters. A variant of this approach adds extra gappy features to the n-gram features. Gappy features at order p test whether some label u occurs in the remote past anywhere between position t − p + 1 11 and t − n. They take the following form:</p><p>G w,u (y 1 , . . . , y t ) =I(y t−n+1 . . . y t = w∧ u ∈ {y t−p+1 . . . y t−n }),</p><p>and likewise for features testing components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training protocol</head><p>The following protocol is used throughout: (a) identify W ( §3) -note that this may imply to tune a regularization parameter; (b) train a full model (in- cluding tests on the observations for each pattern in W) using 1 regularization and a very small 2 term to stabilize convergence. The best regulariza- tion in (a) and (b) is selected on development data and targets either perplexity (for LMs) or label ac- curacy (for CRFs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Features</head><p>Experiments are run on two MRLs: for Czech, we use the CoNLL 2009 data set <ref type="bibr" target="#b8">(Hajič et al., 2009)</ref> and for German, the Tiger Treebank with the split of <ref type="bibr" target="#b6">Fraser et al. (2013)</ref>). Both datasets include rich morphological attributes (cf. <ref type="table">Table 1)</ref>.</p><p>All the patterns in W are combined with lexical features testing the current word x t , its prefixes and suffixes of length 1 to 4, its capitalization and the presence of digit or punctuation symbols. Ad- ditional contextual features also test words in a lo- cal window around position t. These tests greatly increase the feature count and are not provided for all label patterns: for unigram patterns, we test the presence of all unigrams and bigrams of words in a window of 5 words; for bigrams patterns we only test for all unigrams in a window of 3 words. Con- textual features are not used for larger patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We consider several baselines: Maxent and MEMM models, neither of which considers la- bel dependencies in training, a linear chain CRF 12 and our own implementation of the group lasso of <ref type="bibr" target="#b27">Vieira et al. (2016)</ref>. For the latter, we contrast two setups: one where each pattern in W gives rise to one single feature, and one where it is conjoined with tests on the observation. <ref type="bibr">13</ref> All scores in Ta- ble 2 are label accuracies on unseen test data.</p><p>As expected, Maxent and MEMM are outper- formed by almost all variants of CRFs, and their scores are only reported for completeness. Group lasso results demonstrate the effectiveness of using contextual information with high order fea- tures: the gain is ≈ 0.7 points for both languages and all values of p. Greedy 1 achieves accu- racy results similar to group lasso, suggest- ing that 1 penalty alone is effective to select high- order features. It also yields slighly smaller mod- els and very comparable training time across the board: indeed, greedy parameter selection strate- gies imply multiple rounds of training which are overall quite costly, due to the size of the full la- bel set. Testing individual subtags ( § 3.2) results in a slight improvement (≈+0.3) in accuracy over Greedy 1 . When using an additional automata for the full tag, we get a larger gain of ≈ 0.6 points for Czech, slightly less for German: including a model for complete tags also prevents to gener- 12 Using the implementation of <ref type="bibr" target="#b10">Lavergne et al. (2010)</ref>. <ref type="bibr">13</ref> As suggested by the authors themselves in fn 4. cz de p = 2 p = 3 p = 4 p = 5 p = 2 p = 3 p = 4 p = 5 90.01% 91.12% 91.17% 91.14% 85.62% 85.84% 85.96% 86. <ref type="table" target="#tab_2">02%  Maxent  1924  1924  1924  1924  781  781  781  781  191min  219min  286min  349min  142min  193min  252min  297min  90.96%</ref> 92.09% 92.13% 92.12% 86.48% 86.88% 87.13% 87. <ref type="table" target="#tab_2">19%  MEMM  1924  1924  1924  1924  781  781  781  781  191min  219min  286min  349min  142min  193min  252min  297min  91.93%</ref> 86.95%  ate invalid combinations of subtags. These models represent different tradeoffs between accuracy and training time: the 4-gram Component-wise ex- periment only took 14 hrs to complete on German data and outperforms the corresponding Greedy 1 setup while containing approximately 100 times less features. Component-wise+Full is more comparable in size and training time to Greedy 1 , but yields a larger improvement in perfor- mance. The last sets of experiments with LMs yields even better operating points, as the first stage of pattern selection is performed with a cheap model. They are our best trade-off to date, yielding the best performance for all values of p.</p><formula xml:id="formula_7">Linear Chain CRF 3.7e6 - - - 6.1e5 - - -<label>657min</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have explored ways to take advan- tage of the flexibility offered by implementations of VoCRFs based on finite-state techniques. We have proposed strategies to include tests on sub- parts of complex tags, as well as to select useful label patterns with auxiliary unconditional LMs. Experiments with two MRLs with large tagsets yielded consistent improvements (≈ +0.8 points) over strong baselines. They offer new perspectives to perform feature selection in high order CRFs.</p><p>In our future work, we intend to also explore how to complement 1 penalties with terms penalizing more explicitely the processing time; we also wish to study how these ideas can be used in combina- tion with neural models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results. Each cell reports accuracy, number of states in A[W] and total training 
time. Group lasso is our reimplementation of Vieira et al. (2016) (+Ctx = +context features) ; Greedy 
1 is described in section 3.1, Component-wise is the decomposition approach of  § 3.2, PrunedLM and 
MELM (+Gaps) were described in  § 3.3 and  § 3.4. 

</table></figure>

			<note place="foot" n="1"> This is reminiscent of variable order HMMs, introduced eg. in (Schütze and Singer, 1994; Ron et al., 1996).</note>

			<note place="foot" n="2"> More precisely, Vieira et al. (2016) consider W, the closure of W under suffix and last character substitution, which factors as W = H × Y. The complexity of training depends on the size of the finite-state automaton representing W. 3 A trie has one state for each prefix. 4 This was also suggested by Cotterell and Eisner (2015) as a way to build a more compact pattern automaton. 5 Upon reaching a state v, we need to access the features that fire for that pattern, and also for all its suffixes. Each state thus stores a set of pattern; each pattern is associated with a set of tests on the observation (cf. 2.1). 6 Recall that the size of parameter set is exponential wrt. the model order.</note>

			<note place="foot" n="10"> As the LM building step only look at labels, we tune the regularization to optimize the perplexity of the LM on a development set. 11 We use p = 6 in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to thank the reviewers for their useful comments and suggestions. This work has been partly funded by the European Union's Hori-zon 2020 research and innovation programme un-der grant agreement No. 645452 (QT21).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized algorithms for constructing statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<idno type="doi">10.3115/1075096.1075102</idno>
		<ptr target="https://doi.org/10.3115/1075096.1075102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wolfgang Lezius, and George Smith. 2002. The TIGER treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on treebanks and linguistic theories</title>
		<meeting>the workshop on treebanks and linguistic theories</meeting>
		<imprint>
			<biblScope unit="page" from="24" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Penalized expectation propagation for graphical models over strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno type="doi">10.3115/v1/N15-1094</idno>
		<ptr target="https://doi.org/10.3115/v1/N15-1094" />
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="932" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Viet Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chieu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/cuong14a.html" />
	</analytic>
	<monogr>
		<title level="m">Conditional Random Field with High-order Dependencies for Sequence Labeling and Segmentation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="981" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge sources for constituent parsing of german, a morphologically rich and less-configurational language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CL</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="85" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Morphological tagging: Data vs. dictionaries</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference</title>
		<meeting>the 1st North American chapter of the Association for Computational Linguistics conference<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task. CoNLL &apos;09</title>
		<editor>Štěpánek, Pavel Straňák, Mihai Surdeanu, Nianwen Xue, and Yi Zhang</editor>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task. CoNLL &apos;09</meeting>
		<imprint>
			<date type="published" when="2009-01" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning<address><addrLine>San Francisco, CA, Williamstown, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical very large scale CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lavergne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Uppsala, Sweden</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured sparsity in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Aguiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1500" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint lemmatization and morphological tagging with lemming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse forward-backward using minimum divergence beams for fast training of conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="doi">10.1109/ICASSP.2006.1661342</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2006.1661342" />
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The power of amnesia: Learning probabilistic automata with variable memory length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="117" to="149" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to adaptive statistical learning modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="187" to="228" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convex structure learning in log-linear models: Beyond pairwise potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy, AISTATS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="709" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Still not there? comparing traditional sequence-to-sequence models to encoderdecoder neural networks on monotone string translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Schnober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik-Lân Do</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1703" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Part-ofspeech tagging using a variable memory Markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Las Cruces, New Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="181" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exact decoding for jointly labeling and chunking sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuyuki</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING/ACL</title>
		<meeting>COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="763" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context-based morphological disambiguation with random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">W</forename><surname>Tromble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Entropy-based pruning of backoff language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>DARPA Broadcast News Transcription and Understanding Workshop<address><addrLine>Lansdowne, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="270" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Spoken Langage Processing (ICSLP)</title>
		<meeting>the International Conference on Spoken Langage Processing (ICSLP)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An introduction to conditional random fields for relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Introduction to Statistical Relational Learning</title>
		<editor>Lise Getoor and Ben Taskar</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A global model for joint lemmatization and part-of-speech prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P09-1055" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="486" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speed-accuracy tradeoffs in tagging with variableorder crfs and structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1973" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Conditional random fields with high-order features for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/3815-conditional-random-fields-with-high-order-features-for-sequence-labeling.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Bengio, D. Schuurmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2196" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
