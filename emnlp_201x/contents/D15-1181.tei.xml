<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
							<email>huah@cs.umd.edu, kgimpel@ttic.edu, jimmylin@uwaterloo.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression. To cope with these challenges, we propose a model for comparing sentences that uses a multiplicity of perspectives. We first model each sentence using a convolutional neural network that extracts features at multiple levels of gran-ularity and uses multiple types of pooling. We then compare our sentence representations at several granularities using multiple similarity metrics. We apply our model to three tasks, including the Microsoft Research paraphrase identification task and two SemEval semantic textual similarity tasks. We obtain strong performance on all tasks, rivaling or exceeding the state of the art without using external resources such as WordNet or parsers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Measuring the semantic relatedness of two pieces of text is a fundamental problem in language processing tasks like plagiarism detection, query ranking, and question answering. In this paper, we address the sentence similarity measurement prob- lem: given a query sentence S 1 and a comparison sentence S 2 , the task is to compute their similar- ity in terms of a score sim(S 1 , S 2 ). This simi- larity score can be used within a system that de- termines whether two sentences are paraphrases, e.g., by comparing it to a threshold.</p><p>Measuring sentence similarity is challenging because of the variability of linguistic expression and the limited amount of annotated training data. This makes it difficult to use sparse, hand-crafted features as in conventional approaches in NLP. Re- cent successes in sentence similarity have been ob- tained by using neural networks ( <ref type="bibr" target="#b37">Tai et al., 2015;</ref><ref type="bibr" target="#b43">Yin and Schütze, 2015)</ref>. Our approach is also based on neural networks: we propose a modular functional architecture with two components, sen- tence modeling and similarity measurement.</p><p>For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as part- of-speech tag embeddings (Sec. 4). For similar- ity measurement, we compare pairs of local re- gions of the sentence representations, using multi- ple distance functions: cosine distance, Euclidean distance, and element-wise difference <ref type="bibr">(Sec. 5)</ref>.</p><p>We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks <ref type="bibr" target="#b0">(Agirre et al., 2012;</ref><ref type="bibr" target="#b29">Marelli et al., 2014)</ref>, and highly com- petitive performance on the Microsoft Research paraphrase (MSRP) identification task ( <ref type="bibr" target="#b10">Dolan et al., 2004</ref>). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long Short- Term Memory (LSTM) neural networks of <ref type="bibr" target="#b37">Tai et al. (2015)</ref> without using parsers or part-of- speech taggers. On the MSRP task, we outper- form the recently-proposed convolutional neural network model of <ref type="bibr" target="#b43">Yin and Schütze (2015)</ref> with- out any pretraining. In addition, we perform ab- lation experiments to show the contribution of our modeling decisions for all three datasets, demon- strating clear benefits from our use of multiple per- spectives both in sentence modeling and structured similarity measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most previous work on modeling sentence simi- larity has focused on feature engineering. Sev- eral types of sparse features have been found use- ful, including: (1) string-based, including n-gram overlap features on both the word and character levels ( <ref type="bibr" target="#b38">Wan et al., 2006</ref>) and features based on machine translation evaluation metrics <ref type="bibr" target="#b26">(Madnani et al., 2012)</ref>; (2) knowledge-based, using exter- nal lexical resources such as WordNet <ref type="bibr" target="#b11">(Fellbaum, 1998;</ref><ref type="bibr" target="#b12">Fern and Stevenson, 2008)</ref>; (3) syntax- based, e.g., modeling divergence of dependency syntax between the two sentences ( <ref type="bibr" target="#b9">Das and Smith, 2009)</ref>; (4) corpus-based, using distributional mod- els such as latent semantic analysis to obtain fea- tures <ref type="bibr" target="#b17">(Hassan, 2011;</ref><ref type="bibr" target="#b16">Guo and Diab, 2012</ref>).</p><p>Several strongly-performing approaches used system combination ( <ref type="bibr" target="#b9">Das and Smith, 2009;</ref><ref type="bibr" target="#b26">Madnani et al., 2012)</ref> or multi-task learning. <ref type="bibr" target="#b42">Xu et al. (2014)</ref> developed a feature-rich multi-instance learning model that jointly learns paraphrase rela- tions between word and sentence pairs. Recent work has moved away from hand- crafted features and towards modeling with dis- tributed representations and neural network archi- tectures. <ref type="bibr" target="#b8">Collobert and Weston (2008)</ref> used con- volutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. <ref type="bibr" target="#b22">Kalchbrenner et al. (2014)</ref> introduced a convolutional neural net- work for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. <ref type="bibr" target="#b24">Kim (2014)</ref> proposed several modifications to the convolutional neural network architecture of <ref type="bibr" target="#b8">Collobert and Weston (2008)</ref>, including the use of both fixed and learned word vectors and varying window sizes of the convolution filters.</p><p>For the MSRP task, <ref type="bibr" target="#b35">Socher et al. (2011)</ref> used a recursive neural network to model each sen- tence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. <ref type="bibr" target="#b20">Ji and Eisenstein (2013)</ref> used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features us- ing an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incor- porated sparse features to improve performance, which we do not use in this work. <ref type="bibr" target="#b18">Hu et al. (2014)</ref> used convolutional neural net- works that combine hierarchical sentence mod- eling with layer-by-layer composition and pool- ing. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks <ref type="bibr" target="#b40">(Weston et al., 2011;</ref><ref type="bibr" target="#b19">Huang et al., 2013;</ref><ref type="bibr" target="#b1">Andrew et al., 2013;</ref><ref type="bibr" target="#b7">Bromley et al., 1993</ref>).</p><p>Most recently, <ref type="bibr" target="#b37">Tai et al. (2015)</ref> and <ref type="bibr" target="#b45">Zhu et al. (2015)</ref> concurrently proposed a tree-based LSTM neural network architecture for sentence model- ing. Unlike them, we do not use syntactic parsers, yet our performance matches <ref type="bibr" target="#b37">Tai et al. (2015)</ref> on the similarity task. This result is appealing because high-quality parsers are difficult to ob- tain for low-resource languages or specialized do- mains. <ref type="bibr" target="#b43">Yin and Schütze (2015)</ref> concurrently de- veloped a convolutional neural network architec- ture for paraphrase identification, which we com- pare to in our experiments. Their best results rely on an unsupervised pretraining step, which we do not need to match their performance.</p><p>Our model architecture differs from previous work in several ways. We exploit multiple per- spectives of input sentences in order to maxi- mize information utilization and perform struc- tured comparisons over particular regions of the sentence representations. We now proceed to de- scribe our model in detail, and we compare to the above related work in our experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Overview</head><p>Modeling textual similarity is complicated by the ambiguity and variability of linguistic expression. We designed a model with these phenomena in mind, exploiting multiple types of input which are processed by multiple types of convolution and pooling. Our similarity architecture likewise uses multiple similarity functions.</p><p>To summarize, our model (shown in <ref type="figure">Figure 1</ref>) consists of two main components:</p><p>1. A sentence model for converting a sentence into a representation for similarity measure- ment; we use a convolutional neural network architecture with multiple types of convolution and pooling in order to capture different granu- larities of information in the inputs.</p><p>2. A similarity measurement layer using multi- ple similarity measurements, which compare lo- cal regions of the sentence representations from the sentence model.</p><p>Our model has a "Siamese" structure ( <ref type="bibr" target="#b7">Bromley et al., 1993</ref>) with two subnetworks each process- ing a sentence in parallel. The subnetworks share all of their weights, and are joined by the simi- larity measurement layer, then followed by a fully connected layer for similarity score output.  <ref type="figure">Figure 1</ref>: Model overview. Two input sentences (on the bottom) are processed in parallel by iden- tical neural networks, outputting sentence repre- sentations. The sentence representations are com- pared by the structured similarity measurement layer. The similarity features are then passed to a fully-connected layer for computing the similarity score (top).</p><p>Importantly, we do not require resources like WordNet or syntactic parsers for the language of interest; we only use optional part-of-speech tags and pretrained word embeddings. The main dif- ference from prior work lies in our use of multiple types of convolution, pooling, and structured sim- ilarity measurement over local regions. We show later in our experiments that the bulk of our perfor- mance comes from this use of multiple "perspec- tives" of the input sentences.</p><p>We describe our sentence model in Section 4 and our similarity measurement layer in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sentence Modeling</head><p>In this section we describe our convolutional neu- ral network for modeling each sentence. We use two types of convolution filters defined on differ- ent perspectives of the input (Sec. 4.1), and also use multiple types of pooling (Sec. 4.2).</p><p>Our inputs are streams of tokens, which can be interpreted as a temporal sequence where nearby words are likely to be correlated. Let sent ∈ R len×Dim be a sequence of len input words rep- resented by Dim-dimensional word embeddings, where sent i ∈ R Dim is the embedding of the i-th word in the sequence and sent i:j represents the concatenation of embeddings from word i up to and including word j. We denote the k-th dimen- sion of the i-th word vector by sent <ref type="bibr">[k]</ref> i and we de- note the vector containing the k-th dimension of words i to j by sent <ref type="bibr">[k]</ref> i:j . w 1 w 2 w 3 w 4 w 5 w 1 w 2 w 3 w 4 w 5 <ref type="figure">Figure 2</ref>: Left: a holistic filter matches entire word vectors (here, ws = 2). Right: per-dimension fil- ters match against each dimension of the word em- beddings independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convolution on Multiple Perspectives</head><p>We define a convolution filter F as a tuple</p><formula xml:id="formula_0">ws, w F , b F , h F ,</formula><p>where ws is the sliding window width, w F ∈ R ws×Dim is the weight vector for the filter, b F ∈ R is the bias, and h F is the activa- tion function (a nonlinear function such as tanh).</p><p>When filter F is applied to sequence sent, the inner product is computed between w F and each possible window of word embeddings of length ws in sent, then the bias is added and the activa- tion function is applied. This results in an output vector out F ∈ R 1+len−ws where entry i equals</p><formula xml:id="formula_1">out F [i] = h F (w F · sent i:i+ws−1 + b F ) (1)</formula><p>where i ∈ [1, 1 + len − ws]. This filter can be viewed as performing "temporal" convolution, as it matches against regions of the word sequence. Since these filters consider the entirety of each word embedding at each position, we call them holistic filters; see the left half of <ref type="figure">Figure 2</ref>. In addition, we target information at a finer granularity by constructing per-dimension filters F <ref type="bibr">[k]</ref> for each dimension k of the word embed- dings, where w F <ref type="bibr">[k]</ref> ∈ R ws . See the right half of <ref type="figure">Figure 2</ref>. The per-dimension filters are simi- lar to "spatial convolution" filters except that we limit each to a single, predefined dimension. We include separate per-dimension filters for each di- mension of the input word embeddings.</p><formula xml:id="formula_2">Applying a per-dimension filter F [k] = ws, w F [k] , b F [k] , h F [k] for dimension k re- sults in an output vector out F [k] ∈ R 1+len−ws where entry i (for i ∈ [1, 1 + len − ws]) equals out F [k] [i] = h F [k] (w F [k] · sent [k] i:i+ws−1 + b F [k] )</formula><p>Our use of word embeddings in both ways allows more information to be extracted for richer sen- tence modeling. While we typically do not expect individual dimensions of neural word embeddings  We define a convolution layer as a set of con- volution filters that share the same type (holistic or per-dimension), activation function, and width ws. The type, width, activation function, and num- ber of filters numFilter in the layer are chosen by the modeler and the weights of each filter (w F and b F ) are learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multiple Pooling Types</head><p>The output vector out F of a convolution filter F is typically converted to a scalar for subsequent use by the model using some method of pooling. For example, "max-pooling" applies a max operation across the entries of out F and returns the max- imum value. In this paper, we experiment with two additional types of pooling: "min-pooling" and "mean-pooling".</p><p>A group, denoted group(ws, pooling, sent), is an object that contains a convolution layer with width ws, uses pooling function pooling, and op- erates on sentence sent. We define a building block to be a set of groups. We use two types of building blocks, block A and block B , as shown in <ref type="figure">Figure 3</ref>. We define block A as {group A (ws a , p, sent) : p ∈ {max, min, mean}}.</p><p>That is, an instance of block A has three convolu- tion layers, one corresponding to each of the three pooling functions; all have the same window size ws a . An alternative choice would be to use the multiple types of pooling on the same filters <ref type="bibr" target="#b32">(Rennie et al., 2014</ref>); we instead use independent sets of filters for the different pooling types. <ref type="bibr">1</ref> We use blocks of type A for all holistic convolution layers.</p><p>We define block B as {group B (ws b , p, sent) : p ∈ {max, min}}.</p><p>That is, block B contains two groups of convolu- tion layers of width ws b , one with max-pooling and one with min-pooling. Each group B ( * ) con- tains a convolution layer with Dim per-dimension convolution filters. That is, we use blocks of type B for convolution layers that operate on individual dimensions of word vectors.</p><p>We use these multiple types of pooling to ex- tract different types of information from each type of filter. The design of each group( * ) allows a pooling function to interact with its own underly- ing convolution layers independently, so each con- volution layer can learn to recognize distinct phe- nomena of the input for richer sentence modeling.</p><p>For a group A (ws a , pooling a , sent) with a con- volution layer with numFilter A filters, we define the output oG A as a vector of length numFilter A where entry j is</p><formula xml:id="formula_3">oG A [j] = pooling a (out F j )<label>(2)</label></formula><p>where filters are indexed as F j . That is, the output of group A ( * ) is a numFilter A -length vector con- taining the output of applying the pooling function on each filter's vector of filter match outputs. <ref type="bibr">2</ref> A component group B ( * ) of block B contains Dim filters, each operating on a particular di- mension of the word embeddings. We define the output oG B of group B (ws b , pooling b , sent) as a Dim × numFilter B matrix where entry</p><formula xml:id="formula_4">[k][j] is oG B [k][j] = pooling b (out F [k ] j )</formula><p>where filter F <ref type="bibr">[k]</ref> j is filter j for dimension k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multiple Window Sizes</head><p>Similar to traditional n-gram-based models, we use multiple window sizes ws in our building blocks in order to learn features of different lengths. For example, in <ref type="figure">Figure 4</ref> we use four building blocks, each with one window size ws = <ref type="figure">Figure 4</ref>: Example neural network architecture for a single sentence, containing 3 instances of block A (with 3 types of pooling) and 2 instances of block B (with 2 types) on varying window sizes ws = 1, 2 and ws = ∞; block A operates on entire word vec- tors while block B contains filters that operate on individual dimensions independently.</p><note type="other">Cats Sit On The Mat Window Size Building Block A ws = 1 Window Size ws = 1 Window Size ws = 2 Window Size ws = 2 Building Block B Window Size ws = ∞</note><p>1 or 2 for its own convolution layers. In order to retain the original information in the sentences, we also include the entire matrix of word embeddings in the sentence, which essentially corresponds to ws = ∞. The width ws represents how many words are matched by a filter, so using larger values of ws corresponds to matching longer n-grams in the input sentences. The ranges of ws values and the numbers of filters numFilter of block A and block B are empirical choices tuned based on vali- dation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Similarity Measurement Layer</head><p>In this section we describe the second part of our model, the similarity measurement layer.</p><p>Given two input sentences, the first part of our model computes sentence representations for each of them in parallel. One straightforward way to compare them is to flatten the sentence represen- tations into two vectors, then use standard met- rics like cosine similarity. However, this may not be optimal because different regions of the flattened sentence representations are from differ- ent underlying sources (e.g., groups of different widths, types of pooling, dimensions of word vec- tors, etc.). Flattening might discard useful com- positional information for computing similarity. We therefore perform structured comparisons over particular regions of the sentence representations.</p><p>One important consideration is how to iden- tify suitable local regions for comparison so that we can best utilize the compositional information in the sentence representations. There are many possible ways to group local comparison regions. In doing so, we consider the following four as- pects: 1) whether from the same building block; 2) whether from convolutional layers with the same window size; 3) whether from the same pooling layer; 4) whether from the same filter of the under- lying convolution layers. <ref type="bibr">3</ref> We focus on comparing regions that share at least two of these conditions.</p><p>To concretize this, we provide two algorithms below to identify meaningful local regions. While there exist other sets of comparable regions that share the above conditions, we do not explore them all due to concerns about learning efficiency; we find that the subset we consider performs strongly in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Similarity Comparison Units</head><p>We define two comparison units for comparing two local regions in the sentence representations:</p><formula xml:id="formula_5">comU 1( − → x , − → y ) = {cos( − → x , − → y ), L2Euclid ( − → x , − → y ), | − → x − − → y |} (3) comU 2( − → x , − → y ) = {cos( − → x , − → y ), L2Euclid ( − → x , − → y )} (4)</formula><p>Cosine distance (cos) measures the distance of two vectors according to the angle between them, while L 2 Euclidean distance (L 2 Euclid ) and element-wise absolute difference measure magni- tude differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison over Local Regions</head><p>Algorithms 1 and 2 show how the two sentence representations are compared in our model. Algo- rithm 1 works on the output of block A only, while Algorithm 2 deals with both block A and block B , focusing on regions from the output of the same pooling type and same block type, but with differ- ent filters and window sizes of convolution layers. Given two sentences S 1 and S 2 , we set the max- imum window size ws of block A and block B to be n, let regM * represent a numFilter A by n + 1 ma- trix, and assume that each group * outputs its cor- responding oG * . The output features are accumu- lated in a final vector fea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">One Simplified Example</head><p>We provide a simplified working example to show how the two algorithms compare outputs of block A only. If we arrange the sentence representations into the shape of sentence matrices as in <ref type="figure">Figure 5</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Horizontal Comparison</head><p>1: for each pooling p = max, min, mean do 2:</p><p>for each width ws1 = 1...n, ∞ do 3:</p><formula xml:id="formula_6">regM 1 [ * ][ws1] = groupA(ws1, p, S1) 4: regM 2 [ * ][ws1] = groupA(ws1, p, S2) 5:</formula><p>end for 6:</p><p>for each i = 1...numFilterA do 7:</p><formula xml:id="formula_7">fea h = comU 2(regM 1 [i], regM 2 [i]) 8:</formula><p>accumulate fea h for final layer 9:</p><p>end for 10: end for Algorithm 2 Vertical Comparison 1: for each pooling p = max, min, mean do 2:</p><p>for each width ws1 = 1...n, ∞ do 3: oG1A = groupA(ws1, p, S1) 4:</p><p>for each width ws2 = 1...n, ∞ do 5: oG2A = groupA(ws2, p, S2) 6: feaa = comU 1(oG 1A, oG2A) 7:</p><p>accumulate feaa for final layer 8:</p><p>end for 9:</p><p>end for 10:</p><p>for each width ws1 = 1...n do 11: oG1B = groupB (ws1, p, S1) 12: oG2B = groupB (ws1, p, S2) 13:</p><p>for each i = 1...numFilterB do 14:</p><formula xml:id="formula_8">fea b = comU 1(oG 1B [ * ][i], oG2B[ * ][i]) 15:</formula><p>accumulate fea b for final layer 16:</p><p>end for 17:</p><p>end for 18: end for then in Algorithms 1 and 2 we are essentially com- paring local regions of the two matrices in two di- rections: along rows and columns.</p><p>In <ref type="figure">Figure 5</ref>, each column of the max/min/mean groups is compared with all columns of the same pooling group for the other sentence. This is shown in red dotted lines in the Figure and listed in lines 2 to 9 in Algorithm 2. Note that both ws 1 and ws 2 columns within each pooling group should be compared using red dotted lines, but we omit this from the figure for clarity.</p><p>In the horizontal direction, each equal-sized max/min/mean group is extracted as a vector and is compared to the corresponding one for the other sentence. This process is repeated for all rows and comparisons are shown in green solid lines, as per- formed by Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Other Model Details</head><p>Output Fully-Connected Layer. On top of the similarity measurement layer (which outputs a vector containing all fea * ), we stack two linear layers with an activation layer in between, fol- lowed by a log-softmax layer as the final output layer, which outputs the similarity score.  <ref type="figure">Figure 5</ref>: Simplified example of local region com- parisons over two sentence representations that use block A only. The "horizontal comparison" (Algorithm 1) is shown with green solid lines and "vertical comparison" (Algorithm 2) with red dot- ted lines. Each sentence representation uses win- dow sizes ws 1 and ws 2 with max/min/mean pool- ing and numFilter A = 3 filters.</p><p>as the activation function for all convolution filters and for the activation layer placed between the fi- nal two layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>Everything necessary to replicate our experimen- tal results can be found in our open-source code repository. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Tasks and Datasets</head><p>We consider three sentence pair similarity tasks:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Microsoft Research Paraphrase Corpus</head><p>(MSRP). This data was collected from news sources ( <ref type="bibr" target="#b10">Dolan et al., 2004</ref>) and contains 5,801 pairs of sentences, with 4,076 for training and the remaining 1,725 for testing. Each sentence pair is annotated with a binary label indicating whether the two sentences are paraphrases, so the task here is binary classification.</p><p>2. Sentences Involving Compositional Knowl- edge (SICK) dataset. This data was collected for the 2014 SemEval competition ( <ref type="bibr" target="#b29">Marelli et al., 2014</ref>) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a devel- opment set, and the remaining 4,927 in the test set. The sentences are drawn from image and video descriptions. Each sentence pair is annotated with a relatedness score ∈ <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>, with higher scores indicating the two sen- tences are more closely-related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Microsoft Video Paraphrase Corpus</head><p>(MSRVID). This dataset was collected for the 2012 SemEval competition and consists of 1,500 pairs of short video de- scriptions which were then annotated ( <ref type="bibr" target="#b0">Agirre et al., 2012</ref>). Half of it is for training and the other half is for testing. Each sentence pair has a relatedness score ∈ <ref type="bibr">[0,</ref><ref type="bibr">5]</ref>, with higher scores indicating the two sentences are more closely-related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Training</head><p>We use a hinge loss for the MSRP paraphrase identification task. This is simpler than log loss since it only penalizes misclassified cases. The training objective is to minimize the following loss (summed over examples x, y gold ):</p><formula xml:id="formula_9">loss(θ, x, y gold ) = y =y gold max(0, 1 + f θ (x, y ) − f θ (x, y gold )) (5)</formula><p>where y gold is the ground truth label, input x is the pair of sentences x = {S 1 , S 2 }, θ is the model weight vector to be trained, and the func- tion f θ (x, y) is the output of our model. We use regularized KL-divergence loss for the semantic relatedness tasks (SICK and MSRVID), since the goal is to predict the similarity of the two sentences. The training objective is to minimize the KL-divergence loss plus an L 2 regularizer:</p><formula xml:id="formula_10">loss(θ) = 1 m m k=1 KL f k || f k θ + λ 2 ||θ|| 2 2 (6)</formula><p>where f θ is the predicted distribution with model weight vector θ, f is the ground truth, m is the number of training examples, and λ is the regu- larization parameter. Note that we use the same KL-loss function and same sparse target distribu- tion technique as <ref type="bibr" target="#b37">Tai et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiment Settings</head><p>We conduct experiments with ws values in the range <ref type="bibr">[1,</ref><ref type="bibr">3]</ref> as well as ws = ∞ (no convolution).</p><p>We use multiple kinds of embeddings to rep- resent each sentence, both on words and part-of- speech (POS) tags. We use the Dim g = 300- dimensional GloVe word embeddings ( <ref type="bibr" target="#b31">Pennington et al., 2014</ref>) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vec- tors ( <ref type="bibr" target="#b41">Wieting et al., 2015</ref>) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Para- phrase Database ( <ref type="bibr" target="#b14">Ganitkevitch et al., 2013)</ref>. For POS embeddings, we run the Stanford POS tag- ger ( ) on the English side of the Xinhua machine translation parallel cor- pus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings us- ing the word2vec toolkit <ref type="figure">(Mikolov et al., 2013)</ref>. Adding POS embeddings is expected to retain syn- tactic information which is reported to be effec- tive for paraphrase identification <ref type="bibr" target="#b9">(Das and Smith, 2009)</ref>. We use POS embeddings only for the MSRP task.</p><p>Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dim g + Dim p + Dim k = 525-dimension vectors for each input word; for SICK and MSRVID we only use Dim = 300-dimension GloVe embeddings.</p><p>We use 5-fold cross validation on the MSRP training data for tuning, then largely re-use the same hyperparameters for the other two datasets. However, there are two changes: 1) for the MSRP task we update word embeddings during train- ing but not so on SICK and MSRVID tasks; 2) we set the fully connected layer to contain 250 hidden units for MSRP, and 150 for SICK and MSRVID. These changes were done to speed up our experimental cycle on SICK and MSRVID; on SICK data they are the same experimental settings as used by <ref type="bibr" target="#b37">Tai et al. (2015)</ref>, which makes for a cleaner empirical comparison.</p><p>We set the number of holistic filters in block A to be the same as the input word embeddings, therefore numFilter A = 525 for MSRP and numFilter A = 300 for SICK and MSRVID. We set the number of per-dimension filters in block B to be numFilter B = 20 per dimension for all three datasets, which corresponds to 20 * Dim fil- ters in total.</p><p>We perform optimization using stochastic gra- dient descent <ref type="bibr" target="#b6">(Bottou, 1998)</ref>. The backpropaga- tion algorithm is used to compute gradients for all parameters during training <ref type="bibr" target="#b15">(Goller and Kuchler, 1996</ref>). We fix the learning rate to 0.01 and regularization parameter λ = 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results on Three Datasets</head><p>Results on MSRP Data. We report F1 scores and accuracies from prior work in <ref type="table" target="#tab_4">Table 1</ref>  <ref type="formula" target="#formula_3">(2012)</ref> 73.0% 82.3% <ref type="bibr" target="#b12">Fern and Stevenson (2008)</ref> 74.1% 82.4% <ref type="bibr" target="#b13">Finch (2005)</ref> 75.0% 82.7% <ref type="bibr" target="#b9">Das and Smith (2009)</ref> 76.1% 82.7% <ref type="bibr" target="#b38">Wan et al. (2006)</ref> 75.6% 83.0% <ref type="bibr" target="#b35">Socher et al. (2011)</ref> 76.8% 83.6% <ref type="bibr" target="#b26">Madnani et al. (2012)</ref> 77.4% 84.1% <ref type="bibr" target="#b20">Ji and Eisenstein (2013)</ref> 80.41% 85.96% <ref type="bibr">Yin and Schütze (2015) (without pretraining)</ref> 72.5% 81.4% <ref type="bibr">Yin and Schütze (2015) (with pretraining)</ref> 78.1% 84.4% <ref type="bibr">Yin and Schütze (2015) (pretraining+sparse features)</ref> 78.4% 84.6%</p><p>This work 78.60% 84.73% When comparing to their model without pre- training, we outperform them by 6% absolute in accuracy and 3% in F1. Our model is also supe- rior to other recent neural network models ( <ref type="bibr" target="#b18">Hu et al., 2014;</ref><ref type="bibr" target="#b35">Socher et al., 2011</ref>) without requiring sparse features or unlabeled data as in <ref type="bibr" target="#b43">(Yin and Schütze, 2015;</ref><ref type="bibr" target="#b35">Socher et al., 2011</ref>). The best re- sult on MSRP is from <ref type="bibr" target="#b20">Ji and Eisenstein (2013)</ref> which uses unsupervised learning on the MSRP test set and rich sparse features.</p><p>Results on SICK Data. Our results on the SICK task are summarized in <ref type="table" target="#tab_7">Table 2</ref>, showing Pearson's r, Spearman's ρ, and mean squared error (MSE). We include results from the literature as reported by <ref type="bibr" target="#b37">Tai et al. (2015)</ref>, including prior work using re- current neural networks (RNNs), the best submis- sions in the SemEval-2014 competition, and vari- ants of LSTMs. When measured by Pearson's r, the previous state-of-the-art approach uses a tree- structured LSTM <ref type="bibr" target="#b37">(Tai et al., 2015)</ref>; note that their best results require a dependency parser.</p><p>On the contrary, our approach does not rely on parse trees, nor do we use POS/PARAGRAM em- beddings for this task. The word embeddings,  <ref type="bibr" target="#b25">Lai and Hockenmaier (2014)</ref> 0.7993 0.7538 0.3692 <ref type="bibr" target="#b21">Jimenez et al. (2014)</ref> 0.8070 0.7489 0.3550 <ref type="bibr" target="#b4">Bjerva et al. (2014)</ref> 0.8268 0.7721 0.3224 <ref type="bibr" target="#b44">Zhao et al. (2014)</ref> 0    <ref type="table" target="#tab_8">Table 3</ref>, which includes the top 2 submissions in the Seman- tic Textual Similarity (STS) task from SemEval- 2012. We find that we outperform the top system from the task by nearly 3 points in Pearson's r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Model Ablation Study</head><p>We report the results of an ablation study in <ref type="table">Ta</ref>  <ref type="table">Table 4</ref>: Ablation study over test sets of all three datasets. Nine components are divided into four groups. We remove components one at a time and show differences.</p><p>The nine components can be divided into four groups: (1) input embeddings (components 1-2); (2) sentence modeling (components 3-5); (3) sim- ilarity measurement metrics (component 6); (4) similarity measurement layer (components 7-9). For MSRP, we use all nine components. For SICK and MSRVID, we use components 3-9 (as de- scribed in Sec. 6.3).</p><p>From <ref type="table">Table 4</ref> we find drops in performance for all components, with the largest differences ap- pearing when removing components of the simi- larity measurement layer. For example, conduct- ing comparisons over flattened sentence represen- tations (removing component 9) leads to large drops across tasks, because this ignores struc- tured information within sentence representations. Groups (1) and (2) are also useful, particularly for the MSRP task, demonstrating the extra benefit obtained from our multi-perspective approach in sentence modeling.</p><p>We see consistent drops when ablating the Ver- tical/Horizontal algorithms that target particular regions for comparison. Also, removing group (3) hinders both the Horizontal and Vertical al- gorithms (as described in Section 5.1), so its removal similarly causes large drops in perfor- mance. Though convolutional neural networks al- ready perform strongly when followed by flattened vector comparison, we are able to leverage the full richness of the sentence models by performing structured similarity modeling on their outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>On the SICK dataset, the dependency tree LSTM ( <ref type="bibr" target="#b37">Tai et al., 2015)</ref> and our model achieve comparable performance despite taking very dif- ferent approaches. Tai et al. use syntactic parse trees and gating mechanisms to convert each sen- tence into a vector, while we use large sets of flex- ible feature extractors in the form of convolution filters, then compare particular subsets of features in our similarity measurement layer.</p><p>Our model architecture, with its many paths of information flow, is admittedly complex. Though we have removed hand engineering of features, we have added a substantial amount of functional architecture engineering. This may be necessary when using the small training sets provided for the tasks we consider here. We conjecture that a sim- pler, deeper neural network architecture may out- perform our model when given large amounts of training data, but we leave an investigation of this direction to future work.</p><p>In summary, we developed a novel model for sentence similarity based on convolutional neural networks. We improved both sentence modeling and similarity measurement. Our model achieves highly competitive performance on three datasets. Ablation experiments show that the performance improvement comes from our use of multiple per- spectives in both sentence modeling and structured similarity measurement over local regions of sen- tence representations. Future work could extend this model to related tasks including question an- swering and information retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Filters</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1577</head><label>1577</label><figDesc></figDesc><table>Cats Sit On The Mat 




On The Mat There Sit Cats 




Structured Similarity Measurement Layer 

Fully Connected Layer 

Output: Similarity Score 















</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>ws1</head><label></label><figDesc></figDesc><table>Max Pooling 

ws1 

Min Pooling 

ws1 

Mean Pooling 

ws1 

Max Pooling 

ws1 

Min Pooling 

Building Block A 
Building Block B 

Holistic 
Dimension 
Per-

Filters 
Holistic 
Filters 
Holistic 
Filters 
Dimension 
Per-

Filters 

Figure 3: Each building block consists of multiple 
independent pooling layers and convolution layers 
with width ws 1 . Left: block A operates on entire 
vectors of word embeddings. Right: block B oper-
ates on individual dimensions of word vectors to 
capture information of a finer granularity. 

to be interpretable to humans, there may still be 
distinct information captured by the different di-
mensions that our model could exploit. Further-
more, if we update the word embeddings during 
learning, different dimensions could be encour-
aged further to capture distinct information. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Test set results on MSRP for paraphrase 
identification. Rows in grey are neural network-
based approaches. 

proaches shown in gray rows of the table are 
based on neural networks. The recent approach 
by Yin and Schütze (2015) includes a pretraining 
technique which significantly improves results, as 
shown in the table. We do not use any pretrain-
ing but still slightly outperform their best results 
which use both pretraining and additional sparse 
features from Madnani et al. (2012). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test set results on SICK, as reported 
by Tai et al. (2015), grouped as: (1) RNN vari-
ants; (2) SemEval 2014 systems; (3) sequential 
LSTM variants; (4) dependency and constituency 
tree LSTMs (Tai et al., 2015). Evaluation metrics 
are Pearson's r, Spearman's ρ, and mean squared 
error (MSE). 

Model 
Pearson's r 
Rios et al. (2012) 
0.7060 
Wang and Cer (2012) 
0.8037 
Beltagy et al. (2014) 
0.8300 
Bär et al. (2012) 
0.8730Šari´c 8730ˇ8730Šari´8730Šari´c et al. (2012) 
0.8803 
This work 
0.9090 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test set results on MSRVID data. The Bär 
et al. (2012) andŠari´candˇandŠari´andŠari´c et al. (2012) results were 
the top two submissions in the Semantic Textual 
Similarity task at the SemEval-2012 competition. 

sparse distribution targets, and KL loss function 
are exactly the same as used by Tai et al. (2015), 
therefore representing comparable conditions. 

Results on MSRVID Data. Our results on the 
MSRVID data are summarized in </table></figure>

			<note place="foot" n="1"> We note that max and min are not both strictly necessary when using certain activation functions, but they still may help us find a more felicitous local optimum. 2 We note that there is no pooling across multiple filters in a layer/group, or across groups. Each pooling operation is performed independently on the matches of a single filter.</note>

			<note place="foot" n="3"> We note that since we apply the same network to both sentences, the same filters are used to match both sentences, so we can directly compare filter matches of individual filters across the two sentences.</note>

			<note place="foot" n="4"> http://hohocode.github.io/textSimilarityConvNet/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.S. National Sci-ence Foundation under awards IIS-1218043 and CNS-1405688. Any opinions, findings, conclu-sions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsor. We would like to thank the anony-mous reviewers for their feedback and CLIP lab-mates for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: a pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">UKP: computing semantic textual similarity by combining multiple content similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic soft logic for semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1210" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The meaning factory: formal semantics for recognizing textual entailment and determining semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Bjerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Rob van der Goot, and Malvina Nissim. International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Online learning and stochastic approximations. On-line learning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine learning</title>
		<meeting>the 25th International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Paraphrase identification as probabilistic quasi-synchronous recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A semantic similarity approach to paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics UK 11th Annual Research Colloquium</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using machine translation evaluation techniques to determine sentence-level semantic equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Paraphrasing</title>
		<meeting>the International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PPDB: the Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Networks</title>
		<meeting>the International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling sentences in the latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Measuring Semantic Relatedness Using Salient Encyclopedic Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Denton, Texas, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of North Texas</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative improvements to distributional sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="891" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">UNAL-NLP: combining soft cardinality features for semantic textual similarity, relatedness and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Duenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Baquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Av Juan Dios Bátiz, and Av Mendizábal. International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Illinois-LH: a denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">SemEval-2014 task 1: evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at International Conference on Learning Representations</title>
		<meeting>Workshop at International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GloVe: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep order statistic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Spoken Language Technology</title>
		<meeting>the IEEE Workshop on Spoken Language Technology</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">UOW: semantically informed text similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="673" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TakeLab: systems for measuring semantic text similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franě</forename><surname>Sari´csari´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mladen</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Snajder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojana Dalbelo</forename><surname>Baši´baši´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using Dependency-based Features to Take the &quot;Para-farce&quot; out of Paraphrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic edit distance metrics for STS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="648" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wsabie: scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extracting lexically divergent paraphrases from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="435" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convolutional neural network for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ECNU: one stone two birds: ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Tian Tian Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
