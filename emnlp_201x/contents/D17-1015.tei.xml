<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="157" to="166"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a model for locating regions in space based on natural language descriptions. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as on top of or next to, and finally locate the region described in the sentence. All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation. To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions. We achieve a 32% relative error reduction compared to a strong neural baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work, we present a model for grounding spatial descriptors in 3D scenes. Consider inter- preting the instructions: Take the book and put it on the shelf. One critical element of being able to interpret this sentence is associating the refer- ring expression the book with the corresponding object in the world. Another important component of understanding the command above is translat- ing the phrase on the shelf to a location in space. We call such phrases spatial descriptors. While spatial descriptors are closely related to referring expressions, they are distinct in that they can refer to locations even when there is nothing there. An intuitive way to model this is to reason over spatial regions as first-class entities, rather than taking an object-centric approach.</p><p>Following a long tradition of using game envi- ronments for AI, we adopt Minecraft as the setting for our work. Minecraft has previously been used Misty is hanging in the air next to the wooden shelf with the plant on it. for work on planning and navigation <ref type="bibr" target="#b10">(Oh et al., 2016;</ref><ref type="bibr" target="#b14">Tessler et al., 2016)</ref>, and we expand on this by using it for grounded language understanding. As a sandbox game, it can be used to construct a wide variety of environments that capture many interesting aspects of the real world. At the same time, it is easy to extract machine-interpretable representations from the game.</p><p>We construct a dataset of Minecraft scenes with natural-language annotations, and propose a task that evaluates understanding spatial descriptors. Our task is formulated in terms of locating a pink, cube-shaped character named Misty given a scene, a natural language description, and a set of loca- tions to choose from. An example from our dataset is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The Minecraft scene repre- sentation does not provide ground-truth informa- tion about object identity or segmentation, reflect- ing the fact that perceptual ambiguity is always present in real-world scenarios. We do, however, assume the availability of 3D depth information (which, for real-world conditions, can be acquired using depth sensors such as RGBD cameras or Li- DAR).</p><p>We propose and evaluate a neural network that combines convolutional layers operating over 3D regions in space with recurrent layers for process- ing language. Our model jointly learns to seg- ment objects, associate them with words, and un- derstand spatial relationships -all in an end-to-end manner. We compare with a strong neural baseline and demonstrate a relative error reduction of 32%. The dataset and model described in this paper are available online. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our task includes some of the same elements as referring-expression generation and interpretation. Past work on these tasks includes <ref type="bibr" target="#b2">Golland et al. (2010)</ref>, <ref type="bibr" target="#b8">Krishnamurthy and Kollar (2013)</ref>, <ref type="bibr" target="#b12">Socher et al. (2014)</ref> and <ref type="bibr" target="#b5">Kazemzadeh et al. (2014)</ref>. A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone. For example, <ref type="bibr" target="#b8">Krishnamurthy and Kollar (2013)</ref> convert natural language to a logical form that is matched against image seg- ments, an approach that is only capable of rea- soning about objects already present in the scene (and not skipped over by the segmentation pro- cess). Our model's ability to reason over spatial regions also differentiates it from past approaches to tasks beyond referring expressions, such as the work by <ref type="bibr" target="#b13">Tellex et al. (2011)</ref> on natural-language commanding of robots. Recent work by <ref type="bibr" target="#b3">Hu et al. (2016)</ref> on interpreting referring expressions can capture relationships between objects, relying on the construction of (subject, object, relation) tu- ples. Their model is limited in that it can only handle one such tuple per utterance. Our model does not have such a restriction, and it addition- ally expands to a 3D setting.</p><p>Our task is also related to work on Visual Ques- tion Answering, or VQA ( <ref type="bibr" target="#b0">Agrawal et al., 2015)</ref>. While VQA uses free-form textual answers, our task places targeted emphasis on spatial reasoning by requiring outputs to be locations in the scene. Spatial reasoning remains an important capabil- ity for VQA systems, and is one of the elements featured in CLEVR ( <ref type="bibr" target="#b4">Johnson et al., 2016</ref>), a di-agnostic dataset for VQA. Like in our dataset, vi- sual percepts in CLEVR are based on machine- generated scenes. CLEVR also makes use of machine-generated language, while all language in our dataset is written by humans.</p><p>Another related task in NLP is spatial role la- beling, which includes the identification of spatial descriptors and the assigning of roles to each of their constituent words. This task was studied by <ref type="bibr" target="#b7">Kordjamshidi et al. (2011)</ref> and led to the creation of shared tasks such as SpaceEval ( <ref type="bibr" target="#b11">Pustejovsky et al., 2015)</ref>. Our setting differs in that we con- sider grounded environments instead of studying text in isolation, and evaluate on task performance rather than logical correctness of interpretation.</p><p>Spatial descriptors are also present in the task of generating 3D scenes given natural language de- scriptions. Compared to a recent model by <ref type="bibr" target="#b1">Chang et al. (2017)</ref> for scene generation, our model works with lower-level 3D percepts rather than li- braries of segmented and tagged objects. We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task</head><p>At its core, the ability to understand spatial de- scriptors can be formulated as mapping from a natural-language description to a particular loca- tion in space. In <ref type="figure" target="#fig_0">Figure 1</ref>, we show an instance of our task, which consists of the following compo- nents:</p><p>• W : a perceptual representation of the world • x: the natural language description</p><p>• {y 1 , y 2 , . . . , y n }: the candidate set of loca- tions that are under consideration</p><p>• y : the true location that is being referred to in the scene Given W and x, a model must select which candi- date location y i best matches the description x.</p><p>We will address the particulars of the above representation as we discuss the process for constructing our dataset.</p><p>Each example (W, x, {y 1 , . . . , y n }, y ) in the dataset is made by generating a Minecraft scene (Section 3.1) and selecting a location as the target of description (Section 3.2). We then crowdsource natural language descriptions of the target location in space. To better anchor the language, we populate the target location with a cube-shaped character we name Misty, and ask workers to describe Misty's location (Section 3.3). We repeat this process for each example in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scene Generation and Representation</head><p>Each of our Minecraft scenes is set in a randomly- generated room. We select a random size for this room, and then populate it with a variety of ob- jects. We include objects that can be placed on the floor (e.g. tables), mounted on the wall (e.g. torches), embedded in the wall (e.g. doors), or hanging from the ceiling (e.g. cobwebs).</p><p>We then discard ground-truth knowledge about object segmentation or identity in the process of saving our dataset. This allows our task to eval- uate not only models' capacity for understand- ing language, but also their ability to integrate with perceptual systems. One way of approxi- mating real-world observations would be to take a screenshot of the scene -however, a 2D projec- tion does not provide all of the spatial informa- tion that a language user would reasonably have access to. We would like to use a 3D encoding instead, and Minecraft naturally offers a low-level (albeit low-resolution) voxel-based representation that we adopt for this work.</p><p>Each Minecraft world W is encoded as a 3D grid of voxels, where a voxel may be empty or contain a particular type of "block," e.g. stone or wood. In general, what humans would inter- pret as single objects will be made of multiple Minecraft blocks -for example, the table in Fig- ure 1 consists of a "wooden pressure plate" block on top of a "wooden fencepost" block. These same blocks can be used for other purposes as well: the "wooden fencepost" block is also part of fences, lamp-posts, and pillars, while the "wooden pres- sure plate" block can form shelves, countertops, as well as being placed on the ground to detect when something walks over it. We construct our Minecraft scenes specifically to include examples of such re-use, so that models capable of achieving high performance on this task must demonstrate the capacity to work without ground-truth segmen- tation or perfect object labeling.</p><p>The voxel-grid 3D representation is not spe- cific to the virtual Minecraft setting: it is equally applicable to real-world data where depth infor- mation is available. The main difference is that each voxel would need to be associated with a fea- ture vector rather than a block type. One use of such a representation is in <ref type="bibr" target="#b9">Maturana and Scherer (2015)</ref>'s work on object classification from data collected with RGBD cameras and LiDAR, which uses a 3D convolutional neural network over a voxel grid. We do not explicitly handle occlusion in this work, but we imagine that real-world ex- tensions can approach it using a combination of multi-viewpoint synthesis, occlusion-aware voxel embeddings, and restricting the set of voxels con- sidered by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Location Sampling</head><p>After constructing a scene with representation W , we proceed to sample a location y in the scene. Given our voxel-based scene representation, our location sampling is at voxel granularity. The can- didate set we sample from, {y 1 , . . . , y n }, consists of empty voxels in the scene. Locations that occur in the middle of a large section of empty space are hard to distinguish visually and to describe pre- cisely, so we require that each candidate y i be ad- jacent to at least one object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Natural Language Descriptions</head><p>For each scene-location pair (W, y ) we crowd- source a natural language description x.</p><p>The choice of prompt for human annotators is important in eliciting good descriptions. At the lo- cation we are asking workers to refer to, we insert a pink-colored cube that we personify and name "Misty." We then ask workers to describe Misty's location such that someone can find her if she were to turn invisible. Having a visually salient target helps anchor human perception, which is why we chose a pink color that contrasts with other visual elements in the scene. We make sure to empha- size the name "Misty" in the instructions, which results in workers almost always referring to Misty by name or with the pronoun she. This avoids hav- ing to disambiguate a myriad of generic descrip- tions (the pink block, the block, the target, etc.) for what is fundamentally an artificial construct.</p><p>To make sure that humans understand the 3D structure of the scene as they describe it, we give them access to a 3D view of the environment and require that they move the camera before submit- ting a description. This helped increase the quality of our data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>We next present our model for this task. Our model architecture is shown in <ref type="figure" target="#fig_3">Figure 2</ref>, with some of the quantities it operates over highlighted in <ref type="figure" target="#fig_5">Figure 3</ref>. Throughout this section, we will use the example description Misty is to the right of the table and just under the torch. Note that while the accompanying scene illustrations are shown in 2D for visual clarity, our actual model operates in 3D and on larger scene sizes.</p><p>Our model first associates words with regions in the world. There is no notion of object segmenta- tion in the dataset, so the references it produces are just activations over space given a word. Activa- tions are computed for all words in the sentence, though they will only be meaningful for words such as table and torch <ref type="figure" target="#fig_5">(Figure 3a)</ref>. Our model next determines the spatial relationships between referenced objects and Misty, using information provided by context words such as right and under. These relationships are represented as 3D convo- lutional offset filters <ref type="figure" target="#fig_5">(Figure 3b</ref>). For each word, its reference and offset filter are convolved to get a localization, i.e. an estimate of Misty's location <ref type="figure" target="#fig_5">(Figure 3c)</ref>. Finally, our model aggregates local- izations across all words in the sentence, combin- ing the information provided by the phrases to the right of the table and just under the torch <ref type="figure" target="#fig_5">(Fig- ure 3e)</ref>.</p><p>The following sections describe in more detail how references (Section 4.1), offsets (Section 4.2), and localizations (Section 4.3) are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Input and References</head><p>The first component of our model is responsible for associating words with the voxels that they re- fer to. It assigns a real-valued score s(x t , y) to each pair consisting of word x t and voxel coordi- nate y.</p><p>High scores correspond to high compatibility; for any given word, we can visualize the set s(x t , ·) of scores assigned to different voxels by interpreting it as logits that encode a probability distribution over blocks in the scene. In the exam- ple, the word table would ideally be matched to the uniform reference distribution over blocks that are part of a table, and similarly for the word torch <ref type="figure" target="#fig_5">(Figure 3a)</ref>.</p><p>The word-voxel scores are computed by com- bining word and block embeddings. To take ad- vantage of additional unsupervised language and world data, we start with pretrained word em- beddings and context-aware location embeddings f (W, y). The function f consists of the first two layers of a convolutional neural network that is pretrained on the task of predicting a voxel's identity given the 5x5x5 neighborhood around it. Since f fails to take into account the actual voxel's identity, we add additional embeddings V that only consider single blocks. The score is then computed as s(x t , y) = w t Af (W, y) + w t v y , where w t is the word embedding and v y is the single-block embedding. The parameter matrix (b) Offset distributions that will be applied to the refer- ences for table and torch. These are calculated based on the language context, including the words right and un- der.</p><p>(c) Localizations for Misty, given the words table and torch in context.  A and the single-block embedding matrix V are trained end-to-end with the rest of the model.</p><p>References are computed for all words in the sentence -including function words like to or the. To signify that a word does not refer to any objects in the world, the next layer of the network expects that we output a uniform distribution over all vox- els. Outputting uniform distributions also serves as a good initialization for our model, so we set the elements of A and V to zero at the start of training (our pretrained word embeddings are sufficient to break symmetry).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Offsets</head><p>The per-word references described in Section 4.1 do not themselves indicate Misty's location. Rather, they are used in a spatial descriptor like to the right of the table. For every word, our model outputs a distribution over offset vectors that is used to redistribute scores from object locations to possible locations for Misty <ref type="figure" target="#fig_5">(Figure 3b</ref>). For ex- ample, if probability mass is placed on the "one- block-to-the-right" offset vector, this corresponds to predicting that Misty will be one block to the right of the voxels that a word refers to. Offset scores o t are assigned based on the context the word x t occurs in, which allows the model to in- corporate information from words such as right or under in its decisions. This is accomplished by running a bidirectional LSTM over the embed- dings w t of the words in the sentence, and using its output to compute offset probabilities:</p><formula xml:id="formula_0">[z 0 , z 1 , . . .] = BiLSTM([w 0 , w 1 , . . .]) o t = M z t o t (i) ∝ exp o t (i)</formula><p>Each set of offset scores o t is reshaped into a 3x3x3 convolutional filter, except that we struc- turally disallow assigning any probability to the no-offset vector in the center. As a parameter- tying technique, the trainable matrix M is not full- rank; we instead decompose it such that the log- probability of an offset vector factors additively over the components in a cylindrical coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Localizations and Output</head><p>For each word, the 3D tensor of word-voxel scores s(x t , ·) is convolved with the offset distribution o t to produce a distribution of localizations for Misty, d t (y). A 2D illustration of the result is shown in <ref type="figure" target="#fig_5">Figure 3c</ref>. Localizations are then summed across all words in the sentence, resulting in a single score for each voxel in the scene <ref type="figure" target="#fig_5">(Figure 3e</ref>). These scores are interpreted as logits correspond- ing to a probability distribution over possible loca- tions for Misty:</p><formula xml:id="formula_1">d t (y) = s(x t , y) * o t p(y) ∝ exp t d t (y)</formula><p>Not all words will have localizations that pro- vide information about Misty -for some words the localizations will just be a uniform distribu- tion. We will refer to words that have low-entropy localizations as landmarks, with the understand- ing that being a landmark is actually a soft notion in our model. Our offset filters o t are much smaller than our voxel grid, which means that convolving any off- set filter with a uniform reference distribution over the voxel grid will also result in a uniform localiza- tion distribution (edge effects are immaterial given the small filter size and the fact that Misty is gener- ally not at the immediate edges of the scene). Con- versely, given non-uniform references almost any set of offsets will result in a non-uniform local- ization. The architecture for computing references can output uniform references for function words (like to or the), but it lacks the linguistic context to determine when words refer to objects but should not be interpreted as landmarks (e.g. when they are part of exposition or a negated expression). We therefore include an additional not-a-landmark class that is softmax-normalized jointly with the offset vector distribution o t . Probability assigned to this class subtracts from the probability mass for the true offset directions (and therefore from the localizations) -if this class receives a prob- ability of 1, the corresponding localizations will not contribute to the model output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Loss and Training</head><p>We use a softmax cross-entropy loss for training our model. During training, we find that it helps to not use the candidate set {y 1 , y 2 , . . . , y n } and in- stead calculate a probability p(y) for all blocks in the scene, including solid blocks that cannot possi- bly contain Misty (perhaps because this penalizes inferring nonsensical spatial relationships).</p><p>We run the Adam optimizer ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) with step size 0.001 for 100 epochs using batch size 10. We keep an exponential moving av- erage of our trainable parameters, which we save every two epochs. We then select the saved model that has the highest performance on our develop- ment set.</p><p>We perform several regularization and data aug- mentation techniques in order to achieve better generalization. Each time we sample a training example, we select a random 19x19x19 crop from the full scene (as long as Misty's location is not cropped out). We also disallow using the context- based block embeddings for the first 20 epochs by holding the parameter matrix A described in Sec- tion 4.1 fixed at zero, forcing the model to first learn to associate vocabulary with local features and only later expand to capture the compositional aspects of the environment.</p><p>For the natural language descriptions, all to- kens are converted to lowercase as part of pre- processing. During training we apply word-level dropout (i.e. replacing words with an UNK token) in the LSTM responsible for computing offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Metric</head><p>In evaluating this task, we would like to use a metric that can provide meaningful comparison of our model with baseline and human performance. The set of all possible locations for Misty is large enough that it is hard even for a human to guess the correct block on the first try, especially when some descriptions are only precise to within 1 or 2 blocks. The size of this set also varies from scene to scene.</p><p>Therefore for our evaluation, we restrict the set {y 1 , . . . , y n } to 6 possible locations: Misty's true location and 5 distractors. This represents a less ambiguous problem that is much easier for hu- mans, while also allowing for the evaluation of fu- ture models that may require an expensive com- putation for each candidate location considered. Our procedure for selecting the distractors is de- signed to ensure that we test both local and global scene understanding. Each set of six choices is constructed to consist of three clusters of two can- didates each. Each cluster location is anchored to a landmark -we sample a landmark block adjacent to Misty and two additional landmark blocks from the entire scene, such that the pairwise distances between landmarks are at least 4 units. We then sample one distractor near Misty's landmark and two distractors near both of the other landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset</head><p>To make our development and test sets, we con- struct this six-option variation from a subset of our collected data. For each such example we crowdsource two human solutions using Mechani- cal Turk. Examples where both humans answered correctly are partitioned into a development and a test set. This filtering procedure serves as our primary method of excluding confusing or unin- formative descriptions from the evaluation con-</p><note type="other">(a) When you come in the door, she's on the floor to the right, just in front of the flower.</note><p>(b) Misty is facing to the right of the brown door. (c) If you were to walk through the door that is on the same wall as the table and plank of floating wood, Misty would be to the left of the door. She is eye level with the plank of wood and floating in front of it. (d) Misty is in the ground and she is front of door. (e) Misty is located under a table that is connected to the wall. She is at ground level. <ref type="table">Table 1</ref>: Five natural-language descriptions sampled at random from our dataset.</p><p>ditions. We also collect a third human solution to each example in the development and test sets to get an independent estimate of human perfor- mance on our task. The final dataset consists of 2321 training examples, 120 dev set examples, and 200 test set examples.</p><p>The natural-language descriptions across the full dataset use a vocabulary of 1015 distinct to- kens (case-insensitive but including punctuation). The average description length is 19.02 tokens, with a standard deviation of 10.00 tokens. The large spread partially reflects the fact that some people gave short descriptions that referenced a few landmarks, while others gave sequences of in- structions on how to find Misty. As a point of com- parison, the ReferIt dataset ( <ref type="bibr" target="#b5">Kazemzadeh et al., 2014</ref>) has a larger vocabulary of 9124 tokens, but a shorter average description length of 3.52 tokens (with a standard deviation of 2.67 tokens).</p><p>A random sampling of descriptions from our dataset is shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantitative Results</head><p>Quantitative results are shown in <ref type="table" target="#tab_2">Table 2</ref>. Our evaluation metric is constructed such that there is an easily interpretable random baseline. We also evaluate a strong neural baseline that uses an ap- proach we call Seq2Emb. This baseline converts the sentence into a vector using a bidirectional LSTM encoder, and also assigns vector embed- dings to each voxel using a two-layer convolu- tional neural network. The voxel with an embed- ding that most closely matches the sentence em- bedding is chosen as the answer.</p><p>Our model achieves noticeable gains over the baseline approaches. At the same time, there re- mains a gap between our model and individual hu- man performance. We see this as an indication that we have constructed a task with appropriate diffi- culty: it is approachable by building on the cur- rent state-of-the-art in machine learning and NLP, while presenting challenges that can motivate con- tinued work on understanding language and how it   relates to descriptions of the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We next conduct an ablation study to evaluate the contribution of the individual elements in our model. Our ablation results on the development set are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>In our first ablation, we remove the composi- tional block embeddings that make use of multiple blocks. The resulting performance drop of 2.5% reflects the fact that our model uses multi-block information to match words with objects.</p><p>We next replace the LSTM in our full model with a 3-word-wide convolutional layer. A sin- gle word of left-and right-context provides lim- ited ability to incorporate spatial descriptor words like left and right, or to distinguish landmarks used to locate Misty from words providing exposition about the scene. This ablation solves 5% fewer ex- amples than our full model, reflecting our LSTM's ability to capture such phenomena.</p><p>Finally, we try holding the distribution over off- set vectors fixed, by making it a trainable variable rather than a function of the language. This cor- responds to enforcing the use of only one spatial Misty is floating in the middle of the room. She in the upper half of the room, between the two poles. <ref type="figure">Figure 4</ref>: Reference distribution representing our model's be- lief of which blocks the word poles refers to. Our model as- signs the majority of the probability mass to the poles, while ignoring a table leg that is made of the same block type. Note that the seven numbers overlaid on top account for more than 99% of the total probability mass, and that each of the remaining blocks in the scene has a probability of at most 0.025%.</p><p>operator that roughly means 'near.' We retain the LSTM for the sole purpose of assigning a score to the not-a-landmark class, meaning that contextual information is still incorporated in the decision of whether to classify a word as a landmark or not. The resulting accuracy is 5.8% lower than our full model, which makes this the worst-performing of our ablations. These results suggest that the abil- ity to infer spatial directions is important to our model's overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Examination</head><p>The modular design of our model allows us to ex- amine the individual behavior of each component in the network, which we explore in this section.</p><p>We find that our algorithm is able to learn to associate words with the corresponding voxels in the world. <ref type="figure">Figure 4</ref> shows the reference distribu- tion associated with the word poles, which is con- structed by applying a softmax operation to the word-voxel scores for that word. Our algorithm is able to correctly segment out the voxels that are a part of the pole. Moreover, the table on the right side of the scene has a table leg made of the same block type as the pole -and yet, it is is assigned a low probability. This shows that our model is ca- pable of representing compositional objects, and can learn to do so in an end-to-end manner.</p><p>We next examine the offset distributions com- puted by our model. Consider the scene and de- scription shown in <ref type="figure" target="#fig_7">Figure 5a</ref>. The offset vector distribution at the word platform, shown in <ref type="figure" target="#fig_7">Fig- ure 5b</ref>, shows that the model assigns high proba- (a) To the left of the room, there is a bookcase with a platform directly in front of it. Misty is right above the platform.</p><p>(b) Misty is right above the platform.  In (a), we show the scene and its description. In (b), we visualize the offset vector distribution at the word platform, i.e. the 3D convolutional filter that is applied af- ter finding the platform location. The red dot that indicates the center of the filter will be matched with the platform lo- cation. In (c), we have artificially replaced the words right above with in front of, resulting in a substantial change to this distribution. bility to Misty being above the platform. In <ref type="figure" target="#fig_7">Fig- ure 5c</ref>, we show the effects of replacing the phrase right above with the words in front of. This ex- ample illustrates our model's capacity for learn- ing spatial directions. We note that the offset dis- tribution given the phrase in front of is not as peaked as it is for right above, and that distribu- tions for descriptions saying left or right are even less peaked (and are mostly uniform on the hori- zontal plane). One explanation for this is the am- biguity between speaker-centric and object-centric reference frames. The reference frame of our con- volutional filters is the same as the initial camera frame for our our annotators, but this may not be the true speaker-centric frame because we man- date that annotators move the camera before sub- mitting a description.</p><p>We next highlight our model's ability to incor- porate multiple landmarks in making its decisions. Consider the scene and description shown in <ref type="figure" target="#fig_8">Fig- ure 6</ref>. The room has four walls, two flowers, and four corners -no single landmark is sufficient to correctly guess Misty's location. Our model is able to localize the flowers, walls, and corners in this scene and intersect them to locate Misty. Strictly speaking, this approach is not logically equivalent to applying a two-argument between operator and recognizing the role of that as a rela- tivizer. This is a limitation of our specific model, but the general approach of manipulating spatial region masks need not be constrained in this way. It would be possible to introduce operations into the neural network to model recursive structure in the language. In practice, however, we find that the intersective interpretation suffices for many of the descriptions that occur in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we define the task of interpreting spatial descriptors, construct a new dataset based on Minecraft, and propose a model for this task. We show that convolutional neural networks can be used to reason about regions in space as first- class entities. This approach is trainable end-to- end while also having interpretable values at the intermediate stages of the neural network.</p><p>Our architecture handles many of the linguis- tic phenomena needed to solve this task, includ- ing object references and spatial regions. How- ever, there is more work to be done before we can say that the network completely understands the sentences that it reads. Our dataset can be used to investigate future models that expand to han- dle relativization and other recursive phenomena in language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example from our dataset. (a) The Minecraft scene and its natural language description. (b) Given the choice between six possible locations, our model assigns the highest probability to the location consistent with the natural language description.</figDesc><graphic url="image-2.png" coords="1,324.57,338.43,183.68,69.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Misty is to the right of the table and just under the torch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our model architecture. Note that while the schematic illustrations are shown in 2D, our actual model operates in 3D. Zoomed-in versions of the marked references, offsets, localizations, and output are shown in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>d) Output (e) Output distribution produced by intersecting the lo- calizations for each word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A schematic 2D depiction of the representations used throughout our neural network, zoomed in from Figure 2. Our model (a) matches words with objects in the scene, (b) determines offsets from landmark objects to Misty's location, (c) combines these pieces of information to form perword localizations of Misty, and then (d) uses all localizations to guess Misty's location.</figDesc><graphic url="image-24.png" coords="5,152.56,362.13,57.14,71.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Misty is in front of the platform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of language context on offset vector distributions. In (a), we show the scene and its description. In (b), we visualize the offset vector distribution at the word platform, i.e. the 3D convolutional filter that is applied after finding the platform location. The red dot that indicates the center of the filter will be matched with the platform location. In (c), we have artificially replaced the words right above with in front of, resulting in a substantial change to this distribution.</figDesc><graphic url="image-28.png" coords="8,344.98,433.62,142.87,142.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Our algorithm interprets this sentence as Misty is near the wall and the flowers and close to the corner. This intersective interpretation is sufficient to correctly guess Misty's location in this scene (as well as others in the dataset).</figDesc><graphic url="image-29.png" coords="9,79.09,62.81,204.09,114.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table torch</head><label>torch</label><figDesc></figDesc><table>(a) Reference distributions for the words table and torch. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Success rates for our dataset split. Our model is able 
to outperform a strong neural baseline (Seq2Emb). 

% correct 

Full model 
67.5 
−contextual block embeddings 
65.0 
−LSTM (use 3-word convolutions instead) 62.5 
−language-dependent spatial operators 
61.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Development set results for our full model and three 
independent ablations. 

</table></figure>

			<note place="foot" n="1"> https://github.com/nikitakit/voxelworld</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the many workers on Mechanical Turk who contributed to the creation of our dataset.</p><p>This work was made possible by the open source tooling developed around and inspired by Minecraft; in particular we would like to thank the developers of the voxel.js project and as-sociated plugins, as well as the developers of mcedit2.</p><p>Nikita Kitaev is supported by an NSF Graduate Research Fellowship. This research was supported by DARPA through the XAI program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m">VQA: Visual Question Answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00050</idno>
		<title level="m">SceneSeer: 3d Scene Design with Natural Language</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09978</idno>
		<title level="m">Modeling Relationships in Referential Expressions with Compositional Modular Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06890</idno>
		<title level="m">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatial role labeling: Towards extraction of spatial relations from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martijn</forename><surname>Van Otterlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valliappa</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09128</idno>
		<title level="m">Control of Memory, Active Perception, and Action in Minecraft</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Kordjamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Dworman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Yocum</surname></persName>
		</author>
		<title level="m">SemEval-2015 Task 8: SpaceEval. In Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="884" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><forename type="middle">A</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fleming</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">R</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashis</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Seth Teller, and Nicholas Roy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahar</forename><surname>Givony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Zahavy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07255</idno>
		<title level="m">A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
