<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analogs of Linguistic Structure in Deep Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>jda,klein@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Division</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analogs of Linguistic Structure in Deep Representations</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2893" to="2897"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the compositional structure of message vectors computed by a deep network trained on a communication game. By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning. We then search for struc-tured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation , conjunction, and disjunction. Our results suggest that neural representations are capable of spontaneously developing a &quot;syntax&quot; with functional analogues to qualitative properties of natural language. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past year has seen a renewal of interest in end- to-end learning of communication strategies be- tween pairs of agents represented with deep net- works ( <ref type="bibr" target="#b14">Wagner et al., 2003</ref>). Approaches of this kind make it possible to learn decentralized poli- cies from scratch ( <ref type="bibr" target="#b6">Foerster et al., 2016;</ref><ref type="bibr" target="#b12">Sukhbaatar et al., 2016)</ref>, with multiple agents coordinating via learned communication protocol. More gener- ally, any encoder-decoder model <ref type="bibr" target="#b13">(Sutskever et al., 2014</ref>) can be viewed as implementing an analo- gous communication protocol, with the input en- coding playing the role of a message in an arti- ficial "language" shared by the encoder and de- coder ( <ref type="bibr" target="#b15">Yu et al., 2016)</ref>. Earlier work has found that under suitable conditions, these protocols acquire simple interpretable lexical ( <ref type="bibr" target="#b4">Dircks and Stoness, 1999;</ref><ref type="bibr" target="#b7">Lazaridou et al., 2016</ref>) and sequential struc- ture <ref type="bibr" target="#b9">(Mordatch and Abbeel, 2017)</ref>, even without natural language training data. e f</p><formula xml:id="formula_0">e W f W e W f W W W W</formula><p>Figure 1: Overview of our task. Given a dataset of referring expression games, example human expressions, and their as- sociated logical forms, we compute explicit denotations both for the original task and in other possible tasks-giving rise to a truth-conditional representation of the natural language. We train a recurrent encoder-decoder model to solve the same tasks directly, and use the decoder to generate comparable truth-conditional representations of neural encodings.</p><p>One of the distinguishing features of natural language is compositionality: the existence of op- erations like negation and coordination that can be applied to utterances with predictable effects on meaning. RNN models trained for natural lan- guage processing tasks have been found to learn representations that encode some of this composi- tional structure-for example, sentence represen- tations for machine translation encode explicit fea- tures for certain syntactic phenomena ( <ref type="bibr" target="#b11">Shi et al., 2016)</ref> and represent some semantic relationships translationally ( <ref type="bibr" target="#b8">Levy et al., 2014)</ref>. It is thus nat- ural to ask whether these "language-like" struc- tures also arise spontaneously in models trained directly from an environment signal. Rather than using language as a form of supervision, we pro- pose to use it as a probe-exploiting post-hoc sta- tistical correspondences between natural language descriptions and neural encodings to discover reg- ular structure in representation space.</p><p>To do this, we need to find (vector, string) pairs with matching semantics, which requires first aligning unpaired examples of human-human communication with network hidden states. This is similar to the problem of "translating" RNN rep- resentations recently investigated in <ref type="bibr" target="#b0">Andreas et al. (2017)</ref>. Here we build on that approach in order to perform a detailed analysis of compositional struc- ture in learned "languages". We investigate a com- munication game previously studied by <ref type="bibr" target="#b5">FitzGerald et al. (2013)</ref>, and make two discoveries: in a model trained without any access to language data, 1. The strategies employed by human speakers in a given communicative context are surpris- ingly good predictors of RNN behavior in the same context: humans and RNNs send mes- sages whose interpretations agree on nearly 90% of object-level decisions, even outside the contexts in which they were produced.</p><p>2. Interpretable language-like structure natu- rally arises in the space of representations.</p><p>We identify geometric regularities corre- sponding to negation, conjunction, and dis- junction, and show that it is possible to lin- early transform representations in ways that approximately correspond to these logical operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>We focus our evaluation on a communication game due to <ref type="bibr" target="#b5">FitzGerald et al. (2013)</ref>  <ref type="figure">(Figure 1</ref>, top). In this game, the speaker observes (1) a world W of 1-20 objects labeled with with at- tributes and (2) a designated target subset X of ob- jects in the world. The listener observes only W , and the speaker's goal is to communicate a rep- resentation of X that enables the listener to accu- rately reconstruct it. The GENX dataset collected for this purpose contains 4170 human-generated natural-language referring expressions and corre- sponding logical forms for 273 instances of this game. Because these human-generated expres- sions have all been pre-annotated, we treat lan- guage and logic interchangeably and refer to both with the symbol e. We write e(W ) for the expres- sion generated by a human for a particular world W , and e W for the result of evaluating the logi- cal form e against W . We are interested in using language data of this kind to analyze the behavior of a deep model trained to play the same game. We focus our anal- ysis on a standard RNN encoder-decoder, with the encoder playing the role of the speaker and the decoder playing the role of the listener. The en- coder is a single-layer RNN with GRU cells <ref type="bibr" target="#b2">(Cho et al., 2014</ref>) that consumes both the input world and target labeling and outputs a 64-dimensional hidden representation. We write f (W ) for the output of this encoder model on a world W . To make predictions, this representation is passed to a decoder implemented as a multilayer perceptron. The decoder makes an independent labeling deci- sion about every object in W (taking as input both f and a feature representation of a particular object W i ). We write f W for the full vector of decoder outputs on W . We train the model maximize clas- sification accuracy on randomly-generated scenes and target sets of the same form as in the GENX dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We are not concerned with the RNN model's raw performance on this task (it achieves nearly per- fect accuracy). Instead, our goal is to explore what kinds of messages the model computes in order to achieve this accuracy-and specifically whether these messages contain high-level seman- tics and low-level structure similar to the referring expressions produced by humans. But how do we judge semantic equivalence between natural lan- guage and vector representations? Here, as in An- dreas et al. (2017), we adopt an approach inspired by formal semantics, and represent the meaning of messages via their truth conditions <ref type="figure">(Figure 1</ref>).</p><p>For every problem instance W in the dataset, we have access to one or more human messages e(W ) as well as the RNN encoding f (W ). The truth-conditional account of meaning suggests that we should judge e and f to be equivalent if they designate the same set of of objects in the world <ref type="bibr" target="#b3">(Davidson, 1967)</ref>. But it is not enough to com- pare their predictions solely in the context where they were generated-testing if e W = f W - because any pair of models that achieve perfect ac- curacy on the referring expression task will make the same predictions in this initial context, regard- less of the meaning conveyed.</p><p>Instead, we sample a collection of alternative worlds {W i } observed elsewhere in the dataset, and compute a tabular meaning representation rep(e) = {e W i } by evaluating e in each world W i . We similarly compute rep(f ) = {f W i }, allowing the learned decoder model to play the role of logical evaluation for message vectors. For logically equivalent messages, these tabular rep- resentations are guaranteed to be identical, so the sampling procedure can be viewed as an approxi- mate test of equivalence. It additionally allows us to compute softer notions of equivalence by mea- suring agreement on individual worlds or objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interpreting the meaning of messages</head><p>We begin with the simplest question we can an- swer with this tool: how often do the messages generated by the encoder model have the same meaning as messages generated by humans for the same context? Again, our goal is not to evaluate the performance of the RNN model, but instead our ability to understand its behavior. Does it send messages with human-like semantics? Is it more explicit? Or does it behave in a way indistinguish- able from a random classifier? For each scene in the GENX test set, we com- pute the model-generated message f and its tabu- lar representation rep(f ), and measure the extent to which this agrees with representations produced by three "theories" of model behavior <ref type="figure">(Figure 2</ref>): (1) a random theory that accepts or rejects ob- jects with uniform probability, (2) a literal the- ory that predicts membership only for objects that exactly match some object in the original target set, and (3) a human theory that predicts accord- ing to the most frequent logical form associated with natural language descriptions of the target set (as described in the preceding section). We eval- uate agreement at the level of individual objects, worlds, and full tabular meaning representations.</p><p>Results are shown in <ref type="table">Table 1</ref>. Model behavior is well explained by human decisions in the same context: object-level decisions can be predicted with close to 90% accuracy based on human judg- ments alone, and a third of message pairs agree exactly in every sampled scene, providing strong evidence that they carry the same semantics.</p><p>These results suggest that the model has learned a communication strategy that is at least super- ficially language-like: it admits representations of the same kinds of communicative abstractions that humans use, and makes use of these abstrac- tions with some frequency. But this is purely a statement about the high-level behavior of the model, and not about the structure of the space of representations. Our primary goal is to deter- mine whether this behavior is achieved using low- level structural regularities in vector space that can themselves be associated with aspects of natural language communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Interpreting the structure of messages</head><p>For this we turn to a focused investigation of three specific logical constructions used in natural lan- guage: a unary operation (negation) and two bi- nary operations (conjunction and disjunction). All are used in the training data, with a variety of scopes (e.g. all green objects that are not a tri- angle, all the pieces that are not tan arches).</p><p>Because humans often find it useful to specify the target set by exclusion rather than inclusion, we first hypothesize that the RNN language might find it useful to incorporate some mechanism cor-  <ref type="figure">Figure 2</ref>: Evaluating theories of model behavior. First, the encoder is run on an initial world (a), producing a represen- tation whose meaning we would like to understand (see <ref type="figure">Fig- ure 1)</ref>. We then observe the behavior of the decoder holding this representation fixed but replacing the underlying world representation with alternatives like (b). We compare the true decoder output to a number of theories of its behavior. The random theory (d) outputs a random decision for every object. The literal theory (e) predicts that the decoder will output a positive label only on those objects that exactly match some object in the initial observation. The human theory (f) assigns labels according to the logical semantics of the utterance pro- duced by a human presented with the initial observation.  <ref type="table">Table 2</ref>: Agreement with predicted model behavior for nega- tion, conjunction, and disjunction tasks (top to bottom). Eval- uation is performed on transformed message vectors as de- scribed in Section 5. We discover a robust linear transforma- tion of message vectors corresponding to negation, as well as evidence of structured representations of binary operations.</p><p>responding to negation, and that messages can be predictably "negated" in vector space. To test this hypothesis, we first collect examples of the form (e, f, e , f ), where e = ¬e, rep(e) = rep(f ), and rep(e ) = rep(f ). In other words, we find pairs of pairs of RNN representations f and f for which the natural language messages (e, e ) serve as a denotational certificate that f behaves as a negation of f . If the learned model does not have any kind of primitive notion of negation, we ex- pect that it will not be possible to find any kind of predictable relationship between pairs (f, f ). (As an extreme example, we could imagine every pos- sible prediction rule being associated with a differ- ent point in the representation space, with the cor- respondence between position and behavior essen- tially random.) Conversely, if there is a first-class notion of negation, we should be able to select an arbitrary representation vector f with an associ- ated referring expression e, apply some transfor- mation N to f , and be able to predict a priori how the decoder model will interpret the representation N f -i.e. in correspondence with ¬e.</p><p>Here we make the strong assumption that the negation operation is not only predictable but lin- ear. Previous work has found that linear opera- tors are powerful enough to capture many hier- archical and relational structures ( <ref type="bibr" target="#b10">Paccanaro and Hinton, 2002;</ref><ref type="bibr" target="#b1">Bordes et al., 2014</ref>). Using ex- amples (f, f ) collected from the training set as described above, we compute the least-squares estimatê N = arg min N ||N f − f || 2 2 . To evaluate, we collect example representations from the test set that are equivalent to known logical forms, and measure how frequently model behav- iors rep(N f ) agree with the logical predictions x.¬red(x)</p><p>x.red(x)</p><p>x.green(x) blue(x)</p><formula xml:id="formula_1">x.¬(green(x) blue(x)) (b) 4 3 2 1 0 1 2 3 4 3 2 1 0 1 2 3</formula><p>x.red(x)</p><p>x.yellow(x)</p><p>x.blue(x)</p><p>x.red(x) yellow(x)</p><p>x.blue(x) yellow(x)</p><p>x.red(x) blue(x) rep(¬e)-in other words, how often the linear operator N actually corresponds to logical nega- tion. Results are shown in the top portion of Ta- ble 2. Correspondence with the logical form is quite high, resulting in 97% agreement at the level of individual objects and 45% agreement on full representations. We conclude that the estimated linear operatorˆNoperatorˆ operatorˆN is analogous to negation in nat- ural language. Indeed, the behavior of this opera- tor is readily visible in <ref type="figure" target="#fig_3">Figure 3</ref>: predicted negated forms (in red) lie close in vector space to their true values, and negation corresponds roughly to mir- roring across a central point.</p><p>In our final experiment, we explore whether the same kinds of linear maps can be learned for the binary operations of conjunction and disjunction. As in the previous section, we collect examples from the training data of representations whose de- notations are known to correspond to groups of logical forms in the desired relationship-in this case tuples (e, f, e , f , e , f ), where rep(e) = rep(f ), rep(e ) = rep(f ), rep(e ) = rep(f ) and either e = e ∧ e (conjunction) or e = e ∨ e (disjunction). Since we expect that our operator will be symmetric in its arguments, we solve forˆM forˆ forˆM = arg min M ||M f + M f − f || 2 2 .</p><p>Results are shown in the bottom portions of <ref type="table">Table 2</ref>. Correspondence between the behavior predicted by the contextual logical form and the model's actual behavior is less tight than for nega- tion. At the same time, the estimated operators are clearly capturing some structure: in the case of disjunction, for example, model interpretations are correctly modeled by the logical form 92% of the time at the object level and 19% of the time at the denotation level. This suggests that the operations of conjunction and disjunction do have some func- tional counterparts in the RNN language, but that these functions are not everywhere well approxi- mated as linear.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Principal components of structured message transformations discovered by our experiments. (a) Negation: black and white dots show raw message vectors denotationally equivalent to the provided logical cluster label (Section 3). Red dots show the result of transforming black dots with the estimated negation operation N. (b) The corresponding experiment for disjunction using the transformation M .</figDesc></figure>

			<note place="foot" n="1"> Code and data are available at http://github. com/jacobandreas/rnn-syn.</note>

			<note place="foot" n="6"> Conclusions Building on earlier tools for identifying neural codes with natural language strings, we have presented a technique for exploring compositional structure in a space of vector-valued representations. Our analysis of an encoder-decoder model trained on a reference game identified a number of language-like properties in the model&apos;s representation space, including transformations corresponding to negation, disjunction, and conjunction. One major question left open by this analysis is what happens when multiple transformations are applied hierarchically, and future work might focus on extending the techniques in this paper to explore recursive structure. We believe our experiments so far highlight the usefulness of a denotational perspective from formal semantics when interpreting the behavior of deep models.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating neuralese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anca</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
		<title level="m">Question answering with subgraph embeddings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Truth and meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="323" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Effective lexicon change in the absence of population flux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dircks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Stoness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Artificial Life</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="720" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning distributions over logical forms for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2137" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards multi-agent communicationbased language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07133</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04908</idno>
		<title level="m">Emergence of grounded compositional language in multi-agent populations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning hierarchical structures with linear relational embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Paccanaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jefferey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">857</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Progress in the simulation of emergent communication and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">A</forename><surname>Reggia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Uriagereka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald S</forename><surname>Wilkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="69" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A joint speaker-listener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.09542</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
