<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">CIS) LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">CIS) LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">CIS) LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global Normalization of Convolutional Neural Networks for Joint Entity and Relation Classification</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1723" to="1729"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce globally normalized convo-lutional neural networks for joint entity classification and relation extraction. In particular, we propose a way to utilize a linear-chain conditional random field output layer for predicting entity types and relations between entities at the same time. Our experiments show that global normal-ization outperforms a locally normalized softmax layer on a benchmark dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity classification (EC) and relation ex- traction (RE) are important topics in natural lan- guage processing. They are relevant, e.g., for pop- ulating knowledge bases or answering questions from text, such as "Where does X live?"</p><p>Most approaches consider the two tasks inde- pendent from each other or treat them as a se- quential pipeline by first applying a named entity recognition tool and then classifying relations be- tween entity pairs. However, named entity types and relations are often mutually dependent. If the types of entities are known, the search space of possible relations between them can be reduced and vice versa. This can help, for example, to resolve ambiguities, such as in the case of "Mer- cedes", which can be a person, organization and location. However, knowing that in the given context, it is the second argument for the rela- tion "live in" helps concluding that it is a loca- tion. Therefore, we propose a single neural net- work (NN) for both tasks. In contrast to joint train- ing and multitask learning, which calculate task- wise costs, we propose to learn a joint classifi- cation layer which is globally normalized on the outputs of both tasks. In particular, we train the NN parameters based on the loss of a linear-chain conditional random field (CRF) ( <ref type="bibr" target="#b6">Lafferty et al., 2001</ref>). CRF layers for NNs have been introduced for token-labeling tasks like named entity recog- nition (NER) or part-of-speech tagging <ref type="bibr" target="#b1">(Collobert et al., 2011;</ref><ref type="bibr" target="#b7">Lample et al., 2016;</ref><ref type="bibr" target="#b0">Andor et al., 2016)</ref>. Instead of labeling each input token as in previous work, we model the joint entity and rela- tion classification problem as a sequence of length three for the CRF layer. In particular, we identify the types of two candidate entities (words or short phrases) given a sentence (we call this entity clas- sification to distinguish it from the token-labeling task NER) as well as the relation between them.</p><p>To the best of our knowledge, this architecture for combining entity and relation classification in a single neural network is novel. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of how we model the task: For each sen- tence, candidate entities are identified. Every pos- sible combination of candidate entities (query en- tity pair) then forms the input to our model which predicts the classes for the two query entities as well as for the relation between them.</p><p>To sum up, our contributions are as follows: We introduce globally normalized convolutional neu- ral networks for a sentence classification task. In particular, we present an architecture which al- lows us to model joint entity and relation clas- sification with a single neural network and clas- sify entities and relations at the same time, nor- malizing their scores globally. Our experiments confirm that a CNN with a CRF output layer out- performs a CNN with locally normalized softmax layers. Our source code is available at http: //cistern.cis.lmu.de.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Some work on joint entity and relation classifica- tion uses distant supervision for building their own datasets, e.g., <ref type="bibr" target="#b18">(Yao et al., 2010;</ref><ref type="bibr" target="#b17">Yaghoobzadeh et al., 2016)</ref>. Other studies, which are described in more detail in the following, use the "entity and re- lation recognition" (ERR) dataset from ( <ref type="bibr">Yih, 2004, 2007)</ref> as we do in this paper. <ref type="bibr" target="#b12">Roth and Yih (2004)</ref> develop constraints and use linear pro- gramming to globally normalize entity types and relations. <ref type="bibr" target="#b3">Giuliano et al. (2007)</ref> use entity type in- formation for relation extraction but do not train both tasks jointly. <ref type="bibr" target="#b5">Kate and Mooney (2010)</ref> train task-specific support vector machines and develop a card-pyramid parsing algorithm to jointly model both tasks. <ref type="bibr" target="#b9">Miwa and Sasaki (2014)</ref> use the same dataset but model the tasks as a table filling prob- lem (see Section 4.2). Their model uses both a lo- cal and a global scoring function. Recently,  apply recurrent neural networks to fill the table. They train them in a multitask fashion. Previous work also uses a variety of linguistic fea- tures, such as part-of-speech tags. In contrast, we use convolutional neural networks and only word embeddings as input. Furthermore, we are the first to adopt global normalization of neural networks for this task.</p><p>Several studies propose different variants of non-neural CRF models for information extraction tasks but model them as token-labeling problems <ref type="bibr" target="#b14">(Sutton and McCallum, 2006;</ref><ref type="bibr" target="#b13">Sarawagi et al., 2004;</ref><ref type="bibr" target="#b2">Culotta et al., 2006</ref>; <ref type="bibr" target="#b19">Zhu et al., 2005;</ref><ref type="bibr" target="#b10">Peng and McCallum, 2006</ref>). In contrast, we propose a simpler linear-chain CRF model which directly connects entity and relation classes instead of as- signing a label to each token of the input sequence. This is more similar to the factor graph by <ref type="bibr" target="#b18">Yao et al. (2010)</ref> but computationally simpler. <ref type="bibr" target="#b16">Xu and Sarikaya (2013)</ref> also apply a CRF layer on top of continuous representations obtained by a CNN. However, they use it for a token labeling task (se- mantic slot filling) while we apply the model to a sentence classification task, motivated by the fact that a CNN creates single representations for whole phrases or sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Context and Entities</head><p>Figure 2 illustrates our model.</p><p>Input. Given an input sentence and two query entities, our model identifies the types of the en- tities and the relation between them; see Fig- ure 1. The input tokens are represented by word embeddings trained on Wikipedia with word2vec ( <ref type="bibr" target="#b8">Mikolov et al., 2013</ref>). For identifying the class of an entity e k , the model uses the context to its left, the words constituting e k and the context to its right. For classifying the relation between two en- tities e i and e j , the sentence is split into six parts: left of e i , e i , right of e i , left of e j , e j , right of e j . 1 For the example sentence in <ref type="figure" target="#fig_0">Figure 1</ref>  Sentence Representation. For representing the different parts of the input sentence, we use convo- lutional neural networks (CNNs). CNNs are suit- able for RE since a relation is usually expressed by the semantics of a whole phrase or sentence. Moreover, they have proven effective for RE in previous work ( <ref type="bibr" target="#b15">Vu et al., 2016</ref>). We train one CNN layer for convolving the entities and one for the contexts. Using two CNN layers instead of one gives our model more flexibility. Since entities are usually shorter than contexts, the filter width for entities can be smaller than for contexts. Fur- thermore, this architecture simplifies changing the entity representation from words to characters in future work.</p><p>After convolution, we apply k-max pooling for both the entities and the contexts and concatenate the results. The concatenated vector c z ∈ R Cz , z ∈ {EC, RE} is forwarded to a task-specific hid- den layer of size H z which learns patterns across the different input parts:  <ref type="figure">Figure 2</ref>: Model overview; the colors/shades show which model parts share parameters with weights V z ∈ R Cz×Hz and bias b z ∈ R Hz .</p><formula xml:id="formula_0">h z = tanh(V T z c z + b z )<label>(1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Normalization Layer</head><p>For global normalization, we adopt the linear- chain CRF layer by <ref type="bibr" target="#b7">Lample et al. (2016)</ref>. <ref type="bibr">2</ref> It expects scores for the different classes as input. Therefore, we apply a linear layer first which maps the representations h z ∈ R Hz to a vector v z of the size of the output classes N = N EC + N RE :</p><formula xml:id="formula_1">v z = W T z h z<label>(2)</label></formula><p>with W z ∈ R Hz×N . For a sentence classification task, the input sequence for the CRF layer is not inherentely clear. Therefore, we propose to model the joint entity and relation classification problem with the following sequence of scores (cf., <ref type="figure">Figure  2</ref>):</p><formula xml:id="formula_2">d = [v EC (e 1 ), v RE (r 12 ), v EC (e 2 )]<label>(3)</label></formula><p>with r ij being the relation between e i und e j . Thus, we approximate the joint probability of en- tity types T e 1 , T e 2 and relations R e 1 e 2 as follows:</p><formula xml:id="formula_3">P (T e 1 R e 1 e 2 T e 2 ) ≈P (T e 1 ) · P (R e 1 e 2 |T e 1 ) · P (T e 2 |R e 1 e 2 )<label>(4)</label></formula><p>Our intuition is that the dependence between re- lation and entities is stronger than the dependence between the two entities. The CRF layer pads its input of length n = 3 with begin and end tags and computes the follow- ing score for a sequence of predictions y:</p><formula xml:id="formula_4">s(y) = n i=0 Q y i y i+1 + n i=1 d i,y i<label>(5)</label></formula><p>2 https://github.com/glample/tagger with Q k,l being the transition score from class k to class l and d p,q being the score of class q at posi- tion p in the sequence. The scores are summed because all the variables of the CRF layer live in the log space. The matrix of transition scores Q ∈ R (n+2)×(n+2) is learned during training. <ref type="bibr">3</ref> For training, the forward algorithm computes the scores for all possible label sequences Y to get the log-probability of the correct label sequencê y:</p><formula xml:id="formula_5">log(p(ˆ y)) = e s(ˆ y) ˜ y∈Y e s(˜ y)<label>(6)</label></formula><p>For testing, Viterbi is applied to obtain the label sequence y * with the maximum score:</p><formula xml:id="formula_6">y * = arg max˜y∈Y max˜ max˜y∈Y s(˜ y)<label>(7)</label></formula><p>4 Experiments and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Evaluation Measure</head><p>We use the "entity and relation recognition" (ERR) dataset from (Roth and Yih, 2004) 4 with the train-test split by . We tune the parameters on a held-out part of train. The data is labeled with entity types and relations (see <ref type="table" target="#tab_2">Ta- ble 1</ref>). For entity pairs without a relation, we use the label N. Dataset statistics and model parame- ters are provided in the appendix. Following previous work, we compute F 1 of the individual classes for EC and RE, as well as a task- wise macro F 1 score. We also report the average of scores across tasks (Avg EC+RE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setups</head><p>Setup 1: Entity Pair Relations. <ref type="bibr">Yih (2004, 2007)</ref>; <ref type="bibr" target="#b5">Kate and Mooney (2010)</ref> train separate models for EC and RE on the ERR dataset. For RE, they only identify relations be- tween named entity pairs. In this setup, the query entities for our model are only named entity pairs. Note that this facilitates EC in our experiments.</p><p>Setup 2: <ref type="table">Table Filling</ref>. Following Miwa and Sasaki (2014); , we also model the joint task of EC and RE as a table filling task. For a sentence with length m, we create a quadratic table. Cell (i, j) contains the relation between word i and word j (or N for no relation). A diagonal cell (k, k) contains the entity type of word k. Following previous work, we only predict classes for half of the table, i.e. for m(m + 1)/2 cells. <ref type="figure">Figure 3</ref> shows the table for the example sentence from <ref type="figure" target="#fig_0">Figure 1</ref>. In this setup, each cell (i, j) with i = j is a separate input query to our model. Our model outputs a prediction for cell (i, j) (the relation between i and j) and predic- tions for cells (i, i) and (j, j) (the types of i and j). To fill the diagonal with entity classes, we ag- gregate all predictions for the particular entity by using majority vote. Section 4.4 shows that the in- dividual predictions agree with the majority vote in almost all cases.</p><p>Setup 3: <ref type="table">Table Filling</ref> Without Entity Boundaries. The table from setup 2 includes one row/column per multi-token entity, utilizing the given entity boundaries of the ERR dataset. In or- der to investigate the impact of the entity bound- aries on the classification results, we also con- sider another table filling setup where we ignore the boundaries and assign one row/column per to- ken. Note that this setup is also used by prior work on table filling <ref type="bibr" target="#b9">(Miwa and Sasaki, 2014;</ref>). For evaluation, we follow  and score a multi-token entity as correct if at least one of its comprising cells has been classi- fied correctly.</p><p>Comparison. The most important difference between setup 1 and setup 2 is the number of en- tity pairs with no relation (test set: ≈3k for setup 1, ≈121k for setup 2). This makes setup 2 more challenging. The same holds for setup 3 which considers the same number of entity pairs with no relation as setup 2. To cope with this, we randomly subsample negative instances in the train set of setup 2 and 3. Setup 3 considers the most query <ref type="table" target="#tab_2">Anderson  ,  41  ,  was  the  chief  Middle East  correspondent  for  The Associated Press  Anderson  ,  41  ,  was  the  chief   Middle East  correspondent  for</ref> The Associated Press</p><formula xml:id="formula_7">Peop N N N N N N live N N work N O N N N N N N N N N N N O N N N N N N N N N N N O N N N N N N N N N N N O N N N N N N N N N N N O N N N N N N N N N N N O N N N N live N N N N N N Loc N N based N N N N N N N N O N N N N N N N N N N N O N work N N N N N N based N N Org</formula><p>Figure 3: Entity-relation table entity pairs in total since multi-token entities are split into their comprising tokens. However, setup 3 represents a more realistic scenario than setup 1 or setup 2 because in most cases, entity boundaries are not given. In order to apply setup 1 or 2 to an- other dataset without entity boundaries, a prepro- cessing step, such as entity boundary recognition or chunking would be required. <ref type="table" target="#tab_2">Table 1</ref> shows the results of our globally normal- ized model in comparison to the same model with locally normalized softmax output layers (one for EC and one for RE). For setup 1, the CRF layer performs comparable or better than the softmax layer. For setup 2 and 3, the improvements are more apparent. We assume that the model can benefit more from global normalization in the case of table filling because it is the more challenging setup. The comparison between setup 2 and setup 3 shows that the entity classification suffers from not given entity boundaries (in setup 3). A reason could be that the model cannot convolve the token embeddings of the multi-token entities anymore when computing the entity representation (context B and D in <ref type="figure">Figure 2)</ref>. Nevertheless, the relation classification performance is comparable in setup 2 and setup 3. This shows that the model can in- ternally account for potentially wrong entity clas- sification results due to missing entity boundaries. The overall results (Avg EC+RE) of the CRF are better than the results of the softmax layer for all three setups. To sum up, the improvements of the linear-chain CRF show that (i) joint EC and RE benefits from global normalization and (ii) our way of creating the input sequence for the CRF for joint EC and RE is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>Comparison to State of the Art.   Our results are best comparable with (Gupta et al., 2016) since we use the same setup and train- test splits. However, their model is more compli- cated with a lot of hand-crafted features and var- ious iterations of modeling dependencies among entity and relation classes. In contrast, we only use pre-trained word embeddings and train our model end-to-end with only one iteration per entity pair. When we compare with their model without ad- ditional features <ref type="bibr">(G et al. 2016 (2)</ref>), our model performs worse for EC but better for RE and com- parable for Avg EC+RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Entity Type Aggregation</head><p>As described in Section 4.2, we aggregate the EC results by majority vote. Now, we analyze their disagreement. For our best model, there are only 9 entities (0.12%) with disagreement in the test data. For those, the max, min and median disagreement with the majority label is 36%, 2%, and 8%, resp. Thus, the disagreement is negligibly small. <ref type="bibr">5</ref> We only show results of single models, no ensembles. Following previous studies, we omit the entity class "Other" when computing the EC score. <ref type="bibr">6</ref> Our results on EC in setup 1 are also not comparable </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of CRF Transition Matrix</head><p>To analyze the CRF layer, we extract which tran- sitions have scores above 0.5. <ref type="figure" target="#fig_2">Figure 4</ref> shows that the layer has learned correct correlations between entity types and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we presented the first study on global normalization of neural networks for a sentence classification task without transforming it into a token-labeling problem. We trained a convolu- tional neural network with a linear-chain condi- tional random field output layer on joint entity and relation classification and showed that it outper- formed using a locally normalized softmax layer. An interesting future direction is the extension of the linear-chain CRF to jointly normalize all predictions for table filling in a single model pass. Furthermore, we plan to verify our results on other datasets in future work.  <ref type="table" target="#tab_4">Table 4</ref> provides the hyperparameters we opti- mized on dev (nk C : number of convolutional fil- ters for the CNN convolving the contexts, nk E : number of convolutional filters for the CNN con- volving the entities; h C : number of hidden units for creating the final context representation, h E : number of hidden units for creating the final entity representation).</p><p>For all models, we use a filter width of 3 for the context CNN and a filter width of 2 for the entity CNN (tuned in prior experiments and fixed for the optimization of the parameters in <ref type="table" target="#tab_4">Table 4</ref>).</p><p>For training, we apply gradient descent with a batch size of 10 and an initial learning rate of 0.1. When the performance on dev decreases, we halve the learning rate. The model is trained with early stopping on dev, with a maximum number of 20 epochs. We apply L2 regularization with λ = 10 −3 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of our task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 illustrates our model. Input. Given an input sentence and two query entities, our model identifies the types of the entities and the relation between them; see Figure 1. The input tokens are represented by word embeddings trained on Wikipedia with word2vec (Mikolov et al., 2013). For identifying the class of an entity e k , the model uses the context to its left, the words constituting e k and the context to its right. For classifying the relation between two entities e i and e j , the sentence is split into six parts: left of e i , e i , right of e i , left of e j , e j , right of e j. 1 For the example sentence in Figure 1 and the entity pair ("Anderson", "chief"), the context split is: [] [Anderson] [, 41 , was the chief Middle ...] [Anderson , 41 , was the] [chief] [Middle East correspondent for ...] Sentence Representation. For representing the different parts of the input sentence, we use convolutional neural networks (CNNs). CNNs are suitable for RE since a relation is usually expressed by the semantics of a whole phrase or sentence. Moreover, they have proven effective for RE in previous work (Vu et al., 2016). We train one CNN layer for convolving the entities and one for the contexts. Using two CNN layers instead of one gives our model more flexibility. Since entities are usually shorter than contexts, the filter width for entities can be smaller than for contexts. Furthermore, this architecture simplifies changing the entity representation from words to characters in future work. After convolution, we apply k-max pooling for both the entities and the contexts and concatenate the results. The concatenated vector c z ∈ R Cz , z ∈ {EC, RE} is forwarded to a task-specific hidden layer of size H z which learns patterns across the different input parts:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Most strongly correlated entity types and relations according to CRF transition matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>)</head><label></label><figDesc></figDesc><table>Anderson , 41 , was the chief Middle East correspondent for The Associated Press when he was kidnapped in Beirut 

context1 
A 

entity1 
B 

context2 
C 

entity2 
D 

context3 
E 

Anderson , 41 , was the chief Middle East correspondent for The Associated Press when he was kidnapped in Beirut 

(Middle East, The Associated Press) 

h EC 

left of e 1 
A 

e 1 
B 

right of e 1 
CDE 

e 2 
D 

right of e 2 
E 

left of e 1 
A 

e 1 
B 

right of e 1 
CDE 

left of e 2 
ABC 

left of e 2 
ABC 

e 2 
D 

right of e 2 
E 

v EC (e 1 ) 

Loc 
OrgBased_in 
Org 

v RE (r 12 ) 
v EC (e 2 ) 

c EC (e 1 ) 
c RE (r 12 ) 
c EC (e 2 ) 

CNN context 
CNN context 
CNN context 
CNN entity 
CNN entity 

split 
context 

convolution 
k-max pooling 

copy and 
concatenate 

V EC 

h RE 
h EC 

V RE 
V EC 

W EC 
W EC 
W RE 

hidden layer 

linear layer 

CRF layer 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>our results in the context of state-of-the-art results: 
(Roth and Yih, 2007), (Kate and Mooney, 2010), </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : F 1 results for entity classification (EC) and relation extraction (RE) in the three setups</head><label>1</label><figDesc></figDesc><table>Model 
S Feats 
EC 
RE 
EC+RE 
R &amp; Y 2007 
1 
yes 
85.8 58.1 
72.0 
K &amp; M 2010 
1 
yes 
91.7 62.2 
77.0 
Ours (NN CRF) 
1 
no 
92.1 65.3 
78.7 
Ours (NN CRF) 
2 
no 
88.2 62.1 
75.2 
M &amp; S 2014 
3 
yes 
92.3 71.0 
81.7 
G et al. 2016 (1) 3 
yes 
92.4 69.9 
81.2 
G et al. 2016 (2) 3 
no 
88.8 58.3 
73.6 
Ours (NN CRF) 
3 
no 
82.1 62.5 
72.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison to state of the art (S: setup) 

(Miwa and Sasaki, 2014), (Gupta et al., 2016). 5 
Note that the results are not comparable because of 
the different setups and different train-test splits. 6 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Hyperparameter optimization results</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The ERR dataset we use provides boundaries for entities to concentrate on the classification task (Roth and Yih, 2004).</note>

			<note place="foot" n="3"> 2 is added because of the padded begin and end tag 4 http://cogcomp.cs.illinois.edu/page/resource view/43</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Heike Adel is a recipient of the Google European Doctoral Fellowship in Natural Language Process-ing and this research is supported by this fel-lowship. This work was also supported by DFG (SCHU 2246/4-2).</p><p>since we only input named entities into our model.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Statistics</head><p>Note that the sum of numbers of relation labels is slightly different to the numbers reported in ( <ref type="bibr" target="#b12">Roth and Yih, 2004</ref>). According to their web- site https://cogcomp.cs.illinois. edu/page/resource_view/43, they have updated the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>Setup Output layer nkC nkE hC <ref type="table">hE  1  softmax  500  100  100 50  2  softmax  500  100  100 50  3  softmax  500  100  100 50  1  CRF  200  50  100 50  2  CRF  500  100  200 50  3  CRF  500  100  100 50</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating probabilistic extraction models and data mining to discover relations and patterns in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Betz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="296" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relation extraction and the influence of automatic named-entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Giuliano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Lavelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction using card-pyramid parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at 1st International Conference on Learning Representations (ICLR)</title>
		<meeting>Workshop at 1st International Conference on Learning Representations (ICLR)<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Information extraction from research papers using conditional random fields. Information processing &amp; management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="963" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global inference for entity and relation identification via a linear programming formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Introduction to Statistical Relational Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004)</title>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semimarkov conditional random fields for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An introduction to conditional random fields for relational learning. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="93" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining recurrent and convolutional neural networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting><address><addrLine>Olomouc, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Noise mitigation for neural entity typing and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective cross-document relation extraction without labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2d conditional random fields for web information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiqing</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1044" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
