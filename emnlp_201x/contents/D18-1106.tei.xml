<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Domain Enablement Attention for Personalized Domain Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amazon</forename><surname>Alexa</surname></persName>
						</author>
						<title level="a" type="main">Supervised Domain Enablement Attention for Personalized Domain Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="894" to="899"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>894</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In large-scale domain classification for natural language understanding, leveraging each user&apos;s domain enablement information, which refers to the preferred or authenticated domains by the user, with attention mechanism has been shown to improve the overall domain classification performance. In this paper, we propose a supervised enablement attention mechanism, which utilizes sigmoid activation for the attention weighting so that the attention can be computed with more expressive power without the weight sum constraint of softmax attention. The attention weights are explicitly encouraged to be similar to the corresponding elements of the ground-truth&apos;s one-hot vector by supervised attention, and the attention information of the other enabled domains is leveraged through self-distillation. By evaluating on the actual utterances from a large-scale IPDA, we show that our approach significantly improves domain classification performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Due to recent advances in deep learning tech- niques, intelligent personal digital assistants (IP- DAs) such as Amazon Alexa, Google Assistant, Microsoft Cortana, and Apple Siri have been widely used as real-life applications of natural language understanding ( <ref type="bibr" target="#b22">Sarikaya et al., 2016;</ref><ref type="bibr" target="#b21">Sarikaya, 2017)</ref>.</p><p>In natural language understanding, domain clas- sification is a task that finds the most relevant do- main given an input utterance (Tur and <ref type="bibr" target="#b23">de Mori, 2011</ref>). For example, "make a lion sound" and "find me an apple pie recipe" should be classified as ZooKeeper and AllRecipe, respectively. Recent IPDAs cover more than several thousands of diverse domains by including third-party devel- oped domains such as Alexa Skills ( <ref type="bibr" target="#b8">Kim et al., 2018a;</ref><ref type="bibr" target="#b5">Kim and Kim, 2018)</ref>, Google Actions, and Cortana Skills, which makes domain classification to be a more challenging task.</p><p>Given a large number of domains, leverag- ing user's enabled domain information 1 has been shown to improve the domain classification per- formance since enabled domains reflect the user's context in terms of domain usage <ref type="bibr" target="#b9">(Kim et al., 2018b)</ref>. For an input utterance, <ref type="bibr" target="#b9">Kim et al. (2018b)</ref> use attention mechanism so that a weighted sum of the enabled domain vectors are used as an input signal as well as the utterance vector. The enabled domain vectors and the attention weights are au- tomatically trained in an end-to-end fashion to be helpful for the domain classification.</p><p>In this paper, we propose a supervised enable- ment attention mechanism for more effective at- tention on the enabled domains. First, we use lo- gistic sigmoid instead of softmax as the attention activation function to relax the constraint that the weight sum over all the enabled domains is 1 to the constraint that each attention weight is between 0 and 1 regardless of the other weights <ref type="bibr" target="#b16">(Martins and Astudillo, 2016;</ref><ref type="bibr" target="#b7">Kim et al., 2017)</ref>. There- fore, all the attention weights can be very low if there are no enabled domains relevant to a ground- truth so that we can disregard the irrelevant en- abled domains, and multiple attention weights can have high values when multiple enabled domains are helpful for disambiguating an input utterance. Second, we encourage each attention weight to be high if the corresponding enabled domain is a ground-truth domain and if otherwise, to be low, by a supervised attention method ( <ref type="bibr" target="#b17">Mi et al., 2016)</ref> so that the attention weights can be directly tuned for the downstream classification task. Third, we í µí±¦ " í µí±¦ + í µí±¦ ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised attention</head><p>í µí±¦: ground-truth one hot vector í µí± " &lt; í µí± + &lt; í µí± , &lt; Self-distillation apply self-distillation ( <ref type="bibr">Furlanello et al., 2018</ref>) on top of the enablement attention weights so that we can better utilize the enabled domains that are not ground-truth domains but still relevant. Evaluating on datasets obtained from real usage in a large-scale IPDA, we show that our approach significantly improves domain classification per- formance by utilizing the domain enablement in- formation effectively. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of the pro- posed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Given an input utterance, each word of the utter- ance is represented as a dense vector through word embedding followed by bidirectional long short- term memory (BiLSTM) ( <ref type="bibr" target="#b0">Graves and Schmidhuber, 2005</ref>). Then, an utterance vector is composed by concatenating the last outputs of the forward LSTM and the backward LSTM. <ref type="bibr">2</ref> To represent the domain enablement informa- tion, we obtain a weighted sum of domain enable- ment vector where the weights are calculated by logistic sigmoid function on top of the multiplica- tive attention ( <ref type="bibr" target="#b15">Luong et al., 2015</ref>) for the utterance vector and the domain enablement vectors. The attention weight of an enabled domain e is formu- <ref type="bibr">2</ref> We have also evaluated word vector summation, CNN <ref type="bibr" target="#b6">(Kim, 2014)</ref>, BiLSTM mean-pooling, and BiLSTM max- pooling ( <ref type="bibr">Conneau et al., 2017)</ref> as alternative utterance repre- sentation methods, but they did not show better performance on our task. lated as follows:</p><formula xml:id="formula_0">a e = σ (u · v e ) ,</formula><p>where u is the utterance vector, v e is the enable- ment vector of enabled domain e, and σ is sig- moid function. Compared to conventional atten- tion mechanism using softmax function, which constraints the sum of the attention weights to be 1, sigmoid attention has more expressive power, where each attention weight can be between 0 and 1 regardless of the other weights. We show that using sigmoid attention is actually more effective for improving prediction performance in Section 3.</p><p>The utterance vector and the weighted sum of the domain enablement vectors are concatenated to represent the utterance and the domain enable- ment as a single vector. Given the concatenated vector, a feed-forward neural network with a sin- gle hidden layer 3 is used to predict the confidence score by logistic sigmoid function for each do- main.</p><p>One issue of the proposed architecture is that the domain enablement can be trained to be a very strong signal, where one of the enabled domains would be the predicted domains regardless of the relevancy of the utterances to the predicted do- mains in many cases. To reduce this prediction bias, we use randomly sampled enabled domains instead of the correct enabled domains of an input utterance with 50% probability during training so that the domain enablement is used as an auxil- iary signal rather than determining signal. During inference, we always use the correct domain en- ablements of the given utterances.</p><p>The main loss function of our model is formu- lated as binary log loss between the confidence score and the ground-truth vector as follows:</p><formula xml:id="formula_1">L m = − n i=1 y i log o i + (1 − y i ) log (1 − o i ) ,</formula><p>where n is the number of all domains, o is an n-dimensional confidence score vector from the model, and y is an n-dimensional one-hot vector whose element corresponding to the position of the ground-truth domain is set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Enablement Attention</head><p>Attention weights are originally intended to be au- tomatically trained in an end-to-end fashion (Bah- danau et al., 2015), but it has been shown that applying proper explicit supervision to the atten- tion improves the downstream tasks such as ma- chine translation given the word alignment and constituent parsing given annotations between sur- face words and nonterminals <ref type="bibr" target="#b17">(Mi et al., 2016;</ref><ref type="bibr" target="#b4">Kamigaito et al., 2017)</ref>.</p><p>We hypothesize that if the ground-truth domain is one of the enabled domains, the attention weight for the ground-truth domain should be high and vice versa. To apply this hypothesis in the model training as a supervised attention method, we for- mulate an auxiliary loss function as follows:</p><formula xml:id="formula_2">L a = − e∈E y e log a e + (1 − y e ) log (1 − a e ) ,</formula><p>where E is a set of enabled domains and a e is the attention weight for the enabled domain e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Distilled Attention</head><p>One issue of supervised attention in Section 2.1 is that enabled domains that are not ground-truth domains are encouraged to have lower attention weights regardless of their relevancies to the in- put utterances and the ground-truth domains. Dis- tillation methods utilize not only the ground-truth but also all the output activations of a source model so that all the prediction information from the source model can be utilized for more effec- tive knowledge transfer between the source model and the target model ( <ref type="bibr" target="#b2">Hinton et al., 2014</ref>). Self- distillation, which trains a model leveraging the outputs of the source model with the same archi- tecture or capacity, has been shown to improve the target model's performance with a distillation method ( <ref type="bibr">Furlanello et al., 2018)</ref>.</p><p>We use a variant of self-distillation methods, where the model outputs at the previous epoch with the best dev set performance are used as the soft targets for the distillation, 4 so that the en- abled domains that are not ground-truths can also be used for the supervised attention. While con- ventional distillation methods utilize softmax acti- vations as the target values, we show that distilla- tion on top of sigmoid activations is also effective without loss of generality. The loss function for the self-distillation on the attention weights is for- mulated as follows:</p><formula xml:id="formula_3">L d = − e∈Eã e∈E˜e∈Eã e log a e + (1 − ˜ a e ) log (1 − a e ) ,</formula><p>whereãwhere˜whereã e is the attention weight of the model showing the dev set performance in the previous epochs. It is formulated as:</p><formula xml:id="formula_4">˜ a e = σ u · v e T ,</formula><p>where T is the temperature for sufficient usage of all the attention weights as the soft target. In this work, we set T to be 16, which shows the best dev set performance. We have also evaluated soft-target regulariza- tion <ref type="bibr">(Aghajanyan, 2017)</ref>, where a weighted sum of the hard ground-truth target vector and the soft target vector is used as a single target vector, but it did not show better performance than self- distillation.</p><p>All the described loss functions are added to compose a single loss function as follows:</p><formula xml:id="formula_5">L = L m + α (1 − β) L a + β t L d ,</formula><p>where α is a coefficient representing the degree of supervised enablement attention and β t denotes the degree of the self-distillation. We set α to be 0.01 in this work. Following <ref type="bibr" target="#b3">Hu et al. (2016)</ref>, β t = 1 − 0.95 t , where t denotes the current training epoch starting from 0 so that the hard ground-truth targets are more influential in the early epochs and the self-distillation is more utilized in the late epochs.  <ref type="table">Table 2</ref>: Sample utterances correctly predicted with model (4) but not with model <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model no Attention method Biased ground-truth inclusion Unbiased ground-truth inclusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our proposed model on domain clas- sification leveraging enabled domains. The en- abled domains can be a crucial disambiguat- ing signal especially when there are multiple similar domains. For example, assume that the input utterance is "what's the weather" and there are multiple weather-related domains such as NewYorkWeather, AccuWeather, and WeatherChannel. In this case, if WeatherChannel is included as an enabled do- main of the current user, it is likely to be the most relevant domain to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Following the data collection methods used in <ref type="bibr" target="#b9">Kim et al. (2018b)</ref>, our models are trained us- ing utterances with explicit invocation patterns. For example, given a user's utterance, "Ask {ZooKeeper} to {play peacock sound}," "play peacock sound" and ZooKeeper are extracted to compose a pair of the utterance and the ground- truth, respectively. In this way, we have gener- ated train, development, and test sets containing 4.4M, 500K, and 500K utterances, respectively. All the utterances are from the usage log of Ama- zon Alexa and the ground-truth of each utterance is one of 1K frequently used domains. The aver- age number of enabled domains per utterance in the test sets is 8.47.</p><p>One issue of this collected data sets is that the <ref type="bibr" target="#b13">(Laine and Aila, 2017</ref>), but we just leverage the model out- puts at the previous epoch rather than accumulating the out- puts over multiple epochs.</p><p>ground-truth is included in the enabled domains for more than 90% of the utterances, where the ground-truths are biased to enabled domains. <ref type="bibr">5</ref> For more correct and unbiased evaluation of the mod- els on the input utterances from real live traffic, we also evaluate the models on the same sized train, development, and test sets where the utterances are sampled to set the ratio of ground-truth inclusion in enabled domains to be 70%, which is closer to the ratio for actual input traffic. <ref type="table">Table 1</ref> shows the accuracies of our proposed models on the two test sets. We also show mean reciprocal rank (MRR) and top-3, accuracy 6 which is meaningful when utilizing post reranker, but we do not cover reranking issues in this paper ( <ref type="bibr" target="#b20">Robichaud et al., 2014;</ref><ref type="bibr" target="#b8">Kim et al., 2018a</ref>). From <ref type="table">Table 1</ref>, we can first see that chang- ing softmax attention to sigmoid attention signif- icantly improves the performance. This means that having more expressive power for the do- main enablement information by relaxing the soft- max constraint is effective in terms of leveraging the domain enablement information for domain classification. Along with sigmoid attention, su- pervised attention leveraging ground-truth slightly improves the performance, and supervised atten- tion combined with self-distillation shows sig- nificant performance improvement. It demon-strates that supervised domain enablement atten- tion leveraging ground-truth enabled domains is helpful, and utilizing attention information from other enabled domains is synergistic. <ref type="bibr" target="#b9">Kim et al. (2018b)</ref>'s model also adds a domain enablement bias vector to the final output, which is helpful when the ground-truth domain is one of the enabled domains. Such models <ref type="formula">(5)</ref> and <ref type="formula">(6)</ref> also show good performance for the test set where the ground-truth is one of the enabled domains with more than 90% probability. However, for the un- biased test set where the ground-truth is included in the enabled domains with a smaller probability, not adding the bias vector is shown to be better overall. <ref type="table">Table 2</ref> shows sample utterances correctly pre- dicted with model (4) but not with model <ref type="formula">(1)</ref> and (2). For the first two utterances, the ground- truths are included in the enabled domains, but there were only hundreds or fewer training in- stances whose ground-truths are CryptoPrice or Expedia. In these cases, we can see that model (1) attends to unrelated domains, model <ref type="formula">(2)</ref> attends to none of the enabled domains, but model (4), which uses supervised attention, is shown to attend to the ground-truth even without many training examples. "find my phone" has a single enabled domain which is not a ground-truth. In this case, model (1) still fully attends to the unre- lated domain because of softmax attention while model (2) and (4) do not highly attend to it so that the unrelated enabled domain is not impactive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>The word vectors are initialized with off-the-shelf GloVe vectors ( <ref type="bibr" target="#b19">Pennington et al., 2014)</ref>, and all the other model parameters are initialized with Xavier initialization <ref type="bibr">(Glorot and Bengio, 2010)</ref>. Each model is trained for 25 epochs and the pa- rameters showing the best performance on the de- velopment set are chosen as the model parameters. We use ADAM ( <ref type="bibr" target="#b10">Kingma and Ba, 2015)</ref> for the op- timization with the initial learning rate 0.0002 and the mini-batch size 128. We use gradient clipping, where the threshold is set to 5. We use a variant of LSTM, where the input gate and the forget gate are coupled and peephole connections are used <ref type="bibr">(Gers and Schmidhuber, 2000;</ref><ref type="bibr" target="#b1">Greff et al., 2017</ref>). We also use variational dropout for the LSTM regular- ization ( <ref type="bibr">Gal and Ghahramani, 2016)</ref>. All the mod- els are implemented with DyNet ( <ref type="bibr" target="#b18">Neubig et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have introduced a novel domain enablement attention mechanism improving domain classifi- cation performance utilizing domain enablement information more effectively. The proposed at- tention mechanism uses sigmoid attentions for more expressive power of the attention weights, supervised attention leveraging ground-truth in- formation for explicit guidance of the attention weight training, and self-distillation for the atten- tion supervision leveraging enabled domains that are not ground truth domains. Evaluating on utter- ances from real usage in a large-scale IPDA, we have demonstrated that our proposed model sig- nificantly improves domain classification perfor- mance by better utilizing domain enablement in- formation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Armen Aghajanyan. 2017. Softtarget regularization:</p><p>An effective technique to reduce over-fitting in neu- ral networks. In IEEE Conference on Cybernetics (CYBCONF). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture: the input utterance is represented as a dense vector through word embedding and BiLSTM. Domain enablement vector is computed as a weighted sum of enabled domain vectors through the proposed attention mechanism. The two vectors are concatenated for the final domain prediction thorough a feed-forward neural network.</figDesc></figure>

			<note place="foot" n="1"> Enabled domain information specifically refers to preferred or authenticated domains in Amazon Alexa, but it can be extended to other information such as the list of recently used domains.</note>

			<note place="foot" n="3"> We utilize scaled exponential linear units (SeLU) as the activation function for the hidden layer(Klambauer et al., 2017).</note>

			<note place="foot" n="4"> This approach is closely related to Temporal Ensembling</note>

			<note place="foot" n="5"> Since the data collection method leverages utterances where users already know the exact domain names, such domains are likely to be the enabled domains of the users. 6 Top-3 accuracy is calculated as # (utterances one of whose top three predictions is a ground-truth) / # (total utterances).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Neural Network Learning and Systems (TNNLS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Harnessing Deep Neural Networks with Logic Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2410" to="2420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised attention for sequence-to-sequence constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Joint learning of domain classification and out-of-domain detection with dynamic class weighting for satisficing false acceptance rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1292" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable neural shortlisting-reranking approach for large-scale domain classification in natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Kyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient Large-Scale Neural Domain Classification with Personalized Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjishnu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjishnu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Hoffmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Gandhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Filiminov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariya</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnika</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Conversational AI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernandez Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised Attentions for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2283" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The Dynamic Neural Network Toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hypotheses ranking for robust domain classification and tracking in dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Robichaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">Zia</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page" from="145" to="149" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The technology behind personal digital assistants: An overview of the system architecture and key components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE Signal Processing Magazine</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An overview of endto-end language understanding and dialog management for personal digital assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Robichaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">Zia</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Spoken Language Understanding: Systems for Extracting Semantic Information from Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>De Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
