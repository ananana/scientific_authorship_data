<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. After-wards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gat-ed recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document level sentiment classification is a fun- damental task in sentiment analysis, and is cru- cial to understand user generated content in so- cial networks or product reviews <ref type="bibr" target="#b30">(Manning and Schütze, 1999;</ref><ref type="bibr" target="#b19">Jurafsky and Martin, 2000;</ref><ref type="bibr" target="#b36">Pang and Lee, 2008</ref>; Liu, 2012). The task calls for iden- tifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches fol- low ( <ref type="bibr" target="#b37">Pang et al., 2002</ref>) and exploit machine learn-ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features ( <ref type="bibr" target="#b40">Qu et al., 2010;</ref><ref type="bibr" target="#b34">Paltoglou and Thelwall, 2010)</ref> or learning discriminate features from data, since the performance of a machine learner is heavily de- pendent on the choice of data representation .</p><p>Document level sentiment classification re- mains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of documen- t. This is crucial for sentiment classification be- cause relations like "contrast" and "cause" have great influences on determining the meaning and the overall polarity of a document. However, ex- isting studies typically fail to effectively capture such information. For example, <ref type="bibr" target="#b37">Pang et al. (2002)</ref> and <ref type="bibr" target="#b48">Wang and Manning (2012)</ref> represent docu- ments with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its "sparse" and "discrete" char- acteristics make it clumsy in taking into account of side information like relations between sentences. Recently, <ref type="bibr" target="#b24">Le and Mikolov (2014)</ref> exploit neural networks to learn continuous document represen- tation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it out in a se- quential, bottom-up fashion, analyze the meanings of sentences before considering semantic relation- s between them. This motivates us to develop an end-to-end and bottom-up algorithm to effectively model document representation.</p><p>In this paper, we introduce a neural network ap- proach to learn continuous document representa- tion for sentiment classification. The method is on the basis of the principle of compositionality <ref type="bibr">(Frege, 1892)</ref>, which states that the meaning of a longer expression (e.g. a sentence or a docu-  <ref type="figure">Figure 1</ref>: The neural network model for document level sentiment classification. w n i stands for the i-th word in the n-th sentence, l n is sentence length. ment) depends on the meanings of its constituents. Specifically, the approach models document rep- resentation in two steps. In the first step, it us- es convolutional neural network (CNN) or long short-term memory (LSTM) to produce sentence representations from word representations. After- wards, gated recurrent neural network is exploit- ed to adaptively encode semantics of sentences and their inherent relations in document represen- tations. These representations are naturally used as features to classify the sentiment label of each document. The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification 2 .</p><p>We conduct document level sentiment classi- fication on four large-scale review datasets from IMDB 3 and Yelp Dataset Challenge <ref type="bibr">4</ref> . We com- pare to neural network models such as paragraph vector ( <ref type="bibr" target="#b24">Le and Mikolov, 2014</ref>), convolutional neu- ral network, and baselines such as feature-based SVM ( <ref type="bibr" target="#b37">Pang et al., 2002</ref>), recommendation algo- rithm JMARS ( <ref type="bibr" target="#b7">Diao et al., 2014</ref>). Experimental results show that: (1) the proposed neural model shows superior performances over all baseline al- gorithms; (2) gated recurrent neural network dra- matically outperforms standard recurrent neural <ref type="bibr">2</ref> A similar work can be found at:</p><p>http: //deeplearning.net/tutorial/lstm.html 3 http://www.imdb.com/ 4 http://www.yelp.com/dataset_challenge network in document modeling. The main con- tributions of this work are as follows:</p><p>• We present a neural network approach to en- code relations between sentences in document rep- resentation for sentiment classification.</p><p>• We report empirical results on four large-scale datasets, and show that the approach outperforms state-of-the-art methods for document level senti- ment classification.</p><p>• We report empirical results that traditional re- current neural network is weak in modeling docu- ment composition, while adding neural gates dra- matically improves the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Approach</head><p>We introduce the proposed neural model in this section, which computes continuous vector repre- sentations for documents of variable length. These representations are further used as features to clas- sify the sentiment label of each document. An overview of the approach is displayed in <ref type="figure">Figure 1</ref>.</p><p>Our approach models document semantics based on the principle of compositionality <ref type="bibr">(Frege, 1892)</ref>, which states that the meaning of a longer expression (e.g. a sentence or a document) comes from the meanings of its constituents and the rules used to combine them. Since a document consist- s of a list of sentences and each sentence is made up of a list of words, the approach models docu- ment representation in two stages. It first produces continuous sentence vectors from word represen-tations with sentence composition (Section 2.1). Afterwards, sentence vectors are treated as inputs of document composition to get document repre- sentation (Section 2.2). Document representations are then used as features for document level senti- ment classification (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Composition</head><p>We first describe word vector representation, be- fore presenting a convolutional neural network with multiple filters for sentence composition.</p><p>Each word is represented as a low dimension- al, continuous and real-valued vector, also known as word embedding ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>. Al- l the word vectors are stacked in a word embed- ding matrix L w ∈ R d×|V | , where d is the dimen- sion of word vector and |V | is vocabulary size. These word vectors can be randomly initialized from a uniform distribution <ref type="bibr" target="#b42">(Socher et al., 2013b</ref>), or be pre-trained from text corpus with embedding learning algorithms ( <ref type="bibr" target="#b32">Mikolov et al., 2013;</ref><ref type="bibr" target="#b39">Pennington et al., 2014;</ref><ref type="bibr" target="#b45">Tang et al., 2014</ref>). We adopt the latter strategy to make better use of semantic and grammatical associations of words.</p><p>We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with se- mantic composition. CNN and LSTM are state- of-the-art semantic composition models for senti- ment classification <ref type="bibr" target="#b21">(Kim, 2014;</ref><ref type="bibr" target="#b18">Johnson and Zhang, 2015;</ref><ref type="bibr" target="#b25">Li et al., 2015a</ref>). They learn fixed-length vectors for sen- tences of varying length, captures words order in a sentence and does not depend on external de- pendency or constituency parse results. One could also use tree-based composition method such as s and trigrams in a sentence. Each filter consists of a list of linear layers with shared parameter- s. Formally, let us denote a sentence consisting of n words as {w 1 , w 2 , ...w i , ...w n }, let l c be the width of a convolutional filter, and let W c , b c be the shared parameters of linear layers in the fil- ter. Each word w i is mapped to its embedding representation e i ∈ R d . The input of a linear lay- er is the concatenation of word embeddings in a fixed-length window size l c , which is denoted as</p><formula xml:id="formula_0">I c = [e i ; e i+1 ; ...; e i+lc−1 ] ∈ R d·lc .</formula><p>The output of a linear layer is calculated as</p><formula xml:id="formula_1">O c = W c · I c + b c (1)</formula><p>where W c ∈ R loc×d·lc , b c ∈ R loc , l oc is the output length of linear layer. To capture global semantics of a sentence, we feed the outputs of linear layers to an average pooling layer, resulting in an output vector with fixed-length. We further add hyperbol- ic tangent (tanh) to incorporate pointwise nonlin- earity, and average the outputs of multiple filters to get sentence representation. We also try lstm as the sentence level semantic calculator, the performance comparison between these two variations is given in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document Composition with Gated Recurrent Neural Network</head><p>The obtained sentence vectors are fed to a docu- ment composition component to calculate the doc- ument representation. We present a gated recur- rent neural network approach for document com- position in this part. Given the vectors of sentences of variable length as input, document composition produces a fixed-length document vector as output. To this end, a simple strategy is ignoring the order of sen-</p><formula xml:id="formula_2">GNN Softmax GNN GNN í µí± 1 í µí± 2 í µí± í µí± Softmax Average (a) GatedNN (b) GatedNN Avg GNN GNN GNN í µí± 1 í µí± 2 í µí± í µí±</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…… ……</head><p>Figure 3: Document modeling with gated recurrent neural network. GNN stands for the basic computa- tional unit of gated recurrent neural network.</p><p>tences and averaging sentence vectors as docu- ment vector. Despite its computational efficiency, it fails to capture complex linguistic relations (e.g. "cause" and "contrast") between sentences. Con- volutional neural network <ref type="bibr" target="#b6">(Denil et al., 2014</ref>) is an alternative for document composition, which mod- els local sentence relations with shared parameters of linear layers. Standard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector s t with the output vector of the previous step h t−1 . The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh.</p><formula xml:id="formula_3">h t = tanh(W r · [h t−1 ; s t ] + b r )<label>(2)</label></formula><p>where </p><formula xml:id="formula_4">W r ∈ R l h ×(l h +loc) , b r ∈ R l h , l</formula><formula xml:id="formula_5">i t = sigmoid(W i · [h t−1 ; s t ] + b i )<label>(3)</label></formula><formula xml:id="formula_6">f t = sigmoid(W f · [h t−1 ; s t ] + b f )<label>(4)</label></formula><formula xml:id="formula_7">g t = tanh(W r · [h t−1 ; s t ] + b r )<label>(5)</label></formula><formula xml:id="formula_8">h t = tanh(i t g t + f t h t−1 )<label>(6)</label></formula><p>where stands for element-wise multiplication,</p><formula xml:id="formula_9">W i , W f , b i , b</formula><p>f adaptively select and remove histo- ry vector and input vector for semantic composi- tion. The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to dis- carding any part of the semantics of sentences to get a better document representation. <ref type="figure">Figure 3</ref> (a) displays a standard sequential way where the last hidden vector is regarded as the document rep- resentation for sentiment classification. We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical seman- tics with different granularities. The method is il- lustrated in <ref type="figure">Figure 3 (b)</ref>, which shares some char- acteristics with ( <ref type="bibr" target="#b55">Zhao et al., 2015)</ref>. We can go one step further to use preceding histories and fol- lowing evidences in the same way, and exploit bi- directional ( <ref type="bibr" target="#b14">Graves et al., 2013</ref>) gated RNN as the calculator. The model is embedded in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentiment Classification</head><p>The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineer- ing. Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a sof tmax layer to convert real values to con- ditional probabilities, which is calculated as fol- lows.</p><formula xml:id="formula_10">P i = exp(x i ) C i =1 exp(x i )<label>(7)</label></formula><p>We conduct experiments in a supervised learn- ing setting, where each document in the training data is accompanied with its gold sentiment label. For model training, we use the cross-entropy er- ror between gold sentiment distribution P g (d) and predicted sentiment distribution P (d) as the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><note type="other">#docs #s/d #w/d |V | #class Class Distribution Yelp 2013 335,</note><formula xml:id="formula_11">loss = − d∈T C i=1 P g i (d) · log(P i (d)) (8)</formula><p>where T is the training data, C is the number of classes, d represents a document. P g (d) has a 1-of-K coding scheme, which has the same dimension as the number of classes, and only the dimension corresponding to the ground truth is 1, with all others being 0. We take the deriva- tive of loss function through back-propagation with respect to the whole set of</p><formula xml:id="formula_12">parameters θ = [W c ; b c ; W i ; b i ; W f ; b f ; W r ; b r ; W sof tmax , b sof tmax ],</formula><p>and update parameters with stochastic gradient descent. We set the widths of three convolutional filters as 1, 2 and 3, output length of convolutional filter as 50. We learn 200-dimensional word em- beddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U (−0.01, 0.01), and set learning rate as 0.03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>We conduct experiments to empirically evaluate our method by applying it to document level senti- ment classification. We describe experimental set- tings and report empirical results in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setting</head><p>We  <ref type="bibr" target="#b30">and Schütze, 1999;</ref><ref type="bibr" target="#b19">Jurafsky and Martin, 2000</ref>) and M SE ( <ref type="bibr" target="#b7">Diao et al., 2014)</ref> as evaluation metrics, where accuracy is a stan- dard metric to measure the overall sentiment clas- sification performance. We use MSE to measure the divergences between predicted sentiment la- bels and ground truth sentiment labels because re- view labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive).</p><formula xml:id="formula_13">M SE = N i (gold i − predicted i ) 2 N<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline Methods</head><p>We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline meth- ods for document level sentiment classification.</p><p>(1) Majority is a heuristic baseline, which as- signs the majority sentiment label in training set to each document in test set.</p><p>(2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with <ref type="bibr">LibLinear (Fan et al., 2008)</ref>   (4) In AverageSG, we learn 200-dimensional word vectors with word2vec 6 ( <ref type="bibr" target="#b32">Mikolov et al., 2013)</ref>, average word embeddings to get document representation, and train a SVM classifier.</p><p>(5) We learn sentiment-specific word embed- dings (SSWE), and use max/min/average pooling ( <ref type="bibr" target="#b45">Tang et al., 2014</ref>) to get document representation.</p><p>(6) We compare with a state-of-the-art recom- mendation algorithm JMARS ( <ref type="bibr" target="#b7">Diao et al., 2014</ref>), which utilizes user and aspects of a review with collaborative filtering and topic modeling.</p><p>(7) We implement a convolutional neural net- work (CNN) baseline as it is a state-of-the-art se- mantic composition method for sentiment analysis <ref type="bibr" target="#b21">(Kim, 2014;</ref><ref type="bibr" target="#b6">Denil et al., 2014)</ref>. <ref type="formula">(8)</ref> We implement a state-of-the-art neural net- work baseline Paragraph Vector ( <ref type="bibr" target="#b24">Le and Mikolov, 2014</ref>) because its codes are not officially provided. Window size is tuned on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison to Other Methods</head><p>Experimental results are given in <ref type="table" target="#tab_5">Table 2</ref>. We e- valuate each dataset with two metrics, namely ac- curacy (higher is better) and MSE (lower is better). The best method in each dataset and each evalua- tion metric is in bold.</p><p>From <ref type="table" target="#tab_5">Table 2</ref>, we can see that majority is the worst method because it does not capture any tex- tual semantics. SVM classifiers with unigram and bigram features ( <ref type="bibr" target="#b37">Pang et al., 2002</ref>) are extremely strong, which are almost the strongest performers among all baseline methods. Designing complex features are also effective for document level sen- timent classification, however, it does not surpass the bag-of-ngram features significantly as on Twit- ter corpora ( <ref type="bibr" target="#b22">Kiritchenko et al., 2014)</ref>. Further- more, the aforementioned bag-of-features are dis- crete and sparse. For example, the feature dimen- sion of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features. Based on them, we try to con- catenate several discourse-driven features, but the classification performances remain unchanged.</p><p>AverageSG is a straight forward way to com- pose document representation without feature en- gineering. Unfortunately, we can see that it does not work in this scenario, which appeals for pow- erful semantic composition models for documen- t level sentiment classification. We try to make better use of the sentiment information to learn a better SSWE ( <ref type="bibr" target="#b45">Tang et al., 2014</ref>), e.g. setting a large window size. However, its performance is still worse than context-based word embedding. This stems from the fact that there are many sen- timent shifters (e.g. negation or contrast words) in document level reviews, while <ref type="bibr" target="#b45">Tang et al. (2014)</ref> learn SSWE by assigning sentiment label of a tex- t to each phrase it contains. How to learn SSWE effectively with document level sentiment super- vision remains as an interesting future work.</p><p>Since JMARS outputs real-valued outputs, we only evaluate it in terms of M SE. We can see that sophisticated baseline methods such as JMARS, paragraph vector and convolutional NN obtain sig- nificant performance boosts over AverageSG by  capturing deeper semantics of texts. Comparing between CNN and AverageSG, we can conclude that deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents. However, it is somewhat disappoint- ing that these models do not significantly outper- form discrete bag-of-ngrams and bag-of-features.</p><note type="other">GatedNN 0.636 0.58 0.656 0.52 0.651 0.51 0.430 2.95 GatedNN Avg 0.635 0.57 0.659 0.52 0.657 0.56 0.416 2.78 Bi GatedNN Avg 0.637 0.56 0.655 0.51 0.660 0.50 0.425 2.71</note><p>The reason might lie in that semantic meanings of documents, e.g. relations between sentences, are not well captured. We can see that the proposed method Conv-GRNN and LSTM-GRNN yield the best performance on all four datasets in two evalu- ation metrics. Compared with CNN, Conv-GRNN shows its superior power in document composi- tion component, which encodes semantics of sen- tences and their relations in document representa- tion with gated recurrent neural network. We al- so find that LSTM (almost) consistently performs better than CNN in modeling the sentence repre- sentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Analysis</head><p>As discussed before, document composition con- tributes a lot to the superior performance of Conv- GRNN and LSTM-GRNN. Therefore, we take Conv-GRNN as an example and compare differen- t neural models for document composition in this part. Specifically, after obtaining sentence vectors with convolutional neural network as described in Section 2.1, we carry out experiments in following settings.</p><p>(1) Average. Sentence vectors are averaged to get the document vector.</p><p>(2) Recurrent / GatedNN. Sentence vectors are fed to standard (or gated) recurrent neural network in a sequential way from the beginning of the input document. The last hidden vector is regarded as document representation.</p><p>(3) Recurrent Avg / GatedNN Avg. We extend setting <ref type="formula" target="#formula_3">(2)</ref> by averaging hidden vectors of recur- rent neural network as document vector.</p><p>(4) Bi Recurrent Avg / Bi GatedNN Avg. We ex- tend setting (3) by calculating hidden vectors from both preceding histories and following contexts. Bi-directional hidden vectors are averaged as doc- ument representation. <ref type="table" target="#tab_7">Table 3</ref> shows the experimental results. We can see that standard recurrent neural network (RN- N) is the worst method, even worse than the sim- ple vector average. This is because RNN suf- fers from the vanishing gradient problem, stating that the influence of a given input on the hidden layer decays exponentially over time on the net- work output. In this paper, it means that doc- ument representation encodes rare semantics of the beginning sentences. This is further justified by the great improvement of Recurrent Avg over Recurrent. Bi Recurrent Avg and Recurrent Avg perform comparably, but disappointingly both of them fail to transcend Average. After adding neu- ral gates, GatedNN obtains dramatic accuracy im- provements over Recurrent and significantly out- performs previous settings. The results indicate that Gated RNN is capable of handling the van- ishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Document level sentiment classification is a fun- damental problem in sentiment analysis <ref type="bibr" target="#b36">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b28">Liu, 2012)</ref>, which aims at identifying the sentiment label of a document ( <ref type="bibr" target="#b37">Pang et al., 2002;</ref><ref type="bibr" target="#b47">Turney, 2002</ref>). <ref type="bibr" target="#b37">Pang and Lee (2002;</ref><ref type="bibr" target="#b35">2005)</ref> cast this problem as a classification task, and use machine learning method in a supervised learning framework. <ref type="bibr" target="#b47">Turney (2002)</ref> introduces an unsuper- vised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. <ref type="bibr" target="#b13">Goldberg and Zhu (2006)</ref> place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow <ref type="bibr" target="#b37">Pang et al. (2002)</ref> and work on designing effective features for building a powerful sentiment classifier. Rep- resentative features include word ngrams ( <ref type="bibr" target="#b48">Wang and Manning, 2012</ref>), text topic ( <ref type="bibr" target="#b11">Ganu et al., 2009)</ref>, bag-of-opinions ( <ref type="bibr" target="#b40">Qu et al., 2010)</ref>, syntactic rela- tions ( <ref type="bibr" target="#b49">Xia and Zong, 2010)</ref>, sentiment lexicon fea- tures ( <ref type="bibr" target="#b22">Kiritchenko et al., 2014</ref>).</p><p>Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and or- ganize the discriminative information from data ( . Recently, neural network e- merges as an effective way to learn continuous text representation for sentiment classification. Exist- ing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embed- ding learning algorithms typically leverage con- texts of words in a context-prediction way <ref type="bibr" target="#b2">(Bengio et al., 2003;</ref><ref type="bibr" target="#b32">Mikolov et al., 2013;</ref><ref type="bibr" target="#b0">Baroni et al., 2014</ref>). Since these methods typically map word- s with similar contexts but opposite polarity (e.g. "good" and "bad") to neighboring vectors, sever- al studies <ref type="bibr" target="#b29">(Maas et al., 2011;</ref><ref type="bibr" target="#b23">Labutov and Lipson, 2013;</ref><ref type="bibr" target="#b45">Tang et al., 2014</ref>) learn sentiment-specific word embeddings by taking sentiment of texts in- to account. Another line of research concentrates on semantic composition <ref type="bibr" target="#b33">(Mitchell and Lapata, 2010)</ref>. <ref type="bibr" target="#b52">Yessenalina and Cardie (2011)</ref>   <ref type="bibr" target="#b27">(Li, 2014)</ref>, deep recursive layer ( <ref type="bibr" target="#b17">Irsoy and Cardie, 2014)</ref>, adaptive composi- tion functions ( <ref type="bibr" target="#b8">Dong et al., 2014)</ref>, combined with Combinatory Categorial Grammar ( <ref type="bibr">Hermann and Blunsom, 2013)</ref>, and used for opinion relation de- tection ( ). <ref type="bibr" target="#b12">Glorot et al. (2011)</ref> use s- tacked denoising autoencoder. Convolutional neu- ral networks are widely used for semantic compo- sition <ref type="bibr" target="#b21">(Kim, 2014;</ref><ref type="bibr" target="#b6">Denil et al., 2014;</ref><ref type="bibr" target="#b18">Johnson and Zhang, 2015)</ref> by auto- matically capturing local and global semantics. <ref type="bibr" target="#b24">Le and Mikolov (2014)</ref> introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural net- work or long short-term memory (LSTM) are also verified as strong approaches for semantic compo- sition ( <ref type="bibr" target="#b25">Li et al., 2015a)</ref>.</p><p>In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations. A recent work in ( <ref type="bibr" target="#b26">Li et al., 2015b)</ref> also investigate LSTM to model document meaning. They verify the effectiveness of LSTM in text generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce neural network models (Conv- GRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in doc- ument representation, and is effectively trained end-to-end with supervised sentiment classifica- tion objectives. We conduct extensive experiments on four review datasets with two evaluation met- rics. Empirical results show that our approaches achieve state-of-the-art performances on all these datasets. We also find that (1) traditional recurren- t neural network is extremely weak in modeling document composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in mod- eling sentence representation.</p><p>We briefly discuss some future plans. How to effectively compose sentence meanings to docu- ment meaning is a central problem in natural lan- guage processing. In this work, we develop neu- ral models in a sequential way, and encode sen- tence semantics and their relations automatically without using external discourse analysis result- s. From one perspective, one could carefully de- fine a set of sentiment-sensitive discourse relation- s ( <ref type="bibr" target="#b56">Zhou et al., 2011</ref>), such as "contrast", "condi- tion", "cause", etc. Afterwards, relation-specific gated RNN can be developed to explicitly mod- el semantic composition rules for each relation <ref type="bibr" target="#b41">(Socher et al., 2013a</ref>). However, defining such a relation scheme is linguistic driven and time con- suming, which we leave as future work. From an- other perspective, one could compose document representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network ( <ref type="bibr" target="#b42">Socher et al., 2013b</ref>) and Struc- tured LSTM <ref type="bibr" target="#b44">(Tai et al., 2015;</ref><ref type="bibr" target="#b57">Zhu et al., 2015)</ref> can be used as composition algorithms. Howev- er, existing discourse structure learning algorithm- s are difficult to scale to massive review texts on the web. How to simultaneously learn document structure and composition function is an interest- ing future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentence composition with convolutional neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>represent each word as a matrix and use iterated matrix mul- tiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recur- sive neural networks for sentence-level semantic composition. Recursive neural network is extend- ed with global feedbackward (Paulus et al., 2014), feature weight tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 1 .</head><label>1</label><figDesc>We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and test- ing sets with 80/10/10. We run tokenization and sentence splitting with Stanford CoreNLP (Man- ning et al., 2014) on all these datasets. We use accuracy (Manning</figDesc><table>conduct experiments on large-scale datasets 
consisting of document reviews. Specifically, we 
use one movie review dataset from IMDB (Diao 
et al., 2014) and three restaurant review dataset-
s from Yelp Dataset Challenge in 2013, 2014 and 
2015. Human labeled review ratings are regarded 
as gold standard sentiment labels, so that we do 
not need to manually annotate sentiment labels of 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sentiment classification on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are 
accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Sentiment classification on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accu-
racy (higher is better) and MSE (lower is better). The best method in each setting is in bold. 

</table></figure>

			<note place="foot" n="5"> We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier.</note>

			<note place="foot" n="6"> We use Skipgram as it performs slightly better than CBOW in the experiment. We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions. We also would like to thank three anonymous reviewer-s for their valuable comments and suggestions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning. Book in preparation for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Gated feedback recurrent neural networks. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modelling, visualising and summarising documents with a single convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno>arXiv preprint:1406.3830</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1537" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gottlob Frege. 1892. On sense and reference</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="563" to="584" />
			<pubPlace>Ludlow</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond the stars: Improving rating predictions using review text content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayatree</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amélie</forename><surname>Marian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seeing stars when there aren&apos;t many stars: graph-based semisupervised learning for sentiment categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GraphBased Method for NLP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speech &amp; language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James H Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentiment analysis of short informal texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="723" to="762" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eudard</forename><surname>Hovy</surname></persName>
		</author>
		<idno>iv:1503.00185</idno>
		<title level="m">When are tree structures necessary for deep learning of representations? arXiv preprint arX</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>arX- iv:1506.01057</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Feature weight tuning for recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno>1412.3714</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A study of information retrieval weighting schemes for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1386" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Global belief recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2888" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The bag-of-opinions method for review rating prediction from sparse text patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="913" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1014" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Exploring the use of word relation features for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1336" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Joint opinion relation detection using one-class deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention. ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Do users rate or review?: boost phrase-level sentiment labeling with review-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1027" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanjun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Long short-term memory over tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
