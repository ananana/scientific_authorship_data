<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dataset for Document Grounded Conversations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangyan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Dataset for Document Grounded Conversations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="708" to="713"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a document grounded dataset for conversations. We define &quot;Docu-ment Grounded Conversations&quot; as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>At present, dialog systems are considered to be ei- ther task-oriented, where a specific task is the goal of the conversation (e.g. getting bus information or weather for a particular location); or non-task oriented where conversations are more for the sake of themselves, be it entertainment or passing the time. Ultimately, we want our agents to smoothly interleave between task-related information flow and casual chat for the given situation. There is a dire need of a dataset which caters to both these objectives. <ref type="bibr" target="#b6">Serban et al. (2015)</ref> provide a comprehensive list of available datasets for building end-to-end conversational agents. Datasets based on movie scripts ( <ref type="bibr" target="#b5">Lison and Tiedemann, 2016</ref>; Danescu- Niculescu-Mizil and Lee, 2011a) contain artifi- cial conversations. The Ubuntu Dialogue Corpus ( <ref type="bibr" target="#b6">Lowe et al., 2015</ref>) is based on technical support logs from the Ubuntu forum. The Frames dataset <ref type="bibr" target="#b0">(Asri et al., 2017</ref>) was collected to solve the prob- lem of frame tracking. These datasets do not pro- vide grounding of the information presented in the conversations. <ref type="bibr" target="#b13">Zhang et al. (2018)</ref> focuses on per- sonas in dialogues: each worker has a set of pre- defined facts about the persona that they can talk about. Most of these datasets lack conversations with large number of on-topic turns.</p><p>We introduce a new dataset which addresses the concerns of grounding in conversation responses, context and coherence in responses. We present a dataset which has real human conversations with grounding in a document. Although our exam- ples use Wikipedia articles about movies, we see the same techniques being valid for other external documents such as manuals, instruction booklets, and other informational documents. We build a generative model with and without the document information and find that the responses generated by the model with the document information is more engaging (+7.5% preference) and more flu- ent (+0.96 MOS). The perplexity also shows a 11.69 point improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Document Grounded Dataset</head><p>To create a dataset for document grounded con- versations, we seek the following things: (1) A set of documents (2) Two humans chatting about the content of the document for more than 12 turns. We collected conversations about the documents through Amazon Mechanical Turk (AMT). We re- strict the topic of the documents to be movie- related articles to facilitate the conversations. We initially experimented with different potential do- mains. Since movies are engaging and widely known, people actually stay on task when dis- cussing them. In fact in order to make the task interesting, we offered a choice of movies to the participants so that they are invested in the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Set Creation</head><p>We choose Wikipedia (Wiki) 1 articles to cre- ate a set of documents D = {d 1 , . . . , d 30 } for grounding of conversations. We randomly select 30 movies, covering various genres like thriller, super-hero, animation, romantic, biopic etc. We extract the key information provided in the Wiki article and divide it into four separate sections. This was done to reduce the load of the users to read, absorb and discuss the information in the document. Hence, each movie document d i con- sists of four sections {s 1 , s 2 , s 3 , s 4 } correspond- ing to basic information and three key scenes of the movie. The basic information section s 1 con- tains data from the Wikipedia article in a stan- dard form such as year, genre, director. It also includes a short introduction about the movie, rat- ings from major review websites, and some crit- ical responses. Each of the key scene sections {s 2 , s 3 , s 4 } contains one short paragraph from the plot of the movie. Each paragraph contains on an average 7 sentences and 143 words. These para- graphs were extracted automatically from the orig- inal articles, and were then lightly edited by hand to make them of consistent size and detail. An ex- ample of the document is attached in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Creation</head><p>To create a dataset of conversations which uses the information from the document, involves the par- ticipation of two workers. Hence, we explore two scenarios: (1) Only one worker has access to the document and the other worker does not and <ref type="formula" target="#formula_1">(2)</ref> Both the workers have access to the document. In both settings, they are given the common instruc- tions of chatting for at least 12 turns.</p><p>Scenario 1: One worker has document. In this scenario, only one worker has access to the docu- ment. The other worker cannot see the document. The instruction to the worker with the document is: Tell the other user what the movie is, and try to persuade the other user to watch/not to watch the movie using the information in the document; and the instruction to the worker without the doc- ument is: After you are told the name of the movie, pretend you are interested in watching the movie, and try to gather all the information you need to make a decision whether to watch the movie in the end. An example of part of the dialogue for this <ref type="bibr">1</ref> https://en.wikipedia.org user2: Hey have you seen the inception? user1: No, I have not but have heard of it.</p><p>What is it about user2: It's about extractors that perform experiments using military technology on people to retrieve info about their targets.  scenario is shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Scenario 2: Both workers have document. In this scenario, both the workers have access to the same Wiki document. The instruction given to the workers are: Discuss the content in the doc- ument with the other user, and show whether you like/dislike the movie. An example of the dialogue for this scenario is shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Workflow: When the two workers enter the chat-room, they are initially given only the first section on basic information s 1 of the document d i . After the two workers complete 3 turns (for the first section 6 turns is needed due to initial greet- ings), the users will be shown the next section. The users are encouraged to discuss information in the new section, but are not constrained to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset Statistics</head><p>The dataset consists of total 4112 conversations with an average of 21.43 turns. The number of conversations for scenario 1 is 2128 and for sce- nario 2 it is 1984. We consider a turn to be an exchange between two workers (say w1 and w 2 ).</p><p>Hence an exchange of w 1 , w 2 , w 1 has 2 turns (w 1 , w 2 ) and (w 2 , w 1 ). We show the comparison of our dataset as CMU Document Grounded Conversa- tions (CMU DoG) with other datasets in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p># Utterances Avg. # of Turns CMU DoG 130000 31 Persona-chat ( <ref type="bibr" target="#b13">Zhang et al., 2018)</ref> 164,356 14 Cornell Movie (Danescu-Niculescu-Mizil and Lee, 2011b) 304,713</p><p>1.38 Frames dataset ( <ref type="bibr" target="#b0">Asri et al., 2017)</ref> 19,986 15  One of the salient features of CMUDoG dataset is that it has mapping of the conversation turns to each section of the document, which can then be used to model conversation responses. Another useful aspect is that we report the quality of the conversations in terms of how much the conversa- tion adheres to the information in the document.</p><p>Split Criteria: We automatically measure the quality of the conversations using BLEU <ref type="bibr" target="#b8">(Papineni et al., 2002</ref>) score. We use BLEU because we want to measure the overlap of the turns of the conversation with the sections of the docu- ment. Hence, a good quality conversation should use more information from the document than a low quality conversation. We divide our dataset into three ratings based on this measure. The BLEU score is calculated between all the utter- ances {x 1 , . . . , x n } of a conversation C i and the document d i corresponding to C i . We eliminate incomplete conversations that have less than 10 turns. The percentiles for the remaining conver- sations are shown in  <ref type="table" target="#tab_4">Table 5</ref>: The distribution of BLEU score for conversa- tions with more than 10 turns.</p><p>Rating 1: Conversations are given a rating of 1 if their BLEU score is less than or equal to 0.1. We consider these conversations to be of low-quality.</p><p>Rating 2: All the conversations that do not fit in rating 1 and 3 are marked with a rating of 2.</p><p>Rating 3: Conversations are labeled with a rat- ing of 3, only if the conversation has more than 12 turns and has a BLEU score larger than 0.587. This threshold was calculated by summing the mean (0.385) and the standard deviation (0.202) of BLEU scores of the conversations that do not belong rating 1. The average BLEU score for workers who have access to the document is 0.22 whereas the aver- age BLEU score for the workers without access to the document is 0.03. This suggests that even if the workers had external knowledge about the movie, they have not extensively used it in the conversa- tion. It also suggests that the workers with the doc- ument have not used the information from the doc- ument verbatim in the conversation. <ref type="table" target="#tab_3">Table 4</ref> shows the statistics on the total number of conversations, utterances, and average number of utterances per conversation and average length of utterances for all the three ratings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>In this section we discuss models which can lever- age the information from the document for gener- ating responses. We explore generative models for this purpose. Given a dataset X = {x 0 , . . . , x n } of utterances in a conversation C i , we consider two settings: (1) to generate a response x i+1 when given only the current utterance x i and (2) to gen- erate a response x i+1 when given the correspond- ing section s i and the previous utterance x i .</p><p>Without section: We use the sequence-to- sequence model <ref type="bibr" target="#b12">(Sutskever et al., 2014</ref>) to build our baseline model. Formally, let θ E represent the parameters of the encoder. Then the representation h x i of the current utterance x i is given by:</p><formula xml:id="formula_0">h x i = Encoder(x i ; θ E )<label>(1)</label></formula><p>Samples of x i+1 are generated as follows:</p><formula xml:id="formula_1">p(ˆ x|h x i ) = t p(ˆ x t |ˆx|ˆx &lt;t , h x i )<label>(2)</label></formula><p>where, ˆ x &lt;t are the tokens generated beforê x t . We also use global attention ( <ref type="bibr" target="#b7">Luong et al., 2015</ref>) with copy mechanism ( <ref type="bibr" target="#b9">See et al., 2017</ref>) to guide our generators to replace the unknown (UNK) tokens. We call this model SEQ.</p><p>With section: We extend the sequence-to- sequence framework to include the section s i cor- responding the current turn. We use the same en- coder to encode both the utterance and the section. We get the representation h x i of the current utter- ance x i using Eq. 1. The representation of the section is given by:</p><formula xml:id="formula_2">h s i = Encoder(s i ; θ E )<label>(3)</label></formula><p>The input at each time step t to the generative model is given by h t = [x t−1 ; h s ], where x t−1 is the embedding of the word at the previous time step. We call this model SEQS.</p><p>Experimental Setup: For both SEQ and SEQS model, we use a two-layer bidirectional LSTM as the encoder and a LSTM as the decoder. The dropout rate of the LSTM output is set to be 0.3. The size of hidden units for both LSTMs is 300. We set the word embedding size to be 100, since the size of vocabulary is relatively small 2 . The models are trained with adam ( <ref type="bibr" target="#b4">Kingma and Ba, 2014</ref>) optimizer with learning rate 0.001 until they converge on the validation set for the perplexity criteria. We use beam search with size 5 for re- sponse generation. We use all the data (i.e all the conversations regardless of the rating and sce- nario) for training and testing. The proportion of train/validation/test split is 0.8/0.05/0.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In what follows, we first present an analysis of the dataset, then provide an automatic metric for evaluation of our models-perplexity and finally present the results of human evaluation of the gen- erated responses for engagement and fluency.</p><p>scenario NW LT 1 0.78 12.85 2 5.84 117.12 Dataset analysis: We perform two kinds of au- tomated evaluation to investigate the usefulness of the document in the conversation. The first one is to investigate if the workers use the informa- tion from the document in the conversation. The second analysis is to show that the document adds value to the conversation. Let the set of tokens in the current utterance x i be N , the set of tokens in the current section s i be M , the set of tokens in the previous three utterances be H, and the set of stop words be S. In scenario 1, we calculate the set operation (NW) as</p><formula xml:id="formula_3">|((N ∩ M ) \ H) \ S|.</formula><p>Let the tokens that appear in all the utterances (x i , . . . , x i+k ) corresponding to the current sec- tion s i be K and the tokens that appear in all the utterances (x i , . . . , x i+p ) corresponding to the pre- vious section s i−1 be P . In scenario 2, we calcu- late the set operation (NW) as |((K ∩M )\P )\S|.</p><p>The results in <ref type="table" target="#tab_5">Table 6</ref> show that people use the in- formation in the new sections and are not fixated on old sections. It also shows that they use the information to construct the responses.</p><p>Perplexity: To automatically evaluate the flu- ency of the models, we use perplexity measure.</p><p>We build a language model on the train set of re- sponses using ngrams up to an order of 3 3 . The generated test responses achieve a perplexity of 21.8 for the SEQ model and 10.11 for the SEQS model. This indicates that including the sections of document helps in the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human Evaluation</head><p>We also perform two kinds of human evaluations to evaluate the quality of predicted utterances -en- gagement and fluency. These experiments are per- formed on Amazon Mechanical Turk.</p><p>Engagement: We set up a pairwise comparison following <ref type="bibr" target="#b1">Bennett (2005)</ref> to evaluate the engage- ment of the generated responses. The test presents the chat history (1 utterance) and then, in random order, its corresponding response produced by the SEQ and SEQS models. A third option "No Pref- erence" was given to participants to mark no pref- erence for either of the generated responses. The instruction given to the participants is "Given the above chat history as context, you have to pick the one which can be best used as the response based on the engagingness." We randomly sam- ple 90 responses from each model. Each response was annotated by 3 unique workers and we take majority vote as the final label. The result of the test is that SEQ generated responses were chosen only 36.4% times as opposed to SEQS generated responses which were chosen 43.9% and the "No Preference" option was chosen 19.6% of times. This result shows the information from the sec- tions improves the engagement of the generated responses.</p><p>Fluency: The workers were asked to evaluate the fluency of the generated response on a scale of 1 to 4, where 1 is unreadable and 4 is perfectly readable. We randomly select 120 generated re- sponses from each model and each response was annotated by 3 unique workers. The SEQ model got a low score of 2.88, contrast to the SEQS score of 3.84. This outcome demonstrates that the in- formation in the section also helps in guiding the generator to produce fluent responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we introduce a crowd-sourced con- versations dataset that is grounded in a predefined set of documents which is available for download <ref type="bibr">4</ref> . We perform multiple automatic and human judgment based analysis to understand the value the information from the document provides to the generation of responses. The SEQS model which uses the information from the section to generate responses outperforms the SEQ model in the eval- uation tasks of engagement, fluency and perplex- ity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>An example conversation for scenario 1. User 
1 does not have access to the document, while User 2 
does. The full dialogue is attached in the Appendix. 

User 2: I thought The Shape of Water was 
one of Del Toro's best works. 
What about you? 
User 1: Did you like the movie? 
User 1: Yes, his style really extended the story. 
User 2: I agree. He has a way with fantasy 
elements that really helped this story 
be truly beautiful. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>An example conversation for scenario 2. Both 
User 1 and User 2 have access to the Wiki document. 
The full dialogue is attached in the Appendix. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with other datasets. The average number of turns are calculated as the number of utterances 
divided by the number of conversations for each of the datasets. 

Rating 1 
Rating 2 
Rating 3 
Rating 2&amp; 3 
Total # of conversations 
1443 
2142 
527 
2669 
Total # of utterances 
28536 
80104 
21360 
101464 
Average # utterances/conversation 19.77(13.68) 35.39(8.48) 40.53(12.92) 38.01(9.607) 
Average length of utterance 
7.51(50.19) 
10.56(8.51) 16.57(15.23) 11.83(10.58) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>The statistics of the dataset. Standard deviation in parenthesis.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>We split the dataset 
into three ratings based on BLEU score. 

Percentile 20 
40 
60 
80 
99 
BLEU 
0.09 0.20 0.34 0.53 0.82 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The results of data analysis. LT refers to the 
average length of x i in scenario 1 and x i , . . . , x i+k in 
scenario 2. 

</table></figure>

			<note place="foot" n="2"> The total number of tokens is 46000, and we limit the vocabulary to be 10000 tokens. 3 We use the SRILM toolkit (Stolcke, 2002)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by a fellowship from Robert Bosch, and in part by Facebook Inc. and Microsoft Corporation. This work was performed as a part of The Conversational Intelligence Chal-lenge (ConvAI, NIPS 2017) and we would like to thank the ConvAI team. We are also grateful <ref type="bibr">4</ref> https://www.github.com/festvox/datasets/CMU DoG to the anonymous reviewers for their constructive feedback and to Carolyn Penstein Rose, Shivani Poddar, Sreecharan Sankaranarayanan, Samridhi Shree Choudhary and Zhou Yu for valuable discussions at earlier stages of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Frames: A corpus for adding memory to goal-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Zumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emery</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00057</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale evaluation of corpus-based synthesizers: Results and lessons from the blizzard challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Christina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 2nd Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05742</idno>
		<title level="m">Laurent Charlin, and Joelle Pineau. 2015. A survey of available corpora for building data-driven dialogue systems</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh international conference on spoken language processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Personalizing dialogue agents: I have a dog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07243</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>do you have pets too? arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
