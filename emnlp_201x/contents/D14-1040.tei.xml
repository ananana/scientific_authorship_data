<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><forename type="middle">Vuli´c</forename><surname>Vuli´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
							<email>{ivan.vulic|marie-francine.moens}@cs.kuleuven.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science KU Leuven</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Models of Cross-Lingual Semantic Similarity in Context Based on Latent Cross-Lingual Concepts Induced from Comparable Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="349" to="362"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose the first probabilistic approach to modeling cross-lingual semantic similarity (CLSS) in context which requires only comparable data. The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts (e.g., cross-lingual topics obtained by a multilingual topic model). These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources. Word meaning is represented as a probability distribution over the latent concepts, and a change in meaning is represented as a change in the distribution over these latent concepts. We present new models that modulate the isolated out-of-context word representations with contex-tual knowledge. Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-lingual semantic similarity (CLSS) is a met- ric that measures to which extent words (or more generally, text units) describe similar semantic concepts and convey similar meanings across lan- guages. Models of cross-lingual similarity are typ- ically used to automatically induce bilingual lexi- cons and have found numerous applications in in- formation retrieval (IR), statistical machine trans- lation (SMT) and other natural language process- ing (NLP) tasks. Within the IR framework, the output of the CLSS models is a key resource in the models of dictionary-based cross-lingual in- formation retrieval <ref type="bibr">(Ballesteros and Croft, 1997;</ref><ref type="bibr" target="#b0">Lavrenko et al., 2002;</ref><ref type="bibr" target="#b3">Levow et al., 2005</ref>; <ref type="bibr" target="#b34">Wang and Oard, 2006</ref>) or may be utilized in query ex- pansion in cross-lingual IR models <ref type="bibr">(Adriani and van Rijsbergen, 1999;</ref>). These CLSS models may also be utilized as an addi- tional source of knowledge in SMT systems <ref type="bibr" target="#b13">(Och and Ney, 2003;</ref><ref type="bibr" target="#b35">Wu et al., 2008)</ref>. Additionally, the models are a crucial component in the cross- lingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling <ref type="bibr" target="#b15">(Padó and Lapata, 2009;</ref><ref type="bibr" target="#b30">van der Plas et al., 2011</ref>), parsing ( <ref type="bibr" target="#b38">Zhao et al., 2009;</ref><ref type="bibr">Durrett et al., 2012;</ref><ref type="bibr" target="#b27">Täckström et al., 2013b</ref>), POS tagging ( <ref type="bibr" target="#b36">Yarowsky and Ngai, 2001;</ref><ref type="bibr">Das and Petrov, 2011;</ref><ref type="bibr" target="#b26">Täckström et al., 2013a;</ref><ref type="bibr">Ganchev and Das, 2013)</ref>, verb clas- sification ( <ref type="bibr" target="#b6">Merlo et al., 2002</ref>), inducing selectional preferences <ref type="bibr" target="#b16">(Peirsman and Padó, 2010)</ref>, named entity recognition ( <ref type="bibr">Kim et al., 2012)</ref>, named en- tity segmentation ( <ref type="bibr">Ganchev and Das, 2013)</ref>, etc.</p><p>The models of cross-lingual semantic similar- ity from parallel corpora rely on word alignment models ( <ref type="bibr">Brown et al., 1993;</ref><ref type="bibr" target="#b13">Och and Ney, 2003)</ref>, but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently.</p><p>All these models from parallel and compara- ble corpora provide ranked lists of semantically similar words in the target language in isolation or invariably, that is, they do not explicitly iden-tify and encode different senses of words. In practice, it means that, given the sentence "The coach of his team was not satisfied with the game yesterday.", these context-insensitive models of similarity are not able to detect that the Spanish word entrenador is more similar to the polyse- mous word coach in the context of this sentence than the Spanish word autocar, although auto- car is listed as the most semantically similar word to coach globally/invariably without any observed context. In another example, while Spanish words partido, encuentro, cerilla or correspondencia are all highly similar to the ambiguous English word match when observed in isolation, given the Span- ish sentence "She was unable to find a match in her pocket to light up a cigarette.", it is clear that the strength of semantic similarity should change in context as only cerilla exhibits a strong seman- tic similarity to match within this particular sen- tential context.</p><p>Following this intuition, in this paper we inves- tigate models of cross-lingual semantic similarity in context. The context-sensitive models of sim- ilarity target to re-rank the lists of semantically similar words based on the co-occurring contexts of words. Unlike prior work (e.g., <ref type="bibr" target="#b10">(Ng et al., 2003;</ref><ref type="bibr" target="#b18">Prior et al., 2011;</ref><ref type="bibr">Apidianaki, 2011)</ref>), we explore these models in a particularly difficult and min- imalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper ex- tends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cross-lingual concepts ( <ref type="bibr">Haghighi et al., 2008;</ref><ref type="bibr">Daumé III and Jagarlamudi, 2011;</ref><ref type="bibr" target="#b32">Vuli´cVuli´c et al., 2011;</ref>) induced from non-parallel data. While all the previous mod- els in this framework are context-insensitive mod- els of semantic similarity, we demonstrate how to build context-aware models of semantic similarity within the same probabilistic framework which re- lies on the same shared set of latent concepts.</p><p>The main contributions of this paper are:</p><p>• We present a new probabilistic approach to modeling cross-lingual semantic similarity in context based on latent cross-lingual seman- tic concepts induced from non-parallel data.</p><p>• We show how to use the models of cross- lingual semantic similarity in the task of sug- gesting word translations in context. <ref type="table">• We provide results for three language</ref> pairs which demonstrate that contextualized models of similarity significantly outscore context-insensitive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Towards Cross-Lingual Semantic Similarity in Context</head><p>Latent Cross-Lingual Concepts. Latent cross- lingual concepts/senses may be interpreted as language-independent semantic concepts present in a multilingual corpus (e.g., document-aligned Wikipedia articles in English, Spanish and Dutch) that have their language-specific representations in different languages. For instance, having a multi- lingual collection in English, Spanish and Dutch, and then discovering a latent semantic concept on Soccer, that concept would be represented by words (actually probabilities over words P (w|z k ), where w denotes a word, and z k denotes k-th latent concept): {player, goal, coach, . . . } in English, balón (ball), futbolista (soccer player), equipo (team), . . . } in Spanish, and {wedstrijd (match), elftal (soccer team), doelpunt (goal), . . . } in Dutch. Given a multilingual corpus C, the goal is to learn and extract a set Z of K latent cross- lingual concepts {z 1 , . . . , z K } that optimally de- scribe the observed data, that is, the multilingual corpus C. Extracting cross-lingual concepts ac- tually implies learning per-document concept dis- tributions for each document in the corpus, and discovering language-specific representations of these concepts given by per-concept word distri- butions in each language. Z = {z 1 , . . . , z K } represents the set of K la- tent cross-lingual concepts present in the multilin- gual corpus. These K semantic concepts actually span a latent cross-lingual semantic space. Each word w, irrespective of its actual language, may be represented in that latent semantic space as a K-dimensional vector, where each vector compo- nent is a conditional concept score P (z k |w).</p><p>A number of models may be employed to in- duce the latent concepts. For instance, one could use cross-lingual Latent Semantic Indexing ( <ref type="bibr">Dumais et al., 1996)</ref>, probabilistic Principal Compo- nent Analysis <ref type="bibr" target="#b29">(Tipping and Bishop, 1999)</ref>, or a probabilistic interpretation of non-negative matrix factorization ( <ref type="bibr" target="#b1">Lee and Seung, 1999;</ref><ref type="bibr">Gaussier and Goutte, 2005;</ref><ref type="bibr">Ding et al., 2008</ref>) on concatenated documents in aligned document pairs. Other more recent models include matching canonical correla- tion analysis <ref type="bibr">(Haghighi et al., 2008;</ref><ref type="bibr">Daumé III and Jagarlamudi, 2011</ref>) and multilingual probabilistic topic models ( <ref type="bibr" target="#b11">Ni et al., 2009;</ref><ref type="bibr">De Smet and Moens, 2009;</ref><ref type="bibr" target="#b7">Mimno et al., 2009;</ref><ref type="bibr">Boyd-Graber and Blei, 2009;</ref><ref type="bibr" target="#b37">Zhang et al., 2010;</ref><ref type="bibr">Fukumasu et al., 2012)</ref>.</p><p>Due to its inherent language pair indepen- dent nature and state-of-the-art performance in the tasks such as bilingual lexicon extraction <ref type="bibr" target="#b32">(Vuli´cVuli´c et al., 2011</ref>) and cross-lingual information retrieval , the description in this pa- per relies on the multilingual probabilistic topic modeling (MuPTM) framework. We draw a di- rect parallel between latent cross-lingual concepts and latent cross-lingual topics, and we present the framework from the MuPTM perspective, but the proposed framework is generic and allows the usage of all other models that are able to com- pute probability scores P (z k |w). These scores in MuPTM are induced from their output language- specific per-topic word distributions. The mul- tilingual probabilistic topic models output prob- ability scores P (w S i |z k ) and P (w T j |z k ) for each w S i ∈ V S and w T j ∈ V T and each z k ∈ Z, and it holds w S i ∈V S P (w S i |z k ) = 1 and w T j ∈V T P (w T j |z k ) = 1. The scores are then used to compute scores P (z k |w S i ) and P (z k |w T j ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. With- out observing any context, the standard models of semantic word similarity that rely on the seman- tic space spanned by latent cross-lingual concepts in both monolingual ( <ref type="bibr">Dinu and Lapata, 2010a;</ref><ref type="bibr">Dinu and Lapata, 2010b</ref>) and multilingual set- tings <ref type="bibr" target="#b32">(Vuli´cVuli´c et al., 2011</ref>) typically proceed in the following manner. Latent language-independent concepts (e.g., cross-lingual topics or latent word senses) are estimated on a large corpus. The K-dimensional vector representation of the word w S 1 ∈ V S is:</p><formula xml:id="formula_0">vec(w S 1 ) = [P (z1|w S 1 ), . . . , P (zK |w S 1 )]<label>(1)</label></formula><p>Similarly, we are able to represent any target lan- guage word w T 2 in the same latent semantic space by a K-dimensional vector with scores P (z k |w T 2 ).</p><p>Each word regardless of its language is repre- sented as a distribution over K latent concepts. The similarity between w S 1 and some word w T 2 ∈ V T is then computed as the similarity between their K-dimensional vector representations using some of the standard similarity measures (e.g., the Kullback-Leibler or the Jensen-Shannon diver- gence, the cosine measure). These methods use only global co-occurrence statistics from the train- ing set and do not take into account any contex- tual information. They provide only out-of-context word representations and are therefore able to de- liver only context-insensitive models of similarity.</p><p>Defining Context. Given an occurrence of a word w S 1 , we build its context set Con(w S 1 ) = {cw S 1 , . . . , cw S r } that comprises r words from V S that co-occur with w S 1 in a defined contextual scope or granularity. In this work we do not in- vestigate the influence of the context scope (e.g., document-based, paragraph-based, window-based contexts). Following the recent work from <ref type="bibr">Huang et al. (2012)</ref> in the monolingual setting, we limit the contextual scope to the sentential context. However, we emphasize that the proposed models are designed to be fully functional regardless of the actual chosen context granularity. e.g., when operating in the sentential context, Con(w S 1 ) con- sists of words occurring in the same sentence with the particular instance of w S 1 . Following Mitchell and Lapata <ref type="bibr">(2008)</ref>, for the sake of simplicity, we impose the bag-of-words assumption, and do not take into account the order of words in the context set as well as context words' dependency relations to w S 1 . Investigating different context types (e.g., dependency-based) is a subject of future work.</p><p>By using all words occurring with w S 1 in a con- text set (e.g., a sentence) to build the set Con(w S 1 ), we do not make any distinction between "infor- mative and "uninformative" context words. How- ever, some context words bear more contextual in- formation about the observed word w S 1 and are stronger indicators of the correct word meaning in that particular context. For instance, in the sen- tence "The coach of his team was not satisfied with the game yesterday", words game and team are strong clues that coach should be translated as entrenador while the context word yesterday does not bring any extra contextual information that could resolve the ambiguity.</p><p>Therefore, in the final context set Con(w S 1 ) it is useful to retain only the context words that re-ally bring extra semantic information. We achieve that by exploiting the same latent semantic space to provide the similarity score between the ob- served word w S 1 and each word cw S i , i = 1, . . . , r from its context set Con(w S 1 ). Each word cw S i may be represented by its vector vec(cw S i ) (see eq. <ref type="formula" target="#formula_0">(1)</ref>) in the same latent semantic space, and there we can compute the similarity between its vec- tor and vec(w S 1 ). We can then sort the similarity scores for each cw S i and retain only the top scoring M context words in the final set Con(w S 1 ). The procedure of context sorting and pruning should improve the semantic cohesion between w S 1 and its context since only informative context features are now present in Con(w S 1 ), and we reduce the noise coming from uninformative contextual fea- tures that are not semantically related to w S 1 . Other options for the context sorting and pruning are possible, but the main goal in this paper is to il- lustrate the core utility of the procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Lingual Semantic Similarity in Context via Latent Concepts</head><p>Representing Context. The probabilistic frame- work that is supported by latent cross-lingual con- cepts allows for having the K-dimensional vector representations in the same latent semantic space spanned by cross-lingual topics for: (1) Single words regardless of their actual language, and (2) Sets that comprise multiple words. Therefore, we are able to project the observed source word, all target words, and the context set of the observed source word to the same latent semantic space spanned by latent cross-lingual concepts. Eq. <ref type="formula" target="#formula_0">(1)</ref> shows how to represent single words in the latent semantic space. Now, we present a way to address compositionality, that is, we show how to build the same representations in the same latent semantic space beyond the word level. We need to compute a conditional concept distribution for the context set Con(w S 1 ), that is, we have to compute the probability scores P (z k |Con(w S 1 )) for each z k ∈ Z. Remember that the context Con(w S 1 ) is actually a set of r (or M after pruning) words Con(w S 1 ) = {cw S 1 , . . . , cw S r }. Under the single- topic assumption ( <ref type="bibr" target="#b25">Griffiths et al., 2007</ref>) and fol- lowing Bayes' rule, it holds:</p><formula xml:id="formula_1">P (z k |Con(w S 1 )) = P (Con(w S 1 )|z k )P (z k ) P (Con(w S 1 )) = P (cw S 1 , . . . , cw S r |z k )P (z k ) K l=1 P (cw S 1 , . . . , cw S r |z l )P (z l ) (2) = r j=1 P (cw S j |z k )P (z k ) K l=1 r j=1 P (cw S j |z l )P (z l )<label>(3)</label></formula><p>Note that here we use a simplification where we assume that all cw S j ∈ Con(w S 1 ) are condition- ally independent given z k . The assumption of the conditional independence of unigrams is a stan- dard heuristic applied in bag-of-words model in NLP and IR (e.g., one may observe a direct anal- ogy to probabilistic language models for IR where the assumption of independence of query words is imposed <ref type="bibr" target="#b17">(Ponte and Croft, 1998;</ref><ref type="bibr">Hiemstra, 1998;</ref><ref type="bibr">Lavrenko and Croft, 2001)</ref>), but we have to forewarn the reader that in general the equa-</p><formula xml:id="formula_2">tion P (cw S 1 , . . . , cw S r |z k ) = r j=1 P (cw S j |z k )</formula><p>is not exact. However, by adopting the conditional independence assumption, in case of the uniform topic prior P (z k ) (i.e., we assume that we do not posses any prior knowledge about the importance of latent cross-lingual concepts in a multilingual corpus), eq. (3) may be further simplified:</p><formula xml:id="formula_3">P (z k |Con(w S 1 )) ≈ r j=1 P (cw S j |z k ) K l=1 r j=1 P (cw S j |z l )<label>(4)</label></formula><p>The representation of the context set in the latent semantic space is then:</p><formula xml:id="formula_4">vec(Con(w S 1 )) = [P (z1|Con(w S 1 )), . . . , P (zK |Con(w S 1 ))]</formula><p>We can then compute the similarity between words and sets of words given in the same latent semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS mod- els.</p><p>One remark: As a by-product of our modeling approach, by this procedure for computing repre- sentations for sets of words, we have in fact paved the way towards compositional cross-lingual mod- els of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings <ref type="bibr" target="#b9">(Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b21">Rudolph and Giesbrecht, 2010;</ref><ref type="bibr">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b23">Socher et al., 2011;</ref><ref type="bibr">Grefenstette and Sadrzadeh, 2011;</ref><ref type="bibr">Blacoe and Lapata, 2012;</ref><ref type="bibr">Clarke, 2012;</ref><ref type="bibr" target="#b24">Socher et al., 2012</ref>) and multilingual settings ( <ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr">Kočisk´Kočisk´y et al., 2014)</ref>, the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and text units beyond words is similar to <ref type="bibr">(Klementiev et al., 2012;</ref><ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr">Kočisk´Kočisk´y et al., 2014</ref>), but unlike them, we do not need high-quality sentence-aligned parallel data to induce bilingual text representations. Moreover, this work on compositionality in multilingual set- tings is only preliminary (e.g., we treat phrases and sentences as bags-of-words), and in future work we will aim to include syntactic information in the composition models as already done in monolin- gual settings <ref type="bibr" target="#b24">(Socher et al., 2012;</ref><ref type="bibr">Hermann and Blunsom, 2013)</ref>. Intuition behind the Approach. Going back to our novel CLSS models in context, these models rely on the representations of words and their con- texts in the same latent semantic space spanned by latent cross-lingual concepts/topics. The models differ in the way the contextual knowledge is fused with the out-of-context word representations.</p><p>The key idea behind these models is to repre- sent a word w S 1 in the latent semantic space as a distribution over the latent cross-lingual concepts, but now with an additional modulation of the rep- resentation after taking its local context into ac- count. The modulated word representation in the semantic space spanned by K latent cross-lingual concepts is then:</p><formula xml:id="formula_5">vec(w S 1 , Con(w S 1 )) = [P (z1|w S 1 ), . . . , P (zK |w S 1 )] (5)</formula><p>where P (z K |w S 1 ) denotes the recalculated (or modulated) probability score for the conditional concept/topic distribution of w S 1 after observing its context Con(w S 1 ). For an illustration of the key idea, see <ref type="figure">fig. 1</ref>. The intuition is that the context helps to disambiguate the true meaning of the oc- currence of the word w S 1 . In other words, after observing the context of the word w S 1 , fewer latent cross-lingual concepts will share most of the prob- ability mass in the modulated context-aware word representation. Model I: Direct-Fusion. The first approach makes the conditional distribution over latent se- mantic concepts directly dependent on both word w S 1 and its context Con(w S 1 ). The probability score P (z k |w S 1 ) from eq. (5) for each z k ∈ Z is then given as P (z k |w S 1 ) = P (z k |w S 1 , Con(w S 1 )). We have to estimate the probability P (z k |w S 1 , Con(w S 1 )), that is, the probability that word w S 1 is assigned to the latent concept/topic z k given its context Con(w S 1 ):</p><formula xml:id="formula_6">P (z k |w S 1 , Con(w S 1 )) = P (z k , w S 1 )P (Con(w S 1 )|z k ) K l=1 P (z l , w S 1 )P (Con(w S 1 )|z l )<label>(6)</label></formula><p>Since P (z k , w S 1 ) = P (w S 1 |z k )P (z k ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent semantic space (and again assume the uniform topic prior P (z k )), we finally obtain the following formula:</p><formula xml:id="formula_7">P (z k |w S 1 ) ≈ P (w S 1 |z k ) r j=1 P (cw S j |z k ) K l=1 P (w S 1 |z l ) r j=1 P (cw S j |z l )<label>(7)</label></formula><p>The ranking of all words w T 2 ∈ V T according to their similarity to w S 1 may be computed by detect- ing the similarity score between their representa- tion in the K-dimensional latent semantic space and the modulated source word representation as given by eq. <ref type="formula">(5)</ref> and eq. <ref type="formula" target="#formula_7">(7)</ref> using any of the ex- isting similarity functions <ref type="bibr" target="#b2">(Lee, 1999;</ref><ref type="bibr">Cha, 2007)</ref>. The similarity score Sim(w S 1 , w T 2 , Con(w S 1 )) be- tween some w T 2 ∈ V T represented by its vector vec(w T 2 ) and the observed word w S 1 given its con- text Con(w S 1 ) is computed as:</p><formula xml:id="formula_8">sim(w S 1 , w T 2 , Con(w S 1 )) = SF vec w S 1 , Con(w S 1 ) , vec w T 2<label>(8)</label></formula><p>where SF denotes a similarity function. Words are then ranked according to their respective sim- ilarity scores and the best scoring candidate may be selected as the best translation of an oc- currence of the word w S 1 given its local con- text. Since the contextual knowledge is inte- grated directly into the estimation of probability P (z k |w S 1 , Con(w S 1 )), we name this context-aware CLSS model the Direct-Fusion model. Model II: Smoothed-Fusion. The next model follows the modeling paradigm established within the framework of language modeling (LM), where the idea is to "back off" to a lower order N- gram in case we do not possess any evidence about a higher-order N-gram <ref type="bibr">(Jurafsky and Martin, 2000</ref>). The idea now is to smooth the repre- sentation of a word in the latent semantic space induced only by the words in its local context with the out-of-context type-based representation of that word induced directly from a large training corpus. In other words, the modulated probability score P (z k |w S 1 ) from eq. (5) is calculated as:</p><formula xml:id="formula_9">P (z k |w S 1 ) = λ1P (z k |Con(w S 1 )) + (1 − λ1)P (z k |w S 1 ) (9)</formula><p>where λ 1 is the interpolation parameter, P (z k |w S 1 ) is the out-of-context conditional concept probabil- ity score as in eq. <ref type="formula" target="#formula_0">(1)</ref>, and P (z k |Con(w S 1 )) is given by eq. (3). This model compromises be- tween the pure contextual word representation and</p><formula xml:id="formula_10">z 3 z 2 z 1 coach (in isolation) entrenador autocar z 3 z 2 z 1 coach (contextualized) entrenador autocar</formula><p>The coach of his team was not satisfied with the game yesterday.</p><formula xml:id="formula_11">K coach K coach CONTEXT-INSENSITIVE</formula><p>CONTEXT-SENSITIVE <ref type="figure">Figure 1</ref>: An illustrative toy example of the main intuitions in our probabilistic framework for building context sensitive models with only three latent cross-lingual concepts (axes z 1 , z 2 and z 3 ): A change in meaning is reflected as a change in a probability distribution over latent cross-lingual concepts that span a shared latent semantic space. A change in the probability distribution may then actually steer an English word coach towards its correct (Spanish) meaning in context.</p><p>the out-of-context word representation. In cases when the local context of word w S 1 is informa- tive enough, the factor P (z k |Con(w S 1 )) is suffi- cient to provide the ranking of terms in V T , that is, to detect words that are semantically similar to w S 1 based on its context. However, if the context is not reliable, we have to smooth the pure context- based representation with the out-of-context word representation (the factor P (z k |w S 1 )). We call this model the Smoothed-Fusion model.</p><p>The ranking of words w T 2 ∈ V T then finally proceeds in the same manner as in Direct-Fusion following eq. (8), but now using eq. (9) for the modulated probability scores P (z k |w S 1 ). Model III: Late-Fusion. The last model is con- ceptually similar to Smoothed-Fusion, but it per- forms smoothing at a later stage. It proceeds in two steps: (1) Given a target word w T 2 ∈ V T , the model computes similarity scores separately be- tween (i) the context set Con(w S 1 ) and w T 2 , and (ii) the word w S 1 in isolation and w T 2 (again, on the type level); (2) It linearly combines the obtained similarity scores. More formally, we may write:</p><formula xml:id="formula_12">Sim(w S 1 , w T 2 , Con(w S 1 )) = λ2SF vec Con(w S 1 ) , vec w T 2 + (1 − λ2)SF vec w S 1 , vec w T 2<label>(10)</label></formula><p>where λ 2 is the interpolation parameter. Since this model computes the similarity with each tar- get word separately for the source word in isola- tion and its local context, and combines the ob- tained similarity scores after the computations, this model is called Late-Fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Evaluation Task: Suggesting Word Transla- tions in Context. Given an occurrence of a pol- ysemous word w S 1 ∈ V S in the source language L S with vocabulary V S , the task is to choose the correct translation in the target language L T of that particular occurrence of w S 1 from the given set T = {t T 1 , . . . , t T q }, T ⊆ V T , of its q possible translations/meanings (i.e., its translation or sense inventory). The task of suggesting a word trans- lation in context may be interpreted as ranking the q translations with respect to the observed local context Con(w S 1 ) of the occurrence of the word w S 1 . The best scoring translation candidate in the ranked list is then the suggested correct translation for that particular occurrence of w S 1 after observ- ing its local context Con(w S 1 ). Training Data. We use the following corpora for inducing latent cross-lingual concepts/topics, i.e., for training our multilingual topic model: (i) a col- lection of 13, 696 Spanish-English Wikipedia arti- cle pairs (Wiki-ES-EN), (ii) a collection of 18, 898 Italian-English Wikipedia article pairs, (iii) a col- lection of 7, 612 Dutch-English Wikipedia arti- cle pairs (Wiki-NL-EN), and (iv) the Wiki-NL- EN corpus augmented with 6,206 Dutch-English document pairs from Europarl ( <ref type="bibr">Koehn, 2005</ref>) (Wiki+EP-NL-EN). The corpora were previously used in . No explicit use is made of sentence-level alignments in Europarl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence in Italian</head><p>Correct Translation (EN) 1. I primi calci furono prodotti in legno ma recentemente... stock 2. In caso di osteoporosi si verifica un eccesso di rilascio di calcio dallo scheletro... calcium 3. La crescita del calcio femminile professionistico ha visto il lancio di competizioni... football 4. Il calcio di questa pistola (Beretta Modello 21a, calibro .25) ha le guancette in materiale... stock  <ref type="table">Table 2</ref>: Sets of 15 ambiguous words in Spanish, Italian and Dutch from our test set accompanied by the sets of their respective possible senses/translations in English.</p><p>All corpora are theme-aligned comparable cor- pora, i.e, the aligned document pairs discuss sim- ilar themes, but are in general not direct trans- lations (except for Europarl). By training on Wiki+EP-NL-EN we want to test how the training corpus of higher quality affects the estimation of latent cross-lingual concepts that span the shared latent semantic space and, consequently, the over- all results in the task of suggesting word transla- tions in context. Following prior work ( <ref type="bibr">Koehn and Knight, 2002;</ref><ref type="bibr">Haghighi et al., 2008;</ref><ref type="bibr" target="#b19">Prochasson and Fung, 2011;</ref>, we re- tain only nouns that occur at least 5 times in the corpus. We record lemmatized word forms when available, and original forms otherwise. We use TreeTagger ( <ref type="bibr" target="#b22">Schmid, 1994)</ref> for POS tagging and lemmatization.</p><p>Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in En- glish (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible transla- tions) in each of the 3 languages, and have man- ually extracted 24 sentences (not present in the training data) for each noun that capture different meanings of the noun from Wikipedia. In order to construct datasets that are balanced across dif- ferent possible translations of a noun, in case of q different translation candidates in T for some word w S 1 , the dataset contains exactly 24/q sen- tences for each translation from T . In total, we have designed 360 sentences for each language pair (ES/IT/NL-EN), 1080 sentences in total. <ref type="bibr">1</ref> . We have used 5 extra nouns with 20 sentences each as a development set to tune the parameters of our models. As a by-product, we have built an initial repository of ES/IT/NL ambiguous words. Tab. 1 presents a small sample from the IT evaluation dataset, and illustrates the task of suggesting word translations in context. Evaluation Procedure. Our task is to present the system a list of possible translations and let the system decide a single most likely translation given the word and its sentential context. Ground truth thus contains one word, that is, one correct translation for each sentence from the evaluation dataset. We have manually annotated the correct translation for the ground truth 1 by inspecting the discourse in Wikipedia articles and the interlingual Wikipedia links. We measure the performance of all models as Top 1 accuracy (Acc 1 ) ( <ref type="bibr">Gaussier et al., 2004;</ref><ref type="bibr" target="#b28">Tamura et al., 2012)</ref>. It denotes the num- ber of word instances from the evaluation dataset whose top proposed candidate in the ranked list of translation candidates from T is exactly the cor- rect translation for that word instance as given by ground truth over the total number of test word in- stances (360 in each test dataset). Parameters. We have tuned λ 1 and λ 2 on the de- velopment sets. We set λ 1 = λ 2 = 0.9 for all language pairs. We use sorted context sets (see sect. 2) and perform a cut-off at M = 3 most de- scriptive context words in the sorted context sets for all models. In the following section we discuss the utility of this context sorting and pruning, as well as its influence on the overall results. Inducing Latent Cross-Lingual Concepts. Our context-aware models are generic and allow ex- perimentations with different models that induce latent cross-lingual semantic concepts. However, in this particular work we present results obtained by a multilingual probabilistic topic model called bilingual LDA ( <ref type="bibr" target="#b7">Mimno et al., 2009;</ref><ref type="bibr" target="#b11">Ni et al., 2009;</ref><ref type="bibr">De Smet and Moens, 2009</ref>). The BiLDA model is a straightforward multilingual extension of the standard LDA model ( <ref type="bibr">Blei et al., 2003)</ref>. For the details regarding the modeling, generative story and training of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature.</p><p>We have used the Gibbs sampling procedure <ref type="bibr">(Geman and Geman, 1984)</ref> tailored for BiLDA in particular for training and have experimented with different number of topics K in the interval 300 − 2500. Here, we present only the results ob- tained with K = 2000 for all language pairs which also yielded the best or near-optimal performance in ( <ref type="bibr">Dinu and Lapata, 2010b;</ref><ref type="bibr" target="#b32">Vuli´cVuli´c et al., 2011</ref>).</p><p>Other parameters of the model are set to the typical values according to <ref type="bibr" target="#b25">Steyvers and Griffiths (2007)</ref>: α = 50/K and β = 0.01. 2 Models in Comparison. We test the performance of our Direct-Fusion, Smoothed-Fusion and Late- Fusion models, and compare their results with the context-insensitive CLSS models described in sect. 2 (No-Context). We provide results with two different similarity functions:</p><p>(1) We have tested different SF-s (e.g., the Kullback-Leibler and the Jensen-Shannon divergence, the cosine measure) on the K-dimensional vector represen- tations, and have detected that in general the best scores are obtained with the Bhattacharyya coef- ficient (BC) <ref type="bibr">(Cha, 2007;</ref><ref type="bibr">Kazama et al., 2010)</ref>, (2) Another similarity method we use is the so- called Cue method ( <ref type="bibr" target="#b25">Griffiths et al., 2007;</ref><ref type="bibr" target="#b32">Vuli´cVuli´c et al., 2011</ref>), which models the probability that a target word t T i will be generated as an as- sociation response given some cue source word w S 1 . In short, the method computes the score P (t T i |w S 1 ) = P (t T i |z k )P (z k |w S 1 ). We can use the scores P (t T i |w S 1 ) obtained by inputting out-of- context probability scores P (z k |w S 1 ) or modulated probability scores P (z k |w S 1 ) to produce the rank- ing of translation candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>The performance of all the models in comparison is displayed in tab. 3. These results lead us to several conclusions: (i) All proposed context-sensitive CLSS models suggesting word translations in context signifi- cantly outperform context-insensitive CLSS mod- els, which are able to produce only word trans- lations in isolation. The improvements in re- sults when taking context into account are ob-  (ii) The choice of a similarity function influences the results. On average, the Cue method as SF out- performs other standard similarity functions (e.g., Kullback-Leibler, Jensen-Shannon, cosine, BC) in this evaluation task. However, it is again impor- tant to state that regardless of the actual choice of SF, context-aware models that modulate out-of- context word representations using the knowledge of local context outscore context-insensitive mod- els that utilize non-modulated out-of-context rep- resentations (with all other parameters equal).</p><formula xml:id="formula_13">Direction: ES→EN IT→EN NL→EN (Wiki) NL→EN (Wiki+EP) Model Acc1 Acc1 Acc1 Acc1 Acc1 Acc1 Acc1 Acc1 (SF=BC) (SF=Cue) (SF=BC) (SF=Cue) (SF=BC) (SF=Cue) (SF=BC) (</formula><p>(iii) The Direct-Fusion model, conceptually sim- ilar to a model of word similarity in context in monolingual settings ( <ref type="bibr">Dinu and Lapata, 2010a)</ref>, is outperformed by the other two context-sensitive models. In Direct-Fusion, the observed word and its context are modeled in the same fashion, that is, the model does not distinguish between the word and its surrounding context when it computes the modulated probability scores P (z k |w S 1 ) (see eq. <ref type="formula" target="#formula_7">(7)</ref>). Unlike Direct-Fusion, the modeling assump- tions of Smoothed-Fusion and Late-Fusion pro- vide a clear distinction between the observed word w S 1 and its context Con(w S 1 ) and combine the out- of-context representation of w S 1 and its contextual knowledge into a smoothed LM-inspired proba- bilistic model. As the results reveal, that strategy leads to better overall scores. The best scores in general are obtained by Smoothed-Fusion, but it is also outperformed by Late-Fusion in several ex- perimental runs where BC was used as SF. How- ever, the difference in results between Smoothed- Fusion and Late-Fusion in these experimental runs is not statistically significant according to a chi- squared significance test (p &lt; 0.05).</p><p>(iv) The results for Dutch-English are influenced by the quality of training data. The performance of our models of similarity is higher for models that rely on latent-cross lingual topics estimated from the data of higher quality (i.e., compare the results when trained on Wiki and Wiki+EP in tab. 3). The overall quality of our models of similarity is of course dependent on the quality of the latent cross-lingual topics estimated from training data, and the quality of these latent cross-lingual con- cepts is further dependent on the quality of multi- lingual training data. This finding is in line with a similar finding reported for the task of bilingual lexicon extraction . (v) Although Dutch is regarded as more similar to English than Italian or Spanish, we do not ob- serve any major increase in the results on both test datasets for the English-Dutch language pair compared to English-Spanish/Italian. That phe- nomenon may be attributed to the difference in size and quality of our training Wikipedia datasets. Moreover, while the probabilistic framework pro- posed in this chapter is completely language pair agnostic as it does not make any language pair dependent modeling assumptions, we acknowl- edge the fact that all three language pairs com- prise languages coming from the same phylum, that is, the Indo-European language family. Future extensions of our probabilistic modeling frame- work also include porting the framework to other more distant language pairs that do not share the same roots nor the same alphabet (e.g., English- Chinese/Hindi). Analysis of Context Sorting and Pruning. We also investigate the utility of context sorting and pruning, and its influence on the overall results in our evaluation task. Therefore, we have con- ducted experiments with sorted context sets that were pruned at different positions, ranging from 1 (only the most similar word to w S 1 in a sentence is included in the context set Con(w S 1 )) to All (all words occurring in a same sentence with w S 1 are included in Con(w S 1 )). The monolingual similar- ity between w S 1 and each potential context word in a sentence has been computed using BC on their out-of-context representations in the latent seman- tic space spanned by cross-lingual topics. <ref type="figure" target="#fig_0">Fig. 2</ref> shows how the size of the sorted context influences the overall results. The presented results have been obtained by the Cue+Smoothed-Fusion combina- tion, but a similar behavior is observed when em- ploying other combinations. <ref type="figure" target="#fig_0">Fig. 2</ref> clearly indicates the importance of con- text sorting and pruning. The procedure ensures that only the most semantically similar words in a given scope (e.g., a sentence) influence the choice of a correct meaning. In other words, closely semantically similar words in the same sentence are more reliable indicators for the most probable word meaning. They are more informative in mod- ulating the out-of-context word representations in context-sensitive similarity models. We observe large improvements in scores when we retain only the top M semantically similar words in the con- text set (e.g., when M =5, the scores are 0.694, 0.758, 0.717, and 0.767 for ES-EN, IT-EN, NL- EN (Wiki) and NL-EN (Wiki+EP), respectively; while the same scores are 0.572, 0.703, 0.639 and 0.672 when M =All).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have proposed a new probabilistic approach to modeling cross-lingual semantic similarity in con- text, which relies only on co-occurrence counts and latent cross-lingual concepts which can be es- timated using only comparable data. The approach is purely statistical and it does not make any ad- ditional language-pair dependent assumptions; it does not rely on a bilingual lexicon, orthographic clues or predefined ontology/category knowledge, and it does not require parallel data.</p><p>The key idea in the approach is to represent words, regardless of their actual language, as dis- tributions over the latent concepts, and both out- of-context and contextualized word representa- tions are then presented in the same latent space spanned by the latent semantic concepts. A change in word meaning after observing its con- text is reflected in a change of its distribution over the latent concepts. Results for three lan- guage pairs have clearly shown the importance of the newly developed modulated or "contextual- ized" word representations in the task of suggest- ing word translations in context.</p><p>We believe that the proposed framework is only a start, as it ignites a series of new research ques- tions and perspectives. One may further exam- ine the influence of context scope (e.g., document- based vs. sentence-based vs. window-based con- texts), as well as context selection and aggregation (see sect. 2) on the contextualized models. For instance, similar to the model from´Ofrom´ from´O <ref type="bibr" target="#b12">Séaghdha and Korhonen (2011)</ref> in the monolingual setting, one may try to introduce dependency-based con- texts <ref type="bibr" target="#b14">(Padó and Lapata, 2007)</ref> and incorporate the syntax-based knowledge in the context-aware CLSS modeling. It is also worth studying other models that induce latent semantic concepts from multilingual data (see sect. 2) within this frame- work of context-sensitive CLSS modeling. One may also investigate a similar approach to context- sensitive CLSS modeling that could operate with explicitly defined concept categories ( <ref type="bibr">Gabrilovich and Markovitch, 2007;</ref><ref type="bibr">Cimiano et al., 2009;</ref><ref type="bibr">Hassan and Mihalcea, 2009;</ref><ref type="bibr">Hassan and Mihalcea, 2011;</ref><ref type="bibr" target="#b5">McCrae et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Mirna Adriani and C. J. van <ref type="bibr">Rijsbergen. 1999</ref>. Term similarity-based query expansion for cross-language information retrieval. <ref type="table" target="#tab_2">In Proceedings of the 3rd Eu- ropean Conference on Research and Advanced Tech- nology for Digital Libraries (</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The influence of the size of sorted context on the accuracy of word translation in context. The model is Cue+Smoothed-Fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Example sentences from our IT evaluation dataset with corresponding correct translations.</head><label>1</label><figDesc></figDesc><table>Spanish 
Italian 
Dutch 

Ambiguous word 
Ambiguous word 
Ambiguous word 
(Possible senses/translations) 
(Possible senses/translations) 
(Possible senses/translations) 

1. estación 
1. raggio 
1. toren 
(station; season) 
(ray; radius; spoke) 
(rook; tower) 
2. ensayo 
2. accordo 
2. beeld 
(essay; rehearsal; trial) 
(chord; agreement) 
(image; statue) 
3. núcleo 
3. moto 
3. blade 
(core; kernel; nucleus) 
(motion; motorcycle) 
(blade; leaf; magazine) 
4. vela 
4. calcio 
4.fusie 
(sail; candle) 
(calcium; football; stock) 
(fusion; merger) 
5. escudo 
5. terra 
5. stam 
(escudo; escutcheon; shield) 
(earth; land) 
(stem; trunk; tribe) 
6. papa 
6. tavola 
6. koper 
(Pope; potato) 
(board; panel; table) 
(copper; buyer) 
7. cola 
7. campione 
7. bloem 
(glue; coke; tail; queue) 
(champion; sample) 
(flower; flour) 
8. cometa 
8. carta 
8. spanning 
(comet; kite) 
(card; paper; map) 
(voltage; tension; stress) 
9. disco 
9. piano 
9. noot 
(disco; discus; disk) 
(floor; plane; plan; piano) 
(note; nut) 
10. banda 
10. disco 
10. akkoord 
(band; gang; strip) 
(disco; discus; disk) 
(chord; agreement) 
11. cinta 
11. istruzione 
11. munt 
(ribbon; tape) 
(education; instruction) 
(coin; currency; mint) 
12. banco 
12. gabinetto 
12. pool 
(bank; bench; shoal) 
(cabinet; office; toilet) 
(pole; pool) 
13. frente 
13. torre 
13. band 
(forehead; front) 
(rook; tower) 
(band; tyre; tape) 
14. fuga 
14. campo 
14. kern 
(escape; fugue; leak) 
(camp; field) 
(core; kernel; nucleus) 
15. gota 
15. gomma 
15. kop 
(gout; drop) 
(rubber; gum; tyre) 
(cup; head) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the 3 evaluation datasets. Translation direction is ES/IT/NL→EN. The improvements 
of all contextualized models over non-contextualized models are statistically significant according to a 
chi-square statistical significance test (p&lt;0.05). The asterisk (*) denotes significant improvements of 
Smoothed-Fusion over Late-Fusion using the same significance test. 

served for all 3 language pairs. The large im-
provements in the results (i.e., we observe an aver-
age relative increase of 51.6% for the BC+Direct-
Fusion combination, 64.3% for BC+Smoothed-
Fusion, 64.9% for BC+Late-Fusion, 49.1% for 
Cue+Direct-Fusion, 76.7% for Cue+Smoothed-
Fusion, and 64.5% for Cue+Late-Fusion) confirm 
that the local context of a word is essential in ac-
quiring correct word translations for polysemous 
words, as isolated non-contextualized word repre-
sentations are not sufficient. 
</table></figure>

			<note place="foot" n="1"> Available at http://people.cs.kuleuven.be/ ∼ivan.vulic/software/</note>

			<note place="foot" n="2"> We are well aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned latent cross-lingual concepts/topics and, consequently, the quality of latent semantic space, but that analysis is not the focus of this work. Additionally, we perform semantic space pruning (Reisinger and Mooney, 2010; Vuli´cVuli´c and Moens, 2013). All computations are performed over the best scoring 100 cross-lingual topics according to their respective scores P (z k |w S i ) similarly to (Vuli´cVuli´c and Moens, 2013).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their comments and suggestions. This re-search has been carried out in the framework of the Smart Computer-Aided Translation Environment (SCATE) project (IWT-SBO 130041).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual relevance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 12th Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measures of distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dictionary-based techniques for cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="523" to="547" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Investigating task performance of probabilistic topic models: An empirical study of PLSA and LDA. Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="178" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Orthonormal explicit topic analysis for cross-lingual document matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Philip Mccrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1732" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multilingual paradigm for automatic verb classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivian</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Allaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="207" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Polylingual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="880" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting parallel texts for word sense disambiguation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Seng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="455" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining multilingual topics from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International World Wide Web Conference (WWW)</title>
		<meeting>the 18th International World Wide Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1155" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic models of similarity in syntactic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1047" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crosslingual annotation projection for semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="307" to="340" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crosslingual induction of selectional preferences with bilingual vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 11th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="921" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Translation ambiguity in and out of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Prior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuly</forename><surname>Wintner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Macwhinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Psycholinguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="111" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rare word translation extraction from aligned comparable documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Prochasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACLHLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1327" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A mixture model with sharing for lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenie</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="907" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on New Methods in Language Processing</title>
		<meeting>the International Conference on New Methods in Language Processing</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 24th Annual Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">801</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Probabilistic topic models. Handbook of Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="424" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1061" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilingual lexicon extraction from comparable corpora using label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="24" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analysers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scaling up automatic cross-lingual semantic role annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Lonneke Van Der Plas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="299" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crosslingual semantic similarity of words as the similarity of their semantic word responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 14th Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="106" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Identifying word translations from comparable corpora using latent topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="479" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-language information retrieval models based on latent topic models trained with documentaligned comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wim</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="368" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining bidirectional translation and synonymy for cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 22nd International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="200" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-lingual latent topic extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1128" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross language dependency parsing using a bilingual lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 47th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
