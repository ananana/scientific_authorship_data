<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Globally Coherent Text Generation with Neural Checklist Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Globally Coherent Text Generation with Neural Checklist Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="329" to="339"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent neural networks can generate locally coherent text but often have difficulties representing what has already been generated and what still needs to be said-especially when constructing long texts. We present the neural checklist model, a recurrent neural network that models global coherence by storing and updating an agenda of text strings which should be mentioned somewhere in the output. The model generates output by dynamically adjusting the interpolation among a language model and a pair of attention models that encourage references to agenda items. Evaluations on cooking recipes and dialogue system responses demonstrate high coherence with greatly improved semantic coverage of the agenda.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural network (RNN) architectures have proven to be well suited for many natural language generation tasks ( <ref type="bibr" target="#b17">Mikolov et al., 2010;</ref><ref type="bibr" target="#b18">Mikolov et al., 2011;</ref><ref type="bibr" target="#b22">Sordoni et al., 2015;</ref><ref type="bibr" target="#b12">Xu et al., 2015;</ref><ref type="bibr" target="#b29">Wen et al., 2015;</ref><ref type="bibr">Mei et al., 2016)</ref>. Previous neu- ral generation models typically generate locally co- herent language that is on topic; however, overall they can miss information that should have been in- troduced or introduce duplicated or superfluous con- tent. These errors are particularly common in situ- ations where there are multiple distinct sources of input or the length of the output text is sufficiently long. In this paper, we present a new recurrent neu- ral model that maintains coherence while improv- "salt," "lime," etc.) have already been used (checked boxes).</p><p>The model is trained to interpolate an RNN (e.g., encode "pico de gallo" and decode a recipe) with attention models over new (left column) and used (middle column) items that identify likely items for each time step (shaded boxes; "tomatoes," etc.).</p><p>ing coverage by globally tracking what has been said and what is still left to be said in complete texts. For example, consider the challenge of generat- ing a cooking recipe, where the title and ingredient list are provided as inputs and the system must gen- erate a complete text that describes how to produce the desired dish. Existing RNN models may lose track of which ingredients have already been men- tioned, especially during the generation of a long recipe with many ingredients. Recent work has fo- cused on adapting neural network architectures to improve coverage ( <ref type="bibr" target="#b29">Wen et al., 2015</ref>) with applica- tion to generating customer service responses, such as hotel information, where a single sentence is gen- erated to describe a few key ideas. Our focus is in- stead on developing a model that maintains coher- ence while producing longer texts or covering longer input specifications (e.g., a long ingredient list).</p><p>More specifically, our neural checklist model gen- erates a natural language description for achieving a goal, such as generating a recipe for a particu- lar dish, while using a new checklist mechanism to keep track of an agenda of items that should be mentioned, such as a list of ingredients (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The checklist model learns to interpolate among three components at each time step: (1) an encoder-decoder language model that generates goal-oriented text, (2) an attention model that tracks remaining agenda items that need to be introduced, and (3) an attention model that tracks the used, or checked, agenda items. Together, these compo- nents allow the model to learn representations that best predict which words should be included in the text and when references to agenda items should be checked off the list (see check marks in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>We evaluate our approach on a new cooking recipe generation task and the dialogue act genera- tion from <ref type="bibr" target="#b29">Wen et al. (2015)</ref>. In both cases, the model must correctly describe a list of agenda items: an in- gredient list or a set of facts, respectively. Gener- ating recipes additionally tests the ability to main- tain coherence in long procedural texts. Experi- ments in dialogue generation demonstrate that our approach outperforms previous work with up to a 4 point BLEU improvement. Our model also scales to cooking recipes, where both automated and manual evaluations demonstrate that it maintains the strong local coherence of baseline RNN techniques while significantly improving the global coverage by ef- fectively integrating the agenda items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>Given a goal g and an agenda E = {e 1 , . . . , e |E| }, our task is to generate a goal-oriented text x by mak- ing use of items on the agenda. For example, in the cooking recipe domain, the goal is the recipe title ("pico de gallo" in <ref type="figure" target="#fig_0">Fig. 1)</ref>, and the agenda is the in- gredient list (e.g., "lime," "salt"). For dialogue sys- tems, the goal is the dialogue type (e.g., inform or query) and the agenda contains information to be mentioned (e.g., a hotel name and address). For example, if g ="inform" and E = {name(Hotel Stratford), has internet(no)}, an output text might be x ="Hotel Stratford does not have internet."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Attention models have been used for many NLP tasks such as machine translation ( <ref type="bibr" target="#b2">Balasubramanian et al., 2013;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>), abstractive sen- tence summarization <ref type="bibr" target="#b21">(Rush et al., 2015)</ref>, machine reading ( <ref type="bibr" target="#b4">Cheng et al., 2016)</ref>, and image caption gen- eration ( <ref type="bibr" target="#b12">Xu et al., 2015)</ref>. Our model uses new types of attention to record what has been said and to se- lect new agenda items to be referenced.</p><p>Recently, other researchers have developed new ways to use attention mechanisms for related gen- eration challenges. Most closely related, <ref type="bibr" target="#b29">Wen et al. (2015)</ref> and <ref type="bibr" target="#b30">Wen et al. (2016)</ref> present neural network models for generating dialogue system responses given a set of agenda items. They focus on gener- ating short texts (1-2 sentences) in a relatively small vocabulary setting and assume a fixed set of possi- ble agenda items. Our model composes substantially longer texts, such as recipes, with a more varied and open ended set of possible agenda items. We also compare performance for our model on their data.</p><p>Maintaining coherence and avoiding duplication have been recurring challenges when generating text using RNNs for other applications, including image captioning ( <ref type="bibr" target="#b12">Jia et al., 2015;</ref><ref type="bibr" target="#b12">Xu et al., 2015)</ref> and ma- chine translation ( <ref type="bibr" target="#b27">Tu et al., 2016b;</ref><ref type="bibr" target="#b26">Tu et al., 2016a)</ref>. A variety of solutions have been developed to ad- dress infrequent or out-of-vocabulary words in par- ticular <ref type="bibr" target="#b9">(Gülçehre et al., 2016;</ref><ref type="bibr" target="#b11">Jia and Liang, 2016)</ref>. Instead of directly copying input words or determin- istically selecting output, our model can learn how to generate them (e.g., it might prefer to produce the word "steaks" when the original recipe ingre- dient was "ribeyes"). Finally, recent work in ma- chine translation models has introduced new train- ing objectives to encourage attention to all input words ( <ref type="bibr" target="#b16">Luong et al., 2015)</ref>, but these models do not accumulate attention while decoding.</p><p>Generating recipes was an early task in planning <ref type="bibr" target="#b10">(Hammond, 1986)</ref> and generating referring expres- sion research <ref type="bibr" target="#b6">(Dale, 1988)</ref>. These can be seen as key steps in classic approaches to generating natu- ral language text: a formal meaning representation is provided as input and the model first does content selection to determine the non-linguistic concepts to be conveyed by the output text (i.e., what to say) and then does realization to describe those concepts  The top portion shows how the checklist and available/used agenda item matrices are updated.</p><formula xml:id="formula_0">330 E t Generate output E t+1 α t σ h t-1 g E t x t + r t s t q t z t h t ref-type(h t ) Ph t E t x x x x α t f t o t + a t-1 a t f t new E t+1 new 1-a t E a t x E x E E 2 x GRU</formula><p>in natural language text (i.e., how to say it) <ref type="bibr" target="#b25">(Thompson, 1977;</ref><ref type="bibr" target="#b20">Reiter and Dale, 2000</ref>). More recently, machine learning methods have focused on parts of this approach ( <ref type="bibr" target="#b3">Barzilay and Lapata, 2005;</ref><ref type="bibr" target="#b15">Liang et al., 2009</ref>) or the full two-stage approach <ref type="bibr" target="#b0">(Angeli et al., 2010;</ref><ref type="bibr" target="#b14">Konstas and Lapata, 2013</ref>). Most of these models shorter texts, although <ref type="bibr" target="#b19">Mori et al. (2014)</ref> did consider longer cooking recipes. Our approach is a joint model that instead operates with textual input and tries to cover all of the content it is given. <ref type="figure" target="#fig_1">Fig. 2</ref> shows a graphical representation of the neu- ral checklist model. At a high level, our model uses a recurrent neural network (RNN) language model that encodes the goal as a bag-of-words and then generates output text token by token. It additionally stores a vector that acts as a soft checklist of what agenda items have been used so far during genera- tion. This checklist is updated every time an agenda item reference is generated and is used to compute the available agenda items at each time step. The available items are used as an input to the language model and to constrain which agenda items can still be referenced during generation. Agenda embed- dings are also used when generating item references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Input variable definitions</head><p>We assume the goal g and agenda items E (see Sec. 2) are each defined by a set of tokens. Goal tokens come from a fixed vocabulary V goal , the item tokens come from a fixed vocabulary V agenda , and the tokens of the text x t come from a fixed vocab- ulary V text . In an abuse of notation, we represent each goal g, agenda item e i , and text token x t as a k-dimensional word embedding vector. We com- pute these embeddings by creating indicator vec- tors of the vocabulary token (or set of tokens for goals and agenda items) and embed those vectors using a trained k × |V z | projection matrix, where z ∈ {goal, agenda, text} depending whether we are generating a goal, agenda item, or text token. Given a goal embedding g ∈ R k , a matrix of L agenda items E ∈ R L×k , a checklist soft record of what items have been used a t−1 ∈ R L , a previous hidden state h t−1 ∈ R k , and the current input word embedding x t ∈ R k , our architecture computes the next hidden state h t , an embedding used to generate the output word o t , and the updated checklist a t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generating output token probabilities</head><p>To generate the output token probability distribution (see "Generate output" box in <ref type="figure" target="#fig_1">Fig. 2</ref>), w t ∈ R |Vtext| , we project the output hidden state o t into the vocab- ulary space and apply a softmax:</p><formula xml:id="formula_1">w t = softmax(W o o t ),</formula><p>where W o ∈ R |V |×k is a trained projection ma- trix. The output hidden state is the linear interpola- tion of (1) content c gru t from a Gated Recurrent Unit 331 (GRU) language model, (2) an encoding c new t gen- erated from the new agenda item reference model <ref type="bibr">(Sec. 4.</ref>3), and (3) and an encoding c used t generated from a previously used item model (Sec. 4.4):</p><formula xml:id="formula_2">o t = f gru t c gru t + f new t c new t + f used t c used t .</formula><p>The interpolation weights, f gru t , f new t , and f used t , are probabilities representing how much the output token should reflect the current state of the language model or a chosen agenda item. f gru t is the proba- bility of a non-agenda-item token, f new t is the prob- ability of an new item reference token, and f used t is the probability of a used item reference. In the <ref type="figure" target="#fig_0">Fig. 1</ref> example, f new t is high in the first row when new ingredient references "tomatoes" and "onion" are generated; f used t is high when the reference back to "tomatoes" is made in the second row, and f gru t is high the rest of the time.</p><p>To generate these weights, our model uses a three- way probabilistic classifier, ref -type(h t ), to deter- mine whether the hidden state of the GRU h t will generate non-agenda tokens, new agenda item refer- ences, or used item references. ref -type(h t ) gener- ates a probability distribution f t ∈ R 3 as</p><formula xml:id="formula_3">f t = ref -type(h t ) = sof tmax(βSh t ),</formula><p>where S ∈ R 3×k is a trained projection matrix and β is a temperature hyper-parameter.</p><formula xml:id="formula_4">f gru t = f 1 t , f new t = f 2</formula><p>t , and f used t = f 3 t . ref -type() does not use the agenda, only the hidden state h t : h t must encode when to use the agenda, and ref -type() is trained to identify that in h t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">New agenda item reference model</head><p>The two key features of our model are that it (1) pre- dicts which agenda item is being referred to, if any, at each time step and (2) stores those predictions for use during generation. These components allow for improved output texts that are more likely to men- tion agenda items while avoiding repetition and ref- erences to irrelevant items not in the agenda. These features are enabled by a checklist vector a t ∈ R L that represents the probability each agenda item has been introduced into the text. The checklist vector is initialized to all zeros at t = 1, representing that all items have yet to be introduced. The check- list vector is a soft record with each a t,i ∈ [0, 1]. <ref type="bibr">1</ref> We introduce the remaining items as a matrix E new t ∈ R L×k , where each row is an agenda item embedding weighted by how likely it is to still need to be referenced. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, after the first "tomatoes" is generated, the row representing "chopped tomatoes" in the agenda will be weighted close to 0. We calculate E new t using the checklist vector (see "Update [...] items" box in <ref type="figure" target="#fig_1">Fig. 2</ref>):</p><formula xml:id="formula_5">E new t = ((1 L − a t−1 ) ⊗ 1 k ) • E,</formula><p>where 1 L = {1} L , 1 k = {1} k , and the outer prod- uct ⊗ replicates 1 L − a t−1 for each dimension of the embedding space.</p><p>• is the Hadamard product (i.e., element-wise multiplication) of two matrices with the same dimensions.</p><p>The model predicts when an agenda item will be generated using ref -type() (see Sec. 4.2 for de- tails). When it does, the encoding c new t approxi- mates which agenda item is most likely. c new t is computed using an attention model that generates a learned soft alignment α new t ∈ R L between the hid- den state h t and the rows of E new t (i.e., available items). The alignment is a probability distribution representing how close h t is to each item:</p><formula xml:id="formula_6">α new t ∝ exp(γE new t P h t ),</formula><p>where P ∈ R k×k is a learned projection matrix and γ is a temperature hyper-parameter. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the shaded squares in the top line (i.e., the first "tomatoes" and the onion references) represent this alignment. The attention encoding c new t is then the attention-weighted sum of the agenda items:  </p><formula xml:id="formula_7">c new t = E T α new t . At each</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Previously used item reference model</head><p>We also allow references to be generated for previ- ously used agenda items through the previously used item encoding c used t . This is useful in longer texts -when agenda items can be referred to more than once -so that the agenda is always responsible for generating its own referring expressions. The exam- ple in <ref type="figure" target="#fig_0">Fig. 1</ref> </p><note type="other">refers back to tomatoes when generating to what to add the diced onion. At each time step t, we use a second atten- tion model to compare h t to a used items matrix E used t ∈ R L×k . Like the remaining agenda item matrix E new t , E used t is calculated using the checklist vector generated at the previous time step:</note><formula xml:id="formula_8">E used t = (a t−1 ⊗ 1 k ) • E.</formula><p>The attention over the used items, α used t ∈ R L , and the used attention encoding c used t are calculated in the same way as those over the available items (see Sec. 4.3 for comparison):</p><formula xml:id="formula_9">α used t ∝ exp(γE used t P h t ), c used t = E T α used t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">GRU language model</head><p>Our decoder RNN adapts a Gated Recurrent Unit (GRU) ( ). Given an input x t ∈ R k at time step t and the previous hidden state h t−1 ∈ R k , a GRU computes the next hidden state h t as</p><formula xml:id="formula_10">h t = (1 − z t )h t−1 + z t ˜ h t .</formula><p>The update gate, z t , interpolates between h t−1 and new content, ˜ h t , defined respectively as</p><formula xml:id="formula_11">z t = σ(W z x t + U z h t−1 ), ˜ h t = tanh(W x t + r t U h t−1 ).</formula><p>is an element-wise multiplication, and the reset gate, r t , is calculated as</p><formula xml:id="formula_12">r t = σ(W r x t + U r h t−1 ). W z , U z , W , U , W r , U r ∈ R k×k are trained projec- tion matrices.</formula><p>We adapted a GRU to allow extra inputs, namely the goal g and the available agenda items E new t (see "GRU language model" box in <ref type="figure" target="#fig_1">Fig. 2</ref>). These extra inputs help guide the language model stay on topic. Our adapted GRU has a change to the computation of the new content˜hcontent˜ content˜h t as follows:</p><formula xml:id="formula_13">˜ h t = tanh(W h x t + r t U h h t−1 + s t Y g + q t (1 T L ZE new t ) T ,</formula><p>where s t is a goal select gate and q t is a item select gate, respectively defined as</p><formula xml:id="formula_14">s t = σ(W s x t + U s h t−1 ), q t = σ(W q x t + U q h t−1 ).</formula><p>1 L sums the rows of the available item matrix E new t . Y , Z, W s , U s , W q , U q ∈ R k×k are trained projec- tion matrices. The goal select gate controls when the goal should be taken into account during genera- tion: for example, the recipe title may be used to de- cide what the imperative verb for a new step should be. The item select gate controls when the avail- able agenda items should be taken into account (e.g., when generating a list of ingredients to combine). The GRU hidden state is initialized with a projec- tion of the goal:</p><formula xml:id="formula_15">h 0 = U g g, where U g ∈ R k×k .</formula><p>The content vector c gru t that is used to compute the output hidden state o t is a linear projection of the GRU hidden state, c gru t = P h t , where P is the same learned projection matrix used in the computation of the attention weights (see Sections 4.3 and 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Training</head><p>Given a training set of (goal, agenda, output text) triples {(g <ref type="bibr">(1)</ref> , E (1) , x (1) ), . . . , (g (J) , E (J) , x (J) )}, we train model parameters by minimizing negative log-likelihood:</p><formula xml:id="formula_16">N LL(θ) = − J j=1 Nj i=2 log p(x (j) i |x (j) 1 , . . . , x (j) i−1 , g (j) , E (j) ; θ), where x (j)</formula><p>1 is the start symbol. We use mini-batch stochastic gradient descent, and back-propagate through the goal, agenda, and text embeddings.</p><p>It is sometimes the case that weak heuristic su- pervision on latent variables can be easily gathered to improve training. For example, for recipe gen- eration, we can approximate the linear interpolation weights f t and the attention updates a new t and a used t using string match heuristics comparing tokens in the text to tokens in the ingredient list. <ref type="bibr">2</ref> When this extra signal is available, we add mean squared loss terms to N LL(θ) to encourage the latent variables to take those values; for example, if f * t is the true value and f t is the predicted value, a loss term −(f * t − f t ) 2 is added. When this signal is not available, as is the case with our dialogue generation task, we instead introduce a mean squared loss term that encourages the final checklist a (j) N j to be a vector of 1s (i.e., every agenda item is accounted for).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Generation</head><p>We generate text using beam search, which has been shown to be fast and accurate for RNN decoding <ref type="bibr" target="#b8">(Graves, 2012;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014</ref>). When the beam search completes, we select the highest prob- ability sequence that uses the most agenda items. This is the count of how many times the three-way classifier, ref -type(h t ), chose to generate an new item reference with high probability (i.e., &gt; 50%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental setup</head><p>Our model was implemented and trained using the Torch scientific computing framework for Lua. <ref type="bibr">3</ref> Experiments We evaluated neural checklist mod- els on two natural language generation tasks. The first task is cooking recipe generation. Given a recipe title (i.e., the name of the dish) as the goal and the list of ingredients as the agenda, the system must generate the correct recipe text. Our second evalua- tion is based on the task from <ref type="bibr" target="#b29">Wen et al. (2015)</ref> for generating dialogue responses for hotel and restau- rant information systems. The task is to generate a natural language response given a query type (e.g., informing or querying) and a list of facts to convey (e.g., a hotel's name and address).</p><p>Parameters We constrain the gradient norm to 5.0 and initialize parameters uniformly on <ref type="bibr">[−0.35, 0.35]</ref>. We used a beam of size 10 for gen- eration. Based on dev set performance, a learning rate of 0.1 was chosen, and the temperature hyper- parameters (β, γ) were (5, 2) for the recipe task and (1, 10) for the dialogue task. The models for the recipe task had a hidden state size of k = 256; the 3 http://torch.ch/ models for the dialogue task had k = 80 to compare to previous models. We use a batch size 30 for the recipe task and 10 for the dialogue task.</p><p>Recipe data and pre-processing We use the Now You're Cooking! recipe library: the data set contains over 150,000 recipes in the Meal-Master TM for- mat. <ref type="bibr">4</ref> We heuristically removed sentences that were not recipe steps (e.g., author notes, nutritional in- formation, publication information). 82,590 recipes were used for training, and 1,000 each for develop- ment and testing. We filtered out recipes to avoid exact duplicates between training and dev (test) sets.</p><p>We collapsed multi-word ingredient names into single tokens using word2phrase <ref type="bibr">5</ref> ran on the train- ing data ingredient lists. Titles and ingredients were cleaned of non-word tokens. Ingredients addition- ally were stripped of amounts (e.g., "1 tsp"). As mentioned in Sec. 4.6, we approximate true values for the interpolation weights and attention updates for recipes based on string match between the recipe text and the ingredient list. The first ingredient ref- erence in a sentence cannot be the first token or after a comma (e.g., the bold tokens cannot be ingredients in "oil the pan" and "in a large bowl, mix [...]").</p><p>Recipe data statistics Automatic recipe genera- tion is difficult due to the length of recipes, the size of the vocabulary, and the variety of possible dishes. In our training data, the average recipe length is 102 tokens, and the longest recipe has 814 tokens. The vocabulary of the recipe text from the training data (i.e., the text of the recipe not including the title or ingredient list) has 14,103 unique tokens. About 31% of tokens in the recipe vocabulary occur at least 100 times in the training data; 8.6% of the tokens oc- cur at least 1000 times. The training data also repre- sents a wide variety of recipe types, defined by the recipe titles. Of 3793 title tokens, only 18.9% of the title tokens in the title vocabulary occur at least 100 times in the training data, which demonstrates the large variability in the titles.</p><p>Dialogue system data and processing We used the hotel and restaurant dialogue system corpus and the same train-development-test split from <ref type="bibr" target="#b29">Wen et al. (2015)</ref>. We used the same pre-processing, sets of reference samples, and baseline output, and we were given model output to compare against. <ref type="bibr">6</ref> For training, slot values (e.g., "Red Door Cafe") were re- placed by generic tokens (e.g., "NAME TOKEN"). After generation, generic tokens were swapped back to specific slot values. Minor post-processing in- cluded removing duplicate determiners from the re- lexicalization and merging plural "-s" tokens onto their respective words. After replacing specific slot values with generic tokens, the training data vocab- ulary size of the hotel corpus is 445 tokens, and that of the restaurant corpus is 365 tokens. The task has eight goals (e.g., inform, confirm).</p><p>Models Our main baseline EncDec is a model us- ing the RNN Encoder-Decoder framework proposed by  and <ref type="bibr" target="#b23">Sutskever et al. (2014)</ref>. The model encodes the goal and then each agenda item in sequence and then decodes the text using GRUs. The encoder has two sets of parameters: one for the goal and the other for the agenda items. For the di- alogue task, we also compare against the SC-LSTM system from <ref type="bibr" target="#b29">Wen et al. (2015)</ref> and the handcrafted rule-based generator described in that paper.</p><p>For the recipe task, we also compare against three other baselines. The first is a basic attention model, Attention, that generates an attention encoding by comparing the hidden state h t to the agenda. That encoding is added to the hidden state, and a non- linear transformation is applied to the result before projecting into the output space. We also present a nearest neighbor baseline (NN) that simply copies over an existing recipe text based on the input simi- larity computed using cosine similarity over the title and the ingredient list. Finally, we present a hybrid approach (NN-Swap) that revises a nearest neighbor recipe using the neural checklist model. The neural checklist model is forced to generate the returned recipe nearly verbatim, except that it can generate new strings to replace any extraneous ingredients.</p><p>Our neural checklist model is labeled Checklist. We also present the Checklist+ model, which in- teractively re-writes a recipe to better cover the in- put agenda: if the generated text does not use every agenda item, embeddings corresponding to missing items are multiplied by increasing weights and a new recipe is generated. This process repeats until the <ref type="bibr">6</ref> We thank the authors for sharing their system outputs.  <ref type="table">Table 1</ref>: Quantitative results on the recipe task. The line with ot = ht has the results for the non-interpolation ablation.</p><note type="other">-4 METEOR Avg. % given items Avg. extra items Attention 2.8 8.6 22.8% 3.0 EncDec 3.1 9.4 26.9% 2.0 NN 7.1 12.1 40.0% 4.2 NN-Swap 7.1 12.8 58.2% 2.1 Checklist 3.0 10.3 67.9% 0.6 -ot = ht 2.1 8.3 29.1% 2.4 -no used 3.0 10.4 62.2% 1.9 -no supervision 3.7 10.1 38.9% 1.8 Checklist+ 3.8 11.5 83.4% 0.8</note><p>new recipe does not contain new items. We also report the performance of our check- list model without the additional weak supervision of heuristic ingredient references (-no supervision) (see Sec. 4.6). <ref type="bibr">7</ref> we also evaluate two ablations of our checklist model on the recipe task. First, we re- move the linear interpolation and instead use h t as the output (see Sec. 4.2). Second, we remove the previously used item reference model by changing ref -type() to a 2-way classifier between new ingre- dient references and all other tokens (see Sec. 4.4).</p><p>Metrics We include commonly used metrics like BLEU-4, <ref type="bibr">8</ref> and METEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014)</ref>. Because neither of these metrics can measure how well the generated recipe follows the input goal and the agenda, we also define two additional met- rics. The first measures the percentage of the agenda items corrected used, while the second measures the number of extraneous items incorrectly introduced. Both these metrics are computed based on simple string match and can miss certain referring expres- sions (e.g., "meat" to refer to "pork"). Because of the approximate nature of these automated metrics, we also report a human evaluation. <ref type="figure" target="#fig_0">Fig. 1</ref>    METEOR; this is due to a number of recipes that have very similar text but make different dishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Recipe generation results</head><p>However, NN baselines are not successful in gen- erating a goal-oriented text that follows the given agenda: compared to Checklist+ (83.4%), they use substantially less % of the given ingredients (40% - 58.2%) while also introducing extra ingredients not provided. EncDec and Attention baselines similarly generate recipes that are not relevant to the given in- put, using only 22.8% -26.9% of the agenda items. Checklist models rarely introduce extraneous ingre- dients not provided (0.6 -0.8), while other baselines make a few mistakes on average (2.0 -4.2).</p><p>The ablation study demonstrates the empirical contribution of different model components. (o t = h t ) shows the usefulness of the attention encodings when generating the agenda references, while (-no used) shows the need for separate attention mech- anisms between new and used ingredient references for more accurate use of the agenda items. Similarly, (-no supervision) demonstrates that the weak super- vision encourages the model to learn more accurate management of the agenda items.</p><p>Human evaluation Because neither BLEU nor METEOR is suitable for evaluating generated text in terms of their adherence to the provided goal and the agenda, we also report human evaluation using Amazon Mechanical Turk. We evaluate the gener- ated recipes on (1) grammaticality, (2) how well the recipe adheres to the provided ingredient list, and <ref type="formula">(3)</ref> how well the generated recipe accomplishes the de- sired dish. We selected 100 random test recipes. For each question we used a Likert scale (∈ <ref type="bibr">[1,</ref><ref type="bibr">5]</ref>) and report averaged ratings among five turkers. <ref type="table" target="#tab_3">Table 2</ref> shows the averaged scores over the re- sponses. The checklist models outperform all base- lines in generating recipes that follow the provided agenda closely and accomplish the desired goal, where NN in particular often generates the wrong dish. Perhaps surprisingly, both the Attention and EncDec baselines and the Checklist model beat the true recipes in terms of having better grammar. This can partly be attributed to noise in the parsing of the true recipes, and partly because the neural models tend to generate shorter, simpler texts. <ref type="figure" target="#fig_5">Fig. 3</ref> shows the counts of the most used vocab- ulary tokens in the true dev set recipes compared to the recipes generated by EncDec and Checklist+. Using the vocabulary from the training data, the true dev recipes use 5206 different tokens. The EncDec's vocabulary is only ∼16% of that size, while the Checklist+ model is a third of the size.</p><p>An error analysis on the dev set shows that the EncDec baseline over-generates catch-all phrases like "all ingredients" or "the ingredients," used in 21% of the generated recipes, whereas only 7.8% of true recipes use that construction. This phrase type simplifies the recipe, but using all ingredients in one step reduces the chance of accomplishing the desired dish correctly. The Checklist model only generates those phrases 13% of the time.</p><p>Qualitative analysis <ref type="figure" target="#fig_6">Fig. 4</ref> shows two dev set recipes with generations from the EncDec and Checklist+ models. The EncDec model is much more likely to both use incorrect ingredients and to introduce ingredients more than once (e.g., "baking power" and "salt" in the bottom example are not in the ingredient list, and "milk" in the top example is duplicated). In the top example, the Checklist+ model refers to both Parmesean and Swiss cheese as "cheese"; generating more precise referring ex- pressions is an important area for future work. The Checklist+ recipes generate the correct dishes to an extent: for example, the top recipe makes a casse- role but does not cook the ingredients together be- fore baking and mixes in biscuits instead of putting    <ref type="table">Table 3</ref>: Quantitative evaluation of the top generations in the hotel and restaurant domains them on top. Future work could better model the full set of steps needed to achieve the overall goal. <ref type="figure" target="#fig_5">Figure 3</ref> shows our results on the hotel and restau- rant dialogue system generation tasks. HDC is the rule-based baseline from <ref type="bibr" target="#b29">Wen et al. (2015)</ref>. For both domains, the checklist model achieved the highest BLEU-4 and METEOR scores, but both neural sys- tems performed very well. The power of our model is in generating long texts, but this experiment shows that our model can generalize well to other tasks with different kinds of agenda items and goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Title: Casino royale casserole Ingredients: 10 oz chopped broccoli, 2 tbsp butter, 2 tbsp flour, 1/2 tsp salt, 1/4 tsp black pepper, 1/4 tsp ground nutmeg, 1 cup milk, 1 1/2 cup shredded swiss cheese, 2 tsp lemon juice, 2 cup cooked cubed turkey, 4 oz mushrooms, 1/4 cup grated Parmesan cheese, 1 can refrigerated biscuits</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Dialogue system results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future work and conclusions</head><p>We present the neural checklist model that gener- ates globally coherent text by keeping track of what has been said and still needs to be said from a pro- vided agenda. Future work includes incorporating referring expressions for sets or compositions of agenda items (e.g., "vegetables"). The neural check- list model is sensitive to hyperparameter initializa- tion, which should be investigated in future work. The neural checklist model can also be adapted to handle multiple checklists, such as checklists over composite entities created over the course of a recipe (see <ref type="bibr" target="#b13">Kiddon (2016)</ref> for an initial proposal).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example checklist recipe generation. A checklist (right dashed column) tracks which agenda items (top boxes;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A diagram of the neural checklist model. The bottom portion depicts how the model generates the output embedding ot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label></label><figDesc>Similar to a new t , a used t = f used t · α used t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Counts of the most used vocabulary tokens (sorted by count) in the true dev set recipes and in generated recipes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example dev set generated recipes. Tokenization, newlines, and capitalization changed for space and readability. Bolded ingredient references are either ingredients not in the list and/or duplicated initial ingredient references.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>results for recipe generation. All BLEU and METEOR scores are low, which is expected for long texts. Our checklist model performs better than both neural network baselines (Attention and EncDec) in all metrics. Nearest neighbor baselines (NN and NN-Swap) perform the best in terms of BLEU and</figDesc><table>Model 

Syntax Ingredient use Follows goal 
Attention 
4.47 
3.02 
3.47 
EncDec 
4.58 
3.29 
3.61 
NN 
4.22 
3.02 
3.36 
NN-Swap 
4.11 
3.51 
3.78 
Checklist 
4.58 
3.80 
3.94 
Checklist+ 
4.39 
3.95 
4.10 
Truth 
4.39 
4.03 
4.34 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Human evaluation results on the generated and true 

recipes. Scores range in [1, 5]. 

1.00 

10.00 

100.00 

1000.00 

10000.00 

0 
500 
1000 
1500 
2000 
Token counts in dev recipes 

Tokens (sorted by count) 

True recipes 
EncDec 
Checklist+ 

</table></figure>

			<note place="foot" n="1"> By definition, at is non-negative. We truncate any values greater than 1 using a hard tanh function.</note>

			<note place="foot" n="4"> Recipes and format at http://www.ffts.com/recipes.htm 5 See https://code.google.com/p/word2vec/</note>

			<note place="foot" n="7"> For this model, parameters were initialized on [-0.2, 0.2] to maximize development accuracy. 8 See Moses system (http://www.statmt.org/moses/)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported in part by the In-tel Science and Technology Center for Pervasive Computing (ISTC-PC), NSF (IIS-1252835 and IIS-1524371), DARPA under the CwC program through the ARO (W911NF-15-1-0543), and gifts by Google and Facebook. We thank our anonymous review-ers for their comments and suggestions, as well as Yannis Konstas, Mike Lewis, Mark Yatskar, Antoine Bosselut, Luheng He, Eunsol Choi, Victoria Lin, Kenton Lee, and Nicholas FitzGerald for helping us read and edit. We also thank Mirella Lapata and An-nie Louis for their suggestions for baselines.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple domain-independent probabilistic approach to generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="502" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating coherent event schemas at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1721" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collective content selection for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2005 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating Referring Expressions in a Domain of Objects and Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
		<respStmt>
			<orgName>Centre for Cognitive Science, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks. Representation Learning Worksop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CHEF: A model of casebased planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kristian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth National Conference on Artificial Intelligence (AAAI-86)</title>
		<meeting>the Fifth National Conference on Artificial Intelligence (AAAI-86)</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="267" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guiding long-short term memory for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2407" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning to Interpret and Generate Instructional Recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Computer Science &amp; Engineering, University of Washington</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A global model for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="305" to="346" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning semantic correspondences with less supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What to talk about and how? Selective generation using lstms with coarse-to-fine alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>September. Hongyuan Mei, Mohit Bansal, and Matthew R. Walter</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ynock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH 2010, the 11th Annual Conference of the International Speech Communication Association</title>
		<meeting>INTERSPEECH 2010, the 11th Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>ICASSP 2011</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FlowGraph2Text: Automatic sentence skeleton compilation for procedural text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuro</forename><surname>Sasada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Funatomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoko</forename><surname>Yamakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Natural Language Generation Conference</title>
		<meeting>the 8th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="118" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Building Natural Language Generation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meg</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Strategy and tactics: a model for language production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Papers from the Thirteenth Regional Meeting of the Chicago Linguistics Society</title>
		<imprint>
			<publisher>Chicago Linguistics Society</publisher>
			<date type="published" when="1977" />
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1608.06043</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th</title>
		<meeting>the 54th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-domain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
