<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coherence-Aware Neural Topic Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Amazon Web Services</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Coherence-Aware Neural Topic Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="830" to="836"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>830</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Topic models are evaluated based on their ability to describe documents well (i.e. low per-plexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework , we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the setting of a topic model <ref type="bibr" target="#b2">(Blei, 2012)</ref>, per- plexity measures the model's capability to describe documents according to a generative process based on the learned set of topics. In addition to de- scribing documents well (i.e. achieving low per- plexity), it is desirable to have topics (represented by top-N most probable words) that are human- interpretable. Topic interpretability or coherence can be measured by normalized point-wise mutual information (NPMI) ( <ref type="bibr" target="#b0">Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b9">Lau et al., 2014</ref>). The calculation of NPMI however is based on look-up operations in a large reference corpus and therefore is non-differentiable and com- putationally intensive. Likely due to these reasons, topic models so far have been solely optimizing for perplexity, and topic coherence is only evalu- ated after training. As has been noted in several publications ( <ref type="bibr" target="#b4">Chang et al., 2009)</ref>, optimization for perplexity alone tends to negatively impact topic coherence. Thus, without introducing topic coher- ence as a training objective, topic modeling likely produces sub-optimal results.</p><p>Compared to classical methods, such as mean- field approximation <ref type="bibr" target="#b6">(Hoffman et al., 2010)</ref> and collapsed Gibbs sampling ( <ref type="bibr" target="#b5">Griffiths and Steyvers, 2004</ref>) for the latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b3">Blei et al., 2003</ref>) model, neural variational infer- ence ( <ref type="bibr" target="#b8">Kingma and Welling, 2013;</ref><ref type="bibr">Rezende et al., 2014</ref>) offers a flexible framework to accommodate more expressive topic models. We build upon the line of work on topic modeling using neural varia- tional inference ( <ref type="bibr" target="#b11">Miao et al., 2016</ref><ref type="bibr" target="#b10">Miao et al., , 2017</ref><ref type="bibr">Srivastava and Sutton, 2017)</ref> and incorporate topic coherence awareness into topic modeling.</p><p>Our approaches of constructing topic coherence training objective leverage pre-trained word em- beddings ( <ref type="bibr" target="#b12">Mikolov et al., 2013;</ref><ref type="bibr" target="#b13">Pennington et al., 2014;</ref><ref type="bibr" target="#b15">Salle et al., 2016;</ref>. The main motivation is that word embeddings carry contextual similarity information that is highly re- lated to the mutual information terms involved in the calculation of NPMI. In this paper, we explore two methods: (1) we explicitly construct a differ- entiable surrogate topic coherence regularization term; (2) we use word embedding matrix as a factor- ization constraint on the topical word distribution matrix that implicitly encourages topic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline: Neural Topic Model (NTM)</head><p>The model architecture shown in <ref type="figure">Figure 1</ref> is a vari- ant of the Neural Variational Document Model (NVDM) ( <ref type="bibr" target="#b11">Miao et al., 2016)</ref>. Let x ∈ R |V |×1 be the bag-of-words (BOW) representation of a document, where |V | is the size of the vocabu- lary and let z ∈ R K×1 be the latent topic variable, where K is the number of topics. In the encoder q φ (z|x), we have π = f M LP (x), µ(x) = l 1 (π), log σ(x) = l 2 (π), h(x, ) = µ + σ , where ∼ N (0, I), and finally z = f (h) = ReLU(h). The functions l 1 and l 2 are linear transformations z = f (h)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G R y V b n z 0 8 D G j c D B u G W G t y y 2 l c E Q = " &gt; A A A C C X i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W o m z I j g t 0 I B T c u K 9 g H t E P J Z D J t a C Y Z k o x Q h 9 m 6 8 V f c u F D E r X / g z r 8 x 0 8 6 i t h 4 I O Z x z L / f e 4 8 e M K u 0 4 P 1 Z p b X 1 j c 6 u 8 X d n Z 3 d s / s A + P O k o k E p M 2 F k z I n o 8 U Y Z S T t q a a k V 4 s C Y p 8 R r r + 5 C b 3 u w 9 E K i r 4 v Z 7 G x I v Q i N O Q Y q S N N L T h w B c s U N P I f O l j d h 3 W F o V x d j 6 0 q 0 7 d m Q G u E r c g V V C g N b S / B 4 H A S U S 4 x g w p 1 X e d W H s p k p p i R r L K I F E k R n i C R q R v K E c R U V 4 6 u y S D Z 0 Y J Y C i k e V z D m b r Y k a J I 5 b u Z y g j p s V r 2 c v E / r 5 / o s O G l l M e J J h z P B 4 U J g 1 r A P B Y Y U E m w Z l N D E J b U 7 A r x G E m E t Q m v Y k J w l 0 9 e J Z 2 L u u v U 3 b v L a r N R x F E G J + A U 1 I A L r k A T 3 I I W a A M M n s A L e A P v 1 r P 1 a n 1 Y n / P S k l X 0 H I M / s L 5 + A Y O o m s 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G R y V b n z 0 8 D G j c D B u G W G t y y 2 l c E Q = " &gt; A A A C C X i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W o m z I j g t 0 I B T c u K 9 g H t E P J Z D J t a C Y Z k o x Q h 9 m 6 8 V f c u F D E r X / g z r 8 x 0 8 6 i t h 4 I O Z x z L / f e 4 8 e M K u 0 4 P 1 Z p b X 1 j c 6 u 8 X d n Z 3 d s / s A + P O k o k E p M 2 F k z I n o 8 U Y Z S T t q a a k V 4 s C Y p 8 R r r + 5 C b 3 u w 9 E K i r 4 v Z 7 G x I v Q i N O Q Y q S N N L T h w B c s U N P I f O l j d h 3 W F o V x d j 6 0 q 0 7 d m Q G u E r c g V V C g N b S / B 4 H A S U S 4 x g w p 1 X e d W H s p k p p i R r L K I F E k R n i C R q R v K E c R U V 4 6 u y S D Z 0 Y J Y C i k e V z D m b r Y k a J I 5 b u Z y g j p s V r 2 c v E / r 5 / o s O G l l M e J J h z P B 4 U J g 1 r A P B Y Y U E m w Z l N D E J b U 7 A r x G E m E t Q m v Y k J w l 0 9 e J Z 2 L u u v U 3 b v L a r N R x F E G J + A U 1 I A L r k A T 3 I I W a A M M n s A L e A P v 1 r P 1 a n 1 Y n / P S k l X 0 H I M / s L 5 + A Y O o m s 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G R y V b n z 0 8 D G j c D B u G W G t y y 2 l c E Q = " &gt; A A A C C X i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W o m z I j g t 0 I B T c u K 9 g H t E P J Z D J t a C Y Z k o x Q h 9 m 6 8 V f c u F D E r X / g z r 8 x 0 8 6 i t h 4 I O Z x z L / f e 4 8 e M K u 0 4 P 1 Z p b X 1 j c 6 u 8 X d n Z 3 d s / s A + P O k o k E p M 2 F k z I n o 8 U Y Z S T t q a a k V 4 s C Y p 8 R r r + 5 C b 3 u w 9 E K i r 4 v Z 7 G x I v Q i N O Q Y q S N N L T h w B c s U N P I f O l j d h 3 W F o V x d j 6 0 q 0 7 d m Q G u E r c g V V C g N b S / B 4 H A S U S 4 x g w p 1 X e d W H s p k p p i R r L K I F E k R n i C R q R v K E c R U V 4 6 u y S D Z 0 Y J Y C i k e V z D m b r Y k a J I 5 b u Z y g j p s V r 2 c v E / r 5 / o s O G l l M e J J h z P B 4 U J g 1 r A P B Y Y U E m w Z l N D E J b U 7 A r x G E m E t Q m v Y k J w l 0 9 e J Z 2 L u u v U 3 b v L a r N R x F E G J + A U 1 I A L r k A T 3 I I W a A M M n s A L e A P v 1 r P 1 a n 1 Y n / P S k l X 0 H I M / s L 5 + A Y O o m s 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G R y V b n z 0 8 D G j c D B u G W G t y y 2 l c E Q = " &gt; A A A C C X i c b V D L S g M x F M 3 U V 6 2 v U Z d u g k W o m z I j g t 0 I B T c u K 9 g H t E P J Z D J t a C Y Z k o x Q h 9 m 6 8 V f c u F D E r X / g z r 8 x 0 8 6 i t h 4 I O Z x z L / f e 4 8 e M K u 0 4 P 1 Z p b X 1 j c 6 u 8 X d n Z 3 d s / s A + P O k o k E p M 2 F k z I n o 8 U Y Z S T t q a a k V 4 s C Y p 8 R r r + 5 C b 3 u w 9 E K i r 4 v Z 7 G x I v Q i N O Q Y q S N N L T h w B c s U N P I f O l j d h 3 W F o V x d j 6 0 q 0 7 d m Q G u E r c g V V C g N b S / B 4 H A S U S 4 x g w p 1 X e d W H s p k p p i R r L K I F E k R n i C R q R v K E c R U V 4 6 u y S D Z 0 Y J Y C i k e V z D m b r Y k a J I 5 b u Z y g j p s V r 2 c v E / r 5 / o s O G l l M e J J h z P B 4 U J g 1 r A P B Y Y U E m w Z l N D E J b U 7 A r x G E m E t Q m v Y k J w l 0 9 e J Z 2 L u u v U 3 b v L a r N R x F E G J + A U 1 I A L r k A T 3 I I W a A M M n s A L e A P v 1 r P 1 a n 1 Y n / P S k l X 0 H I M / s L 5 + A Y O o m s 0 = &lt; / l a t e x i t &gt; µ(x) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 A 3 2 L 1 g 0 n s 4 F L p n s i z f H / G G M 7 8 = " &gt; A A A B / H i c b V D N S 8 M w H E 3 n 1 5 x f 1 R 2 9 B I c w L 6 M V w R 0 H X j x O c B + w l p G m 6 R a W p C V J x V L m v + L F g y J e / U O 8 + d + Y b j 3 o 5 o O Q x 3 u / H 3 l 5 Q c K o 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + / i k r + J U Y t L D M Y v l M E C K M C p I T 1 P N y D C R B P G A k U E w u y n 8 w Q O R i s b i X m c J 8 T m a C B p R j L S R x n b d 4 2 n T C 2 I W q o y b K 3 + c X 4 z t h t N y F o D r x C 1 J A 5 T o j u 0 v L 4 x x y o n Q m C G l R q 6 T a D 9 H U l P M y L z m p Y o k C M / Q h I w M F Y g T 5 e e L 8 H N 4 b p Q Q R r E 0 R 2 i 4 U H 9 v 5 I i r I p u Z 5 E h P 1 a p X i P 9 5 o 1 R H b T + n I k k 1 E X j 5 U J Q y q G N Y N A F D K g n W L D M E Y U l N V o i n S C K s T V 8 1 U 4 K 7 + u V 1 0 r 9 s u U 7 L v b t q d N p l H V V w C s 5 A E 7 j g G n T A L e i C H s A g A 8 / g F b x Z T 9 a L 9 W 5 9 L E c r V r l T B 3 9 g f f 4 A + 8 6 U 8 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 A 3 2 L 1 g 0 n s 4 F L p n s i z f H / G G M 7 8 = " &gt; A A A B / H i c b V D N S 8 M w H E 3 n 1 5 x f 1 R 2 9 B I c w L 6 M V w R 0 H X j x O c B + w l p G m 6 R a W p C V J x V L m v + L F g y J e / U O 8 + d + Y b j 3 o 5 o O Q x 3 u / H 3 l 5 Q c K o 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + / i k r + J U Y t L D M Y v l M E C K M C p I T 1 P N y D C R B P G A k U E w u y n 8 w Q O R i s b i X m c J 8 T m a C B p R j L S R x n b d 4 2 n T C 2 I W q o y b K 3 + c X 4 z t h t N y F o D r x C 1 J A 5 T o j u 0 v L 4 x x y o n Q m C G l R q 6 T a D 9 H U l P M y L z m p Y o k C M / Q h I w M F Y g T 5 e e L 8 H N 4 b p Q Q R r E 0 R 2 i 4 U H 9 v 5 I i r I p u Z 5 E h P 1 a p X i P 9 5 o 1 R H b T + n I k k 1 E X j 5 U J Q y q G N Y N A F D K g n W L D M E Y U l N V o i n S C K s T V 8 1 U 4 K 7 + u V 1 0 r 9 s u U 7 L v b t q d N p l H V V w C s 5 A E 7 j g G n T A L e i C H s A g A 8 / g F b x Z T 9 a L 9 W 5 9 L E c r V r l T B 3 9 g f f 4 A + 8 6 U 8 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 A 3 2 L 1 g 0 n s 4 F L p n s i z f H / G G M 7 8 = " &gt; A A A B / H i c b V D N S 8 M w H E 3 n 1 5 x f 1 R 2 9 B I c w L 6 M V w R 0 H X j x O c B + w l p G m 6 R a W p C V J x V L m v + L F g y J e / U O 8 + d + Y b j 3 o 5 o O Q x 3 u / H 3 l 5 Q c K o 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + / i k r + J U Y t L D M Y v l M E C K M C p I T 1 P N y D C R B P G A k U E w u y n 8 w Q O R i s b i X m c J 8 T m a C B p R j L S R x n b d 4 2 n T C 2 I W q o y b K 3 + c X 4 z t h t N y F o D r x C 1 J A 5 T o j u 0 v L 4 x x y o n Q m C G l R q 6 T a D 9 H U l P M y L z m p Y o k C M / Q h I w M F Y g T 5 e e L 8 H N 4 b p Q Q R r E 0 R 2 i 4 U H 9 v 5 I i r I p u Z 5 E h P 1 a p X i P 9 5 o 1 R H b T + n I k k 1 E X j 5 U J Q y q G N Y N A F D K g n W L D M E Y U l N V o i n S C K s T V 8 1 U 4 K 7 + u V 1 0 r 9 s u U 7 L v b t q d N p l H V V w C s 5 A E 7 j g G n T A L e i C H s A g A 8 / g F b x Z T 9 a L 9 W 5 9 L E c r V r l T B 3 9 g f f 4 A + 8 6 U 8 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 A 3 2 L 1 g 0 n s 4 F L p n s i z f H / G G M 7 8 = " &gt; A A A B / H i c b V D N S 8 M w H E 3 n 1 5 x f 1 R 2 9 B I c w L 6 M V w R 0 H X j x O c B + w l p G m 6 R a W p C V J x V L m v + L F g y J e / U O 8 + d + Y b j 3 o 5 o O Q x 3 u / H 3 l 5 Q c K o 0 o 7 z b V U 2 N r e 2 d 6 q 7 t b 3 9 g 8 M j + / i k r + J U Y t L D M Y v l M E C K M C p I T 1 P N y D C R B P G A k U E w u y n 8 w Q O R i s b i X m c J 8 T m a C B p R j L S R x n b d 4 2 n T C 2 I W q o y b K 3 + c X 4 z t h t N y F o D r x C 1 J A 5 T o j u 0 v L 4 x x y o n Q m C G l R q 6 T a D 9 H U l P M y L z m p Y o k C M / Q h I w M F Y g T 5 e e L 8 H N 4 b p Q Q R r E 0 R 2 i 4 U H 9 v 5 I i r I p u Z 5 E h P 1 a p X i P 9 5 o 1 R H b T + n I k k 1 E X j 5 U J Q y q G N Y N A F D K g n W L D M E Y U l N V o i n S C K s T V 8 1 U 4 K 7 + u V 1 0 r 9 s u U 7 L v b t q d N p l H V V w C s 5 A E 7 j g G n T A L e i C H s A g A 8 / g F b x Z T 9 a L 9 W 5 9 L E c r V r l T B 3 9 g f f 4 A + 8 6 U 8 Q = = &lt; / l a t e x i t &gt;</head><p>log (x)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; f MLP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " N H g u Y J A m h h A h 8 m N s j C z u x Y b G y B A = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 4 U K t g P a E P Z b C f t 0 s 0 m 7 G 6 E E v o j v H h Q x K u / x 5 v / x m 2 b g 7 Y + G H i 8 N 8 P M v C A R X B v X / X Y K G 5 t b 2 z v F 3 d L e / s H h U f n 4 p K 3 j V D F s s V j E q h t Q j Y J L b B l u B H Y T h T Q K B H a C y c 3 c 7 z y h 0 j y W j 2 a a o B / R k e Q h Z 9 R Y q R M O s v u 7 5 m x Q r r h V d w G y T r y c V C B H c 1 D + 6 g 9 j l k Y o D R N U 6 5 7 n J s b P q D K c C Z y V + q n G h L I J H W H P U k k j 1 H 6 2 O H d G L q w y J G G s b E l D F u r v i Y x G W k + j w H Z G 1 I z 1 q j c X / / N 6 q Q n r f s Z l k h q U b L k o T A U x M Z n / T o Z c I T N i a g l l i t t b C R t T R Z m x C Z V s C N 7 q y + u k f V X 1 3 K r 3 c F 1 p 1 P M 4 i n A G 5 3 A J H t S g A b f Q h B Y w m M A z v M K b k z g v z r v z s W w t O P n M K f y B 8 / k D F q q P X A = = &lt; / l a t e x i t &gt;</head><p>✏ ⇠ N (0, I) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 9 O P D Y 9 K w P r 9 N f e 2 c n + v i 4 A X M v E = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X D h a h g p R E B L s s u N G N V L A P a E K Z T K f t 0 H m E m Y l Q Q p d u / B U 3 L h R x 6 y e 4 8 2 + c t F l o 9 c C F w z n 3 c u 8 9 U c y o N p 7 3 5 R S W l l d W 1 4 r r p Y 3 N r e 0 d d 3 e v p W W i M G l i y a T q R E g T R g V p G m o Y 6 c S K I B 4 x 0 o 7 G l 5 n f v i d K U y n u z C Q m I U d D Q Q c U I 2 O l n n s Y k F h T J g U M N O U w 4 M i M M G L p z b T i n c L r k 5 5 b 9 q r e D P A v 8 X N S B j k a P f c z 6 E u c c C I M Z k j r r u / F J k y R M h Q z M i 0 F i S Y x w m M 0 J F 1 L B e J E h + n s k S k 8 t k o f D q S y J Q y c q T 8 n U s S 1 n v D I d m a H 6 k U v E / / z u o k Z 1 M K U i j g x R O D 5 o k H C o J E w S w X 2 q S L Y s I k l C C t q b 4 V 4 h B T C x m Z X s i H 4 i y / / J a 2 z q u 9 V / d v z c r 2 W x 1 E E B + A I V I A P L k A d X I E G a A I M H s A T e A G v z q P z 7 L w 5 7 / P W g p P P 7 I N f c D 6 + A Q 2 Y m K Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 9 O P D Y 9 K w P r 9 N f e 2 c n + v i 4 A X M v E = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X D h a h g p R E B L s s u N G N V L A P a E K Z T K f t 0 H m E m Y l Q Q p d u / B U 3 L h R x 6 y e 4 8 2 + c t F l o 9 c C F w z n 3 c u 8 9 U c y o N p 7 3 5 R S W l l d W 1 4 r r p Y 3 N r e 0 d d 3 e v p W W i M G l i y a T q R E g T R g V p G m o Y 6 c S K I B 4 x 0 o 7 G l 5 n f v i d K U y n u z C Q m I U d D Q Q c U I 2 O l n n s Y k F h T J g U M N O U w 4 M i M M G L p z b T i n c L r k 5 5 b 9 q r e D P A v 8 X N S B j k a P f c z 6 E u c c C I M Z k j r r u / F J k y R M h Q z M i 0 F i S Y x w m M 0 J F 1 L B e J E h + n s k S k 8 t k o f D q S y J Q y c q T 8 n U s S 1 n v D I d m a H 6 k U v E / / z u o k Z 1 M K U i j g x R O D 5 o k H C o J E w S w X 2 q S L Y s I k l C C t q b 4 V 4 h B T C x m Z X s i H 4 i y / / J a 2 z q u 9 V / d v z c r 2 W x 1 E E B + A I V I A P L k A d X I E G a A I M H s A T e A G v z q P z 7 L w 5 7 / P W g p P P 7 I N f c D 6 + A Q 2 Y m K Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 9 O P D Y 9 K w P r 9 N f e 2 c n + v i 4 A X M v E = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X D h a h g p R E B L s s u N G N V L A P a E K Z T K f t 0 H m E m Y l Q Q p d u / B U 3 L h R x 6 y e 4 8 2 + c t F l o 9 c C F w z n 3 c u 8 9 U c y o N p 7 3 5 R S W l l d W 1 4 r r p Y 3 N r e 0 d d 3 e v p W W i M G l i y a T q R E g T R g V p G m o Y 6 c S K I B 4 x 0 o 7 G l 5 n f v i d K U y n u z C Q m I U d D Q Q c U I 2 O l n n s Y k F h T J g U M N O U w 4 M i M M G L p z b T i n c L r k 5 5 b 9 q r e D P A v 8 X N S B j k a P f c z 6 E u c c C I M Z k j r r u / F J k y R M h Q z M i 0 F i S Y x w m M 0 J F 1 L B e J E h + n s k S k 8 t k o f D q S y J Q y c q T 8 n U s S 1 n v D I d m a H 6 k U v E / / z u o k Z 1 M K U i j g x R O D 5 o k H C o J E w S w X 2 q S L Y s I k l C C t q b 4 V 4 h B T C x m Z X s i H 4 i y / / J a 2 z q u 9 V / d v z c r 2 W x 1 E E B + A I V I A P L k A d X I E G a A I M H s A T e A G v z q P z 7 L w 5 7 / P W g p P P 7 I N f c D 6 + A Q 2 Y m K Q = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 9 O P D Y 9 K w P r 9 N f e 2 c n + v i 4 A X M v E = " &gt; A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X D h a h g p R E B L s s u N G N V L A P a E K Z T K f t 0 H m E m Y l Q Q p d u / B U 3 L h R x 6 y e 4 8 2 + c t F l o 9 c C F w z n 3 c u 8 9 U c y o N p 7 3 5 R S W l l d W 1 4 r r p Y 3 N r e 0 d d 3 e v p W W i M G l i y a T q R E g T R g V p G m o Y 6 c S K I B 4 x 0 o 7 G l 5 n f v i d K U y n u z C Q m I U d D Q Q c U I 2 O l n n s Y k F h T J g U M N O U w 4 M i M M G L p z b T i n c L r k 5 5 b 9 q r e D P A v 8 X N S B j k a P f c z 6 E u c c C I M Z k j r r u / F J k y R M h Q z M i 0 F i S Y x w m M 0 J F 1 L B e J E h + n s k S k 8 t k o f D q S y J Q y c q T 8 n U s S 1 n v D I d m a H 6 k U v E / / z u o k Z 1 M K U i j g x R O D 5 o k H C o J E w S w X 2 q S L Y s I k l C C t q b 4 V 4 h B T C x m Z X s i H 4 i y / / J a 2 z q u 9 V / d v z c r 2 W x 1 E E B + A I V I A P L k A d X I E G a A I M H s A T e A G v z q P z 7 L w 5 7 / P W g p P P 7 I</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b V D L S s N A F J 3 4 r P U V d e l m s A h 1 Y U l E s M u C G 5 c V 7 A O a E C b T S T t 0 8 m D m R q y h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s v D H M 4 5 x 7 u v c d P B F d g W T / G y u r a + s Z m a a u 8 v b O 7 t 2 8 e H L Z V n E r K W j Q W s e z 6 R D H B I 9 Y C D o J 1 E 8 l I 6 A v W 8 U f X u d 6 5 Z 1 L x O L q D c c L c k A w i H n B K Q F O e e Z 5 4 D g w Z k K r j x 6 K v x q H + s o c J d r Q L 8 D z 5 O D n z z I p V s 6 a F l 4 F d g A o q q u m Z 3 0 4 / p m n I I q C C K N W z r Q T c j E j g V L B J 2 U k V S w g d k Q H r a R i R k C k 3 m 5 4 1 w a e a 6 e M g l v p F g K f s v C M j o c p 3 0 5 0 h g a F a 1 H L y P 6 2 X Q l B 3 M x 4 l K b C I z g Y F q c A Q 4 z w j 3 O e S U R B j D Q i V X O + K 6 Z B I Q k E n W d Y h 2 I s n L 4 P 2 R c 2 2 a v b t Z a V R L + I o o W N 0 g q r I R l e o g W 5 Q E 7 U Q R U / o B b 2 h d + P Z e D U + j M 9 Z 6 4 p R e I 7 Q n z K + f g H M 1 6 B a &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L z x 9 L f f s r y z P f U Q T z p P 4 J 8 g i s J s = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e l m s A h 1 Y U l E s M u C G 5 c V 7 A O a E C b T S T t 0 8 m D m R q y h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s v D H M 4 5 x 7 u v c d P B F d g W T / G y u r a + s Z m a a u 8 v b O 7 t 2 8 e H L Z V n E r K W j Q W s e z 6 R D H B I 9 Y C D o J 1 E 8 l I 6 A v W 8 U f X u d 6 5 Z 1 L x O L q D c c L c k A w i H n B K Q F O e e Z 5 4 D g w Z k K r j x 6 K v x q H + s o c J d r Q L 8 D z 5 O D n z z I p V s 6 a F l 4 F d g A o q q u m Z 3 0 4 / p m n I I q C C K N W z r Q T c j E j g V L B J 2 U k V S w g d k Q H r a R i R k C k 3 m 5 4 1 w a e a 6 e M g l v p F g K f s v C M j o c p 3 0 5 0 h g a F a 1 H L y P 6 2 X Q l B 3 M x 4 l K b C I z g Y F q c A Q 4 z w j 3 O e S U R B j D Q i V X O + K 6 Z B I Q k E n W d Y h 2 I s n L 4 P 2 R c 2 2 a v b t Z a V R L + I o o W N 0 g q r I R l e o g W 5 Q E 7 U Q R U / o B b 2 h d + P Z e D U + j M 9 Z 6 4 p R e I 7 Q n z K + f g H M 1 6 B a &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L z x 9 L f f s r y z P f U Q T z p P 4 J 8 g i s J s = " &gt; A A A C F n i c b V D L S s N A F J 3 4 r P U V d e l m s A h 1 Y U l E s M u C G 5 c V 7 A O a E C b T S T t 0 8 m D m R q y h X + H G X 3 H j Q h G 3 4 s 6 / c d J m U V s v D H M 4 5 x 7 u v c d P B F d g W T / G y u r a + s Z m a a u 8 v b O 7 t 2 8 e H L Z V n E r K W j Q W s e z 6 R D H B I 9 Y C D o J 1 E 8 l I 6 A v W 8 U f X u d 6 5 Z 1 L x O L q D c c L c k A w i H n B K Q F O e e Z 5 4 D g w Z k K r j x 6 K v x q H + s o c J d r Q L 8 D z 5 O D n z z I p V s 6 a F l 4 F d g A o q q u m Z 3 0 4 / p m n I I q C C K N W z r Q T c j E j g V L B J 2 U k V S w g d k Q H r a R i R k C k 3 m 5 4 1 w a e a 6 e M g l v p F g K f s v C M j o c p 3 0 5 0 h g a F a 1 H L y P 6 2 X Q l B 3 M x 4 l K b C I z g Y F q c A Q 4 z w j 3 O e S U R B j D Q i V X O + K 6 Z B I Q k E n W d Y h 2 I s n L 4 P 2 R c 2 2 a v b t Z a V R L + I o o W N 0 g q r I R l e o g W 5 Q E 7 U Q R U / o B b 2 h d + P Z e D U + j M 9 Z 6 4 p R e I 7 Q n z K + f g H M 1 6 B a &lt; / l a t e x i t &gt; N (0, I) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a h 2 4 J w k F U + a k S q w / e k a b S Y L 6 y y M = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S J U k J K I Y J c F N 7 q R C v Y B b S i T 6 a Q d O p m E m Y l S Y j / F j Q t F 3 P o l 7 v w b J 2 0 W 2 n p g 4 H D O v d w z x 4 8 5 U 9 p x v q 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 y 4 d t F S U S E K b J O K R 7 P h Y U c 4 E b W q m O e 3 E k u L Q 5 7 T t j 6 8 y v / 1 A p W K R u N e T m H o h H g o W M I K 1 k f p 2 q R d i P S K Y p 7 f T i n O G b k 7 7 d t m p O j O g Z e L m p A w 5 G n 3 7 q z e I S B J S o Q n H S n V d J 9 Z e i q V m h N N p s Z c o G m M y x k P a N V T g k C o v n U W f o h O j D F A Q S f O E R j P 1 9 0 a K Q 6 U m o W 8 m s 6 B q 0 c v E / 7 x u o o O a l z I R J 5 o K M j 8 U J B z p C G U 9 o A G T l G g + M Q Q T y U x W R E Z Y Y q J N W 0 V T g r v 4 5 W X S O q + 6 T t W 9 u y j X a 3 k d B T i C Y 6 i A C 5 d Q h 2 t o Q B M I P M I z v M K b 9 W S 9 W O / W x 3 x 0 x c p 3 D u E P r M 8 f m f C S 3 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a h 2 4 J w k F U + a k S q w / e k a b S Y L 6 y y M = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S J U k J K I Y J c F N 7 q R C v Y B b S i T 6 a Q d O p m E m Y l S Y j / F j Q t F 3 P o l 7 v w b J 2 0 W 2 n p g 4 H D O v d w z x 4 8 5 U 9 p x v q 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 y 4 d t F S U S E K b J O K R 7 P h Y U c 4 E b W q m O e 3 E k u L Q 5 7 T t j 6 8 y v / 1 A p W K R u N e T m H o h H g o W M I K 1 k f p 2 q R d i P S K Y p 7 f T i n O G b k 7 7 d t m p O j O g Z e L m p A w 5 G n 3 7 q z e I S B J S o Q n H S n V d J 9 Z e i q V m h N N p s Z c o G m M y x k P a N V T g k C o v n U W f o h O j D F A Q S f O E R j P 1 9 0 a K Q 6 U m o W 8 m s 6 B q 0 c v E / 7 x u o o O a l z I R J 5 o K M j 8 U J B z p C G U 9 o A G T l G g + M Q Q T y U x W R E Z Y Y q J N W 0 V T g r v 4 5 W X S O q + 6 T t W 9 u y j X a 3 k d B T i C Y 6 i A C 5 d Q h 2 t o Q B M I P M I z v M K b 9 W S 9 W O / W x 3 x 0 x c p 3 D u E P r M 8 f m f C S 3 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a h 2 4 J w k F U + a k S q w / e k a b S Y L 6 y y M = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S J U k J K I Y J c F N 7 q R C v Y B b S i T 6 a Q d O p m E m Y l S Y j / F j Q t F 3 P o l 7 v w b J 2 0 W 2 n p g 4 H D O v d w z x 4 8 5 U 9 p x v q 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 y 4 d t F S U S E K b J O K R 7 P h Y U c 4 E b W q m O e 3 E k u L Q 5 7 T t j 6 8 y v / 1 A p W K R u N e T m H o h H g o W M I K 1 k f p 2 q R d i P S K Y p 7 f T i n O G b k 7 7 d t m p O j O g Z e L m p A w 5 G n 3 7 q z e I S B J S o Q n H S n V d J 9 Z e i q V m h N N p s Z c o G m M y x k P a N V T g k C o v n U W f o h O j D F A Q S f O E R j P 1 9 0 a K Q 6 U m o W 8 m s 6 B q 0 c v E / 7 x u o o O a l z I R J 5 o K M j 8 U J B z p C G U 9 o A G T l G g + M Q Q T y U x W R E Z Y Y q J N W 0 V T g r v 4 5 W X S O q + 6 T t W 9 u y j X a 3 k d B T i C Y 6 i A C 5 d Q h 2 t o Q B M I P M I z v M K b 9 W S 9 W O / W x 3 x 0 x c p 3 D u E P r M 8 f m f C S 3 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a h 2 4 J w k F U + a k S q w / e k a b S Y L 6 y y M = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S J U k J K I Y J c F N 7 q R C v Y B b S i T 6 a Q d O p m E m Y l S Y j / F j Q t F 3 P o l 7 v w b J 2 0 W 2 n p g 4 H D O v d w z x 4 8 5 U 9 p x v q 2 V 1 b X 1 j c 3 C V n F 7 Z 3 d v 3 y 4 d t F S U S E K b J O K R 7 P h Y U c 4 E b W q m O e 3 E k u L Q 5 7 T t j 6 8 y v / 1 A p W K R u N e T m H o h H g o W M I K 1 k f p 2 q R d i P S K Y p 7 f T i n O G b k 7 7 d t m p O j O g Z e L m p A w 5 G n 3 7 q z e I S B J S o Q n H S n V d J 9 Z e i q V m h N N p s Z c o G m M y x k P a N V T g k C o v n U W f o h O j D F A Q S f O E R j P 1 9 0 a K Q 6 U m o W 8 m s 6 B q 0 c v E / 7 x u o o O a l z I R J 5 o K M j 8 U J B z p C G U 9 o A G T l G g + M Q Q T y U x W R E Z Y Y q J N W 0 V T g r v 4 5 W X S O q + 6 T t W 9 u y j X a 3 k d B T i C Y 6 i A C 5 d Q h 2 t o Q B M I P M I z v M K b 9 W S 9 W O / W x 3 x 0 x c p 3 D u E P r M 8 f m f C S 3 Q = = &lt; / l a t e x i t &gt; W &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 r 2 I 0 w a d 7 7 b S z y I C V E s p K Q F + 4 G k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z R 8 e p Y t h m s Y h V L 6 A a B Z f Y N t w I 7 C U K a R Q I 7 A b T u 4 X f f U K l e S w f z C x B P 6 J j y U P O q L F S q z s s V 9 y q u w T Z J F 5 O K p C j O S x / D U Y x S y O U h g m q d d 9 z E + N n V B n O B M 5 L g 1 R j Q t m U j r F v q a Q R a j 9 b H j o n V 1 Y Z k T B W t q Q h S / X 3 R E Y j r W d R Y D s j a i Z 6 3 V u I / 3 n 9 1 I R 1 P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S u a l 6 b t V r 3 V Y a 9 T y O I l z A J V y D B z V o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A s T O M 0 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 r 2 I 0 w a d 7 7 b S z y I C V E s p K Q F + 4 G k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z R 8 e p Y t h m s Y h V L 6 A a B Z f Y N t w I 7 C U K a R Q I 7 A b T u 4 X f f U K l e S w f z C x B P 6 J j y U P O q L F S q z s s V 9 y q u w T Z J F 5 O K p C j O S x / D U Y x S y O U h g m q d d 9 z E + N n V B n O B M 5 L g 1 R j Q t m U j r F v q a Q R a j 9 b H j o n V 1 Y Z k T B W t q Q h S / X 3 R E Y j r W d R Y D s j a i Z 6 3 V u I / 3 n 9 1 I R 1 P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S u a l 6 b t V r 3 V Y a 9 T y O I l z A J V y D B z V o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A s T O M 0 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 r 2 I 0 w a d 7 7 b S z y I C V E s p K Q F + 4 G k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z R 8 e p Y t h m s Y h V L 6 A a B Z f Y N t w I 7 C U K a R Q I 7 A b T u 4 X f f U K l e S w f z C x B P 6 J j y U P O q L F S q z s s V 9 y q u w T Z J F 5 O K p C j O S x / D U Y x S y O U h g m q d d 9 z E + N n V B n O B M 5 L g 1 R j Q t m U j r F v q a Q R a j 9 b H j o n V 1 Y Z k T B W t q Q h S / X 3 R E Y j r W d R Y D s j a i Z 6 3 V u I / 3 n 9 1 I R 1 P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S u a l 6 b t V r 3 V Y a 9 T y O I l z A J V y D B z V o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A s T O M 0 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 r 2 I 0 w a d 7 7 b S z y I C V E s p K Q F + 4 G k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E a I 8 F L x 5 b s B / Q h r L Z T t q 1 m 0 3 Y 3 Q g l 9 B d 4 8 a C I V 3 + S N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Q S K 4 N q 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z R 8 e p Y t h m s Y h V L 6 A a B Z f Y N t w I 7 C U K a R Q I 7 A b T u 4 X f f U K l e S w f z C x B P 6 J j y U P O q L F S q z s s V 9 y q u w T Z J F 5 O K p C j O S x / D U Y x S y O U h g m q d d 9 z E + N n V B n O B M 5 L g 1 R j Q t m U j r F v q a Q R a j 9 b H j o n V 1 Y Z k T B W t q Q h S / X 3 R E Y j r W d R Y D s j a i Z 6 3 V u I / 3 n 9 1 I R 1 P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S u a l 6 b t V r 3 V Y a 9 T y O I l z A J V y D B z V o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A s T O M 0 Q = = &lt; / l a t e x i t &gt;</head><formula xml:id="formula_0">h = µ + ✏ &lt; l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " J p 2 U F B O V K 3 w O 0 h Q T G o q g l x 7 j p j s = " &gt; A A A C C H i c b V C 7 S g N B F L 3 r M 8 Z X N K W F g y E g C G H X x j R C w M Y y g n l A N o T Z 2 U k y Z G Z n m Z k V w p J O G 3 / F x k I R W z / B z s 5 P c T Z J o Y k H B g 7 n n M u d e 4 K Y M 2 1 c 9 8 t Z W V 1 b 3 9 j M b e W 3 d 3 b 3 9 g s H h 0 0 t E 0 V o g 0 g u V T v A m n I W 0 Y Z h h t N 2 r C g W A a e t Y H S V + a 0 7 q j S T 0 a 0 Z x 7 Q r 8 C B i f U a w s V K v c D x E l 8 g X C T p D v m Y D g Z E v Q 2 m Q T 2 P N e J Y o u R V 3 C r R M v D k p 1 Y r l + 2 8 A q P c K n 3 4 o S S J o Z A j H W n c 8 N z b d F C v D C K e T v J 9 o G m M y w g P a s T T C g u p u O j 1 k g s p W C V F f K v s i g 6 b q 7 4 k U C 6 3 H I r B J g c 1 Q L 3 q Z + J / X S U y / 2 k 1 Z F C e G R m S 2 q J 9 w Z C T K W k E h U 5 Q Y P r Y E E 8 X s X x E Z Y o W J s d 3 l b Q n e 4 s n L p H l e 8 d y K d 2 P b q M I M O T i C E z g F D y 6 g B t d Q h w Y Q e I A n e I F X 5 9 F 5 d t 6 c 9 1 l 0 x Z n P F O E P n I 8 f L x + a U g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k U T d 8 f p X 2 s w T q j j x g y h e W F O 8 p / U = " &gt; A A A C C H i c b V D L S g M x F M 3 U V 6 2 v a p c u D J a C I J Q Z N 3 Y j F N y 4 r G A f 0 B l K J p N p Q 5 P J k G S E M n S n G 3 / F j Q t F 3 L r w A 9 z p B / g F f o C Z t g t t P R A 4 n H M u N / f 4 M a N K 2 / a H l V t a X l l d y 6 8 X N j a 3 t n e K u 3 s t J R K J S R M L J m T H R 4 o w G p G m p p q R T i w J 4 j 4 j b X 9 4 n v n t a y I V F d G V H s X E 4 6 g f 0 Z B i p I 3 U K x 4 M 4 B l 0 e Q K P o a t o n y P o i k B o 6 J J Y U Z Y l y n b V n g A u E m d G y v V S 5 e b 7 7 e u z 0 S u + u 4 H A C S e R x g w p 1 X X s W H s p k p p i R s Y F N 1 E k R n i I + q R r a I Q 4 U V 4 6 O W Q M K 0 Y J Y C i k e Z G G E / X 3 R I q 4 U i P u m y R H e q D m v U z 8 z + s m O q x 5 K Y 3 i R J M I T x e F C Y N a w K w V G F B J s G Y j Q x C W 1 P w V 4 g G S C G v T X c G U 4 M y f v E h a J 1 X H r j q X p o 0 a m C I P 9 s E h O A I O O A V 1 c A E a o A k w u A X 3 4 B E 8 W X f W g / V s v U y j O W s 2 U w J / Y L 3 + A K k c n O w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k U T d 8 f p X 2 s w T q j j x g y h e W F O 8 p / U = " &gt; A A A C C H i c b V D L S g M x F M 3 U V 6 2 v a p c u D J a C I J Q Z N 3 Y j F N y 4 r G A f 0 B l K J p N p Q 5 P J k G S E M n S n G 3 / F j Q t F 3 L r w A 9 z p B / g F f o C Z t g t t P R A 4 n H M u N / f 4 M a N K 2 / a H l V t a X l l d y 6 8 X N j a 3 t n e K u 3 s t J R K J S R M L J m T H R 4 o w G p G m p p q R T i w J 4 j 4 j b X 9 4 n v n t a y I V F d G V H s X E 4 6 g f 0 Z B i p I 3 U K x 4 M 4 B l 0 e Q K P o a t o n y P o i k B o 6 J J Y U Z Y l y n b V n g A u E m d G y v V S 5 e b 7 7 e u z 0 S u + u 4 H A C S e R x g w p 1 X X s W H s p k p p i R s Y F N 1 E k R n i I + q R r a I Q 4 U V 4 6 O W Q M K 0 Y J Y C i k e Z G G E / X 3 R I q 4 U i P u m y R H e q D m v U z 8 z + s m O q x 5 K Y 3 i R J M I T x e F C Y N a w K w V G F B J s G Y j Q x C W 1 P w V 4 g G S C G v T X c G U 4 M y f v E h a J 1 X H r j q X p o 0 a m C I P 9 s E h O A I O O A V 1 c A E a o A k w u A X 3 4 B E 8 W X f W g / V s v U y j O W s 2 U w J / Y L 3 + A K k c n O w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O t P 3 0 / / 4 8 + J b r W F B F h e 7 S m d Q h 8 Q = " &gt; A A A C C H i c b V D L S g M x F M 3 4 r P V V d e n C Y B E E o c y 4 s R u h 4 M Z l B f u A z l A y m b Q N T S Z D c k c o Q 5 d u / B U 3 L h R x 6 y e 4 8 2 / M t L P Q 1 g O B w z n n c n N P m A h u w H W / n Z X V t f W N z d J W e X t n d 2 + / c n D Y N i r V l L W o E k p 3 Q 2 K Y 4 D F r A Q f B u o l m R I a C d c L x T e 5 3 H p g 2 X M X 3 M E l Y I M k w 5 g N O C V i p X z k Z 4 W v s y x R f Y N / w o S T Y V 5 E C 7 L P E c J E n q m 7 N n Q E v E 6 8 g V V S g 2 a 9 8 + Z G i q W Q x U E G M 6 X l u A k F G N H A q 2 L T s p 4 Y l h I 7 J k P U s j Y l k J s h m h 0 z x m V U i P F D a v h j w T P 0 9 k R F p z E S G N i k J j M y i l 4 v / e b 0 U B v U g 4 3 G S A o v p f N E g F R g U z l v B E d e M g p h Y Q q j m 9 q + Y j o g m F G x 3 Z V u C t 3 j y M m l f 1 j y 3 5 t 2 5 1 U a 9 q K O E j t E p O k c e u k I N d I u a q I U o e k T P 6 B W 9 O U / O i / P u f M y j K 0 4 x c 4 T + w P n 8 A a 1 L m H Y = &lt; / l a t e x i t &gt;</head><p>Figure 1: Model architecture with bias. We choose the multi-layer perceptron (MLP) in the encoder to have two hidden layers with 3 × K and 2 × K hidden units respectively, and we use the sigmoid activation function. The de- coder network p θ (x|z) first maps z to the predicted probability of each of the word in the vocabulary y ∈ R |V |×1 through y = softmax(W z + b), where W ∈ R |V |×K . The log-likelihood of the document can be written as log p θ (x|z) = |V | i=1 {x log y}. We name this model Neural Topic Model (NTM) and use it as our baseline. We use the same encoder MLP configuration for our NVDM implementation and all variants of NTM models used in Section 3. In NTM, the objective function to maximize is the usual evidence lower bound (ELBO) which can be expressed as</p><formula xml:id="formula_1">L ELBO (x i ) ≈ 1 L L l=1 log p θ (x i |z i,l ) − D KL (q φ (h|x)||p θ (h)</formula><p>where</p><formula xml:id="formula_2">z i,l = ReLU(h(x i , l )), l ∼ N (0, I).</formula><p>We approximate E z∼q(z|x) [log p θ (x|z)] with Monte Carlo integration and calculate the Kullback- Liebler (KL) divergence analytically using the fact</p><formula xml:id="formula_3">D KL (q φ (z|x)||p θ (z)) = D KL (q φ (h|x)||p θ (h))</formula><p>due to the invariance of KL divergence under deter- ministic mapping between h and z. Compared to NTM, NVDM uses different ac- tivation functions and has z = h. <ref type="bibr" target="#b10">Miao (2017)</ref> proposed a modification to NVDM called Gaus- sian Softmax Model (GSM) corresponding to hav- ing z = softmax(h). <ref type="bibr">Srivastava (2017)</ref> proposed a model called ProdLDA, which uses a Dirichlet prior instead of Gaussian prior for the latent vari- able h. Given a learned W , the practice to extract top-N most probable words for each topic is to take the most positive entries in each column of W ( <ref type="bibr" target="#b11">Miao et al., 2016</ref><ref type="bibr" target="#b10">Miao et al., , 2017</ref><ref type="bibr">Srivastava and Sutton, 2017)</ref>. This is an intuitive choice, provided that z is non-negative, which is indeed the case for NTM, GSM and ProdLDA. NVDM, GSM, and ProdLDA are state-of-the-art neural topic models which we will use for comparison in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Topic Coherence Regularization: NTM-R</head><p>The topic coherence metric NPMI ( <ref type="bibr" target="#b0">Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b9">Lau et al., 2014</ref>) is defined as</p><formula xml:id="formula_4">NPMI(w) = 1 N (N − 1) N j=2 j−1 i=1 log P (w i ,w j ) P (w i )P (w j ) − log P (w i , w j )</formula><p>where w is the list of top-N words for a topic. N is usually set to 10. For a model generating K topics, the overall NPMI score is an average over all topics. The computational overhead and non-differentiability originate from extracting the co-occurrence frequency from a large corpus <ref type="bibr">1</ref> .</p><p>From the NPMI formula, it is clear that word- pairs that co-occur often would score high, unless they are rare word-pairs -which would be normal- ized out by the denominator. The NPMI scoring bears remarkable resemblance to the contextual similarity produced by the inner product of word embedding vectors. Along this line of reasoning, we construct a differentiable, computation-efficient word embedding based topic coherence (WETC).</p><p>Let E be the row-normalized word embedding matrix for a list of N words, such that E ∈ R N ×D and E i,: = 1, where D is the dimension of the embedding space. We can define pair-wise word embedding topic coherence in a similar spirit as NPMI:</p><formula xml:id="formula_5">WETC P W (E) = 1 N (N − 1) N j=2 j−1 i=1 E i,: , E j,: = {E T E} − N 2N (N − 1)</formula><p>where ·, ·· denotes inner product. Alternatively, we can define centroid word embedding topic co- herence</p><formula xml:id="formula_6">WETC C (E) = 1 N {Et T }</formula><p>where vector t ∈ R 1×D is the centroid of E, nor- malized to have t = 1. Empirically, we found that the two WETC formulations behave very sim- ilarly. In addition, both WETC P W and WETC C correlate to human judgement almost equally well as NPMI when using GloVe ( <ref type="bibr" target="#b13">Pennington et al., 2014</ref>) vectors 2 .</p><p>With the above observations, we propose the following procedure to construct a WETC-based surrogate topic coherence regularization term: (1) let E ∈ R |V |×D be the pre-trained word embed- ding matrix for the vocabulary, rows aligned with W ; (2) form the W -weighted centroid (topic) vec- tors T ∈ R D×K by T = E T W ; (3) calculate the cosine similarity matrix S ∈ R |V |×K between word vectors and topic vectors by S = ET ; (4) calculate the W -weighted sum of word-to-topic cosine similarities for each topic C ∈ R 1×K as C = i (S W ) i,: . Compared to WETC C , in calculating C we do not perform top-N operation in W , but directly use W for weighted sum. Specif- ically, we use W -weighted topic vector construc- tion in Step-2 and W -weighted sum of the cosine similarities between word vectors and topic vectors in Step-4. To avoid unbounded optimization, we normalize the rows of E and the columns of W be- fore Step-2, and normalize the columns of T after</p><p>Step-2. The overall maximization objective func- tion becomes L R (x; θ, φ) = L ELBO + λ i C i , where λ is a hyper-parameter with positive values controlling the strength of topic coherence regular- ization. We name this model NTM-R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Embedding as a Factorization</head><p>Constraint: NTM-F and NTM-FR Instead of allowing all the elements in W to be freely optimized, we can impose a factorization constraint of W = E ˆ T , where E is the pre-trained word embedding matrix that is fixed, and onlyˆTonlyˆ onlyˆT is allowed to be learned through training. Under this configuration, ˆ T lives in the embedding space, and each entry in W is the dot product similarity between a topic vectorˆTvectorˆ vectorˆT i and a word vector E j . As one can imagine, similar words would have similar vector representations in E and would have similar weights in each column of W . Therefore the fac- torization constraint encourages words with similar meaning to be selected or de-selected together thus potentially improving topic coherence.</p><p>We name the NTM model with factorization con- straint enabled as NTM-F. In addition, we can apply </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results on 20NewsGroup</head><p>First, we compare the proposed models to state- of-the-art neural variational inference based topic models in the literature (NVDM, GSM, and ProdLDA) as well as LDA benchmarks, on the 20NewsGroup dataset 3 . In training NVDM and all NTM models, we used Adadelta optimizer <ref type="bibr">(Zeiler, 2012)</ref>. We set the learning rate to 0.01 and train with a batch size of 256. For NTM-R, NTM- F and NTM-FR, the word embedding we used is <ref type="bibr" target="#b13">GloVe (Pennington et al., 2014</ref>) vectors pre- trained on Wikipedia and Gigaword with 400,000 vocabulary size and 50 embedding dimensions 4 . The topic coherence regularization coefficient λ is set to 50. The results are presented in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Overall we can see that LDA trained with col- lapsed Gibbs sampling achieves the best perplexity, while NTM-F and NTM-FR models achieve the best topic coherence (in NPMI). Clearly, there is a trade-off between perplexity and NPMI as identi- fied by other papers. So we constructed <ref type="figure">Figure 2</ref> show the full evolution of these two metrics over training epochs.</p><p>From <ref type="figure">Figure 2</ref>, it becomes clear that although ProdLDA exhibits good performance on NPMI, it is achieved at a steep cost of perplexity, while NTM-R achieves similar or better NPMI at much lower perplexity levels. At the other end of the spectrum, if we look for low perplexity, the best numbers among neural variational models are be- tween 750 and 800. In this neighborhood, NTM-R substantially outperforms the GSM, NVDM and NTM baseline models. Therefore, we consider NTM-R the best model overall. Different down- stream applications may require different tradeoff points between NPMI and perplexity. However, the proposed NTM-R model does appear to provide tradeoff points on a Pareto front compared to other models across most of the range of perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comments on NTM-F and NTM-FR</head><p>It is worth noting that although NTM-F and NTM- FR exhibit high NPMI early on, they fail to main- tain it during the training process. In addition, both models converged to fairly high perplexity levels. Our hypothesis is that this is caused by NTM-F and NTM-FR's substantially reduced parameter space - from |V | × K to D × K, where |V | ranges from 1,000 to 150,000 in a typical dataset, while D is on the order of 100.</p><p>Some form of relaxation could alleviate this problem. For example, we can let W = E ˆ T + A, where A is of size |V | × K but is heavily regu- larized, or let W = EQˆTEQˆ EQˆT where Q is allowed as additional free parameters. We leave fully address- ing this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Validation on other Datasets</head><p>To further validate the performance improvement from using WETC-based regularization in NTM-R, we compare NTM-R with the NTM baseline model on a few more datasets: DailyKOS, NIPS, and NYTimes <ref type="bibr">5 (Asuncion and Newman, 2007)</ref>. These datasets offer a wide range of document length (ranging from ∼100 to ∼1000 words), vocabulary size (ranging from ∼7,000 to ∼140,000), and type of documents (from news articles to long-form sci- entific writing). In this set of experiments, we used the same settings and hyperparameter λ as before and did not fine-tune for each dataset. The results are presented in <ref type="figure">Figure 3</ref>. In a similar style as <ref type="figure">Fig- ure 2</ref>, we show the evolution of NPMI and WETC versus perplexity over epochs until convergence.</p><p>Among all datasets, we observed improved NPMI at the same perplexity level, validating the effectiveness of the topic coherence regularization. However, on the NYTimes dataset, the improve- ment is quite marginal even though WETC im- provements are very noticeable. One particularity about the NYTimes dataset is that approximately 58,000 words in the 140,000-word vocabulary are named entities. It appears that the large number of named entities resulted in a divergence between NPMI and WETC scoring, which is an issue to address in the future. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we proposed regularization and factor- ization constraints based approaches to incorporate awareness of topic coherence into the formulation of topic models: NTM-R and NTM-F respectively. We observed that NTM-R substantially improves topic coherence with minimal sacrifice in perplex- ity. To our best knowledge, NTM-R is the first topic model that is trained with an objective towards topic coherence -a feature directly contributing to its superior performance. We further showed that the proposed WETC-based regularization method is applicable to a wide range of text datasets. <ref type="bibr">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Word Embedding Topic Coherence</head><p>As studied in <ref type="bibr" target="#b0">(Aletras and Stevenson, 2013)</ref> and ( <ref type="bibr" target="#b9">Lau et al., 2014</ref>), the NPMI metric for assessing topic coherence over a list of words w is defined in Eq. 1. </p><p>where P (w i ) and P (w i , w j ) are the probability of words and word pairs, calculated based on a refer- ence corpus. N is usually set to 10, by convention, so that NPMI is evaluated over the topic-10 words for each topic. For a model generating K topics, the overall NPMI score is an average over all the topics. The computational overhead comes from ex- tracting the relevant co-occurrence frequency from a large corpus. This problem is exacerbated when the look-up also requires a small sliding window as the authors of ( <ref type="bibr" target="#b9">Lau et al., 2014</ref>) suggested. A typi- cal calculation of 50 topics based on a few million documents from the Wikipedia corpus takes ∼20 minutes 6 .</p><p>For a list of words w of length N , we can as- semble a corresponding word embedding matrix E ∈ R N ×D with each row corresponding to a word in the list. D is the dimension of the embedding space. Averaging across the rows, we can obtain vector t ∈ R 1×D as the centroid of all the word vectors. It may be regarded as a "topic" vector. In addition, we assume that each row of E and t is normalized, i.e. t = 1 and E i,: = 1. With these, we define pair-wise and centroid word em- bedding topic coherence WETC P W and WETC C as follows:</p><formula xml:id="formula_8">WETC P W (E) = 1 N (N − 1) N j=2 j−1 i=1 E i,: , E j,: = {E T E} − N 2N (N − 1)<label>(2)</label></formula><formula xml:id="formula_9">WETC C (E) = 1 N {Et T }<label>(3)</label></formula><p>where ·, ·· denotes inner product. The simplifica- tion in Eq. 2 is due to the row normalization of E.</p><p>In this setting, we have the flexibility to use any pre-trained word embeddings to construct E. To ex- periment, we compared several recently developed variants <ref type="bibr">7</ref> . The dataset from <ref type="bibr" target="#b0">(Aletras and Stevenson, 2013</ref>) provides human ratings for 300 topics coming from 3 corpora: 20NewsGroup (20NG), New York Times (NYT) and genomics scientific ar- ticles (Genomics), which we use as the human gold standard. We use Pearson and Spearman correla- tions to compare NPMI and WETC scores against human ratings. The results are shown in <ref type="table" target="#tab_4">Table 2</ref>. <ref type="bibr">7</ref> Details of pre-trained word embeddings used in <ref type="table" target="#tab_4">Table 2</ref> Dataset 20NG NYT Genomics  From <ref type="table" target="#tab_4">Table 2</ref> we observed a minimal difference between pair-wise and centroid based WETC in general. Overall, GloVe appears to perform the best across different types of corpora and its cor- relation with human ratings is very comparable to NPMI-based scores. Our NPMI calculation is based on the Wikipedia corpus and should serve as a fair comparison. In addition to the good correla- tion exhibited by WETC, the evaluation of WETC only involves matrix multiplications and summa- tions and thus is fully differentiable and several orders of magnitude faster than NPMI calculations. WETC opens the door of incorporating topic coher- ence as a training objective, which is the key idea we will investigate in the subsequent sections. It is worth mentioning that, for GloVe, the low di- mensional embedding (50d) appears to perform al- most equally well as high dimensional embedding (300d). Therefore, we will use Glove-400k-50d in all subsequent experiments.</p><p>While the WETC metric on its own might be of interest to the topic modeling research community, we leave the task of formally establishing it as a standard metric in place of NPMI to future work. In this work, we still use the widely accepted NPMI as the objective topic coherence metric for model comparisons.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>P</head><label></label><figDesc>(w i ,w j ) P (w i )P (w j ) − log P (w i , w j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison to LDA and neural variational models on the 20NewsGroup dataset. Best numbers are bolded. The blue underlined row highlights the best NPMI and perplexity tradeoff as discussed in text. the regularization discussed in the previous section on the resulting matrix W and we name the result- ing model NTM-FR.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>NPMI and WETC correlation with human 
gold standard (P: Pearson, S: Spearman) 

</table></figure>

			<note place="foot" n="1"> A typical calculation of NPMI over 50 topics based on the Wikipedia corpus takes ∼20 minutes, using code provided by (Lau et al., 2014) at https://github.com/jhlau/ topic_interpretability.</note>

			<note place="foot" n="2"> See Appendix A for details on an empirical study of human judgement of topic coherence, NPMI and WETC with various types of word embeddings.</note>

			<note place="foot" n="3"> We use the exact dataset from (Srivastava and Sutton, 2017) to avoid subtle differences in pre-processing 4 Obtained from https://nlp.stanford.edu/ projects/glove/</note>

			<note place="foot" n="5"> https://archive.ics.uci.edu/ml/ datasets/Bag+of+Words</note>

			<note place="foot" n="6"> Using code provided by (Lau et al., 2014) at https:// github.com/jhlau/topic_interpretability. Running parallel processes on 8 Intel Xeon E5-2686 CPUs.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Perplexity <ref type="table">NPMI   Number of topics  50  200  50</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)-Long Papers</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS 2013)-Long Papers</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2410" to="2419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">pre-trained on GoogleNews, with 3 million vocabulary size and 300 embedding dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Word2vec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="https://code.google.com/archive/p/word2vec/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">000 vocabulary size and 50 and 300 embedding dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Glove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Pennington</surname></persName>
		</author>
		<ptr target="https://nlp.stanford.edu/projects/glove/" />
	</analytic>
	<monogr>
		<title level="m">pre-trained on Wikipedia and Gigaword</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">400</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">pre-trained on Wikipedia with 2.5 million vocabulary size and 300 embedding dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Fasttext</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Joulin</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/fastText" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">pre-trained on Wikipedia with 370,000 vocabulary size and 300 embedding dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Lexvec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Salle</surname></persName>
		</author>
		<ptr target="https://github.com/alexandres/lexvec" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
