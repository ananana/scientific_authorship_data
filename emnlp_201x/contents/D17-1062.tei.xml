<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentence Simplification with Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
							<email>x.zhang@ed.ac.uk,mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentence Simplification with Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="584" to="594"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent , and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The main goal of sentence simplification is to re- duce the linguistic complexity of text, while still retaining its original information and meaning. The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individ- uals alike <ref type="bibr" target="#b31">(Siddharthan, 2014;</ref><ref type="bibr" target="#b29">Shardlow, 2014)</ref>. For instance, a simplification component could be used as a preprocessing step to improve the perfor- mance of parsers ( <ref type="bibr" target="#b3">Chandrasekar et al., 1996)</ref>, sum- marizers <ref type="bibr" target="#b1">(Beigman Klebanov et al., 2004</ref>), and se- mantic role labelers <ref type="bibr" target="#b36">(Vickrey and Koller, 2008;</ref><ref type="bibr" target="#b40">Woodsend and Lapata, 2014</ref>). Automatic simplifi- cation would also benefit people with low-literacy skills ( <ref type="bibr" target="#b37">Watanabe et al., 2009</ref>), such as children and non-native speakers as well as individuals with autism ( <ref type="bibr" target="#b8">Evans et al., 2014</ref>), aphasia <ref type="bibr" target="#b2">(Carroll et al., 1999</ref>), or dyslexia ( <ref type="bibr" target="#b27">Rello et al., 2013</ref>).</p><p>The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, ren- dering syntactically complex structures simpler, and deleting elements of the original text <ref type="bibr" target="#b31">(Siddharthan, 2014</ref>). Earlier work focused on individ- ual aspects of the simplification problem. For ex- ample, several systems performed syntactic sim- plification only, using rules aimed at sentence splitting ( <ref type="bibr" target="#b2">Carroll et al., 1999;</ref><ref type="bibr" target="#b3">Chandrasekar et al., 1996;</ref><ref type="bibr" target="#b36">Vickrey and Koller, 2008;</ref><ref type="bibr" target="#b30">Siddharthan, 2004</ref>) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases <ref type="bibr" target="#b7">(Devlin, 1999;</ref><ref type="bibr" target="#b11">Inui et al., 2003;</ref><ref type="bibr" target="#b13">Kaji et al., 2002</ref>).</p><p>Recent approaches view the simplification pro- cess more holistically as a monolingual text- to-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from exam- ples of complex-simple sentences extracted from online resources such as the ordinary and sim- ple English Wikipedia.</p><p>For example, <ref type="bibr" target="#b48">Zhu et al. (2010)</ref> draw inspiration from syntax-based translation and propose a model similar to <ref type="bibr" target="#b45">Yamada and Knight (2001)</ref> which additionally per- forms simplification-specific rewrite operations (e.g., sentence splitting). <ref type="bibr" target="#b39">Woodsend and Lapata (2011)</ref> formulate simplification in the framework of Quasi-synchronous grammar <ref type="bibr" target="#b32">(Smith and Eisner, 2006</ref>) and use integer linear programming to score the candidate translations/simplifications. <ref type="bibr" target="#b41">Wubben et al. (2012)</ref> propose a two-stage model: initially, a standard phrase-based machine transla- tion (PBMT) model is trained on complex-simple sentence pairs. During inference, the K-best out- puts of the PBMT model are reranked according to their dis-similarity to the (complex) input sen- tence. The hybrid model developed in <ref type="bibr" target="#b22">Narayan and Gardent (2014)</ref> also operates in two phases. Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer <ref type="bibr" target="#b5">(Curran et al., 2007)</ref>. The resulting sentences are fur- ther simplified by a model similar to <ref type="bibr" target="#b41">Wubben et al. (2012)</ref>. <ref type="bibr" target="#b44">Xu et al. (2016)</ref> train a syntax-based machine translation model on a large scale para- phrase dataset ( <ref type="bibr" target="#b9">Ganitkevitch et al., 2013</ref>) using simplification-specific objective functions and fea- tures to encourage simpler output.</p><p>In this paper we propose a simplification model which draws on insights from neural machine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b35">Sutskever et al., 2014</ref>). Central to this approach is an encoder- decoder architecture implemented by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space repre- sentations from which the decoder generates the target sequence. Although our model uses the encoder-decoder architecture as its backbone, it must also meet constraints imposed by the sim- plification task itself, i.e., the predicted output must be simpler, preserve the meaning of the in- put, and grammatical. To incorporate this knowl- edge, the model is trained in a reinforcement learning framework <ref type="bibr" target="#b38">(Williams, 1992)</ref>: it explores the space of possible simplifications while learn- ing to maximize an expected reward function that encourages outputs which meet simplification- specific constraints. Reinforcement learning has been previously applied to extractive summariza- tion <ref type="bibr" target="#b28">(Ryang and Abekawa, 2012)</ref>, information ex- traction ( <ref type="bibr" target="#b21">Narasimhan et al., 2016)</ref>, dialogue gen- eration ( <ref type="bibr" target="#b17">Li et al., 2016)</ref>, machine translation, and image caption generation ( <ref type="bibr" target="#b26">Ranzato et al., 2016)</ref>. We evaluate our system on three publicly available datasets collated automatically from Wikipedia ( <ref type="bibr" target="#b48">Zhu et al., 2010;</ref><ref type="bibr" target="#b39">Woodsend and Lapata, 2011</ref>) and human-authored news articles ( <ref type="bibr" target="#b43">Xu et al., 2015b</ref>). We experimentally show that the re- inforcement learning framework is the key to suc- cessful generation of simplified text bringing sig- nificant improvements over strong simplification models across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Encoder-Decoder Model</head><p>We will first define a basic encoder-decoder model for sentence simplification and then ex- plain how to embed it in a reinforcement learning framework. Given a (complex) source sentence X = (x 1 , x 2 , . . . , x |X| ), our model learns to pre- dict its simplified target Y = (y 1 , y 2 , . . . , y |Y | ). Inferring the target Y given the source X is a typi- cal sequence to sequence learning problem, which can be modeled with attention-based encoder- decoder models ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b19">Luong et al., 2015</ref>). Sentence simplification is slightly different from related sequence transduction tasks (e.g., compression) in that it can involve split- ting operations. For example, a long source sen- tence (In 1883, Faur married Marie Fremiet, with whom he had two sons.) can be simplified as two sentences (In 1883, Faur married Marie Fremiet. They had two sons.). Nevertheless, we still view the target as a sequence, i.e., two or more se- quences concatenated with full stops.</p><p>The encoder-decoder model has two parts (see left hand side in <ref type="figure">Figure 1</ref>). The encoder trans- forms the source sentence X into a sequence of hidden states (h S 1 , h S 2 , . . . , h S |X| ) with a Long Short-Term Memory Network (LSTM; Hochreiter and Schmidhuber 1997), while the decoder uses another LSTM to generate one word y t+1 at a time in the simplified target Y . Generation is condi- tioned on all previously generated words y 1:t and a dynamically created context vector c t , which en- codes the source sentence:</p><formula xml:id="formula_0">P (Y |X) = |Y | t=1 P (y t |y 1:t−1 , X)<label>(1)</label></formula><formula xml:id="formula_1">P (y t+1 |y 1:t , X) = softmax(g(h T t , c t ))<label>(2)</label></formula><p>where g(·) is a one-hidden-layer neural network with the following parametrization:</p><formula xml:id="formula_2">g(h T t , c t ) = W o tanh(U h h T t + W h c t ) (3)</formula><p>where W o ∈ R |V |×d , U h ∈ R d×d , and W h ∈ R d×d ; |V | is the output vocabulary size and d the hidden unit size. h T t is the hidden state of the de- coder LSTM which summarizes y 1:t , i.e., what has been generated so far:</p><formula xml:id="formula_3">h T t = LSTM(y t , h T t−1 )<label>(4)</label></formula><p>The dynamic context vector c t is the weighted sum of the hidden states of the source sentence:</p><formula xml:id="formula_4">c t = |X| i=1 α ti h S i<label>(5)</label></formula><p>whose weights α ti are determined by an attention mechanism:</p><formula xml:id="formula_5">α ti = exp(h T t · h S i ) i exp(h T t · h S i ) (6)</formula><p>where · is the dot product between two vec- tors. We use the dot product here mainly for ef- ficiency reasons; alternative ways to compute at- tention scores have been proposed in the litera- ture and we refer the interested reader to <ref type="bibr" target="#b19">Luong et al. (2015)</ref>. The model sketched above is usually trained by minimizing the negative log-likelihood of the training source-target pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reinforcement Learning for Sentence Simplification</head><p>In this section we present DRESS, our Deep REinforcement Sentence Simplification model. Despite successful application in numerous se- quence transduction tasks ( <ref type="bibr" target="#b12">Jean et al., 2015;</ref><ref type="bibr" target="#b42">Xu et al., 2015a</ref>), a vanilla encoder-decoder model is not ideal for sentence simplification. Although a number of rewrite oper- ations (e.g., copying, deletion, substitution, word reordering) can be used to simplify text, copy- ing is by far the most common. We empirically found that 73% of the target words are copied from the source in the Newsela dataset. This num- ber further increases to 83% when considering Wikipedia-based datasets (we provide details on these datasets in Section 5). As a result, a generic encoder-decoder model learns to copy all too well at the expense of other rewrite operations, often parroting back the source or making only a few trivial changes.</p><p>To encourage a wider variety of rewrite oper- ations while remaining fluent and faithful to the meaning of the source, we employ a reinforce- ment learning framework (see <ref type="figure">Figure 1)</ref>. We view the encoder-decoder model as an agent which first reads the source sentence X; then at each step, it takes an actionˆyactionˆ actionˆy t ∈ V (where V is the output vo- cabulary) according to a policy P RL ( ˆ y t |ˆy|ˆy 1:t−1 , X) (see Equation <ref type="formula" target="#formula_1">(2)</ref>). The agent continues to take actions until it produces an End Of Sentence (EOS) token yielding the action sequencê</p><formula xml:id="formula_6">Y = (ˆ y 1 , ˆ y 2 , . . . , ˆ y | ˆ Y | )</formula><p>, which is also the simplified out- put of our model. A reward r is then received and the REINFORCE algorithm <ref type="bibr" target="#b38">(Williams, 1992)</ref> is used to update the agent. In the following, we first introduce our reward and then present the de- tails of the REINFORCE algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reward</head><p>The reward r( ˆ Y ) for system outputˆYoutputˆ outputˆY is the weighted sum of the three components aimed at capturing key aspects of the target output, namely simplicity, relevance, and fluency:</p><formula xml:id="formula_7">r( ˆ Y ) = λ S r S + λ R r R + λ F r F (7)</formula><p>where</p><formula xml:id="formula_8">λ S , λ R , λ F ∈ [0, 1]; r( ˆ Y ) is a shorthand for r(X, Y, ˆ Y )</formula><p>where X is the source, Y the reference (or target), andˆYandˆ andˆY the system output. r S , r R , and r F are shorthands for simplicity</p><formula xml:id="formula_9">r S (X, Y, ˆ Y ), rel- evance r R (X, ˆ Y )</formula><p>, and fluency r F ( ˆ Y ). We provide details for each reward summand below.</p><p>Simplicity To encourage the model to apply a wide range of simplification operations, we use SARI ( <ref type="bibr" target="#b44">Xu et al., 2016</ref>), a recently proposed metric which compares System output Against References and against the Input sentence. SARI is the arithmetic average of n-gram precision and recall of three rewrite operations: addition, copy- ing, and deletion. It rewards addition operations where system output was not in the input but oc- curred in the references. Analogously, it rewards words retained/deleted in both the system output and the references. In experimental evaluation <ref type="bibr" target="#b44">Xu et al. (2016)</ref> demonstrate that SARI correlates well with human judgments of simplicity, whilst cor- rectly rewarding systems that both make changes and simplify the input.</p><p>One caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplifica- tion. <ref type="bibr" target="#b44">Xu et al. (2016)</ref> provide eight references for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training. The majority of existing simplification datasets (see Section 5 for details) have a single reference for each source sentence. Moreover, they are unavoid- ably noisy as they are mostly constructed automat- ically, e.g., by aligning sentences from the ordi- nary and simple English Wikipedias. When rely- ing solely on a single reference, SARI will try to reward accidental n-grams that should never have occurred in it. To countenance the effect of noise, we apply SARI(X, ˆ Y , Y ) in the expected direc- tion, with X as the source, ˆ Y the system output, and Y the reference as well as in the reverse direc- tion with Y as the system output andˆYandˆ andˆY as the ref- erence. Assuming our system can produce reason- ably good simplifications, by swapping the output <ref type="figure">Figure 1</ref>: Deep reinforcement learning simplification model. X is the complex sentence, Y the reference (simple) sentence andˆYandˆ andˆY the action sequence (simplification) produced by the encoder-decoder model. and the reference, reverse SARI can be used to es- timate how good a reference is with respect to the system output. Our first reward is therefore the weighted sum of SARI and reverse SARI:</p><formula xml:id="formula_10">X = x 1 x 2 x 3 x 4 x 5 ˆ Y = ˆ y 1 ˆ y 2 ˆ y 3 Get Action Seq. ˆ Y Update Agent Simplicity Model Relevance Model Grammar Model REINFORCE algorithmˆY algorithmˆ algorithmˆY X ˆ Y X ˆ Y Y</formula><formula xml:id="formula_11">r S = β SARI(X, ˆ Y , Y )+(1−β) SARI(X, Y, ˆ Y ) (8)</formula><p>Relevance While the simplicity-based re- ward r S tries to encourage the model to make changes, the relevance reward r R ensures that the generated sentences preserve the meaning of the source. We use an LSTM sentence encoder to convert the source X and the predicted targetˆYtargetˆ targetˆY into two vectors q X and q ˆ Y . The relevance reward r R is simply the cosine similarity between these two vectors:</p><formula xml:id="formula_12">r R = cos(q X , q ˆ Y ) = q X · q ˆ Y ||q X || ||qˆY||qˆ ||qˆY ||<label>(9)</label></formula><p>We use a sequence auto-encoder (SAE; <ref type="bibr" target="#b6">Dai and Le 2015)</ref> to train the LSTM sentence encoder on both the complex and simple sentences. Specifi- cally, the SAE uses sentence X = (x 1 , . . . , x |X| ) to infer itself via an encoder-decoder model (with- out an attention mechanism). Firstly, an encoder LSTM converts X into a sequence of hidden states (h 1 , . . . , h |X| ). Then, we use h |X| to initialize the hidden state of the decoder LSTM and re- cover/generate X one word at a time.</p><p>Fluency <ref type="bibr" target="#b44">Xu et al. (2016)</ref> observe that SARI cor- relates less with fluency compared to other met- rics such as BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>). The fluency reward r F models the well-formedness of the generated sentences explicitly. It is the normal- ized sentence probability assigned by an LSTM language model trained on simple sentences:</p><formula xml:id="formula_13">r F = exp   1 | ˆ Y | | ˆ Y | i=1 log P LM (ˆ y i |ˆy|ˆy 0:i−1 )   (10)</formula><p>We take the exponential ofˆYofˆ ofˆY 's perplexity to en- sure that r F ∈ [0, 1] as is the case with r S and r R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The REINFORCE Algorithm</head><p>The goal of the REINFORCE algorithm is to find an agent that maximizes the expected reward. The training loss for one sequence is its negative ex- pected reward:</p><formula xml:id="formula_14">L(θ) = −E (ˆ y 1 ,...,ˆ y | ˆ Y | )∼P RL (·|X) [r(ˆ y 1 , . . ., ˆ y | ˆ Y | )]</formula><p>where P RL is our policy, i.e., the distribution pro- duced by the encoder-decoder model (see Equa- tion(2)) and r(·) is the reward function of an ac- tion sequencê</p><formula xml:id="formula_15">Y = (ˆ y 1 , . . . , ˆ y | ˆ Y | ), i.</formula><p>e., a gener- ated simplification. Unfortunately, computing the expectation term is prohibitive, since there is an infinite number of possible action sequences. In practice, we approximate this expectation with a single sample from the distribution of P LR (·|X). We refer to <ref type="bibr" target="#b38">Williams (1992)</ref> for the full derivation of the gradients. The gradient of L(θ) is:</p><formula xml:id="formula_16">L(θ) ≈ | ˆ Y | t=1 log P RL (ˆ y t |ˆy|ˆy 1:t−1 , X)[r(ˆ y 1:| ˆ Y | ) − b t ]</formula><p>To reduce the variance of gradients, we also intro- duce a baseline linear regression model b t to es- timate the expected future reward at time t (Ran- zato et al., 2016). b t takes the concatenation of h T trained by minimizing mean squared error. We do not back-propagate this error to h T t or c t during training ( <ref type="bibr" target="#b26">Ranzato et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Presented in its original form, the REINFORCE algorithm starts learning with a random policy. This assumption can make model training chal- lenging for generation tasks like ours with large vocabularies (i.e., action spaces). We address this issue by pre-training our agent (i.e., the encoder- decoder model) with a negative log-likelihood ob- jective (see Section 2), making sure it can pro- duce reasonable simplifications, thereby starting off with a policy which is better than random. We follow prior work ( <ref type="bibr" target="#b26">Ranzato et al., 2016</ref>) in adopting a curriculum learning strategy. In the be- ginning of training, we give little freedom to our agent allowing it to predict the last few words for each target sentence. For every target sequence, we use negative log-likelihood to train the first L (initially, L = 24) tokens and apply the reinforce- ment learning algorithm to the (L + 1)th tokens onwards. Every two epochs, we set L = L − 3 and the training terminates when L is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lexical Simplification</head><p>Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification ( <ref type="bibr" target="#b34">Specia et al., 2012</ref>). The model presented so far learns lexical substitu- tion and other rewrite operations jointly. In some cases, words are predicted because they seem nat- ural in the their context, but are poor substitutes for the content of the complex sentence. To coun- tenance this, we learn lexical simplifications ex- plicitly and integrate them with our reinforcement learning-based model.</p><p>We use an pre-trained encoder-decoder model (which is trained on a parallel corpus of com- plex and simple sentences) to obtain probabilis- tic word alignments, aka attention scores (see α t in Equation <ref type="formula">(6)</ref>). Let X = (x 1 , x 2 , . . . , x |X| ) de- note a source sentence and Y = (y 1 , y 2 , . . . , y |Y | ) a target sentence. We convert X into |X| hidden states (v 1 , v 2 , . . . , v |X| ) with an LSTM. Note that v t ∈ R d×1 corresponds to the context dependent representation of x t . Let α t denote the alignment scores α t1 , α t2 , . . . , α t|X| . The lexical simplifica- tion probability of y t given the source sentence and the alignment scores is:</p><formula xml:id="formula_17">P LS (y t |X, α t ) = softmax(W l s t )<label>(11)</label></formula><p>where W l ∈ R |V |×d and s t represents the source:</p><formula xml:id="formula_18">s t = |X| i=1 α ti v i<label>(12)</label></formula><p>The lexical simplification model on its own encourages lexical substitutions, without taking into account what has been generated so far (i.e., y 1:t−1 ) and as a result fluency could be com- promised. A straightforward solution is to inte- grate lexical simplification with our reinforcement learning trained model (Section 3) using linear in- terpolation, where η ∈ [0, 1]: P (y t |y 1:t−1 , X) = (1 − η) P RL (y t |y 1:t−1 , X)</p><formula xml:id="formula_19">+ η P LS (y t |X, α t )<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>In this section we present our experimental setup for assessing the performance of the simplification model described above. We give details on our datasets, model training, evaluation protocol, and the systems used for comparison.  <ref type="bibr" target="#b14">Kauchak (2013)</ref>, the aligned and revision sentence pairs in <ref type="bibr" target="#b39">Woodsend and Lapata (2011)</ref>, and Zhu's (2010) WikiSmall dataset described above. We used the development and test sets created in <ref type="bibr" target="#b44">Xu et al. (2016)</ref>. These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers. The dataset contains 8 (reference) simplifications for 2,359 sentences partitioned into 2,000 for development and 359 for testing. After removing duplicates and sentences in development and test sets, the result- ing training set contains 296,402 sentence pairs.</p><p>Our third dataset is Newsela, a corpus collated by <ref type="bibr" target="#b43">Xu et al. (2015b)</ref> who argue that Wikipedia- based resources are suboptimal due to the auto- matic sentence alignment which unavoidably in- troduces errors, and their uniform writing style which leads to systems that generalize poorly. Newsela 2 consists of 1,130 news articles, each re- written four times by professional editors for chil- dren at different grade levels (0 is the most com- plex level and 4 is simplest). <ref type="bibr" target="#b43">Xu et al. (2015b)</ref> pro- vide multiple aligned complex-simple pairs within each article. We removed sentence pairs corre- sponding to levels 0-1, 1-2, and 2-3, since they were too similar to each other. The first 1,070 documents were used for training <ref type="bibr">(94,</ref><ref type="bibr">208</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sen- tence pairs), the next 30 documents for develop- ment (1,129 sentence pairs) and the last 30 docu- ments for testing (1,076 sentence pairs). 3 We are not aware of any published results on this dataset.</head><p>Training Details We trained our models on an Nvidia GPU card. We used the same hyper- parameters across datasets. We first trained an encoder-decoder model, and then performed re- inforcement learning training (Section 3), and trained the lexical simplification model (Sec- tion 4). Encoder-decoder parameters were uni- formly initialized to [−0.1, 0.1]. We used Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2014</ref>) to optimize the model with learning rate 0.001; the first momentum coefficient was set to 0.9 and the second momentum coeffi- cient to 0.999. The gradient was rescaled when the norm exceeded 5 ( <ref type="bibr" target="#b24">Pascanu et al., 2013)</ref>. Both encoder and decoder LSTMs have two layers with 256 hidden neurons in each layer. We regularized all LSTMs with a dropout rate of 0.2 ( <ref type="bibr">Zaremba et al., 2014</ref>). We initialized the encoder and de- coder word embedding matrices with 300 dimen- sional Glove vectors ( <ref type="bibr" target="#b25">Pennington et al., 2014)</ref>.</p><p>During reinforcement training, we used plain stochastic gradient descent with a learning rate of 0.01. We set β = 0.1, λ S = 1, λ R = 0.25 and λ F = 0.5. <ref type="bibr">4</ref> Training details for the lexical 2 https://newsela.com <ref type="bibr">3</ref> If a sentence has multiple references in the development or test set, we use the reference with highest simplicity level. <ref type="bibr">4</ref> Weights were tuned on the development set of the Newsela dataset and kept fixed for the other two datasets. simplification model are identical to the encoder- decoder model except that word embedding matri- ces were randomly initialized. The weight of the lexical simplification model was set to η = 0.1.</p><p>To reduce vocabulary size, named entities were tagged with the Stanford CoreNLP ( ) and anonymized with a NE@N to- ken, where NE ∈ {PER, LOC, ORG, MISC} and N indicates NE@N is the N -th distinct NE typed entity. For example, "John and Bob are . . . " be- comes "PER@1 and PER@2 are . . . ". At test time, we de-anonymize NE@N tokens in the out- put by looking them up in their source sentences. Note that the de-anonymization may fail, but the chance is small (around 2% of the time on the Newsela development set). We replaced words oc- curring three times or less in the training set with UNK. At test time, when our models predict UNK, we adopt the UNK replacement method proposed in <ref type="bibr" target="#b12">Jean et al. (2015)</ref>.</p><p>Evaluation Following previous work <ref type="bibr" target="#b39">(Woodsend and Lapata, 2011;</ref><ref type="bibr" target="#b44">Xu et al., 2016</ref>) we eval- uated system output automatically adopting met- rics widely used in the simplification literature. Specifically, we used BLEU 5 ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>) to assess the degree to which generated simplifications differed from gold standard refer- ences and the Flesch-Kincaid Grade Level index (FKGL; <ref type="bibr" target="#b15">Kincaid et al. 1975</ref>) to measure the read- ability of the output (lower FKGL 6 implies sim- pler output). In addition, we used SARI ( <ref type="bibr" target="#b44">Xu et al., 2016)</ref>, which evaluates the quality of the output by comparing it against the source and reference simplifications. 7 BLEU, FKGL, and SARI are all measured at corpus-level. We also evaluated system output by eliciting human judgments via Amazon's Mechanical Turk. Specifically (self- reported) native English speakers were asked to rate simplifications on three dimensions: Fluency (is the output grammatical and well formed?), Ad- equacy (to what extent is the meaning expressed in the original sentence preserved in the output?) and Simplicity (is the output simpler than the original sentence?). All ratings were obtained using a five point Likert scale.</p><p>Comparison Systems We compared our model against several systems previously proposed in the literature. These include PBMT-R, a mono-  lingual phrase-based machine translation system with a reranking post-processing step <ref type="bibr">8 (Wubben et al., 2012)</ref> and Hybrid, a model which first performs sentence splitting and deletion opera- tions over discourse representation structures and then further simplifies sentences with PBMT-R ( <ref type="bibr" target="#b22">Narayan and Gardent, 2014</ref>). Hybrid 9 is state of the art on the WikiSmall dataset. Compar- isons with SBMT-SARI, a syntax-based transla- tion model trained on PPDB ( <ref type="bibr" target="#b9">Ganitkevitch et al., 2013)</ref> and tuned with SARI ( <ref type="bibr" target="#b44">Xu et al., 2016</ref>), are problematic due to the size of PPDB which is con- siderably larger than any of the datasets used in this work (it contains 106 million sentence pairs with 2 billion words). Nevertheless, we compare 10 against SBMT-SARI, but only models trained on Wikilarge, our largest dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Since Newsela contains high quality simplifica- tions created by professional editors, we per- formed the bulk of our experiments on this dataset. Specifically, we set out to answer two ques- tions: (a) which neural model performs best and (b) how do neural models which are resource lean and do not have access to linguistic annotations fare against more traditional systems. We there- fore compared the basic attention-based encoder- <ref type="bibr">8</ref> We made a good-faith effort to re-implement their sys- tem following closely the details in <ref type="bibr" target="#b41">Wubben et al. (2012)</ref>. <ref type="bibr">9</ref> We are grateful to Shashi Narayan for running his system on our three datasets. <ref type="bibr">10</ref> The output of SBMT-SARI is publicly available.  <ref type="table">Table 2</ref>: Mean ratings elicited by humans on Newsela, WikiSmall, and WkiLarge test sets. Rat- ings significantly different from DRESS-LS are marked with * (p &lt; 0.05) and ** (p &lt; 0.01). Sig- nificance tests were performed using a student t-test.</p><p>decoder model (EncDecA), with the deep rein- forcement learning model (DRESS; Section 3), and a linear combination of DRESS and the lexi- cal simplification model (DRESS-LS; Section 4). Neural models were further compared against two strong baselines, PBMT-R and Hybrid. <ref type="table">Ta- ble 3</ref> shows example output of all models on the Newsela dataset. The top block in <ref type="table" target="#tab_2">Table 1</ref> summarizes the results of our automatic evaluation. As can be seen, all neural models obtain higher BLEU, lower FKGL and higher SARI compared to PBMT-R. Hybrid has the lowest FKGL and highest SARI. Com- pared to EncDecA, DRESS scores lower on FKGL and higher on SARI, which indicates that the model has indeed learned to optimize the reward function which includes SARI. Integrating lexical simplification (DRESS-LS) yields better BLEU, but slightly worse FKGL and SARI.</p><p>The results of our human evaluation are pre- sented in the top block of <ref type="table">Table 2</ref>. We elicited judgments for 100 randomly sampled test sen- tences.</p><p>Aside from comparing system out- put (PBMT-R, Hybrid, EncDecA, DRESS, and DRESS-LS), we also elicited ratings for the gold standard Reference as an upper bound. We report results for Fluency, Adequacy, and Simplicity in- dividually and in combination (All is the average rating of the three dimensions). As can be seen, DRESS and DRESS-LS outperform PBMT-R and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complex</head><p>There's just one major hitch: the primary purpose of education is to develop citizens with a wide variety of skills. Reference The purpose of education is to develop a wide range of skills. PBMT-R It's just one major hitch: the purpose of education is to make people with a wide variety of skills. Hybrid one hitch the purpose is to develop citizens. EncDecA</p><p>The key of education is to develop people with a wide variety of skills. DRESS There's just one major hitch: the main goal of education is to develop people with lots of skills. DRESS-LS There's just one major hitch: the main goal of education is to develop citizens with lots of skills. Complex "They were so burdened by the past they couldn't think about the future," said Barnet, 62, who was president of Columbia Records, the No.1 record label in the United States, before joining Capitol. Reference Capitol was stuck in the past. It could not think about the future, Barnett said. PBMT-R "They were so affected by the past they couldn't think about the future," said Barnett, 62, was president of Columbia Records, before joining Capitol building. Hybrid 'They were so burdened by the past they couldn't think about the future," said Barnett, 62, who was Columbia Records, president of the No.1 record label in the united states, before joining Capitol. EncDecA "They were so burdened by the past they couldn't think about the future," said Bar- nett, who was president of Columbia Records, the No.1 record labels in the United States. DRESS "They were so sicker by the past they couldn't think about the future," said Barnett, who was president of Columbia Records. DRESS-LS "They were so burdened by the past they couldn't think about the future," said Bar- nett, who was president of Columbia Records. <ref type="table">Table 3</ref>: System output for two sentences (Newsela development set). Substitutions are shown in bold.</p><p>Hybrid on Fluency, Simplicity, and overall. The fact that neural models (EncDecA, DRESS and DRESS-LS) fare well on Fluency, is perhaps not surprising given the recent success of LSTMs in language modeling and neural machine translation ( <ref type="bibr">Zaremba et al., 2014;</ref><ref type="bibr" target="#b12">Jean et al., 2015</ref>). Neural models obtain worse ratings on Ade- quacy but are closest to the human references on this dimension. DRESS-LS (and DRESS) are sig- nificantly better (p &lt; 0.01) on Simplicity than EncDecA, PBMT-R, and Hybrid which indicates that our reinforcement learning based model is ef- fective at creating simpler output. Combined rat- ings (All) for DRESS-LS are significantly different compared to the other models but not to DRESS and the Reference. Nevertheless, integration of the lexical simplification model boosts performance as ratings increase almost across the board (Sim- plicity is slightly worse). Returning to our origi- nal questions, we find that neural models are more fluent than comparison systems, while perform- ing non-trivial rewrite operations (see the SARI scores in <ref type="table" target="#tab_2">Table 1</ref>) which yield simpler output (see the Simplicity column in <ref type="table">Table 2</ref>). Based on our judgment elicitation study, neural models trained with reinforcement learning perform best, with DRESS-LS having a slight advantage.</p><p>We further analyzed model performance by computing various statistics on the simplified out- put. We measured average sentence length and the degree to which DRESS and comparison sys- tems perform rewriting operations. We approxi- mated the latter with Translation Error Rate (TER; <ref type="bibr" target="#b33">Snover et al. 2006</ref>), a measure commonly used to automatically evaluate the quality of machine translation output. We used TER to compute the (average) number of edits required to change an original complex sentence to simpler output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert complex to simple sentences.</p><p>As shown in <ref type="table" target="#tab_5">Table 4</ref>, Hybrid obtains the high- est TER, followed by our models (DRESS and   <ref type="table">(Table 2)</ref>. There is a strong correlation between sentence length and number of deletion operations (i.e., more deleteions lead to shorter sentences) and PBMT-R performs very few dele- tions. Overall, reinforcement learning encourages deletion (see DRESS and DRESS-LS), while per- forming a reasonable amount of additional oper- ations (e.g., substitutions and shifts) compared to EncDecA and PBMT-R.</p><note type="other">Models Len TER Ins Del Sub Shft PBMT</note><p>The middle blocks in <ref type="table" target="#tab_2">Tables 1 and 2</ref> report re- sults on the WikiSmall dataset. FKGL and SARI follow a similar pattern as on Newsela. BLEU scores for PBMT-R, Hybrid, and EncDecA are much higher compared to DRESS and DRESS-LS. Hybrid obtains best BLEU and SARI scores, while DRESS and DRESS-LS do very well on FKGL. In human evaluation, we elicited judgments on the entire WikiSmall test set (100 sentences). We compared DRESS-LS, with PBMT-R, Hybrid, and gold standard Reference simplifications. As hu- man experiments are time consuming and ex- pensive, we did not include other neural models besides DRESS-LS based on our Newsela study which showed that EncDecA is inferior to vari- ants trained with reinforcement learning and that DRESS-LS is the better performing model (how- ever, we do compare all models in <ref type="table" target="#tab_2">Table 1</ref>). DRESS-LS is significantly better on Simplicity than PBMT-R, Hybrid, and the Reference. It per- forms on par with PBMT-R on Fluency and worse on Adequacy (but still closer to the human Ref- erence than PBMT-R or Hybrid). When combin- ing all ratings (All in <ref type="table">Table 2</ref>), DRESS-LS is sig- nificantly better than PBMT-R, Hybrid, and the Reference.</p><p>The bottom blocks in <ref type="table" target="#tab_2">Tables 1 and 2</ref> report re- sults on Wikilarge. We compared our models with PBMT-R, Hybrid, and SBMT-SARI ( <ref type="bibr" target="#b44">Xu et al., 2016)</ref>. The FKGL follows a similar pattern as in the previous datasets. PBMT-R and our mod- els are best in terms of BLEU while SBMT-SARI outperforms all other systems on SARI. 11 Because there are 8 references for each complex sentence in the test set, BLEU scores are much higher com- pared to Newsela and WikiSmall. In human eval- uation, we again elicited judgments for 100 ran- domly sampled test sentences. We randomly se- lected one of the 8 references as the Reference upper bound. On Simplicity, DRESS-LS is sig- nificantly better than all comparison systems, ex- cept Hybrid. On Adequacy, it is better than Hybrid but significantly worse than other comparison sys- tems. On Fluency, it is on par with PBMT-R 12 but better than Hybrid and SBMT-SARI. On All di- mension DRESS-LS significantly outperforms all comparison systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We developed a reinforcement learning-based text simplification model, which can jointly model simplicity, grammaticality, and semantic fidelity to the input. We also proposed a lexical simplifi- cation component that further boosts performance. Overall, we find that reinforcement learning of- fers a great means to inject prior knowledge to the simplification task achieving good results across three datasets. In the future, we would like to ex- plicitly model sentence splitting and simplify en- tire documents (rather than individual sentences). Beyond sentence simplification, the reinforcement learning framework presented here is potentially applicable to generation tasks such as sentence compression ( , generation of programming code ( <ref type="bibr" target="#b18">Ling et al., 2016)</ref>, or poems ( <ref type="bibr" target="#b47">Zhang and Lapata, 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Automatic evaluation on Newsela, Wik-
iSmall, and WikiLarge test sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Output length (average number of to-
kens), TER scores and number of edits by type 
(Insertions, Deletions, Substitutions, Shifts) on 
the Newsela test set. Higher TER means that more 
rewriting operations are performed. 

DRESS-LS), which indicates that they actively 
perform rewriting. Perhaps Hybrid is too ag-
gressive when simplifying a sentence, it obtains 
low Fluency and Adequacy scores in human eval-
uation </table></figure>

			<note place="foot" n="1"> Our code and data are publicly available at https:// github.com/XingxingZhang/dress.</note>

			<note place="foot">t and c t as input and outputs a real value as the expected reward. The parameters of the regressor are</note>

			<note place="foot" n="5"> With the default mtevalv13a.pl settings. 6 FKGL implementation at http://goo.gl/OHP7k3. 7 We used he implementation of SARI in Xu et al. (2016).</note>

			<note place="foot" n="11"> BLEU and SARI scores reported in Xu et al. (2016) are 72.36 and 37.91, and measured at sentence-level. 12 We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Text simplification for informationseeking applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ODBASE</title>
		<meeting>ODBASE<address><addrLine>Agia Napa, Cyprus</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3290</biblScope>
			<biblScope unit="page" from="735" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simplifying text for languageimpaired readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Canning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th EACL</title>
		<meeting>the 9th EACL<address><addrLine>Bergen, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="269" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Motivations and methods for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Srinivas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th COLING</title>
		<meeting>the 16th COLING<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL: HLT</title>
		<meeting>NAACL: HLT<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale nlp with c&amp;c and boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th ACL Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th ACL Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Simplifying Natural Language for Aphasic Readers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siobhan</forename><surname>Devlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Sunderland</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An evaluation of syntactic simplification rules for people with autism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Or Asan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)</title>
		<meeting>the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Text simplification for reading assistance: A project note</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuro</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Iwakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Paraphrasing</title>
		<meeting>the 2nd International Workshop on Paraphrasing<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for WMT15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Workshop on Statistical Machine Translation</title>
		<meeting>the 10th Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Verb paraphrase based on case frame alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th ACL</title>
		<meeting>40th ACL<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st ACL</title>
		<meeting>the 51st ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert P Fishburne</forename><surname>Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Richard L Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 EMNLP</title>
		<meeting>the 2016 EMNLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="599" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 EMNLP</title>
		<meeting>the 2016 EMNLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2355" to="2365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hybrid simplification using deep semantics and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="435" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th ACL</title>
		<meeting>the 40th ACL<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2014</title>
		<meeting>the EMNLP 2014<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dyswebxia 2.0!: More accessible text for people with dyslexia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Bayarri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azuki</forename><surname>Górriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurang</forename><surname>Kanvinde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasile</forename><surname>Topac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility</title>
		<meeting>the 10th International Cross-Disciplinary Conference on Web Accessibility<address><addrLine>Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Framework of automatic text summarization using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonggi</forename><surname>Ryang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Abekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 EMNLP-CoNLL</title>
		<meeting>the 2012 EMNLP-CoNLL<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="256" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of automated text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Shardlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Special Issue on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="581" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Syntactic simplification and text cohesion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research on Language and Computation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="77" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey of research on text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advaith</forename><surname>Siddharthan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Applied Linguistics</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="298" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL 206 Workshop on Statistical Machine Translation</title>
		<meeting>the NAACL 206 Workshop on Statistical Machine Translation<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of association for machine translation in the Americas</title>
		<meeting>association for machine translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">200</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 1: English lexical simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM 2012</title>
		<meeting>*SEM 2012<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="347" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sentence simplification for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="344" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vinícius Rodriguez de Uz˜edaUz˜eda, Renata Pontin de Mattos Fortes, Thiago Alexandre Salgueiro Pardo, and Sandra Maria Alu´sio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willian</forename><surname>Massami Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaldo Candido</forename><surname>Junior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Design of Communication</title>
		<meeting>the 27th ACM International Conference on Design of Communication<address><addrLine>Bloomington, IN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Facilita: reading assistance for low-literacy readers</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences with quasi-synchronous grammar and integer programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 EMNLP</title>
		<meeting>the 2011 EMNLP<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Text rewriting improves semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="133" to="164" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antal</forename><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th ACL</title>
		<meeting>the 50th ACL<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Problems in current text simplification research: New data can help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th ACL</title>
		<meeting>the 39th ACL<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 EMNLP</title>
		<meeting>the 2014 EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd COLING</title>
		<meeting>the 23rd COLING<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
