<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Lingual Sentiment Analysis using modified BRAE</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Engineering</orgName>
								<orgName type="institution">Delhi Technological University DL</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Batra</surname></persName>
							<email>shashankg@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of technology</orgName>
								<address>
									<settlement>Delhi</settlement>
									<region>DL</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Lingual Sentiment Analysis using modified BRAE</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-Lingual Learning provides a mechanism to adapt NLP tools available for label rich languages to achieve similar tasks for label-scarce languages. An efficient cross-lingual tool significantly reduces the cost and effort required to manually annotate data. In this paper, we use the Recursive Autoencoder architecture to develop a Cross Lingual Sentiment Analysis (CLSA) tool using sentence aligned corpora between a pair of resource rich (En-glish) and resource poor (Hindi) language. The system is based on the assumption that semantic similarity between different phrases also implies sentiment similarity in majority of sentences. The resulting system is then analyzed on a newly developed Movie Reviews Dataset in Hindi with labels given on a rating scale and compare performance of our system against existing systems. It is shown that our approach significantly outperforms state of the art systems for Sentiment Analysis, especially when labeled data is scarce.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment Analysis is a NLP task that deals with extraction of opinion from a piece of text on a topic. This is used by a large number of advertising and media companies to get a sense of public opin- ion from their reviews. The ever increasing user generated content has always been motivation for sentiment analysis research, but majority of work has been done for English Language. However, in recent years, there has been emergence of increas- ing amount of text in Hindi on electronic sources but NLP Frameworks to process this data is sadly miniscule. A major cause for this is the lack of annotated datasets in Indian Languages.</p><p>One solution is to create cross lingual tools be- tween a resource rich and resource poor language that exploit large amounts of unlabeled data and sentence aligned corpora that are widely available on web through bilingual newspapers, magazines, etc. Many different approaches have been identi- fied to perform Cross Lingual Tasks but they de- pend on the presence of MT-System or Bilingual Dictionaries between the source and target lan- guage.</p><p>In this paper, we use Bilingually Constrained Recursive Auto-encoder (BRAE) given by ( <ref type="bibr" target="#b16">Zhang et al., 2014</ref>) to perform Cross Lingual sentiment analysis. Major Contributions of this paper are as follows: First, We develop a new Rating scale based Movie Review Dataset for Hindi. Second, a general framework to perform Cross Lingual Classification tasks is developed by modifying the architecture and training procedure for BRAE model. This model exploits the fact that phrases in two languages, that share same semantic meaning, can be used to learn language independent seman- tic vector representations. These embeddings can further be fine-tuned using labeled dataset in En- glish to capture enough class information regard- ing Resource poor language. We train the resultant framework on English-Hindi Language pair and evaluate it against state of the art SA systems on existing and newly developed dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentiment Analysis in Hindi</head><p>In recent years, there have been emergence of works on Sentiment Analysis (both monolingual and cross-lingual) for Hindi. ( <ref type="bibr" target="#b6">Joshi et al., 2010</ref>) provided a comparative analysis of Unigram based In-language, MT based Cross Lingual and Word- Net based Sentiment classifier, achieving highest accuracy of 78.14%. ( <ref type="bibr" target="#b11">Mittal et al., 2013</ref>) described a system based on Hindi SentiWordNet for assign-ing positive/negative polarity to movie reviews. In this approach, overall semantic orientation of the review document was determined by aggregating the polarity values of the words in the document assigned using the WordNet. They also included explicit rules for handling Negation and Discourse relations during preprocessing in their model to achieve better accuracies.</p><p>For Languages where labeled data is not present, approaches based on cross-lingual sentiment anal- ysis are used. Usually, such methods need inter- mediary machine translation system ( <ref type="bibr" target="#b15">Wan et al., 2011;</ref><ref type="bibr" target="#b2">Brooke et al., 2009</ref>) or a bilingual dictionary ( <ref type="bibr" target="#b4">Ghorbel and Jacot, 2011;</ref><ref type="bibr" target="#b8">Lu et al., 2011</ref>) to bridge the language gap. Given the subtle and different ways in which sentiments can be expressed and the cultural diversity amongst different languages, an MT system has to be of a superior quality to per- form well( <ref type="bibr" target="#b0">Balamurali et al., 2012)</ref>.</p><p>( <ref type="bibr" target="#b0">Balamurali et al., 2012</ref>) present an alterna- tive approach to Cross Lingual Sentiment Analy- sis (CLSA) using WordNet senses as features for supervised sentiment classification. A document in Resource Poor Language was tested for polarity through a classifier trained on sense marked and polarity labeled corpora in Resource rich language. The crux of the idea was to use the linked Word- Nets of two languages to bridge the language gap.</p><p>Recently, <ref type="bibr" target="#b12">(Popat et al., 2013</ref>) describes a Cross Lingual Clustering based SA System. In this ap- proach, features were generated using syntagmatic property based word clusters created from unla- beled monolingual corpora, thereby eliminating the need for Bilingual Dictionaries. These features were then used to train a linear SVM to predict positive or negative polarity on a tourism review dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoencoders in NLP Tasks</head><p>Autoencoders are neural networks that learn a low dimensional vector representation of fixed-size in- puts such as image segments or bag-of-word rep- resentations of documents. They can be used to efficiently learn feature encodings that are useful for classification. The Autoencoders were first applied in a recursive setting by <ref type="bibr">Pollack (1990)</ref> in recursive auto-associative memories (RAAMs). However, RAAMs needed fixed recursive data structures to learn vector representations, whereas RAE given by <ref type="bibr" target="#b13">(Socher et al., 2011</ref>) builds recur- sive data structure using a greedy algorithm. The RAE can be pre-trained with an unsupervised algo- rithm and then fine-tuned according to the label of the phrase, such as the syntactic category in pars- ing <ref type="bibr" target="#b14">(Socher et al., 2013)</ref>, the polarity in sentiment analysis, etc. The learned structures are not neces- sarily syntactically accurate but can capture more of the semantic information in the word vectors. <ref type="bibr" target="#b16">(Zhang et al., 2014</ref>) used the RAE along with a Bilingually Constrained Model to simultaneously learn phrase embeddings for two languages in se- mantic vector space. The core idea behind BRAE is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their seman- tic phrase embeddings. Similarly, non-translation pairs should have different semantic meanings, and this information can also be used to guide learning semantic phrase embeddings. In this method, a standard recursive autoencoder (RAE) pre-trains the phrase embedding with an unsuper- vised algorithm by greedily minimizing the re- construction error <ref type="bibr" target="#b13">(Socher et al., 2011</ref>), while the bilingually-constrained model learns to finetune the phrase embedding by minimizing the seman- tic distance between translation equivalents and maximizing the semantic distance between non- translation pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BRAE Framework</head><p>In this section, We will briefly present the struc- ture and training algorithm for BRAE model. Af- ter that, we show how this model can be adapted to perform CLSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recursive Auto-encoder Framework</head><p>In this model, each word w k in the vocabulary V of given language corresponds to a vector x k ∈ R n and stacked into a single word embedding matrix L ∈ R n×|V | . This matrix is learned using DNN <ref type="bibr" target="#b3">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013)</ref> and serves as input to further stages of RAE.</p><p>Using this matrix, a phrase (w 1 w 2 . . . w m ) is first projected into a list of vectors (x 1 , x 2 , . . . x m ). The RAE learns the vector representation of the phrase by combining two children vectors recur- sively in a bottom-up manner. For two children c 1 = x 1 , c 2 = x 2 , the auto-encoder computes the parent vector y 1 :</p><formula xml:id="formula_0">y 1 = f (W (1) [c 1 ; c 2 ] + b (1) ); y 1 ∈ R n (1)</formula><p>To assess how well the parent vector represents its children, the auto-encoder reconstructs the chil- </p><formula xml:id="formula_1">[c ′ 1 ; c ′ 2 ] = W (2) p + b (2)<label>(2)</label></formula><p>and tries to minimize the reconstruction error (Eu- clidean Distance)</p><formula xml:id="formula_2">E rec ([c 1 ; c 2 ]) between the inputs [c 1 ; c 2 ] and their reconstructions [c ′ 1 ; c ′ 2 ]</formula><p>. Given y 1 , Eq.1 is used again to compute y 2 by setting the children to be [c <ref type="bibr">1</ref> ;</p><formula xml:id="formula_3">c 2 ] = [y 1 ; x 3 ].</formula><p>The same auto-encoder is re-used until the vector of the whole phrase is generated. For unsupervised phrase embedding, the sum of reconstruction er- rors at each node in binary tree y is minimized:</p><formula xml:id="formula_4">E rec (x; θ) = arg min y∈A(x) ∑ k∈y E rec ([c 1 ; c 2 ] k )<label>(3)</label></formula><p>Where A(x) denotes all the possible binary trees that can be built from inputs x. A greedy algorithm is used to generate the optimal binary tree y * . The parameters θ rec = (θ (1) , θ <ref type="bibr">(2)</ref> ) are optimized over all the phrases in the training data. For further de- tails, please refer (Socher et al., 2011)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semantic Error</head><p>The BRAE model jointly learns two RAEs for source language L S and target language L T . Each RAE learn semantic vector representation p s and p t of phrases s and t respectively in translation- equivalent phrase pair (s, t) in bilingual corpora (shown in <ref type="figure" target="#fig_0">Fig.1</ref>). The transformation between the two is defined by:</p><formula xml:id="formula_5">p ′ t = f (W t s p s + b t s ), p ′ s = f (W s t p t + b s t ) (4) where θ t s = (W t s , b t s ), θ s t = (W s t , b s t ) are new pa- rameters introduced.</formula><p>The semantic error between learned vector rep- resentations p s and p t is calculated as :</p><formula xml:id="formula_6">E sem (s, t; θ) = E * sem (t|s; θ s t ) + E * sem (s|t; θ t s ) (5) where E * sem (s|t; θ s t )</formula><p>is the semantic distance of p s given p t and vice versa. To calculate it, we first calculate Euclidean distance between origi- nal p t and transformation</p><formula xml:id="formula_7">p ′ t as D sem (s|t, θ t s ) = 1 2 ∥p t − p ′ t ∥ 2 .</formula><p>The max-semantic-margin distance between them is then defined as</p><formula xml:id="formula_8">E * sem (s|t, θ t s ) = max{0, D sem (s|t, θ t s ) −D sem (s|t ′ , θ t s ) + 1} (6)</formula><p>where we simultaneously minimize the distance between translation pairs and maximized between non-translation pairs. Here t ′ in non-translation pair (s, t ′ ) is obtained by replacing the words in t with randomly chosen target language words. We calculate the E * sem (t|s; θ s t ) in similar manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BRAE Objective Function</head><p>Thus, for the phrase pair (s, t), the joint error be- comes:</p><formula xml:id="formula_9">E(s, t, θ) = E(s|t, θ) + E(t|s, θ) E(s|t, θ) = αE rec (s; θ rec s ) + (1 − α)E * sem (s|t, θ t s )) E(t|s, θ) = αE rec (t; θ rec t ) + (1 − α)E * sem (t|s, θ s t )) (7)</formula><p>The hyper-parameter α weighs the reconstruction and semantic errors. The above equation indi- cates that the Parameter sets θ t = (θ s t , θ rec t ) and θ s = (θ t s , θ rec s ) on each side respectively can be optimized independently as long as the phrase rep- resentation of other side is given to compute se- mantic error.</p><p>The final BRAE objective over the phrase pairs training set (S, T ) becomes:</p><formula xml:id="formula_10">J BRAE = 1 N ∑ (s,t)∈(S,T ) E(s, t; θ) + λ BRAE 2 ∥θ∥ 2 (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unsupervised Training of BRAE</head><p>The word embedding matrices L s and L t are pre- trained using unlabeled monolingual data with Word2Vec toolkit ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>). All other parameters are initialized randomly. We use SGD algorithm for parameter optimization. For full gra- dient calculations for each parameter set, please see ( <ref type="bibr" target="#b16">Zhang et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">RAE Training Phase:</head><p>Apply RAE Frame- work (Sec. 3.1) to pre-train the source and target phrase representations p s and p t respectively by optimizing θ rec s and θ rec t using unlabeled monolin- gual datasets.</p><p>2. Cross-Training Phase: Use target-side phrase representation p t to update the source-side parameters θ s and obtain source-side phrase repre- sentation p ′ s , and vice-versa for p s . Calculate the joint error over the bilingual training corpus. On reaching a local minima or predefined no. of iter- ations (30 in our case), terminate this phase, other- wise set p s = p ′ s , p t = p ′ t , and repeat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adapting Model for Classifying Sentiments</head><p>At the end of previous Training procedure, we ob- tain high quality phrase embeddings in both source and target language and transformation function between them. We now extend that model to per- form cross lingual supervised tasks, specifically CLSA.</p><p>To achieve this, we need to modify the learned semantic phrase embeddings such that they can capture information about sentiment. Since we only use monolingual labeled datasets from this point onwards, the supervised learning phases will occur independently for each RAE as we do not have any ''phrase pairs'' now. Thus, the new se- mantic vector space generated for word and phrase embeddings may no longer be in sync with their corresponding transformations.</p><p>We propose following modifications to the sys- tem to deal with this problem. Let L S and L T rep- resent Resource rich and Resource poor language respectively in above model. Modifications in architecture: We first in- clude a softmax (σ) layer on top of each parent node in RAE for L S to predict a K-dimensional multinomial distribution over the set of output classes defined by the task (e.g : polarity, Ratings).</p><formula xml:id="formula_11">d(p; θ ce ) = σ(W ce p)<label>(9)</label></formula><p>Given this layer, we calculate cross entropy er- ror E ce (p k , t, W ce ) generated for node p k in binary tree, where t is target multinomial distribution or one-hot binary vector for target label. We use this layer to capture and predict actual sentiment in- formation about the data in both L S and L T (de- scribed in next section). We show a node in modi- fied architecture in <ref type="figure" target="#fig_1">Fig.2</ref>. Penalty for Movement in Semantic Vector space: During subsequent training phases, we in- clude the euclidean norm of the difference between the original and new phrase embeddings as penalty in reconstruction error at each node of the tree.  Here p is the phrase representation we get during forward propagation of current training iteration and p * is the representation we get if we apply the parameters obtained at the end of the Cross training phase to children [c 1 ; c 2 ] of that node. The reason to do this is twofold.</p><formula xml:id="formula_12">E * rec ([c 1 ; c 2 ]; θ) = E rec ([c 1 ; c 2 ]; θ) + λ p 2 ∥p − p * ∥ 2<label>(10</label></formula><p>First, during supervised training, the error will back propagate through RAEs for both languages affecting their respective weights matrices and word embeddings. This will modify the semantic representation of phrases captured during previous phases of training procedure and adversely affect the transformations derived from them. Therefore we need to include some procedure such that the transformation information learned during Cross- training phase is not lost.</p><p>Secondly, we observe that the information about the semantic similarity of a word or phrase also im- plies sentiment similarity between the two. That is when dealing with bilingual data, words or phrases that appear near each other in semantic space typi- cally represent common sentiment information and we want our model to create a decision boundary around these vectors instead of modifying them too much.</p><p>Disconnecting the RAEs: We fix the trans- formation weights between the two RAEs, i.e. in subsequent training steps the transformation weights(θ t s , θ s t ) are not modified but rather pass the back propagated error as it is to previous lay- ers. We observed that on optimizing the objec- tive along with the penalty term, the transforma- tion weights are preserved between new seman- tic/sentiment vector spaces, resulting in slightly degraded performance, but were still able to preserve enough information about the semantic structure of two languages.Also, it reinforced the penalty imposed on the movement of phrase em- beddings in semantic vector space.On the other hand, if the weights were allowed to be updated, the accuracies were affected severely as infor- mation learned during previous phases was lost and the weights were not been able to capture enough information about the modified phrase em- beddings and generalize well on test phrases not encountered in labeled training set of Resource Scarce Language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Supervised Training Phases</head><p>We now explain supervised training procedure us- ing only monolingual labeled data for each lan- guage. These training phases occur at the end of BRAE training. In each training phase, we use SGD algorithm to perform parameter optimiza- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Phase I : Resource Rich language</head><p>In this phase, we only modify the parameters of RAE L S , i.e. θ rec s and θ ce by optimizing following objective over (sentence, label) pairs (x, t) in its labeled corpus.</p><formula xml:id="formula_13">J S = 1 N ∑ (x,t) E(x, t; θ) + λ S 2 ∥θ∥ 2<label>(11)</label></formula><p>where E(x, t; θ) is the sum over the errors obtained at each node of the tree that is constructed by the greedy RAE:</p><formula xml:id="formula_14">E(x, t; θ) = ∑ k∈RAE L S (x) κE * rec ([c 1 ; c 2 ] k ; θ s ) + (1 − κ)E ce (p k , t; θ ce )<label>(12)</label></formula><p>To compute this gradient, we first greedily con- struct all trees and then derivatives for these trees are computed efficiently via back-propagation through structure <ref type="bibr" target="#b5">(Goller and Kuchler, 1996</ref>). The gradient for our new reconstruction function (Eq. 10) w.r.t to p at a given node is calculated as</p><formula xml:id="formula_15">∂E * rec ∂p = ∂E rec ∂p + λ p (p − p * )<label>(13)</label></formula><p>The first term ∂Erec ∂p is calculated as in standard RAE model. The partial derivative in above equa- tion is used to compute parameter gradients in stan- dard back-propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Phase II : Resource Poor Language</head><p>In this phase, we modify the parameters of RAE L T and θ ce by optimizing Objective J T over (sentence, label) pairs (x, t) in labeled corpus for L T (much smaller than that for L S ). The equation for J T is similar to Eq.11 and Eq.12 but with θ t and η as parameters instead of θ s and κ respectively.</p><p>Since cross-entropy layer is only associated with L S , we need to traverse the transformation param- eters to obtain sentiment distribution for each node (green path in <ref type="figure" target="#fig_1">Fig.2</ref>). That is, we first transform p t to source side phrase p ′ s and then apply the cross entropy weights to it.</p><formula xml:id="formula_16">d(p t , θ ce ) = σ(W ce .f (W t s p t + b t s ))<label>(14)</label></formula><p>We use the similar back-propagation through structure approach for gradient calculation in Phase I. During back propagation, 1) we do not update the transformation weights, 2) we transfer error signals during back-propagation from Cross- entropy layer to θ</p><p>(1) t as if the transformation was an additional layer in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Predicting overall sentiment</head><p>To predict overall sentiment associated with the sentence in L T , we use the phrase embeddings p t of the top layer of the RAE L T and it transforma- tion p ′ s . Together, we train a softmax regression classifier on concatenation of these two vector us- ing weight matrix W ∈ R K×2n</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Work</head><p>We perform experiments on two kind of sentiment analysis systems : (1) that gives +ve/-ve polarity to each review and (2) assigns ratings in range 1 - 4 to each review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">External Datasets Used</head><p>For pre-training the word embeddings and RAE Training, we used HindMonoCorp 0.5( <ref type="bibr" target="#b1">Bojar et al., 2014</ref>) with 44.49M sentences (787M Tokens) and English Gigaword Corpus.</p><p>For Cross Training, we used the bilingual sentence-aligned data from HindEnCorp <ref type="bibr">1</ref>   <ref type="bibr" target="#b9">(Maas et al., 2011</ref>) for +ve/-ve system containing 25000 +ve and 25000 -ve movie reviews.</p><p>For 4-ratings system, we use Rotten Toma- toes Review dataset (scale dataset v1.0) found at http://www.cs.cornell.edu/People/pabo/movie- review-data. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents and each document has accompanying 4-Ratings <ref type="figure" target="#fig_0">({0, 1, 2, 3}</ref>) label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Rating Based Hindi Movie Review (RHMR) Dataset</head><p>We crawled the Hindi Movie Reviews Website 2 to obtain 2945 movie reviews. Each Movie Review on this site is assigned rating in range 1 to 4 by at least three reviewers. We first discard reviews that whose sum of pairwise difference of ratings is greater than two. The final rating for each review is calculated by taking the average of the ratings and rounding up to nearest integer. The fraction of Re- views obtained in ratings 1-4 are <ref type="bibr">[0.20, 0.25, 0.35, 0.20]</ref> respectively. Average length of reviews is 84 words. For +ve/-ve polarity based system, we group the reviews with ratings {1, 2} as negative and {3, 4} as positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Settings</head><p>We used following Baselines for Sentiment Anal- ysis in Hindi : Majority class: Assign the most frequent class in the training set (Rating:3 / Polarity:+ve) Bag-of-words: Softmax regression on Binary Bag-of-words</p><p>We also compare our system with state of the art Monolingual and Cross Lingual System for Senti- ment Analysis in Hindi as described by <ref type="bibr" target="#b12">(Popat et al., 2013</ref>) using the same experimental setup. The best systems in each category given by them are as below:</p><p>WordNet Based: Using Hindi-SentiWordNet 3 , each word in a review was mapped to correspond- ing synset identifiers. These identifiers were used as features for creating sentiment classifiers based on Binary/Multiclass SVM trained on bag of words representation using libSVM library.</p><p>Cross Lingual (XL) Clustering Based: Here, joint clustering was performed on unlabeled bilin- gual corpora which maximizes the joint likelihood of monolingual and cross-lingual factors.. For de- tails, please refer the work of <ref type="bibr" target="#b12">(Popat et al., 2013</ref>).</p><p>Each word in a review was then mapped to its clus- ter identifier and used as features in an SVM.</p><p>Our approaches Basic RAE: We use the Semi-Supervised RAE based classification where we first trained a stan- dard RAE using Hindi monolingual corpora, then applied supervised training procedure as described in <ref type="bibr" target="#b13">(Socher et al., 2011</ref>). This approach doesn't use bilingual corpora, but is dependent on amount of labeled data in Hindi. BRAE-U: We neither include penalty term, nor fix the transformations weights in our proposed system. BRAE-P: We only include the penalty term but allow the transformation weights to be modified in proposed system. BRAE-F: We add the penalty term and fix the transformation weights during back propagation in proposed system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental Setup</head><p>We combined the text data from all English Datasets (English Gigaword + HindEnCorp En- glish Portion + IBMD11 + Scale Dataset) de- scribed above to train the word embeddings us- ing Word2Vec toolkit and RAE. Similarly, we combined text data from all Hindi Datasets (HindMonoCorp + HindiEnCorp Hindi Portion + RHMR) to train word embeddings and RAE for Hindi.</p><p>We used MOSES Toolkit ( <ref type="bibr" target="#b7">Koehn et al., 2007</ref>) to obtain high quality bilingual phrase pairs from HindEnCorp to train our BRAE model. After removing the duplicates, 364.3k bilingual phrase pairs were obtained with lengths ranging from 1- 6, since bigger phrases reduced the performance of the system in terms of Joint Error of BRAE model.</p><p>We randomly split our RHMR dataset into 10 segments and report the average of 10-fold cross validation accuracies for each setting for both Rat- ings and Polarity classifiers.</p><p>We also report 5-fold cross validation accuracy on Standard Movie Reviews Dataset (hereby re- ferred as SMRD) given by ( <ref type="bibr" target="#b6">Joshi et al., 2010)</ref> which contains 125 +ve and 125 -ve reviews in Hindi.</p><p>The dataset can be obtained at http://www.cfilt.iitb.ac.in/Resources.html.</p><p>Since this project is about reducing depen- dence on annotated datasets, we experiment on how accuracy varies with labeled training dataset (RHMR) size. To perform this, we train our model in 10% increments (150 examples) of training set size (each class sampled in proportion of original set). For each size, we sample the data 10 times with replacement and trained the model. For each sample, we calculated 10-fold cross validation ac- curacy as described above. Final accuracy for each size was calculated by averaging the accuracies ob- tained on all 10 samples. Similar kind of evalua- tion is done for all other Baselines explored.</p><p>In subsequent section, the word 'significant' im- plies that the results were statistically significant (p &lt; 0.05) with paired T-test</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">BRAE Hyper Parameters</head><p>We empirically set the learning rate as 0.05. The word vector dimension was selected as 80 from set <ref type="bibr">[40,</ref><ref type="bibr">60,</ref><ref type="bibr">80,</ref><ref type="bibr">100,</ref><ref type="bibr">120]</ref> using Cross Validation. We used joint error of BRAE model to select α as 0.2 from range [0.05, 0.5] in steps of 0.05. Also, λ L was set as 0.001 for DNN trained for word embed- ding and λ BRAE as 0.0001.</p><p>For semi-supervised phases , we used 5-fold cross validation on training set to select κ and η in range   <ref type="table">Table 1</ref> present the results obtained for both rat- ings based and polarity classifier on RHMR and MRD Dataset. Our model gives significantly bet- ter performance for ratings based classification than any other baseline system currently used for SA in Hindi. The margin of accuracy obtained against next best classifier is about 8%. Also, for A ↓ /P → P-1 P-2 P-3 P-4 A-1 83.19 15.28 1.53 0.00 A-2 12.23 82.20 5.57 0.00 A-3 0.00 9.03 81.26 9.71 A-4 0.00 1.87 19.69 78.44 F1-score 0.83 0.78 0.82 0.80 +ve/-ve polarity classifier, the accuracy showed an improvement of 6% over next highest baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RHMR</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we calculate the confusion matrix for our model(BRAE-F) for the 4-Ratings case. Value in a cell (A i , P j ) represents the percentage of ex- amples in actual rating class i that are predicted as rating j. We also show the F1 score calcu- lated for each individual rating class. It clearly shows that our model has low variation in F1- scores and thereby its performance among various rating classes.</p><p>In <ref type="figure" target="#fig_3">Fig. 3</ref>, we show the variation in accuracy of the classifiers with amount of sentiment labeled Training data used. We note that our approach con- sistently outperforms the explored baselines at all dataset sizes. Also, our model was able to attain accuracy comparable to other baselines at about 50% less labeled data showing its strength in ex- ploiting the unlabeled resources. We also experiment with variation of accuracies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New Word/Phrase</head><p>Similar Words/Phrases Sentiment label depressing gloomy उदास Rating : 1 नराशाजनक discouraging नराशाामक Polarity : -ve was painful was difficult ककठन था Rating : 2 दद नाक था was bad खराब था Polarity : -ve should be awarded was appreciated सराहना कक गई Rating : 4 ससमाानत कया जाना चााहए will get accolades वाहवाहह मलना चााहए Polarity : +ve public won't come no one will come कोई नहहं आएगा Rating : 1 लोग नहहं आएगा viewers won't come दश क नहहं आएगा Polarity : -ve  <ref type="figure" target="#fig_4">Fig. 4</ref>, we observed that performance of the proposed approach steadily increases with amount of data added, yet even at about 50000 (20%) phrase pairs, our model produces remark- able gains in accuracy. We also observed that the model which restricts modification to transformation weights during su- pervised phase II does better than the one which allows the modification at all dataset sizes. This result appears to be counterintuitive to normal op- eration of neural network based models, but sup- ports our hypothesis as explained in previous sec- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Performance and Error Analysis</head><p>Analysis on the test results showed that the major advantage given by our model occurs due to pres- ence of unknown words (i.e.words not present in labeled dataset) in test data. Since we restricted the movement in semantic vector space, our model was able to infer the sentiment for a unknown word/phrase by comparing it with semantically similar words/phrases. In <ref type="table" target="#tab_4">Table 3</ref>, we extracted the Top-2 semantically similar phrases in training set for small new phrases and sentiment labeled assigned to them by our model (the phrases are manually translated from Hindi for reader's under- standing). As we can see, our model was able to extract grammatically correct phrases with similar semantic nature as given phrase and assign correct sentiment label to it.</p><p>Secondly, We found that our model was able to correctly infer word sense for polysemous words that adversely affected the quality of sentiment classifiers in our baselines. This eliminates the need for manually constructed fine grained lexi- cal resource like WordNets and development of automated annotation resources. For example, to a phrase like "Her acting of a schizophrenic mother made our hearts weep", the baselines clas- sifiers assigned negative polarity due to presence of words like 'weep', yet our model was correctly able to predict positive polarity and assigned it a rating of 3.</p><p>Error Analysis of test results showed that errors made by our model can be classified in two major categories :</p><p>1) A review may only give description of the object in question (in our case , the description of the film) without actually presenting any individ- ual sentiments about it or it may express conflict- ing sentiments about two different aspects about the same object. This presents difficulty in assign-ing a single polarity/rating to the review.</p><p>2) Presence of subtle contextual references af- fected the quality of predictions made by our clas- sifier. For example, sentence like ''His poor acting generally destroys a movie, but this time it didn't'' got a rating of 2 due to presence of phrase with negative sense (here the phrase doesn't have am- biguous sense), yet the actual sentiment expressed is positive due to temporal dependence and gen- eralization. Also, "This movie made his last one looked good" makes a reference to entities exter- nal to the review, which again forces our model to make wrong prediction of rating 3.</p><p>Analyzing these aspects and making correct pre- dictions on such examples needs further work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This study focused on developing a Cross Lin- gual Supervised Classifier based on Bilingually Constrained Recursive Autoencoder. To achieve this, our model first learns phrase embeddings for two languages using Standard RAE, then fine tune these embeddings using Cross Training procedure. After imposing certain restrictions on these em- beddings, we perform supervised training using labeled sentiment corpora in English and a much smaller one in Hindi to get the final classifier.</p><p>The experimental work showed that our model was remarkably effective for classification of Movie Reviews in Hindi on a rating scale and predicting polarity using least amount of data to achieve same accuracy as other systems explored. Moreover it reduces the need for MT System or lexical resources like Linked WordNets since the performance is not degraded too much even when we lack large quantity of labeled data.</p><p>In Future, we hope to 1) extend this system to learn phrase representations among multiple lan- guages simultaneously, 2) apply this framework to other cross Lingual Tasks such as Paraphrase de- tection, Question Answering, Aspect Based Opin- ion Mining etc and 3) Learning different weight matrices at different nodes to capture complex re- lations between words and phrases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of BRAE structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of BRAE segment with Cross Entropy layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[ 0 .</head><label>0</label><figDesc>0, 1.0] in steps of 0.05 with optimal value obtained at κ = 0.2 and η = 0.35. Parameter λ p was selected as 0.01 , λ S as 0.1 and λ T as 0.04 after selection in range [0.0, 1.0] in steps of 0.01.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Variation of Accuracy (+ve/-ve Polarity) with Size of labeled Dataset(Hindi), x-axis: Fraction of Dataset Used, y-axis: %age Accuracy Obtained</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Variation of Accuracy (+ve/-ve polarity) with Size of Unlabeled Bilingual Corpora, x-axis: Fraction of Training Data Used, y-axis: %age Accuracy Obtained</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>)</head><label></label><figDesc></figDesc><table>Resource Rich Language Resource Poor Language 

Reconstruction 
Reconstruction 
Cross-Entropy 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Confusion Matrix for Ratings by BRAE-
F, Across: Predicted Rating, Downward: Actual 
Rating 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Semantically similar phrases obtained for new phrases and their assigned label 

with amount of Unlabeled Bilingual Training Data 
used for Cross Lingual models explored. Again 
we increase size of bilingual dataset in 10% incre-
ments and calculate the accuracy as described pre-
viously. In </table></figure>

			<note place="foot" n="1"> http://ufal.mff.cuni.cz/hindencorp</note>

			<note place="foot" n="2"> http://hindi.webdunia.com/bollywood-movie-review/ 3 http://www.cfilt.iitb.ac.in/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment analysis for Indian languages using linked wordnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Balamurali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012: Posters</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HindEnCorp-Hindi-English and Hindi-only Corpus for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojtěch</forename><surname>Diatka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Rychlý</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vít</forename><surname>Suchomel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-linguistic sentiment analysis: From english to spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="50" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Further experiments in sentiment analysis of french movie reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hatem</forename><surname>Ghorbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jacot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Web Mastering-3</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fall-back strategy for sentiment analysis in hindi: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Balamurali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ICON</title>
		<meeting>the 8th ICON</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint bilingual sentiment classification with unlabeled parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentiment analysis of hindi review based on negation and discourse relation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namita</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basant</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garvit</forename><surname>Chouhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Bania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Pareek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of International Joint Conference on Natural Language Processing</title>
		<meeting>International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The haves and the have-nots: Leveraging unlabelled corpora for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Balamurali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Biweighting domain adaptation for cross-language text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">1535</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
