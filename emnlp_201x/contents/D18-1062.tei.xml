<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Bilingual Lexicon Induction via Latent Variable Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Bilingual Lexicon Induction via Latent Variable Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="621" to="626"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>621</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Bilingual lexicon extraction has been studied for decades and most previous methods have relied on parallel corpora or bilingual dictionaries. Recent studies have shown that it is possible to build a bilingual dictionary by aligning monolingual word embedding spaces in an unsupervised way. With the recent advances in generative models, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adver-sarial training with no parallel corpora. To demonstrate the effectiveness of our approach, we evaluate our approach on several language pairs and the experimental results show that our model could achieve competitive and even superior performance compared with several state-of-the-art models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning the representations of languages is a fun- damental problem in natural language processing and most existing methods exploit the hypothesis that words occurring in similar contexts tend to have similar meanings <ref type="bibr">(Pennington et al., 2014;</ref><ref type="bibr" target="#b2">Bojanowski et al., 2017)</ref>, which could lead word vectors to capture semantic information. <ref type="bibr">Mikolov et al. (2013)</ref> first point out that word embeddings learned on separate monolingual corpora exhibit similar structures. Based on this finding, they sug- gest it is possible to learn a linear mapping from a source to a target embedding space and then gener- ate bilingual dictionaries. This simple yet effective approach has led researchers to investigate on im- proving cross-lingual word embeddings with the help of bilingual word lexicons <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr">Xing et al., 2015)</ref>.</p><p>For low-resource languages and domains, cross- lingual signal would be hard and expensive to ob- tain, and thus it is necessary to reduce the need for bilingual supervision. <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> suc- cessfully learn bilingual word embeddings with only a parallel vocabulary of aligned digits. <ref type="bibr">Zhang et al. (2017)</ref> utilize adversarial training to obtain cross-lingual word embeddings without any paral- lel data. However, their performance is still signif- icantly worse than supervised methods. By com- bining the merits of several previous works, <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> introduce a model that reaches and even outperforms supervised state-of-the-art methods with no parallel data.</p><p>In recent years, generative models have be- come more and more powerful. Both Genera- tive Adversarial Networks (GANs) ( <ref type="bibr" target="#b8">Goodfellow et al., 2014</ref>) and Variational Autoencoders (VAEs) <ref type="bibr">(Kingma and Welling, 2014</ref>) are prominent ones. In this work, we borrow the ideas from both GANs and VAEs to tackle the problem of bilingual lex- icon induction. The basic idea is to learn latent variables that could capture semantic meaning of words, which would be helpful for bilingual lex- icon induction. We also utilize adversarial train- ing for our model and require no form of super- vision. We evaluate our approach on several lan- guage pairs and experimental results demonstrate that our model could achieve promising perfor- mance. We further combine our model with sev- eral helpful techniques and show our model could perform competitively and even superiorly com- pared with several state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual Lexicon Induction</head><p>Extracting bilingual lexica has been studied by re- searchers for a long time. <ref type="bibr">Mikolov et al. (2013)</ref> first observe there is isomorphic structure among word embeddings trained separately on monolin- gual corpora and they learn the linear transforma- tion between languages. <ref type="bibr">Zhang et al. (2016b)</ref> im- prove the method by constraining the transforma- tion matrix to be orthogonal. <ref type="bibr">Xing et al. (2015)</ref> in- corporate length normalization during training and maximize the cosine similarity instead. They point out that adding an orthogonality constraint can im- prove performance and has a closed-form solution, which was referred to as Procrustes approach in <ref type="bibr">Smith et al. (2017)</ref>. Canonical correlation analy- sis has also been used to map both languages to a shared vector space <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr">Lu et al., 2015)</ref>. To reduce the need for supervision signals, <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> use identical digits and num- bers to form an initial seed dictionary and then iteratively refine their results until convergence. <ref type="bibr">Zhang et al. (2017)</ref> apply adversarial training to align monolingual word vector spaces with no supervision. <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> improve the model by combining adversarial training and Pro- crustes approach, and their unsupervised approach could reach and even outperform state-of-the-art supervised approaches. In this work, we make further improvements and enhance the model pro- posed in ( <ref type="bibr" target="#b4">Conneau et al., 2018</ref>) with latent variable model and iterative training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Models</head><p>VAEs ( <ref type="bibr">Kingma and Welling, 2014</ref>) represent one of the most successful deep generative models. Standard VAEs assume observed variables are generated from latent variables and the latent vari- ables are sampled from a simple Gaussian distri- bution. Typically, VAEs utilize an neural infer- ence model to approximate the intractable poste- rior, and optimize model parameters jointly with a reparameterized variational lower bound. VAEs have been successfully applied in several natural language processing tasks before ( <ref type="bibr">Zhang et al., 2016a;</ref><ref type="bibr" target="#b3">Bowman et al., 2016)</ref>.</p><p>GANs ( <ref type="bibr" target="#b8">Goodfellow et al., 2014</ref>) are another framework for estimating generative models via an adversarial process and have attracted huge at- tention. The basic strategy is to train a generative model and a discriminative model simultaneously via an adversarial process. Adversarial training technique for matching distribution has proven to be powerful in a variety of tasks ( <ref type="bibr" target="#b3">Bowman et al., 2016)</ref>. Adversarial Autoencoder ( <ref type="bibr">Makhzani et al., 2015</ref>) is a probabilistic autoencoder that uses the GANs to perform variational inference. By com- bining a VAE with a GAN, <ref type="bibr">Larsen et al. (2016)</ref> use learned feature representations in the GAN dis- criminator as the basis for the VAE reconstruction objective. GANs have been applied in machine translation before <ref type="bibr">(Yang et al., 2018;</ref><ref type="bibr" target="#b4">Lample et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In this section, we first briefly introduce VAEs, and then we illustrate the details and training tech- niques of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Autoencoder</head><p>Variational Autoencoders (VAEs) are deep genera- tive model which are capable of learning complex density models for data via latent variables. Given a nonlinear generative model p θ (x|z) with input x ∈ R D and associated latent variable z ∈ R L drawn from a prior distribution p 0 (z), the goal of VAEs is to use a recognition model, q φ (z|x) to ap- proximate the posterior distribution of the latent variables by maximizing the following variational lower bound</p><formula xml:id="formula_0">L θ,φ = E q φ (z|x) [log p θ (x|z)] − KL(q φ (z|x)||p 0 (z)),</formula><p>(1) where KL refers to Kullback-Leibler divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Model</head><p>Basically, our model assumes that the source word embedding {x n } and the target word embedding {y n } could be drawn from a same latent variable space {z n }, where {z n } is capable of capturing semantic meaning of words.</p><p>In contrast to the standard VAE prior that as- sumes each latent embedding z n to be drawn from the same latent Gaussian, our model just requires the distribution of latent variables for source and target word embeddings to be equal. To achieve such a goal, we utilize adversarial training to guide the two latent distributions to match with each other.</p><p>As in adversarial training, we have networks φ s and φ t for both source and target space, striving to map words into the same latent space, while the discriminator D is a binary classifier which tries to distinguish between the two languages. We also have reconstruction networks θ s and θ t as in VAEs.</p><p>The objective function for the discriminator D could be formulated as</p><formula xml:id="formula_1">L D = E zy∼q φ t (z|y) [log D(z y )] + E zx∼q φs (z|x) [log(1 − D(z x ))].<label>(2)</label></formula><p>For the source side, the objective is to minimize</p><formula xml:id="formula_2">L φs,θs = E zx∼q φs (z|x) [log p θs (x|z x )] − E zx∼q φs (z|x) [log D(z x )].<label>(3)</label></formula><p>Here we define</p><formula xml:id="formula_3">q φs (z|x) = N (µ s (x), Σ s (x)),</formula><p>where µ s (x) = W µs x and Σ s (x) = exp(W σs x); W µs and W σs are learned parameters. We also de- fine the mean of p θs (x|z) to be W T µs z. The objec- tive function and structure for φ t are similar.</p><p>The basic framework of our model is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As we could see from the figure, our model tries to map the source and target word em- bedding into the same latent space which could capture the semantic meaning of words.</p><p>Theoretical analysis has revealed that adversar- ial training tries to minimize the Jensen-Shannon (JS) divergence between the real and fake distribu- tion. Therefore, one can view our model as replace KL divergence in Equation 1 with JS divergence and change the Gaussian prior to the target distri- bution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Strategy</head><p>Our model has two generators φ s and φ t , and we have found that training them jointly would be ex- tremely unstable. In this paper, we propose an iterative method to train our models. Basically, we first initialize W µt to be identity matrix and train φ s and θ s on the source side. After conver- gence, we freeze W µs , and then train φ t and θ t in the target side. The pseudo-code for this process is shown in Algorithm 1. It should be noted that there is no variance once completing training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Strategy</head><p>1: W µt = I 2: for i = 1, · · · , n iter do </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Small-scale Datasets</head><p>In this section, our experiments focus on small- scale datasets and our main baseline model is ad- versarial autoencoder ( <ref type="bibr">Zhang et al., 2017)</ref>. For justice, we use the same model selection strategy with <ref type="bibr">Zhang et al. (2017)</ref>, i.e. we choose the model whose sum of reconstruction loss and classifica- tion accuracy is the least. The source and target word embeddings would be first mapped into the latent space. For each source word embedding x, it would be first transformed into z x . The the its k nearest target embeddings would be retrieved and be compared against the entry in a ground truth bilingual lexicon. Performance is measured by top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Experiments on Chinese-English Dataset</head><p>For this set of experiments, we use the same data as <ref type="bibr">Zhang et al. (2017)</ref>. The statistics of the fi- nal training data is given in <ref type="table" target="#tab_4">Table 1</ref>. We use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27) as our ground truth bilingual lexi- con for evaluation. The baseline models are MonoGiza system ( <ref type="bibr" target="#b6">Dou et al., 2015)</ref>, translation matrix (TM) ( <ref type="bibr">Mikolov et al., 2013)</ref>, isometric alignment (IA) ( <ref type="bibr">Zhang et al., 2016b</ref>) and adversarial training ap- proach ( <ref type="bibr">Zhang et al., 2017)</ref>. <ref type="table" target="#tab_2">Table 2</ref> summarizes the performance of baseline models and our approach. The results of baseline models are cited from <ref type="bibr">Zhang et al. (2017)</ref>. As we can see from the table, our model could achieve superior performance compared with other base- line models. <ref type="table" target="#tab_3">Table 3</ref> lists some word translation examples given by our model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Experiments on Other Language Pairs Datasets</head><p>We also conduct experiments on Spanish-English and Italian-English language pairs. Again, we use the same dataset with <ref type="bibr">Zhang et al. (2017)</ref>. and the statistics are shown in  The experimental results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset. Our model is able to outperform baseline model in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy (%) en-es en-ru en-zh</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods without refinement</head><p>Adv  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Large-scale Datasets</head><p>In this section, we integrate our method with Con- neau et al. <ref type="formula" target="#formula_1">(2018)</ref>, whose method improves <ref type="bibr">Zhang et al. (2017)</ref> by more sophiscated refinement pro- cedure and validation criterion. We replace their first step, namely the adversarial training step, with our model. Basically, we first map the source and target embeddings into the latent space us- ing our algorithm, and then fine-tune the identity mapping in the latent space with the closed-form Procrustes solution. We use their similarity mea- sure, namely cross-domain similarity local scaling (CSLS), to produce reliable matching pairs and validation criterion for unsupervised model selec- tion.</p><p>We conduct experiments on English-Spanish, English-Russian and English-Chinese datasets, which are the same as <ref type="bibr" target="#b4">Conneau et al. (2018)</ref>. The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. As seen, our model could consistently achieve better perfor- mance compared with adversarial training. After refinement, our model could further achieve com- petitive and even superior results compared with state-of-the-art unsupervised methods. This fur- ther demonstrates the capacity of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Based on the assumption that word vectors in dif- ferent languages could be drawn from a same la- tent variable space, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora. Experimental results on several language pairs have demonstrated the effective- ness and universality of our model. We hope our method could be beneficial to other areas such as unsupervised machine translation ( <ref type="bibr" target="#b4">Lample et al., 2018)</ref>.</p><p>Future directions include validate our model on more realistic scenarios ( <ref type="bibr" target="#b5">Dinu et al., 2015</ref>) as well as combine our algorithms with more sophiscated adversarial networks ( <ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr">Gulrajani et al., 2017)</ref>. <ref type="bibr">Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014</ref>. Glove: Global vectors for word representation. In Conference on Empirical Meth- ods in Natural Language Processing (EMNLP). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Samuel</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our model. φ s and φ t map the source and target word embeddings into latent variables. Discriminator D guides the two latent distributions to be the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In International Conference on Learning Representations (ICLR). Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015. Normalized word embedding and orthogonal trans- form for bilingual word translation. In Annual Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics (NAACL). Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018. Improving neural machine translation with condi- tional sequence generative adversarial nets. In An- nual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Biao Zhang, Deyi Xiong, Hong Duan, Min Zhang, et al. 2016a. Variational neural machine transla- tion. In Conference on Empirical Methods in Nat- ural Language Processing (EMNLP). Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017. Adversarial training for unsupervised bilingual lexicon induction. In Annual Meeting of the Association for Computational Linguistics (ACL). Yuan Zhang, David Gaddy, Regina Barzilay, and Tommi Jaakkola. 2016b. Ten pairs to tag- multilingual pos tagging via coarse mapping be- tween embeddings. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Our experiments could be divided into two parts. In the first part, we conduct experiments on small- scale datasets and our main baseline is Zhang et al. (2017). In the second part, we combine our model with several advanced techniques and we compare our model with Conneau et al. (2018) on large- scale datasets.</figDesc><table>3: 

while φ s and θ s have not converged do 

4: 

Update discriminator D 

5: 

Update φ s and θ s 

6: 

end while 

7: 

while φ t and θ t have not converged do 

8: 

Update discriminator D 

9: 

Update φ t and θ t 

10: 

end while 
11: end for 

4 Experiment 

#tokens vocab. size 

zh-en 
zh 
21m 
3,349 
en 
53m 
5,154 

es-en 
es 
61m 
4,774 
en 
95m 
6,637 

it-en 
it 
73m 
8,490 
en 
93m 
6,597 

Table 1: Statistics of the non-parallel corpora. Lan-
guage codes: zh = Chinese, en = English, es = Spanish, 
it = Italian. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental results on Chinese-English 
dataset. The results of baseline models are cited from 
Zhang et al. (2017). 






airline 
rail 
antiquity 
school 
aviation railway 
era 
education 
airliner railroad century 
college 
service 
freight 
medieval 
student 
flight 
metro 
historian 
teacher 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Word translation examples for Chinese-
English dataset. Ground truths are marked in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>The ground truth 
bilingual lexica for Spanish-English and Italian-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results on Spanish-English and 
Italian-English datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Experimental results on large-scale datasets. 
Language codes: en=English, es = Spanish, ru = Rus-
sian, zh = Chinese. 

</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/MUSE</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviewers for their in-sightful comments. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR), Workshop Track</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unifying bayesian inference and vector space models for improved decipherment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
