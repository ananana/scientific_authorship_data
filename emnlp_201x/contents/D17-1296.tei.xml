<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transition-Based Disfluency Detection using LSTMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Design</orgName>
								<orgName type="institution">Singapore University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transition-Based Disfluency Detection using LSTMs</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2785" to="2794"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a set of transition actions without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our model achieves state-of-the-art F-score on both the commonly used English Switchboard test set and a set of in-house annotated Chinese data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts ( <ref type="bibr" target="#b31">Zayats et al., 2016;</ref><ref type="bibr" target="#b29">Wu et al., 2015)</ref>. As shown in <ref type="figure">Figure 1</ref>, stan- dard annotation of disfluency structure <ref type="bibr" target="#b25">(Shriberg, 1994)</ref> indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.).</p><p>Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. <ref type="table">Table 1</ref> gives a few examples. In- terregnums are easy to detect as they often consist of fixed phrases (e.g. "uh", "you know"). How- ever, reparandums are more difficult to detect, be- cause they can be in arbitrary form. Most previ- ous disfluency detection work focuses on detect- ing reparandums.</p><p>The main challenges of detecting reparandums include that they vary in length, may occur in dif- ferent locations, and are sometimes nested. For example, the longest reparandum in our training set has fifteen words. Hence, it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in <ref type="figure">Figure 1</ref>, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level rep- resentation, which explicitly makes use of resulted infelicity disfluency chunks.</p><p>Common approaches take disfluency detection as a sequence labeling problem, where each sen- tential word is assigned with a label ( <ref type="bibr" target="#b31">Zayats et al., 2016;</ref><ref type="bibr" target="#b13">Hough and Schlangen, 2015;</ref><ref type="bibr" target="#b23">Qian and Liu, 2013;</ref><ref type="bibr" target="#b8">Georgila, 2009)</ref>. These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these ap- proaches is that they are unable to exploit chunk-level features. <ref type="bibr">Semi-CRF (Ferguson et al., 2015)</ref> is used to alleviate this issue to some extent. <ref type="table">Semi- CRF models still have their inefficiencies because  they can only use the local chunk information lim- ited by the markov assumption when decoding.</ref> A different line of work ( <ref type="bibr" target="#b24">Rasooli and Tetreault, 2013;</ref><ref type="bibr" target="#b12">Honnibal and Johnson, 2014;</ref><ref type="bibr" target="#b29">Wu et al., 2015</ref>) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space.</p><p>Inspired by the above observations, we inves- tigate a transition-based model without syntactic information. Our model incrementally constructs and labels the disfluency chunks of input sentences using an algorithm similar to transition-based de- pency parsing. As shown in <ref type="figure">Figure 2</ref>, the model state consists of four components: (i) O, a conven- tional sequential LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>) to store the words that have been la- beled as fluency. (ii) S, a stack LSTM to represent partial disfluency chunks, which captures chunk- level information. (iii) A, a conventional sequen- tial LSTM to represent history of actions. (iiii) B, a Bi-LSTM to represent words that have not yet been processed. A sequence of transition ac- tions are used to consume input tokens and con- struct the output from left to right. To reduce error propagation, we use beam-search ( <ref type="bibr" target="#b5">Collins and Roark, 2004</ref>) and scheduled sampling <ref type="bibr" target="#b1">(Bengio et al., 2015)</ref>, respectively. We evaluate our model on the commonly used English Switchboard test set and a in-house an- notated Chinese data set. Results show that our model outperforms previous state-of-the-art sys- tems. The code is released 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>For a background, we briefly introduce transition- based parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words be- ing processed, a buffer β containing words to be processed and a memory A storing dependency <ref type="figure">Figure 2</ref>: model state when processing the sen- tence "want a flight to boston to denver". arcs which have been generated. There are four types of transition actions <ref type="bibr" target="#b21">(Nivre, 2008)</ref> • Shift : Remove the front of the buffer and push it to the stack.</p><formula xml:id="formula_0">1 https://github.com/hitwsl/transition disfluency TOP e t =max{0,W[s t ;b t ;o t ;a t ]+d} A DEL OUT denver to O want a flight B Bi-LSTM Subtraction DEL S DEL to boston b t s t a t o t</formula><p>• Reduce : Pop the top of the stack.</p><p>• LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer.</p><p>• RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack.</p><p>Many neural network parsers have been con- structed under this framework, such as ( , who use different LSTM structure to repre- sent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recogni- tion (ASR). We denote the word sequence as w n 1 = (w 1 , ..., w n ). The output of the task is a sequence of binary tags denoted as</p><formula xml:id="formula_1">D n 1 = (d 1 , ..., d n ),</formula><p>where each d i corresponds to the word w i , indicat- ing whether w i is a disfluent word or not. Hence the task can be modeled as searching for the best sequenc D * given the stream of words w n</p><formula xml:id="formula_2">1 D * = argmax D P (D n 1 |w n 1 )</formula><p>Wu et al. <ref type="formula">(2015)</ref> proposes a statistical transition- based disfluency detection model, which performs disfluency detection and parsing jointly by aug- menting the Shift-Reduce algorithm with a binary classifier transition (BCT) action:</p><p>• BCT : Classify whether the current word is disfluent or not. If it is, remove it from the buffer, push it into the stack which is similar to Shift and then mark it as disfluent. Other- wise the original parser transition actions will be used.</p><p>Disfluency detection and parsing are jointly opti- mized</p><formula xml:id="formula_3">(D * , T * ) = argmax D,T n i=1 P (d i |w i 1 , T i−1 1 ) ×P (T i 1 |w i 1 , T i−1 1 , d i ),</formula><p>where T i 1 is the partial tree after word w i is con- sumed, d i is the disfluency tag of w i . P (T i 1 |.) is the parsing model and P (d i 1 |.) is the disfluency model used to predict the disluency tags on the contexts of partial trees that have been built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Transition-Based Model</head><p>The BCT model serves as a state-of-the-art transition-based baseline. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. Also, BCT does not explicitly make use of resulting infelicity disflu- ency chunks. Being a discrete model, the perfor- mance relies heavily on manual feature engineer- ing.</p><p>To address the constraints above, we apply a transition-based neural model for disfluency de- tection that does not use any syntax information. Our transition-based method incrementally con- structs and labels the disfluency chunk of input sentences by performing a sequence of actions. The task is modeled as</p><formula xml:id="formula_4">(D * , T * ) = argmax D,T n i=1 P (d i , T i 1 |w i 1 , T i−1 1 ),</formula><p>where T i 1 is the partial model state after word w i is consumed. d i is the disfluency tag of w i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transition-Based Disfluency Detection</head><p>Our model incrementally constructs and labels the disfluency chunks of input sentences, where a state is represented by a tuple (O, S, A, B):</p><p>• output (O) : the output is used to represent the words that have been labeled as fluent.</p><p>• stack (S) : stack is used to represent the par- tially constructed disfluency chunk.</p><p>• action (A) : action is used to represent the complete history of actions taken by the tran- sition system.</p><p>• buffer (B) : buffer is used to represent the sentences that have not yet been processed.</p><p>Given an input disfluent sentence, the stack, output and action are initially empty and the buffer contains all words of the sentence, a sequence of transition actions are used to consume words in the buffer and build the output sentence:</p><p>• OUT: which moves the first word in the buffer to the output and clears out the stack if it is not empty.</p><p>• DEL: which moves the first word in the buffer to the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Algorithm</head><p>Based on the transition system, the decoder searches for an optimal action sequence for a given sentence. The system is initialized by pushing all the input words and their representations (of §3.3) onto B in the reverse order, such that the first word is at the top of B, and S, O and A each contains an empty-stack token. At each step, the system computes a composite representation of the model states (as determined by the current configurations of B, S, O, and A), which is used to predict an action to take. Decoding completes when B is empty (except for the empty-stack symbol), regardless of the state of S. Since each token in B is either moved directly to O or S every step, the total number of actions equals to the length of input sentence. <ref type="table">Table 2</ref> shows the sequence of operations required to process the sentence "want a flight to boston to denver". As shown in <ref type="figure">Figure 2</ref>, the model state represen- tation at time t, which is written as e t , is defined as:</p><formula xml:id="formula_5">e t = max{0, W [s t ; b t ; o t ; a t ] + d},</formula><p>where W is a learned parameter matrix, s t is the representation of S, b t is the representation of B, o t is the representation of O, a t is the representa- tion of A, d is a bias term. (W [s t ; b t ; o t ; a t ] + d) is passed through a component-wise rectified linear unit (ReLU) for nonlinearity <ref type="bibr" target="#b10">(Glorot et al., 2011</ref>).</p><p>Step Action Output Stack Buffer <ref type="table">Table 2</ref>: Segmentation process of a flight to boston to denver Finally, the model state e t is used to compute the probability of the action at time t as:</p><formula xml:id="formula_6">0 [] [] [a, flight, to, boston, to, denver] 1 OUT [a] [] [flight, to, boston, to, denver] 2 OUT [a, flight] [] [to, boston, to, denver] 3 DEL [a, flight] [to] [boston, to, denver] 4 DEL [a, flight] [to, boston] [to, denver] 5 OUT [a, flight, to] [] [denver] 6 OUT [a, flight, to, denver] [] []</formula><formula xml:id="formula_7">p(z t |e t ) = exp(g T zt e t + q zt ) z ∈A(S,B) exp(g T z e t + q z ) ,</formula><p>where g z is a column vector representing the em- bedding of the transition action z, and q z is a bias term for action z. The set A(S, B) represents the valid actions that may be taken given the current state. Since e t = f (s t , b t , a t , o t ) encodes infor- mation about all previous decisions made by the transition system, the probability of any valid se- quence of transition actions z conditioned on the input can be written as:</p><formula xml:id="formula_8">p(z|w) = |z| t=1 p(z t |e t )</formula><p>We then have</p><formula xml:id="formula_9">(D * , T * ) = argmax D,T |z| i=1 P (d i , T i 1 |w i 1 , T i−1 1 ) = argmax D,T |z| t=1 p(z t |e t ),</formula><p>where the disfluency detection task is merged into the transition-based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beam Search</head><p>The mainly drawback of greedy search is error propagation. An incorrect action will have a neg- ative influence to its subsequent actions, leading to an incorrect output sequence. One way to re- duce error propagation is beam-search. Because the number of actions taken always equals to the number of input sentence for every valid path, it is straightforward to use beam search. We use beam- search for both training and testing. The early up- date strategy from <ref type="bibr" target="#b5">Collins and Roark (2004)</ref> is ap- plied for training. In particular, each training se- quence is decoded, and we keep track of the lo- cation of the gold path in the beam. If the gold path falls out of the beam at step t, decoding pro- cess is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method ( <ref type="bibr" target="#b0">Andor et al., 2016;</ref>) to train our beam-search model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheduled Sampling</head><p>Scheduled sampling ( <ref type="bibr" target="#b1">Bengio et al., 2015)</ref> can also be used to reduce error propagation. The train- ing goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrep- ancy between training and inference can yield er- rors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing the training pro- cess from a fully guided scheme using the true pre- vious action, towards a less guided scheme which mostly uses the predicting action instead. We take the action gaining higher p(z t |e t ) with a certain probability p, and a probability (1 − p) for the cor- rect action when training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">State Representation</head><p>For better capturing non-local context informa- tion, we use LSTM structures to represent differ- ent components of each state, including buffer, ac- tion, stack, and output. In particular, we exploit LSTM-Minus ( <ref type="bibr" target="#b27">Wang and Chang, 2016)</ref> to model the buffer segment, conventional LSTM to model the action and ouptut segment, and stack LSTM ( ) to model the stack segments, which demonstrates highly effectively in parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Buffer Representation</head><p>In order to construct more informative represen- tation, we use a Bi-LSTM to represent the buffer following the work of <ref type="bibr" target="#b27">Wang and Chang (2016)</ref>, where the subtraction between a unidirectional boston to denver to hb(to) hb(denver) hf(to) hf(denver) LSTM hidden vectors is utilized to represent a segment's information. We perform a similar method in a Bi-LSTM to obtain the representa- tion of the buffer. The forward and backward subtractions for the buffer can be described as </p><note type="other">O want a flight to boston S to denver B hb(to) hb(denver</note><formula xml:id="formula_10">b f = h f (l) − h f (f ) and b b = h b (f ) − h b (l),</formula><formula xml:id="formula_11">= h f (to) − h f (denver) and b b = h b (denver) − h b (to), respectively.</formula><p>Here to is the first word in buffer and denver is the last. Then b f and b b are concatenated as the representation of buffer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action Representation</head><p>We represent an action a with an embedding e a (a) from a looking-up table E a , and apply a conven- tional LSTM to represent the complete history of actions taken by the transition system. Once an ac- tion a is taken, the embedding e a (a) will be added to the right-most position of the LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stack Representation</head><p>We use a stack LSTM ( ) to rep- resent partial disfluency chunk. The stack LSTM tries to augment the conventional LSTM with a "stack pointer". For a conventional LSTM, new inputs are always added in the right-most position; but in a stack LSTM, the current location of the stack pointer determines which cell in the LSTM provides c t−1 and h t−1 when computing the new memory cell contents. In addition to adding ele- ments to the end of the sequence, the stack LSTM provides a pop operation which moves the stack pointer to the previous element. Thus, the LSTM can be understood as a stack implemented so that contents are never overwritten, When the action OUT is taken, the stack is cleared by moving the stack pointer to the initial position. When the ac- tion DEL is taken, the representation of the buffer will be added directly to the stack LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Representation</head><p>We use a conventional LSTM to represent the out- put. When the action OUT is taken, the repre- sentation of the buffer will be added directly to the right-most position of the LSTM. Because the words in the output are a continuous subsequence of the final output sentence with disfluencies re- moved, the LSTM representation can be seen as a pseudo language model and thus has the ability to keep the generated sentence grammatical, which is very important for disfluency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Token Embeddings</head><p>We use four vectors to represent each input token: a learned word embedding w; a fixed word em- bedding w; a learned POS-tag embedding p; and a hand-crafted feature representation d. The four vectors are concatenated, transformed by a matrix V and fed to a rectified layer to learn a feature combination:</p><formula xml:id="formula_12">x = max{0, V [ w; w; p; d] + b},</formula><p>where V means vector concatenation. Following the work of , we extract two types of hand-crafted discrete features (as shown in <ref type="table" target="#tab_1">Table 3</ref>) for each token in a sentence, and incorporate them into our neural networks by translating them into a 0-1 vector d. The dimension of d is 78, which equals to the number of discrete features. For a token x t , d i fires if x t matches the i-th pattern of the feature templates. The duplicate features indicate whether x t has a duplicated word/POS-tag in certain distance. The similarity features indicate whether the surface string of x t resembles its surrounding words. duplicate features Duplicate(i, w i+k ), −15 ≤ k ≤ +15 and k = 0: if wi equals w i+k , the value is 1, others 0 Duplicate(pi, p i+k ), −15 ≤ k ≤ +15 and k = 0: if pi equals p i+k , the value is 1, others 0 Duplicate(wiwi+1, w i+k w i+k+1 ), −4 ≤ k ≤ +4 and k = 0: if wiwi+1 equals w i+k w i+k+1 , the value is 1, others 0 Duplicate(pipi+1, p i+k p i+k+1 ), −4 ≤ k ≤ +4 and k = 0: if pipi+1 equals p i+k p i+k+1 , the value is 1, others 0 similarity features f uzzyM atch(wi, w i+k ), k ∈ {−1, +1}: similarity = 2 * num same letters/(len(wi) + len(w i+k )). if similarity &gt; 0.8, the value is 1, others 0  <ref type="bibr" target="#b4">Charniak and Johnson (2001)</ref>, the training subcorpus contains directories 2 and 3 in PARSED/MRG/SWBD and directory 4 is split into test, development sets and others. Follow- ing <ref type="bibr" target="#b12">Honnibal and Johnson (2014)</ref>, we lower-case the text and remove all punctuations and partial words 2 . We also discard the 'um' and 'uh' to- kens and merge 'you know' and 'i mean' into sin- gle tokens. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POS- tag in our experiments.</p><p>For Chinese experiments, we collect 25k spo- ken sentences from meeting minutes, which are transcribed using the iflyrec toolkit 3 , and annotate them with only disfluency annotations according to the guideline proposed by <ref type="bibr" target="#b20">Meteer et al. (1995)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance On English Swtichboard</head><p>We build two baseline systems using CRF and Bi-LSTM, respectively. The hand-crafted dis- crete features of CRF refer to those in <ref type="bibr" target="#b7">Ferguson et al. (2015)</ref>. For the Bi-LSTM model, the token embedding is the same with our transition-based method.   score as shown in <ref type="table" target="#tab_4">Table 5</ref>. It achieves 2.4 point im- provements over UBT ( <ref type="bibr" target="#b29">Wu et al., 2015)</ref>, which is the best syntax-based method for disfluency detec- tion. The best performance by linear statistical se- quence labeling methods is the semi-CRF method of <ref type="bibr" target="#b7">Ferguson et al. (2015)</ref>, achieving a 85.4% F- score leveraging prosodic features. Our model ob- tains a 2.1 point improvement compared to this. Our model also achieves 0.8 point improvement over the neural attention-based model ( , which regards the disfluency detection as a sequence-to-sequence problem. We attribute the success to the strong ability to learn global chunk- level features and the good state representation such as the stack-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Result On DPS Corpus</head><p>As described in section 3.1, to directly compare with the transition-based parsing methods <ref type="bibr" target="#b12">(Honnibal and Johnson, 2014;</ref><ref type="bibr" target="#b29">Wu et al., 2015</ref>), we only use MRG files, which are less than the DPS files. In fact, many methods, such as Qian and , have used all the DPS files as train- ing data. We are curious about the performance of our system using all the DPS files. Following the experimental settings of <ref type="bibr" target="#b14">Johnson and Charniak (2004)</ref>, the corpus is split as follows: main train- ing consisting of all sw <ref type="bibr">[23]</ref>*.dps files, develop- ment training consisting of all sw4 <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref><ref type="bibr">[8]</ref><ref type="bibr">[9]</ref>*.dps files and test training consisting of all sw4[0-1]*.mrg files. <ref type="table" target="#tab_6">Table 6</ref> shows the result on the DPS files.    <ref type="formula">(2013)</ref>, which use the same data set and pre-processing.</p><p>Our model achieves a 88.1% F-score by using more training data, obtaining 0.6 point improve- ment compared with the system training on MRG files. The performance is far better than the se- quence labeling methods that use DPS files for training. <ref type="table" target="#tab_7">Table 7</ref> shows the results of Chinese disfluency detection. Our model obtains a 2.4 point im- provement compared with the baseline Bi-LSTM model and a 5.3 point compared with the baseline CRF model. The performance on Chinese is much lower than that on English. Apart from the smaller training set, the main reason is that the proportion of repair type disflueny is much higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance on Chinese</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Tests</head><p>As described in section 3.1, the sate representa- tion has four components. We explicitly compare the impact of different parts. As shown in <ref type="table">Table 8</ref>, the F-score decreases most heavily without stack, which indicates that it is very necessary to cap- ture chunk-level information for disfluency detec- tion and our model can model it effectively. The results also show that output, which can be seen as a pseudo language model, has important influence on model performance. Seen from the result, his- tory of actions represented in action is also useful for predicting at each step. The F-score decreases <ref type="bibr">4</ref> The toolkit is available at https://code.google.com/p/disfluency-detection/downloads.  <ref type="table">Table 8</ref>: Results of feature ablation experiments on English Switchboard test data. "-Bi-LSTM" means using unidirectional LSTM for buffer about 0.4 point, which shows that Bi-LSTM can capture more information compared to simple uni- directional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Repetitions vs Non-repetitions</head><p>Repetition disfluencies are easier to detect and even some simple hand-crafted features can han- dle them well. Other types of reparandums such as repair are more complex ( <ref type="bibr" target="#b31">Zayats et al., 2016;</ref><ref type="bibr" target="#b22">Ostendorf and Hahn, 2013)</ref>. In order to bet- ter understand model performances, we evalu- ate our model's ability to detect repetition vs. non-repetition (other) reparandum. The results are shown in <ref type="table" target="#tab_10">Table 9</ref>. All the three mod- els achieve high score on repetition reparan- dum. Our transition-based model is much bet- ter in predicting non-repetitions compared to CRF and Bi-LSTM. We conjecture that our transition- based structure can capture more of the reparan- dum/repair "rough copy" similarities by learning represention of both chunks and global state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Common approaches take disfluency detection as a sequence labeling problem, where each senten- tial word is assigned with a label <ref type="bibr" target="#b8">(Georgila, 2009;</ref><ref type="bibr" target="#b23">Qian and Liu, 2013)</ref>. These methods achieve good performance, but are not powerful enough to cap- ture complicated disfluencies with longer spans or distances. Another drawback is that they have no ability to exploit chunk-level features. There are also works that try to use recurrent neural network (RNN), which can capture dependencies at any length in theory, on disfluency detection problem ( <ref type="bibr" target="#b31">Zayats et al., 2016;</ref><ref type="bibr" target="#b13">Hough and Schlangen, 2015)</ref>. The RNN method treats sequence tagging as clas- sification on each input token. Hence, it also has no power to exploit chunk-level features. Some works ( ) regard the disfluency detection as a sequence-to-sequence problem and propose a neural attention-based model for it. The  attention-based model can capture a global repre- sentation of the input sentence by using a RNN when encoding. It can strongly capture long-range dependencies and achieves good performance, but are also not powerful enough to capture chunk- level information. To capture chunk-level infor- mation, <ref type="bibr" target="#b7">Ferguson et al. (2015)</ref> try to use semi-CRF for disfluency detection, and reports improved re- sults. Semi-CRF models still have their inefficien- cies because they can only use the local chunk in- formation limited by the markov assumption when decoding.</p><p>Many syntax-based approaches ( <ref type="bibr" target="#b16">Lease and Johnson, 2006;</ref><ref type="bibr" target="#b24">Rasooli and Tetreault, 2013;</ref><ref type="bibr" target="#b12">Honnibal and Johnson, 2014;</ref><ref type="bibr" target="#b29">Wu et al., 2015</ref>) have been proposed which jointly perform dependency parsing and disfluency detection. The main advan- tage of joint models is that they can capture long- range dependency of disfluencies. However, it re- quires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering.</p><p>Transition-based framework has been widely exploited in a number of other NLP tasks, includ- ing syntactic parsing ( <ref type="bibr" target="#b35">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b37">Zhu et al., 2013)</ref>, information extraction ( <ref type="bibr" target="#b17">Li and Ji, 2014)</ref> and joint syntactic models ( <ref type="bibr" target="#b33">Zhang et al., , 2014</ref>).</p><p>Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition ( <ref type="bibr" target="#b15">Lample et al., 2016)</ref>, zero pronoun resolution ( <ref type="bibr" target="#b30">Yin et al., 2017</ref>) and word segmentation ( <ref type="bibr" target="#b34">Zhang et al., 2016)</ref>. The effectiveness of neural features has also been studied for this framework ( <ref type="bibr" target="#b28">Watanabe and Sumita, 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016)</ref>. We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduced a transition-based model for dis- fluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-house annotated Chinese data set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for learning buffer representation based on a Bi-LSTM, h f (*) and h b (*) indicate the hidden vectors of forward and backward LSTM respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>respectively, where h f (f ) and h f (l) are the hidden vectors of the first and the last words in the forward LSTM, respectively. Similarly, h b (f ) and h b (l) are the hidden vectors of the first and the last words in the backward LSTM, respectively. Then these subtractions are concatenated as the repre- sentation of the buffer b t = b f ⊕ b b . As illustrated in Figure 3, the forward and backward subtrac- tions for buffer are b f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>words are recognized as partial words if they are tagged as 'XX' or end with '-' 3 the iflyrec toolkit is available at http://www.iflyrec.com/ 4.2 Neural Network Training Pretrained Word Embeddings. Following Dyer et al. (2015) and Wang et al. (2016), we use a variant of the skip n-gram model introduced by Ling et al. (2015), named "structured skip n-gram", to create word embeddings. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Word embeddings for Chinese are trained on Chinese baike corpus. We use an embedding dimension of 100 for English, 300 for chinese. Hyper-Parameters. Both the Bi-LSTMs and the stack LSTMs have two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions and the learned word embeddings have also 100 dimensions. Pos-tag embeddings have 12 dimensions. The dimension of action embeddings is set to 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>* Email corresponding. I want a flight [ to Boston + {um} to Denver ]</head><label>*</label><figDesc></figDesc><table>RM 
IM 
RP 

Figure 1: Sentence with disfluencies annotated in 
English Switchboard corpus. RM=Reparandum, 
IM=Interregnum, RP=Repair. The preceding RM 
is corrected by the following RP. 

Type 
Annotation 
repair 
[ I just + I ] enjoy working 
repair 
[ we want + {well} in our area we want ] to 
repetition 
[it's + {uh} it's ] almost like 
restart 
[ we would like + ] let's go to the 

Table 1: Different types of disfluencies. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Discrete features used in our transition-based neural networks. p-POS tag. w-word. 

4 Experiments 

4.1 Settings 

Dataset. Our training data include the Switch-
board portion of the English Penn Treebank (Mar-
cus et al., 1993) and a in-house Chinese data 
set. For English, two annotation layers are pro-
vided: one for syntactic bracketing (MRG files), 
and the other for disfluencies (DPS files). The 
Switchboard annotation project was not fully com-
pleted. Because disfluency annotation is cheaper 
to produce, many of the DPS training files do 
not have matching MRG files. Only 619,236 of 
the 1,482,845 tokens in the DPS disfluency de-
tection training data have gold-standard syntac-
tic parses. To directly compare with transition-
based parsing methods (Honnibal and Johnson, 
2014; Wu et al., 2015), we also use the subcor-
pus of PARSED/MRG/SWBD. Following the ex-
periment settings in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 shows</head><label>4</label><figDesc></figDesc><table>the result of our model on 
both the development and test sets. Beam search 
improves the F-score form 87.1% to 87.5%, which 
is consistent with the finding of Buckman et al. 
(2016) on the LSTM parser of (Dyer et al., 2015) 
(improvements by about 0.3 point). Scheduled 
sampling achieves the same improvements com-
pared to beam-search. Because of high training 
speed, we conduct subsequent experiments based 
on scheduled sampling. 
We compare our transition-based neural model 
to five top performing systems. Our model out-
performs the state-of-the-art, achieving a 87.5% F-

2790 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experiment results on the development 
and test data of English Switchboard data. 

Method 
P 
R 
F1 
Our 
91.1 84.1 87.5 
Attention-based (Wang et al., 2016) 91.6 82.3 86.7 
Bi-LSTM (Zayats et al., 2016) 
91.8 80.6 85.9 
semi-CRF (Ferguson et al., 2015) 
90.0 81.2 85.4 
UBT (Wu et al., 2015) 
90.3 80.5 85.1 
M 3 N (Qian and Liu, 2013) 
-
-
84.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with previous state-of-the-
art methods on the test set of English Switchboard. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 : Test result of our transition-based model using DPS files for training.</head><label>6</label><figDesc></figDesc><table>Method 
Dev 
Test 
P 
R 
F1 
P 
R 
F1 
Our 
68.9 40.4 50.9 77.2 37.7 50.6 
Bi-LSTM 60.1 41.3 48.9 65.3 38.2 48.2 
CRF 
73.7 33.5 46.1 77.7 32.0 45.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 : performance on Chinese annotated data</head><label>7</label><figDesc></figDesc><table>The result of M 3 N  *  comes from our experiments 
with the toolkit 4 released by Qian and Liu </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 9 : F-score of different types of reparandums on English Switchboard test data.</head><label>9</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able suggestions. This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natu-ral Science Foundation of China (NSFC) via grant 61370164 and 61632011. The Chinese disfluency data is annotated by IFLYTEK CO.,LTD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with heuristic backtracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stroudsburg (USA): Association for Computational Linguistics (ACL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Texas</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="2313" to="2331" />
			<date type="published" when="2016" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Edit detection and parsing for transcribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</title>
		<meeting>the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">111</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Disfluency detection with a semi-markov model and prosodic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using integer linear programming for detecting speech disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="109" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">275</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint incremental disfluency detection and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for incremental disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tagbased noisy channel model of speech repairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 33. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 33. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Early deletion of fillers in processing conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dysfluency annotation stylebook for the switchboard corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">A</forename><surname>Marie W Meteer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukmini</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A sequential repetition model for improved disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyun</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2624" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disfluency detection using multi-step stacked learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="820" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint parsing and disfluency detection in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Preliminaries to a theory of speech disfluencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">Ellen</forename><surname>Shriberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural attention model for disfluency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient disfluency detection with transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep neural network for chinese zero pronoun resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Artificial Intelligence, IJCAI&apos;17</title>
		<meeting>the 26th International Conference on Artificial Intelligence, IJCAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Zayats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03209</idno>
		<title level="m">Disfluency detection using a bidirectional lstm</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Chinese parsing exploiting characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Character-level chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1326" to="1336" />
		</imprint>
	</monogr>
	<note>Baltimore</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transition-based neural word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="421" to="431" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A neural probabilistic structuredprediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast and accurate shiftreduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
