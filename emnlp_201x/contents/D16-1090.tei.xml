<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arijit</forename><surname>Ray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Christie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="919" to="924"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Visual Question Answering (VQA) is the task of answering natural-language questions about images. We introduce the novel problem of determining the relevance of questions to images in VQA. Current VQA models do not reason about whether a question is even related to the given image (e.g., What is the capital of Argentina?) or if it requires information from external resources to answer correctly. This can break the continuity of a dialogue in human-machine interaction. Our approaches for determining relevance are composed of two stages. Given an image and a question, (1) we first determine whether the question is visual or not, (2) if visual, we determine whether the question is relevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA model uncertainty, and caption-question similarity, are able to outper-form strong baselines on both relevance tasks. We also present human studies showing that VQA models augmented with such question relevance reasoning are perceived as more intelligent , reasonable, and human-like.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual Question Answering (VQA) is the task of predicting a suitable answer given an image and a question about it. VQA models (e.g., <ref type="bibr" target="#b0">(Antol et al., 2015;</ref><ref type="bibr" target="#b17">Ren et al., 2015)</ref>) are typically discriminative models that take in image and question representa- tions and output one of a set of possible answers.</p><p>Our work is motivated by the following key ob- servation -all current VQA systems always output an answer regardless of whether the input question makes any sense for the given image or not. <ref type="figure" target="#fig_0">Fig. 1</ref> Non-Visual Visual True-Premise</p><p>Who is the president of the USA? What is the girl wearing?</p><p>What is the cat wearing?</p><p>Visual False-Premise shows examples of relevant and irrelevant questions. When VQA systems are fed irrelevant questions as input, they understandably produce nonsensical an- swers (Q: "What is the capital of Argentina?" A: "fire hydrant"). Humans, on the other hand, are unlikely to provide such nonsensical answers and will instead answer that this is irrelevant or use an- other knowledge source to answer correctly, when possible. We argue that this implicit assumption by all VQA systems -that an input question is always relevant for the input image -is simply untenable as VQA systems move beyond standard academic datasets to interacting with real users, who may be unfamiliar, or malicious. The goal of this work is to make VQA systems more human-like by providing them the capability to identify relevant questions.</p><p>While existing work has reasoned about cross- modal similarity, being able to identify whether a question is relevant to a given image is a novel prob- lem with real-world applications. In human-robot interaction, being able to identify questions that are dissociated from the perception data available is im- portant. The robot must decide whether to process the scene it perceives or query external world knowl- edge resources to provide a response.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we study three types of question-image pairs: Non-Visual. These questions are not questions about images at all -they do not require information from any image to be answered (e.g., "What is the capital of Argentina?"). Visual False-Premise. While visual, these questions do not apply to the given image. For instance, the ques- tion "What is the girl wearing?" makes sense only for images that contain a girl in them. Visual True- Premise. These questions are relevant to (i.e., have a premise which is true) the image at hand.</p><p>We introduce datasets and train models to rec- ognize both non-visual and false-premise question- image (QI) pairs in the context of VQA. First, we identify whether a question is visual or non-visual; if visual, we identify whether the question has a true- premise for the given image. For visual vs. non- visual question detection, we use a Long Short-Term Memory (LSTM) recurrent neural network (RNN) trained on part of speech (POS) tags to capture visual-specific linguistic structure. For true vs. false- premise question detection, we present one set of ap- proaches that use the uncertainty of a VQA model, and another set that use pre-trained captioning mod- els to generate relevant captions (or questions) for the given image and then compare them to the given question to determine relevance.</p><p>Our proposed models achieve accuracies of 92% for detecting non-visual, and 74% for detecting false-premise questions, which significantly outper- form strong baselines. We also show through human studies that a VQA system that reasons about ques- tion relevance is picked significantly more often as being more intelligent, human-like and reasonable than a baseline VQA system which does not. Our code and datasets are publicly available on the au- thors' webpages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large body of existing work that reasons about cross-modal similarity: how well an image matches a query tag ( <ref type="bibr" target="#b13">Liu et al., 2009</ref>) in text-based image retrieval, how well an image matches a cap- tion <ref type="bibr" target="#b8">(Feng and Lapata, 2013;</ref><ref type="bibr" target="#b20">Xu et al., 2015;</ref><ref type="bibr" target="#b16">Ordonez et al., 2011;</ref><ref type="bibr" target="#b10">Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b7">Fang et al., 2015)</ref>, and how well a video matches a de- scription ( <ref type="bibr" target="#b6">Donahue et al., 2015;</ref><ref type="bibr" target="#b11">Lin et al., 2014a</ref>).</p><p>In our work, if a question is deemed irrelevant, the VQA model says so, as opposed to answering the question anyway. This is related to perception systems that do not respond to an input where the system is likely to fail. Such failure prediction sys- tems have been explored in vision ( <ref type="bibr" target="#b21">Zhang et al., 2014;</ref><ref type="bibr" target="#b4">Devarakota et al., 2007</ref>) and speech ( <ref type="bibr" target="#b22">Zhao et al., 2012;</ref><ref type="bibr" target="#b18">Sarma and Palmer, 2004;</ref><ref type="bibr" target="#b2">Choularton, 2009;</ref><ref type="bibr" target="#b19">Voll et al., 2008)</ref>. Others attempt to provide the most meaningful answer instead of suppressing the output of a model that is expected to fail for a given input. One idea is to avoid a highly specific prediction if there is a chance of being wrong, and instead make a more generic prediction that is more likely to be right ( <ref type="bibr" target="#b3">Deng et al., 2012</ref>). <ref type="bibr" target="#b15">Malinowski and Fritz (2014)</ref> use semantic segmentations in their approach to question answering, where they reason that objects not present in the segmentations should not be part of the answer.</p><p>To the best of our knowledge, our work is the first to study the relevance of questions in VQA. <ref type="bibr" target="#b1">Chen et al. (2012)</ref> classify users' intention of questions for community question answering services. Most re- lated to our work is <ref type="bibr" target="#b5">Dodge et al. (2012)</ref>. They extract visual text from within Flickr photo captions to be used as supervisory signals for training image cap- tioning systems. Our motivation is to endow VQA systems the ability to detect non-visual questions to respond in a human-like fashion. Moreover, we also detect a more fine-grained notion of question rele- vance via true-and false-premise.</p><p>Visual Questions (VNQ).</p><p>We also collect a dataset of true-vs. false-premise questions by showing AMT workers images paired with random questions from the VQA dataset and asking them to annotate whether they are applicable or not. We had three workers annotate each QI pair. We take the majority vote as the final ground truth label. <ref type="bibr">2</ref> We have 10,793 QI pairs on 1,500 unique images out of which 79% are non-applicable (false- premise). We refer to this visual true-vs. false- premise questions dataset as VTFQ.</p><p>Since there is a class imbalance in both of these datasets, we report the average per-class (i.e., nor- malized) accuracy for all approaches. All datasets are publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>Here we present our approaches for detecting (1) vi- sual vs. non-visual QI pairs, and <ref type="formula">(2)</ref> true-vs. false- premise QI pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual vs. Non-Visual Detection</head><p>Recall that the task here is to detect visual questions from non-visual ones. Non-visual questions, such as "Do dogs fly?" or "Who is the president of the USA?", often tend to have a difference in the lin- guistic structure from that of visual questions, such as "Does this bird fly?" or "What is this man do- ing?". We compare our approach (LSTM) with a baseline (RULE-BASED): 1. RULE-BASED. A rule-based approach to detect non-visual questions based on the part of speech (POS) 3 tags and dependencies of the words in the question. E.g., if a question has a plural noun with no determiner before it and is followed by a singular verb ("Do dogs fly?"), it is a non-visual question. <ref type="bibr">4</ref> 2. LSTM. We train an LSTM with 100-dim hid- den vectors to embed the question into a vector and predict visual vs. not. Instead of feeding question words (['what', 'is', 'the', 'man', 'doing', '?']), the input to our LSTM is embeddings of POS tags of the words ( <ref type="bibr">['pronoun', 'verb', 'determiner', 'noun', 'verb']</ref>). Embeddings of the POS tags are learnt end-to-end. This captures the structure of image-grounded questions, rather than visual vs. non- visual topics. The latter are less likely to generalize across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">True-vs. False-Premise Detection</head><p>Our second task is to detect whether a question Q en- tails a false-premise for an image I. We present two families of approaches to measure this QI 'compat- ibility': (i) using uncertainty in VQA models, and (ii) using pre-trained captioning models.</p><p>Using VQA Uncertainty. Here we work with the hypothesis that if a VQA model is uncertain about the answer to a QI pair, the question may be irrele- vant for the given image since the uncertainty may mean it has not seen similar QI pairs in the training data. We test two approaches: 1. ENTROPY. We compute the entropy of the soft- max output from a state-of-the art VQA model <ref type="bibr" target="#b0">(Antol et al., 2015;</ref>) for a given QI pair and train a three-layer multilayer perceptron (MLP) on top with 3 nodes in the hidden layer. 2. VQA-MLP. We feed in the softmax output to a three-layer MLP with 100 nodes in the hidden layer, and train it as a binary classifier to predict whether a question has a true-or false-premise for the given image.</p><p>Using Pre-trained Captioning Models. Here we utilize (a) an image captioning model, and (b) an image question-generation model -to measure QI compatibility. Note that both these models generate natural language capturing the semantics of an im- age -one in the form of statement, the other in the form of a question. Our hypothesis is that a given question is relevant to the given image if it is similar to the language generated by these models for that image. Specifically: 1. Question-Caption Similarity (Q-C SIM). We use NeuralTalk2 ( <ref type="bibr" target="#b10">Karpathy and Fei-Fei, 2015</ref>) pre- trained on the MSCOCO dataset ( <ref type="bibr" target="#b12">Lin et al., 2014b</ref>) (images and associated captions) to generate a cap- tion C for the given image, and then compute a learned similarity between Q and C (details below). 2. Question-Question Similarity (Q-Q' SIM). We use NeuralTalk2 re-trained (from scratch) on the questions in the VQA dataset to generate a question Q' for the image. Then, we compute a learned simi- larity between Q and Q'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual vs. Non-Visual</head><p>True-vs. False-Premise <ref type="table">RULE-BASED  LSTM  ENTROPY  VQA-MLP  Q-GEN SCORE  Q-C SIM  Q-Q'</ref>  We now describe our learned Q-C similarity func- tion (the Q-Q' similarity is analogous). Our Q-C similarity model is a 2-channel LSTM+MLP (one channel for Q, another for C). Each channel se- quentially reads word2vec embeddings of the cor- responding language via an LSTM. The last hid- den state vectors (40-dim) from the 2 LSTMs are concatenated and fed as inputs to the MLP, which outputs a 2-class (relevant vs. not) softmax. The entire model is learned end-to-end on the VTFQ dataset. We also experimented with other represen- tations (e.g., bag of words) for Q, Q', C, which are included in the supplement for completeness. Finally, we also compare our proposed models above to a simpler baseline (Q-GEN SCORE), where we compute the probability of the input question Q under the learned question-generation model. The intuition here is that since the question generation model has been trained only on relevant questions (from the VQA dataset), it will assign a high proba- bility to Q if it is relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>The results for both experiments are presented in Ta- ble 1. We present results averaged over 40 random train/test splits. RULE-BASED and Q-GEN SCORE were not averaged because they are deterministic.</p><p>Visual vs. Non-Visual Detection. We use a ran- dom set of 100,000 questions from the VNQ dataset for training, and the remaining 31,464 for testing. We see that LSTM performs 16.59% (21.92% rela- tive) better than RULE-BASED.</p><p>True-vs. False-Premise Detection. We use a ran- dom set of 7,195 (67%) QI pairs from the VTFQ dataset to train and the remaining 3,597 (33%) to test. While the VQA model uncertainty based ap- proaches (ENTROPY, VQA-MLP) perform reason- ably well (with the MLP helping over raw entropy), the learned similarity approaches perform much bet- ter (10.39% gain in normalized accuracy). High un- certainty of the model may suggest that a similar QI pair was not seen during training; however, that does not seem to translate to detecting irrelevance. The language generation models (Q-C SIM, Q-Q' SIM) seem to work significantly better at modeling the semantic interaction between the question and the image. The generative approach (Q-GEN SCORE) is outperformed by the discriminative approaches (VQA-MLP, Q-C SIM, Q-Q' SIM) that are trained explicitly for the task at hand. We show qualitative examples of Q-Q' SIM for true-vs. false-premise detection in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human Qualitative Evaluation</head><p>We also perform human studies where we compare two agents: (1) AGENT-BASELINE-always answers every question. (2) AGENT-OURS-reasons about question relevance before responding. If question is classified as visual true-premise, AGENT-OURS an- swers the question using the same VQA model as AGENT-BASELINE (using ( ). Other- wise, it responds with a prompt indicating that the question does not seem meaningful for the image.</p><p>A total of 120 questions (18.33% relevant, 81.67% irrelevant, mimicking the distribution of the VTFQ dataset) were used. Of the relevant ques- tions, 54% were answered correctly by the VQA model. Human subjects on AMT were shown the response of both agents and asked to pick the agent that sounded more intelligent, more reasonable, and more human-like after every observed QI pair. Each QI pair was assessed by 5 different subjects. Not all pairs were rated by the same 5 subjects. In total, 28 unique AMT workers participated in the study.</p><p>AGENT-OURS was picked 65.8% of the time as the winner, AGENT-BASELINE was picked only 1.6% of the time, and both considered equally (un)reasonable in the remaining cases. We also mea- sure the percentage of times each robot gets picked Q":"Is"the"event"indoor"or"outdoor? Q'#:#What"is"the"elephant"doing?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>US GT</head><p>(a) Q":"What"type"of"melon"is"that?</p><p>Q' :"What"color"is"the"horse?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>US GT</head><p>(b)</p><p>Q:"Is"this"man"married? Q':"What"is"the"man"holding?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>US GT</head><p>(c)</p><p>Q:"Is"that"graffiti"on"the"wall?" Q':"What"is"the"woman"holding? by the workers for true-premise, false-premise, and non-visual questions. These percentages are shown in <ref type="table" target="#tab_2">Table 2</ref>.  Interestingly, humans often prefer AGENT-OURS over AGENT-BASELINE even when both models are wrong -AGENT-BASELINE answers the question incorrectly and AGENT-OURS incorrectly predicts that the question is irrelevant and refuses to answer a legitimate question. Users seem more tolerant to mistakes in relevance prediction than VQA.</p><formula xml:id="formula_0">US GT (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduced the novel problem of identifying ir- relevant (i.e., non-visual or visual false-premise) questions for VQA. Our proposed models signifi- cantly outperform strong baselines on both tasks. A VQA agent that utilizes our detector and refuses to answer certain questions significantly outperforms a baseline (that answers all questions) in human stud- ies. Such an agent is perceived as more intelligent, reasonable, and human-like.</p><p>There are several directions for future work. One possibility includes identifying the premise entailed in a question, as opposed to just stating true-or false-premise. Another is determining what exter- nal knowledge is needed to answer non-visual ques- tions.</p><p>Our system can be further augmented to com- municate to users what the assumed premise of the question is that is not satisfied by the image, e.g., respond to "What is the woman wearing?" for an image of a cat by saying "There is no woman."</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example irrelevant (non-visual, false-premise) and relevant (visual true-premise) questions in VQA.</figDesc><graphic url="image-2.png" coords="1,391.43,229.01,68.96,71.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Qualitative examples for Q-Q' SIM. (a) and (b) show success cases, and (c) and (d) show failure cases. Our model predicts true-premise in (a) and (c), and false-premise in (b) and (d). In all examples we show the original question Q and the generated question Q'.</figDesc><graphic url="image-4.png" coords="5,88.38,61.13,90.40,109.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Normalized accuracy results (averaged over 40 random train/test splits) for visual vs. non-visual detection and true-vs. false-premise detection. RULE-BASED and Q-GEN SCORE were not averaged because they are deterministic.</head><label>1</label><figDesc></figDesc><table>SIM 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Percentage of times each robot gets picked by 
AMT workers as being more intelligent, more reasonable, 
and more human-like for true-premise, false-premise, and 
non-visual questions. 

</table></figure>

			<note place="foot" n="3"> Datasets For the task of detecting visual vs. non-visual questions, we assume all questions in the VQA dataset (Antol et al., 2015) are visual, since the Amazon Mechanical Turk (AMT) workers were specifically instructed to ask questions about a displayed image while creating it. We also collected non-visual philosophical and general knowledge questions from the internet (see supplementary material). Combining the two, we have 121,512 visual questions from the validation set of VQA and 9,952 1 generic non-visual questions collected from the internet. We call this dataset Visual vs. Non1 High accuracies on this task in our experiments indicate that this suffices to learn the corresponding linguistic structure.</note>

			<note place="foot" n="2"> 78% of the time all three votes agree. 3 We use spaCy POS tagger (Honnibal and Johnson, 2015). 4 See supplement for examples of such hand-crafted rules.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Lucy Vanderwende for helpful sugges-tions and discussions. We also thank the anony-mous reviewers for their helpful comments. This work was supported in part by the following: Na-tional Science Foundation CAREER awards to DB and DP, Alfred P. Sloan Fellowship, Army Research Office YIP awards to DB and DP, ICTAS Junior Fac-ulty awards to DB and DP, Army Research Lab grant W911NF-15-2-0080 to DP and DB, Office of Naval Research grant N00014-14-1-0679 to DB, Paul G. Allen Family Foundation Allen Distinguished Inves-tigator award to DP, Google Faculty Research award to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donation to DB.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding User Intent in Community Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levene</forename><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Early Stage Detection of Speech Recognition Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen Choularton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Macquarie University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hedging Your Bets: Optimizing Accuracy-Specificity Trade-offs in Large Scale Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Confidence estimation in classification decision: A method for detecting unseen patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pandu R Devarakota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>and Björn Ottersten</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting Visual Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From Captions to Visual Concepts and Back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">K</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Automatic Caption Generation for News Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Improved Non-monotonic Transition System for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep VisualSemantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual Semantic Search: Retrieving Videos via Complex Textual Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boost Search Relevance for Tag-based Social Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeper LSTM and normalized CNN Visual Question Answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/VT-vision-lab/VQA_LSTM_CNN" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A MultiWorld Approach to Question Answering about RealWorld Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Im2Text: Describing Images Using 1 Million Captioned Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-based Speech Recognition Error Detection and Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arup</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David D</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the Utility of Speech Recognition Through Error Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Forster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting Failures of Vision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic Chinese Pronunciation Error Detection Using SVM Trained with Structural Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongmu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akemi</forename><surname>Hoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuaki</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keikichi</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
