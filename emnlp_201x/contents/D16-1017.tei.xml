<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event participant modelling with neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ottokar</forename><surname>Tilk</surname></persName>
							<email>ottokar.tilk@phon.ioc.ee</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Cybernetics</orgName>
								<orgName type="institution">Tallinn University of Technology</orgName>
								<address>
									<postCode>12618</postCode>
									<settlement>Tallinn</settlement>
									<country key="EE">Estonia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asad</forename><surname>Sayeed</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
							<email>dietrich.klakow@lsv.uni-sb.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<postCode>66123</postCode>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Event participant modelling with neural networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="171" to="182"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A common problem in cognitive modelling is lack of access to accurate broad-coverage models of event-level surprisal. As shown in, e.g., Bicknell et al. (2010), event-level knowledge does affect human expectations for verbal arguments. For example, the model should be able to predict that mechanics are likely to check tires, while journalists are more likely to check typos. Similarly, we would like to predict what locations are likely for playing football or playing flute in order to estimate the surprisal of actually-encountered locations. Furthermore, such a model can be used to provide a probability distribution over fillers for a thematic role which is not mentioned in the text at all. To this end, we train two neural network models (an incremental one and a non-incremental one) on large amounts of automatically role-labelled text. Our models are probabilistic and can handle several roles at once, which also enables them to learn interactions between different role fillers. Evaluation shows a drastic improvement over current state-of-the-art systems on modelling human thematic fit judgements , and we demonstrate via a sentence similarity task that the system learns highly useful embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our goals in this paper are to learn a representa- tion of events and their thematic roles based on large quantities of automatically role-labelled text and to be able to calculate probability distributions over the possible role fillers of specific missing roles. In this sense, the task is closely related to work on selec- tional preference acquisition (Van de Cruys, 2014). We focus here on the roles agent, patient, loca- tion, time, manner and the predicate itself. The model we develop is trained to represent the event- relevant context and hence systematically captures long-range dependencies. This has been previously shown to be beneficial also for more general lan- guage modelling tasks (e.g., <ref type="bibr" target="#b9">Chelba and Jelinek, 1998;</ref><ref type="bibr" target="#b39">Tan et al., 2012</ref>).</p><p>This type of modelling is potentially relevant to a wide range of tasks, for instance for performing the- matic fit judgment tasks, detecting anomalous events ( <ref type="bibr" target="#b11">Dasigi and Hovy, 2014)</ref>, or predicting event struc- ture that is not explicitly present in the text. The latter could be useful for inferring missing informa- tion in entailment tasks or improving identification of thematic roles outside the sentence containing the predicate. Potential applications also include predi- cate prediction based on arguments and roles, which has been noted to be relevant for simultaneous ma- chine translation for a verb-final to a verb-medial source language <ref type="bibr" target="#b18">(Grissom II et al., 2014</ref>). Within cognitive modelling, our model could help to more accurately estimate semantic surprisal for broad- coverage texts, when used in combination with an incremental role labeller (e.g., <ref type="bibr" target="#b24">Konstas and Keller, 2015)</ref>, or to provide surprisal estimates for content words as a control variable for psycholinguistic ex- perimental materials.</p><p>In this work, we focus on the predictability of verbs and nouns, and we suggest that the predictabil- ity of these words depends to a large extent on the relationship of these words to other nouns and verbs, especially those connected via the same event.</p><p>We choose a neural network (NN) model because we found that results from existing related models, e.g. Baroni and Lenci's Distributional Memory, de- pend heavily on how exactly the distributional space is defined, while having no principled way of opti- mizing the space. A crucial advantage of a neural network-based approach is thus that the model can be trained to optimize the distributional representa- tion for the task.</p><p>Our model is trained specifically to predict miss- ing semantic role-fillers based on the predicate and other available role-fillers of that predicate. The model can also predict the predicate based on the se- mantic roles and their fillers. In our model, there is no difference in how the semantic roles or the pred- icate are treated. Thus, when we refer here to roles, we usually mean both semantic roles and the predi- cate, unless otherwise explicitly stated.</p><p>Our model is compositional in that it has access to several role-fillers (including the verb) at the same time, and can thus represent interdependencies be- tween participants of an event and predict from a combined representation. Consider, for example, the predicate serve, whose likely patients include e.g., drinks. If we had the agent robber, we would like to be able to predict a patient like sentence, in the sense of "the robber will serve his sentence. . . " This task is related to modelling thematic fit. In this pa- per, we evaluate our model on a variety of thematic fit rating datasets as well as on a sentence similarity dataset that tests for successful compositionality in our model's representations. This paper makes the following contributions:</p><p>• We compare two novel NN models for gener- ating a probability distribution over selectional preferences given one or more roles and fillers.</p><p>• We show that our technique outperforms state of the art thematic fit models on many datasets.</p><p>• We show that the embeddings thus obtained are effective in measuring sentence similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Neural networks</head><p>Neural networks have proven themselves to be very well suited for language modeling. By learning distributed representations of words ( <ref type="bibr" target="#b5">Bengio et al., 2003)</ref>, they are able to generalize to new contexts that were not observed word-by-word in the training corpus. They can also use a relatively large number of context words in order to make predictions about the upcoming word. In fact, the recurrent neural net- work (RNN) LM ( <ref type="bibr" target="#b29">Mikolov et al., 2010</ref>) does not ex- plicitly fix the context size at all but is potentially able to compress the relevant information about the entire context in its recurrent layer. These are the properties that we would like to see in our role-filler prediction model as well. Neural networks have also been used for selec- tional preference acquisition, as in Van de Cruys <ref type="bibr">(2014)</ref>. His selectional preference model differs from our model in several aspects. First, unlike our model it is limited to a fixed number of inputs. An- other difference is that his model uses separate em- beddings for all input words, while ours enables par- tial parameter sharing. Finally and crucially for role- filler prediction, selectional preference models score the inputs, while our model gives a probability dis- tribution over all words for the queried target role.</p><p>We discuss the components necessary for our model in more detail in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data source</head><p>Our source of training data is the ukWaC corpus, which is part of the WaCky project, as well as the British National Corpus. The corpus consists of web pages crawled from the .uk web domain, contain- ing approximately 138 million sentences.</p><p>These sentences were run through a semantic role labeller and head words were extracted as described in <ref type="bibr" target="#b35">Sayeed et al. (2015)</ref>. The semantic role labeller used, SENNA <ref type="bibr" target="#b10">(Collobert and Weston, 2007)</ref>, gener- ates PropBank-style role labels. While PropBank ar- gument positions (ARG0, ARG1, etc.) are primarily designed to be verb-specific, rather than directly rep- resenting "classical" thematic roles (agent, patient, etc.), in the majority of cases, ARG0 lines up with agent roles and ARG1 lines up with patient roles. PropBank-style roles have been used in other recent efforts in thematic fit modelling (e.g., <ref type="bibr" target="#b1">Baroni et al., 2014;</ref><ref type="bibr" target="#b41">Vandekerckhove et al., 2009)</ref>, For processing purposes, the corpus was divided into 3500 segments. Fourteen segments (approx 500 thousand sentences) each were used for develop- ment and testing, and the rest were used for training.</p><p>In order to construct our incremental model and compare it to n-gram language models, we needed a precise mapping between the lemmatized argument words and their positions in the original sentence. This required aligning the SENNA tokenization and the original ukWaC tokenization used for Malt- Parser. Because of the heterogeneous nature of web data, this alignment was not always achievable-we skipped a small number of sentences in this case. In the development and testing portions of the data set, we filtered sentences containing predicates where there were multiple role-assignees with the same role for the same predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model design and implementation</head><p>Our model is a neural network with a single non- linear hidden layer and a Softmax output layer. All inputs are one-hot encoded-i.e., represented as a binary vector with size equal to the number of pos- sible input values, where all entries are zero except the entry at the index corresponding to the current input value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-part view of the model</head><p>The parameters of a neural network classifier with a single hidden layer and one-hot encoded inputs can be viewed as serving two distinct purposes: moving from inputs towards outputs, the first weight matrix that we encounter is responsible for learning dis- tributed representations (or embeddings) of the in- puts; the second weight matrix represents the param- eters of a maximum entropy classifier that uses the learned embeddings as inputs.</p><p>Considering the task of role-filler prediction, we would want these two sets of parameters to have the following properties:</p><p>• The classifier layer should be different for each target role, because the suitable filler given the context can clearly be very different depending on the role (e.g., verb vs. agent).</p><p>• The embedding layer should also be different depending on the role of context word. Other- wise, the network would not have any informa- tion about the role of the context word. For ex- ample, the suitable verb filler for context word dog in an agent role is probably very different from what it would be, were it in a patient role (e.g. bark vs. feed).</p><p>We now briefly describe some incrementally im- proved intermediate approaches that we also consid- ered as they help to understand the steps that led to our final solution for achieving the desired proper- ties of the embedding and classifier layer.</p><p>A naive way to accomplish the aspired properties would be to have a separate model for each input role and target role pair. This approach has several drawbacks. For a start, there is no obvious way to model interactions of different input roles and fillers in order to make predictions based on multiple in- put role-word pairs simultaneously. Another prob- lem is that the parameters are trained only on a frac- tion of available training data-e.g., verb embed- ding weights are trained independently for each tar- get role classifier. Finally, given that we have chosen to distinguish between n different roles, it would re- quire us to train and tune hyper-parameters for n 2 models.</p><p>One of these problems (data under-utilization) can be alleviated by sharing role-specific embedding and classifier weights across different models. For ex- ample, the verb embedding matrix would be shared across all models that predict different role fillers based on input verbs. Other problems remain, and training the large number of models becomes even more difficult because of parameter synchronization, but this is a step towards the next improvement.</p><p>Shared role-specific embedding and classifier weights enable us to combine all input-target role pair models into a single model. This can be done by stacking role-specific embedding matrices to form a 3-way embedding tensor and building a classifier pa- rameter tensor analogously. Having a single model saves us from tuning multiple models and makes modelling interactions between inputs possible. Despite these advantages, having two tensors in our model has a drawback of rapidly growing the number of parameters as vocabulary size, number of roles, and hidden layer size increase. This may lead to over-fitting and increases training time.</p><p>A more subtle weakness is the fact that this kind of model lacks parameter sharing across role- specific embedding weight matrices. It is clear that some characteristics of words (e.g., semantics) usu-ally remain the same across different roles. Thus it is practical to share some information across role- specific weights so that the embeddings can benefit from more data and learn better semantic represen- tations while leaving room for role-specific traits.</p><p>For these reasons we replace the tensors with their factored form in our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Factored parameter tensors</head><p>Factoring classifier and embedding tensors helps to alleviate both the efficiency and parameter sharing problems brought out in Section 3.1.</p><p>Given vocabulary size |V |, number of roles |R| and hidden layer size H, each tensor T would re- quire |V | × |R| × H parameters. The number of parameters can be reduced by expressing the tensor as a sum of F rank-one tensors <ref type="bibr" target="#b20">(Hitchcock, 1927)</ref>. This technique enables us to replace the tensor T with three factor matrices A, B and C. Each tensor element T [i, j, k] can then be written as:</p><formula xml:id="formula_0">T [i, j, k] = F f =1 A[i, f ]B[j, f ]C[f, k]<label>(1)</label></formula><p>Assuming lateral slices of T represent role-specific weight matrices (index j denotes roles), we write each role specific weight matrix W as:</p><formula xml:id="formula_1">W = A diag(rB)C (2)</formula><p>where r is a one-hot encoded role vector and diag is a function that returns a square matrix with the ar- gument vector on the main diagonal and zeros else- where. For example, with a vocabulary of 50000 words, 7 roles and number of factors and hid- den units equal to 512, the factorization reduces the number of parameters from 179M to 26M and greatly improves training speed. Factorization also enables parameter sharing, since factor matrices A and C are shared across all roles. Factored tensors have been used in different neu- ral network models before. Starting with restricted Boltzmann machines, <ref type="bibr" target="#b28">Memisevic and Hinton (2010)</ref> used a factored 3-way interaction tensor in their im- age transformation model. Sutskever et al. (2011) created a character level RNN LM that was effi- ciently able to use input character specific recurrent weights by using a factored tensor. <ref type="bibr" target="#b0">Alumäe (2013)</ref> used a factored tensor in a multi-domain LM to be able to use a domain-specific hidden layer weight matrix that would take into account the differences while exploiting similarities between domains. A multi-modal LM by <ref type="bibr" target="#b21">Kiros et al. (2014)</ref> uses a fac- tored tensor to change the effective output layer weights based on image features.</p><p>It has been noticed before, that training models with factored tensors as parameters using gradient descent is difficult <ref type="bibr" target="#b38">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b21">Kiros et al., 2014)</ref>. As explained by <ref type="bibr" target="#b38">Sutskever et al. (2011)</ref>, this is caused by the fact that each tensor element is represented as a product of three param- eters, which may cause disproportionate updates if these three factors have magnitudes that are too dif- ferent. Another problem is that if the factor matrix B happens to have too small or too large values, then this might also cause instabilities in the lower layers as the back-propagated gradients are scaled by role- specific row of B in our model. This situation is magnified in our models, since we have not one, but two factored layers.</p><p>To solve this problem, Sutskever et al. <ref type="formula" target="#formula_0">(2011)</ref> sug- gest using 2nd order methods instead of gradient de- scent. <ref type="bibr" target="#b0">Alumäe (2013)</ref> has alleviated the problem of shrinking back-propagated gradients by adding a bias (initialized with ones) to the domain-specific factor vector. We found that using AdaGrad <ref type="bibr" target="#b12">(Duchi et al., 2011</ref>) to update the parameters is very effec- tive. The method provides parameter-specific learn- ing rates that depend on the historic magnitudes of the gradients of these parameters. This seems to neutralize the effect of vanishing or exploding gra- dients by reducing the step size for parameters that tend to have large gradients and allow a bigger learn- ing rate for parameters with smaller gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">General structure of the model</head><p>Our general approach, common to both role-filler models, is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. First, role-specific word embedding vector e is computed by implicitly taking a fiber (word indexed row of a role indexed slice) from the factored embedding tensor:</p><formula xml:id="formula_2">e = wA e diag(rB e )C e<label>(3)</label></formula><formula xml:id="formula_3">h = PReLU(e + b h )<label>(4)</label></formula><p>where w and r are one-hot encoded word and role vectors respectively, b h is hidden layer bias, and A e , B e and C e represent the factor matrices that the em- bedding tensor is factored into. Next, we apply a parametric rectifier (PReLU; He et al., 2015) non- linearity to the role-specific word embedding to ob- tain the hidden activation vector h. The hidden layer activation vector h is fed to the Softmax output layer through a target role specific classifier weight matrix (a target role-indexed slice of the classifier parameter tensor):</p><formula xml:id="formula_4">c = hA c diag(tB c )C c<label>(5)</label></formula><formula xml:id="formula_5">y = Softmax(c + b y )<label>(6)</label></formula><p>where t is a one-hot encoded target role vector, b y is output layer bias, and y is the output of the model representing the probability distribution over the output vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modeling input interactions</head><p>The general approach described in Section 3.3 also allows us to model interactions between different in- put role-word pairs. If we know the order in which the inputs were introduced, then we can add a recur- rent connection to the hidden layer to implement an incremental role filler predictor. When word order is unknown, then input role-word pair representations can be added together to compose the representa- tion of the entire predicate context 1 . We chose addi- 1 In applications like natural language generation, for exam- ple, where role-fillers need to be predicted, it is not necessarily always the case that the order will be known in advance or that the thematic fit model will be used to generate the full sentence in correct word order. tion over concatenation (often preferred in language models) because the non-incremental model does not need to preserve information about word order, and addition also enables using a variable number of inputs.</p><p>The incremental model adds information about the previous hidden state h t−1 to the current input word role-specific embedding e t through recurrent weights W r . So, Equation 4 is replaced with:</p><formula xml:id="formula_6">h t = PReLU(e t + h t−1 W r + p t W p + b h ) (7)</formula><p>where p t is a binary predicate boundary indicator that informs the model about the start of a new pred- icate and equals 1 when the target word belongs to a new predicate and 0 otherwise. The predi- cate boundary input p t is connected to the network through parameter vector W p . The hidden state h 0 is initialized to zeros.</p><p>The non-incremental model adds role-specific embedding vectors of all input words together to form the representation of the entire predicate con- text and replaces Equation 4 with:</p><formula xml:id="formula_7">h = PReLU( N i=1 e i + b h )<label>(8)</label></formula><p>where N is the number of input role-word pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training details</head><p>First, we give details that are common to both the RNN and NN models. The models are trained with mini-batches of 128 samples. The hidden layer con- sists of 256 PReLU units; embedding and classifier tensor factorization layer sizes are 256 and 512 re- spectively. The input and output vocabularies are the same, consisting of 50,000 most frequent lemma- tized words in the training corpus. The role vocab- ulary consists of 5 argument roles (ARG0, ARG1, ARGM-LOC, ARGM-TMP and ARGM-MNR), the verb is treated as the sixth role, and all the other roles are mapped to a shared OTHER label. Parameters are updated using AdaGrad (Duchi et al., 2011) with a learning rate of 0.1. All models are implemented using Theano ( <ref type="bibr" target="#b4">Bastien et al., 2012;</ref><ref type="bibr" target="#b6">Bergstra et al., 2010</ref>) and trained on GPUs for 8 days. RNN model gradients are computed using back- propagation through time <ref type="bibr" target="#b33">(Rumelhart et al., 1986)</ref> Model Name Dev Test 3-gram LM 450.1 ± 2.6 438.9 ± 2.6 3-gram CWM 859.6 ± 4.6 834.9 ± 4.5 RNN CWM 485.8 ± 2.7 473.2 ± 2.6 RNN RF 244.6 ± 1.4 237.8 ± 1.4 NN RF 248.2 ± 1.4 241.9 ± 1.4 over 3 time steps. The NN model is trained on mini- batches of 128 samples that are randomly drawn with replacement from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model comparison</head><p>Perplexity allows us to compare all our models in similar terms, and evaluate the extent to which ac- cess to thematic roles helps the model to predict missing role fillers. For comparability, the perplexi- ties of all models are computed only on content word probabilities (i.e., predicates and their arguments). We also report the 95% confidence interval for per- plexity, which is computed according to <ref type="bibr" target="#b23">Klakow and Peters (2002)</ref>. All models are trained on exactly the same sentences of lemmatized words. Probabil- ity mass is distributed across the vocabulary of the 50,000 most frequent content words in the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Models</head><p>First, we compare our model to a conventional 3- gram language model 3-gram LM, conditioning on the previous context containing the immediately pre- ceding context of content and function words. All n-grams are discounted with Kneser-Ney smooth- ing, and n-gram probability estimates are interpo- lated with lower order estimates. Sentence onset in all models is padded with a special sentence onset tag. The vocabulary of context words for this model consists of all words from the training corpus.</p><p>As a second model, we train a 3-gram content word model 3-gram CWM, which is an N -gram LM that is trained only on content words.</p><p>Next, we have RNN CWM-an RNN LM ( <ref type="bibr" target="#b29">Mikolov et al., 2010</ref>) trained on content words only. The context size of this model is not explicitly defined and the model can potentially utilize more context words than 3-gram CWM (even from outside the sentence boundary).</p><p>Our incremental role-filler RNN RF is similar to RNN CWM, except for using role-specific embed- ding and classifier weights (slices of factored ten- sor). It thus has additional information about the content word roles 2 .</p><p>Finally, the non-incremental role-filler NN RF loses the information about word order and the abil- ity to use information outside predicate boundaries and trades it for the ability to see the future (i.e., the context includes both the preceding and the follow- ing content words and their roles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Results</head><p>The results of content word perplexity evalua- tion are summarized in <ref type="table" target="#tab_0">Table 1</ref>. The thematic- role informed models outperform all other mod- els by a very large margin, cutting perplexity al- most in half. The incremental model achieves a slightly lower perplexity than the non-incremental one (237.8 vs. 241.9), hinting that the content word order and out-of-predicate role-word pairs can be even more informative than a preview of upcoming role-word pairs.</p><p>The difference between normal LM and the CWM can be explained by the loss of information from function words, combined with additional sparsity in the model because content word sequences are much sparser than sequences of content and func- tion words.</p><p>This also explains why using a neural network- based RNN CWM model improves the performance so much (perplexity drops from 834.9 to 473.2), as neural network based language models are well known for their ability to generalize well to unseen contexts by learning distributed representations of words ( <ref type="bibr" target="#b5">Bengio et al., 2003</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation on thematic fit ratings</head><p>In order to see whether our model accurately repre- sents events and their typical thematic role fillers, we evaluate our model on a range of existing datasets containing human thematic fit ratings. This evalua- tion also allows us to compare our model to existing models that have been used on this task.  <ref type="table" target="#tab_0"># ratings  Roles  NN RF  BL2010 GSD2015 BDK2014  Pado (agent, patient)</ref> 414 ARG0, ARG1, ARG2 0.52 (8)  <ref type="bibr" target="#b3">Baroni and Lenci, 2010</ref>) model, further developed and evaluated in <ref type="bibr">Greenberg et al. (2015a,b)</ref> and the neural network predict model described in <ref type="bibr" target="#b1">Baroni et al. (2014)</ref>. NN RF is the non-incremental model presented in this article. Our model maps ARG2 in Pado to OTHER role. Significances were calculated using paired two-tailed significance tests for correlations <ref type="bibr" target="#b37">(Steiger, 1980)</ref>. NN RF was significantly better than both of the other models on the Greenberg and Ferretti location datasets and significantly better than BL2010 but not GSD2015 on McRae and Pado+McRae+Ferretti; differences were not statistically significant for Pado and Ferretti instruments.</p><formula xml:id="formula_8">0.53 (0) 0.53 (0) 0.41 McRae (agent, patient) 1444 ARG0, ARG1 0.38 (20) 0.32 (70) 0.36 (70) 0.28 Ferretti (location) 274 ARGM-LOC 0.44 (3) 0.23 (3) 0.29 (3) - Ferretti (instrument) 248 ARGM-MNR 0.45 (6) 0.36 (17) 0.42 (17) - Greenberg (patient) 720 ARG1 0.61 (8) 0.46 (18) 0.48 (18) - Pado+McRae+Ferretti 2380 0.41 (37) 0.35 (90) 0.38 (90) -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Related work</head><p>State-of-the-art computational models of thematic fit quantify the similarity between a role filler of a verb and the proto-typical filler for that role for the verb based on distributional vector space models. For example, the thematic fit of grass as a patient for the verb eat would be determined by the cosine of a distributional vector representation of grass and a prototypical patient of eat. The proto-typical pa- tient is in turn obtained from averaging representa- tions of words that typically occur as a patient of eat (e.g., <ref type="bibr" target="#b13">Erk, 2007;</ref><ref type="bibr" target="#b3">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b34">Sayeed and Demberg, 2014;</ref><ref type="bibr" target="#b16">Greenberg et al., 2015b</ref>). For more than one role, information from both the agent and the predicate can be used to jointly to pre- dict a patient (e.g., Lenci, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data</head><p>Previous studies obtained thematic fit ratings from humans by asking experimental participants to rate how common, plausible, typical, or appropriate some test role-fillers are for given verbs on a scale from 1 (least plausible) to 7 (most plausible) ( <ref type="bibr" target="#b27">McRae et al., 1998;</ref><ref type="bibr" target="#b14">Ferretti et al., 2001;</ref><ref type="bibr" target="#b8">Binder et al., 2001;</ref><ref type="bibr" target="#b31">Padó, 2007;</ref><ref type="bibr" target="#b32">Padó et al., 2009;</ref><ref type="bibr" target="#b41">Vandekerckhove et al., 2009;</ref><ref type="bibr" target="#b15">Greenberg et al., 2015a</ref>). The datasets include agent, patient, location and instru- ment roles. For example, in the Padó et al. <ref type="formula">(2009)</ref> dataset, the noun sound has a very low rating of 1.1 as the subject of hear and a very high rating of 6.8 as the object of hear. Each of the verb-role-noun triples was rated by several humans, and our evalua- tions are done against the average human score. The datasets differ from one another in size (as shown in <ref type="table" target="#tab_1">Table 2</ref>), choice of verb-noun pairs, and in how exactly the question was asked of human raters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Methods</head><p>A major difference between what the state-of-the- art models do and what our model does is that our model distributes a probability mass of one across the vocabulary, while the thematic fit models have no such overall constraint; they will assign a high number to all words that are similar to the proto- typical vector, without having to distribute probabil- ity mass. Specifically, this implies that two synony- mous fillers, one of which is a frequent word like fire, and the other of which is an infrequent word, e.g., blaze, will get similar ratings by the distribu- tional similarity models, but quite different ratings by the neural network model, as the more frequent word will have higher probability. <ref type="bibr" target="#b15">Greenberg et al. (2015a)</ref> showed that human ratings are insensitive to noun frequency. Hence, we report results that ad- just for frequency effects by setting the output layer bias of the neural network model to zero. Since the output unit biases of the neural network model are independent from the inputs, they correlate strongly (r s = 0.74, p = 0.0) with training corpus word fre- quencies after being trained. Therefore, setting the learned output layer bias vector to a zero-vector is a simple way to reduce the effect of word frequencies on the model's output probability distribution.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We can see that the neural network model outper- forms the baselines on all the datasets except the Pado dataset. An error analysis on the role filler probabilities generated by the neural net points to the effect of level of constraint of the verb on the es- timates. For a relatively non-constraining verb, the neural net model will have to distribute the probabil- ity mass across many different suitable fillers, while the semantic similarity models do not suffer from this. This implies that filler fit is not directly compa- rable across verbs in the NN model (only filler pre- dictability is comparable).</p><p>Per role results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Surpris- ingly, the model output has the highest correlation with the averaged human judgements for the target role ARG2, despite the fact that ARG2 is mapped to OTHER along with several other roles. The model struggles the most when it comes to predicting fillers for ARG0. There is no noticeable correlation be- tween the role-specific performance and the role oc- currence frequency in the samples of our training set. This implies that parameter sharing between roles does indeed help when it comes to balancing the performance between rare and ubiquitous roles as discussed in section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Compositionality</head><p>The above thematic role fit data sets only assess the fit between two words. Our model can however also model the interaction between different roles; see <ref type="figure" target="#fig_2">Figure 2</ref> for an example of model predictions. We are only aware of one small dataset that can be used to systematically test the effectiveness of the compositionality for this task. The <ref type="bibr" target="#b7">Bicknell et al. (2010)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>dataset contains triples like journalist check</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>NN RF Lenci 2011 Accuracy 1 0.687 0.671 Accuracy 2 0.828 0.844 <ref type="table">Table 4</ref>: Accuracies on the Bicknell evaluation task.</p><p>spelling vs. mechanic check spelling and journalist check tires vs. mechanic check tires together with human congruity judgments. The goal in this task is for the model to repro- duce the human judgments on the 64 sentence pairs. Lenci (2011), which we compare against in <ref type="table">Table  4</ref>, proposed a first compositional model based on TypeDM to evaluate on this task.</p><p>We use two accuracy scores for the evaluation, which we call "Accuracy 1" and "Accuracy 2". "Ac- curacy 1" counts a hit iff the model assigns the composed subject-verb combination a higher score when we test a human-rated better-fitting object in contrast with when we test a worse-fitting one; in other words, a hit is achieved when journalist check spelling should be better than journalist check tires, if we give the model journalist check as the predi- cate to test against different objects. (The result from Lenci for this task was transmitted by private com- munication.) "Accuracy 2" counts a hit iff, given an object, the composed subject-verb combination gives a higher score when the subject is better fitting. That is, a hit is achieved when journalist check spelling has a higher score than mechanic check spelling, setting the query to the model as journalist check and me- chanic check and finding a score for spelling in that context. This accuracy metric is proposed and eval- uated in <ref type="bibr" target="#b25">Lenci (2011)</ref>.</p><p>Evaluation shows that our model performs simi- larly to that of Lenci, although only limited conclu- sions can be drawn due to the small data set size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of event representations: sentence similarity</head><p>To show that our model learns to represent input words and their roles in a useful way that reflects the meaning and interactions between inputs, we evalu- ate our non-incremental model on a sentence simi- larity task from <ref type="bibr" target="#b17">Grefenstette and Sadrzadeh (2015)</ref>. We assign similarity scores to sentence pairs by computing representations for each sentence by tak- ing the hidden layer state (Equation 8) of the non- incremental model given the words in the sentence and their corresponding roles. Sentence similarity is then rated with the cosine similarity between the representations of the two sentences.</p><p>Spearman's rank correlation between the cosine similarities produced by our model and human rat- ings are shown in <ref type="table">Table 5</ref>. Our model achieves much higher correlation with human ratings than the best result reported by <ref type="bibr" target="#b17">Grefenstette and Sadrzadeh (2015)</ref>, showing our model's ability to compose meaningful representations of multiple input words and their roles.</p><p>We also compare our model with another NN word representation model baseline that does not embed role information; by this comparison, we can determine the size of the improvement brought by our role-specific embeddings. The baseline sen- tence representations are constructed by element- wise addition of pre-trained word2vec (Mikolov et al., 2013) word embeddings <ref type="bibr">3</ref> . Scores are again computed by using cosine similarity. The large gap between our model's and word2vec baseline's per- formance illustrates the importance of embedding role information in word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we proposed two neural network archi- tectures for learning proto-typical event representa- 3 https://code.google.com/p/word2vec/ # ratings NN RF Kronecker W2V Humans 199 0.34 0.26 0.13 0.62 <ref type="table">Table 5</ref>: Sentence similarity evaluation scores on GS2013 dataset ( <ref type="bibr" target="#b17">Grefenstette and Sadrzadeh, 2015)</ref>, consisting of Spear- man's ρ correlations between human judgements and model output. Kronecker is the best performing model from <ref type="bibr" target="#b17">Grefenstette and Sadrzadeh (2015)</ref>. NN RF is the non-incremental model presented in this article, and W2V is the word2vec base- line. Human performance (inter-annotator agreement) shows the upper bound.</p><p>tions. These models were trained to generate prob- ability distributions over role fillers for a given se- mantic role. In our perplexity evaluation, we demon- strated that giving the model access to thematic role information substantially improved prediction per- formance. We also compared the performance of our model to the performance of current state-of-the- art models in predicting human thematic fit ratings and showed that our model outperforms the existing models by a large margin. Finally, we also showed that the event representations from the hidden layer of our model are highly effective in a sentence sim- ilarity task. In future work, we intend to test the potential contribution of this model when applied to larger tasks such as entailment and inference tasks as well as semantic surprisal-based prediction tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General structure of role-filler models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of model predictions for the verb serve with different agents and target roles patient and location.</figDesc><graphic url="image-1.png" coords="9,72.00,57.83,226.79,179.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Perplexities on dev/test dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Thematic fit evaluation scores, consisting of Spearman's ρ correlations between average human judgements and model 

output, with numbers of missing values (due to missing vocabulary entries) in brackets. The baseline scores come from the TypeDM 

(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Per role thematic-fit evaluation scores in terms of 

Spearmans ρ correlations between average human judgements 

and model output. 

</table></figure>

			<note place="foot" n="2"> A reviewer kindly points out, as a matter of historical interest, that the high-level architecture of the RNN RF model bears some resemblance to the parallel distributed processing model in McClelland et al. (1989) and St. John and McClelland (1990).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This research was funded by the German Research Foundation (DFG) as part of SFB 1102: "Informa-tion Density and Linguistic Encoding" as well as the Cluster of Excellence "Multimodal Computing and Interaction" (MMCI). Also, the authors wish to thank the anonymous reviewers whose valuable ideas contributed to this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-domain neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alumäe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2182" to="2186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kruszewski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wardefarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effects of event knowledge in processing verbal arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="489" to="505" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effects of thematic fit and discourse context on syntactic ambiguity resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="324" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting syntactic structure for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="225" to="231" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast semantic extraction using a novel neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="560" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling newswire events using neural networks for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1414" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A simple, similarity-based model for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integrating verbs, situation schemas, and thematic role concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Ferretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hatherell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="516" to="547" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Verb polysemy and frequency effects in thematic fit modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the 6th Workshop on Cognitive Modeling and Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving unsupervised vector-space thematic fit evaluation via role-filler prototype clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL HLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Concrete models and empirical evaluations for the categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t until the final verb wait: Reinforcement learning for simultaneous machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Grissom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1342" to="1352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<title level="m">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The expression of a tensor or a polyadic as a sum of products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Physics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="164" to="189" />
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Testing the correlation of word error rate and perplexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="28" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic role labeling improves incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1191" to="1201" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Composing and updating verb argument expectations: A distributional semantic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd Workshop on Cognitive Modeling and Computational Linguistics, CMCL &apos;11</title>
		<meeting>the 2Nd Workshop on Cognitive Modeling and Computational Linguistics, CMCL &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="58" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sentence comprehension: A parallel distributed processing approach. Language and cognitive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Taraban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="287" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the influence of thematic fit (and other constraints) in on-line sentence comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="312" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to represent spatial transformations with factored higher-order Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1473" to="1492" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock`cernock`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09-26" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The integration of syntax and semantic plausibility in a wide-coverage model of human sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Padó</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Saarland University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A probabilistic model of semantic plausibility in sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Crocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="794" to="838" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NATURE</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining unsupervised syntactic and semantic models of thematic fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first Italian Conference on Computational Linguistics</title>
		<meeting>the first Italian Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>CLiCit</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An exploration of semantic features in an unsupervised thematic fit evaluation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sayeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shkadzko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emerging Topics at the First Italian Conference on Computational Linguistics</title>
		<imprint>
			<publisher>Accademia University Press</publisher>
			<date type="published" when="2015-12-01" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="25" to="40" />
		</imprint>
	</monogr>
	<note>IJCoL</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning and applying contextual constraints in sentence comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="217" to="257" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Tests for comparing elements of a correlation matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A scalable distributed syntactic, semantic, and lexical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="631" to="671" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A neural network approach to selectional preference acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A robust and extensible exemplar-based model of thematic fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vandekerckhove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-03-30" />
			<biblScope unit="page" from="826" to="834" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
