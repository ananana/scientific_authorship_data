<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WIKIQA: A Challenge Dataset for Open-Domain Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yiyang@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<postCode>30308</postCode>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WIKIQA: A Challenge Dataset for Open-Domain Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Answer sentence selection is a crucial subtask of the open-domain question answering (QA) prob- lem, with the goal of extracting answers from a set of pre-selected sentences <ref type="bibr" target="#b0">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b8">Yao et al., 2013;</ref><ref type="bibr" target="#b5">Severyn and Moschitti, 2013)</ref>. In order to conduct research on this im- portant problem, <ref type="bibr" target="#b7">Wang et al. (2007)</ref> created a dataset, which we refer to by QASENT, based on the TREC-QA data. The QASENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QASENT has since * Work conducted while interning at Microsoft Research. become the benchmark dataset for the answer se- lection problem, its creation process actually in- troduces a strong bias in the types of answers that are included. The following example illustrates an answer that does not share any content words with the question and would not be selected:</p><p>Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule.</p><p>One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QASENT dataset and might inflate the performance of existing systems in more natu- ral settings. For instance, <ref type="bibr" target="#b9">Yih et al. (2013)</ref> find that simple word matching methods outperform many sophisticated approaches on the dataset. We ex- plore this possibility in Section 3.</p><p>A second, more subtle challenge for question answering is that it normally assumes that there is at least one correct answer for each question in the candidate sentences. During the data construction procedures, all the questions without correct an- swers are manually discarded. <ref type="bibr">1</ref> We address a new challenge of answer triggering, an important com- ponent in QA systems, where the goal is to detect whether there exist correct answers in the set of candidate sentences for the question, and return a correct answer if there exists such one.</p><p>We present WIKIQA, a dataset for open- domain question answering. <ref type="bibr">2</ref> The dataset con- tains 3,047 questions originally sampled from Bing query logs. Based on the user clicks, each question is associated with a Wikipedia page pre- sumed to be the topic of the question. In order to eliminate answer sentence biases caused by key- word matching, we consider all the sentences in the summary paragraph of the page as the candi- date answer sentences, with labels on whether the sentence is a correct answer to the question pro- vided by crowdsourcing workers. Among these questions, about one-third of them contain correct answers in the answer sentence set.</p><p>We implement several strong baselines to study model behaviors in the two datasets, including two previous state-of-the-art systems <ref type="bibr" target="#b9">(Yih et al., 2013;</ref><ref type="bibr" target="#b10">Yu et al., 2014</ref>) on the QASENT dataset as well as simple lexical matching methods. The results show that lexical semantic methods yield better performance than sentence semantic mod- els on QASENT, while sentence semantic ap- proaches (e.g., convolutional neural networks) outperform lexical semantic models on WIKIQA. We propose to evaluate answer triggering using question-level precision, recall and F 1 scores. The best F 1 scores are slightly above 30%, which sug- gests a large room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WIKIQA Dataset</head><p>In this section, we describe the process of creat- ing our WIKIQA dataset in detail, as well as some comparisons to the QASENT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question &amp; Sentence Selection</head><p>In order to reflect the true information need of gen- eral users, we used Bing query logs as the ques- tion source. Taking the logs from the period of May 1st, 2010 to July 31st, 2011, we first se- lected question-like queries using simple heuris- tics, such as queries starting with a WH-word (e.g., "what" or "how") and queries ending with a question mark. In addition, we filtered out some entity queries that satisfy the rules, such as the TV show "how I met your mother." In the end, approx- imately 2% of the queries were selected. To fo- cus on factoid questions and to improve the ques- tion quality, we then selected only the queries is- sued by at least 5 unique users and have clicks to Wikipedia. Among them, we sampled 3,050 ques- tions based on query frequencies.</p><p>Because the summary section of a Wikipedia page provides the basic and usually most impor- tant information about the topic, we used sen- tences in this section as the candidate answers. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example question, as well as the summary section of a linked Wikipedia page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question: Who wrote second Corinthians?</head><p>Second Epistle to the Corinthians The Second Epistle to the Corinthi- ans, often referred to as Second Corinthians (and written as 2 Corinthi- ans), is the eighth book of the New Testament of the Bible. Paul the Apostle and "Timothy our brother" wrote this epistle to "the church of God which is at Corinth, with all the saints which are in all Achaia". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence Annotation</head><p>We employed crowdsourcing workers through a platform, which is similar to Amazon MTurk, to label whether the candidate answer sentences of a question are correct. We designed a cascaded Web UI that consists of two stages. The first stage shows a testing question, along with the ti- tle and the summary paragraph of the associated Wikipedia page, asking the worker "Does the short paragraph answer the question?" If the worker chooses "No", then equivalently all the sentences in this paragraph are marked incorrect and the UI moves to the next question. Otherwise, the sys- tem enters the second stage and puts a checkbox along each sentence. The worker is then asked to check the sentences that can answer the question in isolation, assuming coreference is resolved. To ensure the label quality, each question was labeled by three workers. Sentences with inconsistent la- bels would be verified by a different set of crowd- sourcing workers. The final decision was based on the majority vote of all the workers. In the end, we included 3,047 questions and 29,258 sentences in the dataset, where 1,473 sentences were labeled as answer sentences to their corresponding questions.</p><p>Although not used in the experiments, each of these answer sentence is associated with the an- swer phrase, which is defined as the shortest sub- string of the sentence that answers the question. For instance, the second sentence in the summary paragraph shown in <ref type="figure" target="#fig_0">Figure 1</ref> is an answer sen- tence. Its substring "Paul the Apostle and Tim- othy our brother" can be treated as the answer phrase. The annotations of the answer phrases were given by the authors of this paper. Because the answer phrase boundary can be highly ambigu- ous, each sentence is associated with at most two answer phrases that are both acceptable, given by two different labelers. We hope this addition to the WIKIQA data can be beneficial to future re- searchers for building or evaluating an end-to-end question answering system.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">WIKIQA vs. QASENT</head><p>Our WIKIQA dataset differs from the existing QASENT dataset in both question and candi- date answer sentence distributions. Questions in QASENT were originally used in TREC 8-13 QA tracks and were a mixture of questions from query logs (e.g., Excite and Encarta) and from human editors. The questions might be outdated and do not reflect the true information need of a QA sys- tem user. By contrast, questions in WIKIQA were sampled from real queries of Bing without edi- torial revision. On the sentence side, the can- didate sentences in QASENT were selected from documents returned by past participating teams in the TREC QA tracks, and sentences were only included if they shared content words from the questions. These procedures make the distribu- tion of the candidate sentence skewed and unnat- ural. In comparison, 20.3% of the answers in the WIKIQA dataset share no content words with questions. Candidate sentences in WIKIQA were chosen from relevant Wikipedia pages directly, which could be closer to the input of an answer sentence selection module of a QA system. To make it easy to compare results of dif- ferent QA systems when evaluated on the WIKIQA dataset, we randomly split the data to training (70%), development (10%) and testing (20%) sets.</p><p>Some statistics of the QASENT and WIKIQA datasets are presented in <ref type="table" target="#tab_1">Tables 1 and 2</ref>  magnitude more questions and three times more answer sentences compared to QASENT. Unlike QASENT, we did not filter questions with only incorrect answers, as they are still valuable for model training and more importantly, useful for evaluating the task of answer triggering, as de- scribed in Section 3. Specifically, we find nearly two-thirds of questions contain no correct answers in the candidate sentences. The distributions of question types in these two datasets are also different, as shown in <ref type="table" target="#tab_3">Table 3</ref>. <ref type="bibr">4</ref> WIKIQA contains more description or definition questions, which could be harder to answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Many systems have been proposed and tested on the QASENT dataset, including lexical semantic models ( <ref type="bibr" target="#b9">Yih et al., 2013</ref>) and sentence seman- tic models ( <ref type="bibr" target="#b10">Yu et al., 2014</ref>). We investigate the performance of several systems on WIKIQA and QASENT. As discussed in Section 2, WIKIQA of- fers us the opportunity to evaluate QA systems on answer triggering. We propose simple metrics and perform a feature study on the new task. Finally, we include some error analysis and discussion at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Systems</head><p>We consider two simple word matching methods: Word Count and Weighted Word Count. The first method counts the number of non-stopwords in the question that also occur in the answer sentence. The second method re-weights the counts by the IDF values of the question words.</p><p>We reimplement LCLR ( <ref type="bibr" target="#b9">Yih et al., 2013</ref>), an answer sentence selection approach that achieves very competitive results on QASENT. LCLR sentences that have human annotations.   makes use of rich lexical semantic features, including word/lemma matching, WordNet and vector-space lexical semantic models. We do not include features for Named Entity matching. <ref type="bibr">5</ref> We include two sentence semantic methods, Paragraph Vector 6 (PV; <ref type="bibr" target="#b1">Le and Mikolov, 2014)</ref> and Convolutional Neural Networks (CNN; <ref type="bibr" target="#b10">Yu et al., 2014</ref>). The model score of PV is the cosine similarity score between the question vector and the sentence vector. We follow <ref type="bibr" target="#b10">Yu et al. (2014)</ref> and employ a bigram CNN model with average pool- ing. We use the pre-trained word2vec embeddings provided by <ref type="bibr" target="#b3">Mikolov et al. (2013)</ref> as model input. <ref type="bibr">7</ref> For computational reasons, we truncate sentences up to 40 tokens for our CNN models.</p><p>Finally, we combine each of the two sentence semantic models with the two word matching fea- tures by training a logistic regression classifier, re- ferring as PV-Cnt and CNN-Cnt. CNN-Cnt has been shown to achieve state-of-the-art results on the QASENT dataset ( <ref type="bibr" target="#b10">Yu et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Answer Triggering</head><p>The task of answer sentence selection assumes that there exists at least one correct answer in the can- didate answer sentence set. Although the assump- tion simplifies the problem of question answering, it is unrealistic for practical QA systems. Modern QA systems rely on an independent component to pre-select candidate answer sentences, which uti- lizes various signals such as lexical matching and user behaviors. However, the candidate sentences  <ref type="table">Table 5</ref>: Evaluation of answer triggering on the WIKIQA dataset. Question-level precision, recall and F 1 scores are reported.</p><p>are not guaranteed to contain the correct answers, no matter what kinds of pre-selection components are employed. We propose the answer triggering task, a new challenge for the question answering problem, which requires QA systems to: (1) de- tect whether there is at least one correct answer in the set of candidate sentences for the question; (2) if yes, select one of the correct answer sentences from the candidate sentence set. Previous work adopts MAP and MRR to eval- uate the performance of a QA system on answer sentence selection. Both metrics evaluate the rela- tive ranks of correct answers in the candidate sen- tences of a question, and hence are not suitable for evaluating the task of answer triggering. We need metrics that consider both the presence of answers with respect to a question and the correctness of system predictions.</p><p>We employ precision, recall and F 1 scores for answer triggering, at the question level. In partic- ular, we compute these metrics by aggregating all the candidate sentences of a question. A question is treated as a positive case only if it contains one or more correct answer sentences in its candidate sentence pool. For the prediction of a question, we only consider the sentence in the candidate set that has the highest model score. If the score is above a predefined threshold and the sentence is labeled as a correct answer to the question, then it means that the prediction is correct and the question is answered correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>WIKIQA vs. QASENT The MAP and MRR results are presented in <ref type="table" target="#tab_5">Table 4</ref>. We only evalu- ate questions with answers in the WIKIQA dataset under these metrics. On the QASENT dataset, as found by prior work, the two word matching meth- ods are very strong baselines, in which they sig-nificantly outperform sentence semantic models. By incorporating rich lexical semantic informa- tion, LCLR further improves the results. CNN- Cnt gives results that match LCLR, and PV-Cnt performs worse than CNN-Cnt. <ref type="bibr">8</ref> The story on the WIKIQA dataset is differ- ent. First, methods purely rely on word match- ing are not sufficient to achieve good results. Sec- ond, CNN significantly outperforms simple word matching methods and performs slightly better than LCLR, which suggests that semantic under- standing beyond lexical semantics is important for obtaining good performance on WIKIQA. Finally, word matching features help to further boost CNN results by approximately 3 to 4 points in both MAP and MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of answer triggering on WIKIQA</head><p>We evaluate the best system CNN-Cnt on the task of answer triggering, and the results are shown in <ref type="table">Table 5</ref>. We tune the model scores for making pre- dictions with respect to F 1 scores on the dev set, due to the highly skewed class distribution in train- ing data. The absolute F 1 scores are relative low, which suggests a large room for improvement.</p><p>We further study three additional features: the length of question (QLen), the length of sentence (SLen), and the class of the ques- tion (QClass). The motivation for adding these features is to capture the hardness of the question and comprehensiveness of the sentence. Note that the two question features have no effects on MAP and MRR. As shown in <ref type="table">Table 5</ref>, the question-level F 1 score is substantially improved by adding a simple QLen feature. This suggests that designing features to capture question information is very important for this task, which has been ignored in the past. SLen features also give a small improve- ment in the performance, and QClass feature has slightly negative influence on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Error Analysis &amp; Discussion</head><p>The experimental results show that for the same model, the performance on the WIKIQA dataset is inferior to that on the QASENT dataset, which suggests that WIKIQA is a more challenging dataset. Examining the output of CNN-Cnt, the best performing model, on the WIKIQA dev set seems to suggest that deeper semantic understand- ing and answer inference are often required. Be-low are two examples selected that CNN-Cnt does not correctly rank as the top answers: Q1: What was the GE building in Rockefeller Plaza called before? A1: [GE Building] Known as the RCA Building until 1988, it is most famous for housing the head- quarters of the television network NBC. Q2: How long was I Love Lucy on the air? A2: [I Love Lucy] The black-and-white series originally ran from <ref type="bibr">October 15, 1951</ref><ref type="bibr">, to May 6, 1957</ref>, on the Columbia Broadcasting System (CBS).</p><p>Answering the first question may require a better semantic representation that captures the relation- ship between "called before" and "known ... un- til". As for the second question, knowing that on a TV channel (e.g., CBS) implies "on the air" and a time span between two dates is legitimate to a "how long" question is clearly beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present WIKIQA, a new dataset for open- domain question answering. The dataset is con- structed in a natural and realistic manner, on which we observed different behaviors of various meth- ods compared with prior work. We hope that WIKIQA enables research in the important prob- lem of answer triggering and enables further re- search in answer sentence selection in more real- istic settings. We also hope that our empirical re- sults will provide useful baselines in these efforts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example question and the summary paragraph of a Wikipedia page.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Statistics of the WIKIQA dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. 3 WIKIQA contains an order of 3 We follow experimental settings of Yih et al. (2013) on the QASENT dataset. Although the training set in the original data contains more questions, only 94 of them are paired with</figDesc><table>Class 
QASENT WIKIQA 

Location 
37 (16%) 373 (12%) 
Human 
65 (29%) 494 (16%) 
Numeric 
70 (31%) 658 (22%) 
Abbreviation 2 (1%) 
16 (1%) 
Entity 
37 (16%) 419 (14%) 
Description 
16 (7%) 
1087 (36%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Question classes of the QASENT and 
WIKIQA datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Baseline results on both QASENT and 
WIKIQA datasets. Questions without correct an-
swers in the candidate sentences are removed in 
the WIKIQA dataset. The best results are in bold. 

</table></figure>

			<note place="foot" n="4"> The classifier is trained using a logistic regression model on the UIUC Question Classification Datasets (http: //cogcomp.cs.illinois.edu/Data/QA/QC). The performance is comparable to (Li and Roth, 2002).</note>

			<note place="foot" n="5"> The improvement gains from the features are marginal on the QASENT dataset. 6 We choose the Distributed Bag of Words version of Paragraph Vector, as we found it significantly outperforms the Distributed Memory version of Paragraph Vector. 7 Available at https://code.google.com/p/word2vec/</note>

			<note place="foot" n="8"> Our CNN reimplementation performs slightly worse than (Yu et al., 2014).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree edit models for recognizing textual entailments, paraphrases, and answers to questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1011" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings the International Conference on Computational Linguistics (COLING)</title>
		<meeting>the International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Advances</title>
		<meeting>the Conference on Advances</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="458" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The TREC-8 question answering track evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Retrieval Conference TREC8</title>
		<meeting>the Text Retrieval Conference TREC8</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? A quasisynchronous grammar for QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods for Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</title>
		<meeting>the Annual Meeting of the North American Association of Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Deep Learning and Representation Learning Workshop: NIPS</title>
		<meeting>the Deep Learning and Representation Learning Workshop: NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
