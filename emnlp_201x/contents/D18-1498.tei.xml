<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Source Domain Adaptation with Mixture of Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Darsh J Shah</roleName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Source Domain Adaptation with Mixture of Experts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4694" to="4703"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4694</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship , expressed by a point-to-set metric , determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outper-forms multiple baselines and can robustly handle negative transfer. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Typical domain adaptation methods are designed to transfer supervision from a single source do- main. However, in many practical applications, we have access to multiple sources. For instance, in sentiment analysis of product reviews, we can often transfer from a wide range of product do- mains, rather than one. This can be particularly promising for target domains which do not match any one available source well. For example, the Kitchen product domain may include reviews on pans, cookbooks or electronic devices, which can- not be perfectly aligned to a single source such as Cookware, Books or Electronics. By intelligently aggregating distinct and complementary informa- tion from multiple sources, we may be able to bet- ter fit the target distribution.</p><p>A straightforward approach to utilizing data from multiple sources is to combine them into a single domain. This strategy, however, does not account for distinct relations between individual sources and the target example. Constructing a common feature space for this heterogeneous col- lection may wash out informative characteristics of individual domains and also lead to negative transfer ( <ref type="bibr" target="#b25">Rosenstein et al., 2005</ref>).</p><p>Therefore, we propose to explicitly model the relationship between different source domains and target examples. We hypothesize that different source domains are aligned to different sub-spaces of the target domain. Specifically, in this paper, we model the domain relationship with a mixture- of-experts (MoE) approach <ref type="bibr" target="#b15">(Jacobs et al., 1991b</ref>). For each target example, the predicted posterior is a weighted combination of all the experts' pre- dictions. The weights reflect the proximity of the example to each source domain. Our model learns this point-to-set metric automatically, without ad- ditional supervision.</p><p>We define the point-to-set metric using Maha- lanobis distance <ref type="bibr" target="#b31">(Weinberger and Saul, 2009</ref>) be- tween individual examples and a set (i.e. domain), which are computed within the hidden represen- tation space of our model. The main challenge is to learn this metric in an unsupervised setting. We address it through a meta-training procedure, in which we create multiple meta-tasks of do- main adaptation from the source domains. In each meta-task, we pick one of the source domains as meta-target, and the rest source domains as meta- sources. By minimizing the loss using the MoE predictions on meta-target, we are able to learn both the model and the metric simultaneously. To further improve transfer quality, we align the en- coding space of our target and source domains via adversarial learning.</p><p>We evaluate our approach on sentiment anal- ysis using the benchmark multi-domain Amazon reviews dataset <ref type="bibr" target="#b6">(Chen et al., 2012;</ref><ref type="bibr" target="#b36">Ziser and Reichart, 2017</ref>) as well as on part-of-speech (POS) tagging using the SANCL dataset ). Experiments show that our ap-proach consistently improves the adaptation re- sults over the best single-source model and a uni- fied multi-source model. On average, we achieve a 7% relative error reduction on the Amazon re- views dataset, and a 13% on the SANCL dataset. Importantly, the POS tagging experiments on the SANCL dataset demonstrate that our method is able to robustly handle negative transfer from un- related sources (e.g., Twitter) and utilize it effec- tively to consistently improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised domain adaptation Most exist- ing domain adaptation methods focus on align- ing the feature space between source and target domains to reduce the domain shift <ref type="bibr" target="#b0">(Ben-David et al., 2007;</ref><ref type="bibr" target="#b3">Blitzer et al., , 2006</ref><ref type="bibr" target="#b22">Pan et al., 2010)</ref>. Our approach is close to the representa- tion learning approaches, such as the denoising autoencoder <ref type="bibr" target="#b12">(Glorot et al., 2011</ref>), the marginal- ized stacked denoising autoencoders <ref type="bibr" target="#b6">(Chen et al., 2012)</ref>, and domain adversarial networks ( <ref type="bibr" target="#b29">Tzeng et al., 2014;</ref><ref type="bibr">Ganin et al., 2016;</ref><ref type="bibr" target="#b34">Zhang et al., 2017;</ref><ref type="bibr" target="#b27">Shen et al., 2018)</ref>.</p><p>In contrast to these previous approaches, how- ever, our approach not only learns a shared repre- sentation space that generalizes well to the target domain, but also captures informative characteris- tics of individual source domains.</p><p>Multi-Source domain adaptation The main challenge in using multiple sources for domain adaptation is in learning domain relations. Some approaches assume that all source domains are equally important to the target domain ( <ref type="bibr" target="#b18">Li and Zong, 2008;</ref><ref type="bibr" target="#b20">Luo et al., 2008;</ref><ref type="bibr" target="#b8">Crammer et al., 2008)</ref>. Others learn a global domain similarity metric using labeled data in a supervised fashion <ref type="bibr" target="#b32">(Yang et al., 2007;</ref><ref type="bibr" target="#b9">Duan et al., 2009;</ref>). Alternatively, <ref type="bibr" target="#b21">Mansour et al. (2009)</ref> and <ref type="bibr" target="#b1">Bhatt et al. (2016)</ref> utilize unlabeled data of the tar- get domain to find a distribution weighted com- bination of the source domains or to construct an auxiliary training set of the source domain in- stances close to the target domain instances. Re- cent adversarial methods on multi-source domain adaptation ( <ref type="bibr" target="#b35">Zhao et al., 2018;</ref><ref type="bibr" target="#b7">Chen and Cardie, 2018)</ref> align source domains to the target domains globally, without accounting for the distinct im- portance of each source with respect to a specific target example.</p><p>The work most related to ours is by Kim et al.</p><formula xml:id="formula_0">! "#$ " … … F S 1 F S 2 F S K ↵(x, S 1 ) ↵(x, S 2 )</formula><p>...</p><formula xml:id="formula_1">↵(x, S K ) $ p S1 (y|x) p S2 (y|x) p SK (y|x) p moe (y|x) E(x)</formula><p>Figure 1: Architecture of the MoE model. E is the en- coder which maps an input x to a hidden representation E(x); F Si is the classifier on the i th source domain; D is the critic that is only used during adversarial train- ing. M is the metric learning component, which takes the encoding of x and source domains (S 1:K ) as input and computes α.</p><p>(2017). They also model the example-to-domain relations, but use an attention mechanism. The attention module is learned using limited train- ing data from the target domain in a supervised fashion. Our method, however, works in an unsu- pervised setting without utilizing any labeled data from the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Problem definition We follow the unsupervised multi-source domain adaptation setup, assuming access to labeled training data from K source do- mains:</p><formula xml:id="formula_2">{S i } K i=1 where S i {(x S i t , y S i t )} |S i | t=1</formula><p>, and (optionally) unlabeled data from a target domain:</p><formula xml:id="formula_3">T {x T t } |T |</formula><p>t=1 . The goal is to learn a model us- ing the source domain data, that generalizes well to the target domain.</p><p>Notations For the rest of the paper, we denote an individual example as x, and a batch of exam- ples as x. We use superscript to denote the domain from which an example is sampled, and use sub- script to denote the index of an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Our Approach</head><p>We model the multiple source domains as a mix- ture of experts, and learn a point-to-set metric α to weight the experts for different target examples. The metric is learned in an unsupervised manner.</p><p>Our model consists of four key components as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, namely the encoder (E), classifier (F ), metric (M ) and adversary (D). We use a typical neural multi-task learning archi- tecture <ref type="bibr" target="#b5">(Caruana, 1997)</ref>, with a shared encoder across all sources, and domain-specific classifiers</p><formula xml:id="formula_4">({F S i } K i=1</formula><p>). Each input is first encoded with E, and then fed to each classifier to obtain the domain-specific predictions (i.e. posteriors). The final predictions are then weighted based on the metric (see Equation 1).</p><p>We start by describing the representation learn- ing component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Representation</head><p>Our goal is to design an encoder that sup- ports transfer, while maintaining source domain- specific information. Depending on different tasks and datasets, we select appropriate encoders - MLP, CNN or LSTM (see Section 4.3 for details).</p><p>We further add an adversarial module (D) on top of the encoder, in order to align the target do- main with the sources. D is typically designed as a parameterized classifier in domain adversarial networks ( <ref type="bibr">Ganin et al., 2016;</ref><ref type="bibr" target="#b34">Zhang et al., 2017)</ref>, which is trained jointly with the encoder and the classifiers through a minimax game. Here, we in- stead use Maximum Mean Discrepancy (MMD) ( <ref type="bibr" target="#b13">Gretton et al., 2012</ref>) as our adversary. This dis- tance metric measures the discrepancy between two distributions explicitly in a non-parametric manner, greatly simplifying the training procedure compared to domain adversarial networks which use an additional domain classifier module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mixture of Experts</head><p>Given an example x from the target domain, we model its posterior distribution as a mixture of posteriors produced by models trained on differ- ent source domain data:</p><formula xml:id="formula_5">p moe (y|x) = K i=1 α(x, S i ) · p S i (y|x) = K i=1 α(x, S i ) · softmax W S i E(x)<label>(1)</label></formula><p>p S i is the posterior distribution produced by the i th source classifier F S i (the i th expert). W S i is the output layer weights of F S i , α is a parame- terized metric function that measures how much confidence we put in the specific source expert for a given example x. <ref type="bibr">2</ref> To derive α, we first define a point-to-set Mahalanobis distance metric between an example x and a set S:</p><formula xml:id="formula_6">d(x, S) = E(x) − µ S M S E(x) − µ S 1 2</formula><p>where µ S is the mean encoding of S. In its origi- nal form, the matrix M S played the role of the in- verse covariance matrix. However, computing the inverse of the covariance matrix is both time con- suming and numerically unstable in practice. Here we allow M to denote any positive semi-definite matrix which is to be estimated during training <ref type="bibr" target="#b31">(Weinberger and Saul, 2009)</ref>. To guarantee the positive semi-definiteness of M, we approximate M with M = UU , where U ∈ R h×r , h is the dimension of hidden representations and r is a hyper-parameter controlling the rank of M. Based on the distance metric, we further de- rive a confidence score e(x, S i ) = f d(x, S i ) for each specific expert. The final metric values α(x, S i ) are then obtained by normalizing these confidence scores:</p><formula xml:id="formula_7">α(x, S i ) = exp e(x, S i ) K j=1 exp e(x, S j )<label>(2)</label></formula><p>Here, we explain our design of e(x, S) on two tasks, respectively binary classification and se- quence tagging, which are also used for evaluation in this paper (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary classification</head><p>The point-to-set Maha- lanobis distance metric measures the distance be- tween an example x and the mean encoding of S, i.e. µ S , while taking into account the (pseudo) co- variance of S. In binary classification, however, the mean vector µ S is likely to be located near the decision boundary, particularly under a balanced setting. Therefore, a small d(x, S) actually im- plies lower confidence of the corresponding clas- sifier, which is counter-intuitive. To this end, we instead define the confidence e(x, S) as the differ- ence between the distances from x to each cate- gory of S:</p><formula xml:id="formula_8">e(x, S) = d(x, S + ) − d(x, S − )</formula><p>Here S + and S − stand for the positive space and negative space of S respectively. Consequently, if x is either far away from S (i.e., x is not in the manifold of S) or near the classification boundary, we will get a small e(x, S) indicating a low con- fidence to the corresponding prediction. On the contrary, if x is much closer to a specific category of S than other categories, the classifier will get a higher confidence.</p><p>Sequence tagging For sequence tagging tasks (e.g., POS tagging), we compute the distance met- ric at the token level. 3 Unlike in binary classifi- cation, the decision boundary here is more com- plicated, and the label distribution is typically im- balanced. The mean vector µ S is unlikely to be located at the decision boundary. So we directly use the (reverse) distance as the confidence value for each token x:</p><formula xml:id="formula_9">e(x, S) = −d(x, S)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>Since we do not have annotated data in the target domain, we have to learn our model in an unsu- pervised fashion. Inspired by the recent progress on few-shot learning with metric-based models such as matching network <ref type="bibr" target="#b30">(Vinyals et al., 2016;</ref>) and prototypical network ( <ref type="bibr" target="#b28">Snell et al., 2017)</ref>, we propose the following meta-training ap- proach. Given K source domains, each source domain will be considered as a target, referred to as meta-target, with the rest of the source do- mains as meta-sources. This way, we obtain K (meta-sources, meta-target) training pairs for do- main adaptation. Then, we apply our MoE formu- lation over these meta-training pairs to learn the metric. At testing time, the metric will be applied to all the K source domains for each example in the target domain.</p><p>We optimize two main objectives: the MoE ob- jective and the multi-task learning (MTL) objec- tive.</p><p>MoE objective For each example in each meta- target domain, we compute its MoE posterior us- ing the corresponding meta-sources. Therefore, we get the following MoE loss over the entire multi-source training data:</p><formula xml:id="formula_10">L moe = − K i=1 |S i | j=1 log p moe (y S i j |x S i j ) = − K i=1 |S i | j=1 log K l=1,l =i α(x, S l ) · p S l (y S i j |x S i j )<label>(3)</label></formula><p>Note that α is normalized over the meta-sources for each meta-target, rather than over all the K sources.</p><p>MTL objective For each meta-target, we fur- ther optimize a supervised cross-entropy loss us- ing the corresponding labels. All supervised ob- jectives are optimized jointly with the encoder be- ing shared, resulting in the following multi-task learning objective:</p><formula xml:id="formula_11">L mtl = − K i=1 |S i | j=1 log p S i (y S i j |x S i j )<label>(4)</label></formula><p>Adversary-augmented MoE We use MMD ( <ref type="bibr" target="#b13">Gretton et al., 2012)</ref> as the adversary to minimize the divergence between the marginal distribution of target domain and source domains. Specifi- cally, at each training epoch, given the K batches {x S 1 , x S 2 , ..., x S K } from all the source domains, we sample a batch (unlabeled) x T from our target domain, and minimize the MMD:</p><formula xml:id="formula_12">L adv = MMD 2 (x S 1 ∪ ... ∪ x S K , x T )<label>(5)</label></formula><p>where </p><formula xml:id="formula_13">MMD(D S , D T ) = 1 |D S | xs∈D S φ E(x s ) − 1 |D T | xt∈D T φ E(x t )</formula><formula xml:id="formula_14">κ(h i , h j ) = Σ n exp(− 1 2σn h i − h j 2 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Procedure</head><formula xml:id="formula_15">1: Input: multi-source domain data S = {S i } K i=1</formula><p>, target domain data T 2: Hyper-parameters: mini-batch size m, coefficients for different losses: λ, γ and η 3: repeat 4:</p><p>Sample K source mini-batches {(x S i , y S i )} K i=1 from S and a target mini-batch x T from T 5:</p><formula xml:id="formula_16">L mtl , L moe , L adv , R h ← 0 6: for t = 1 to K do 7:</formula><p>Set meta-target as T metã S t (x St , y St )</p><p>8:</p><p>Set meta-sources as S meta { ˜ S i } K i=1,i =t , where˜Swhere˜ where˜S i (x S i , y S i ) 9:</p><p>Compute cross-entropy loss over T meta , and add to L mtl 10:</p><p>Compute Mahalanobis metric α(x, S ) for each x ∈ T meta and S ∈ S meta Eq. <ref type="formula" target="#formula_7">(2)</ref> 11:</p><p>Compute MoE loss over (S meta , T meta ) using α, and add to L moe 12:</p><p>Compute entropy of α over S, and add to R h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>end for 14:</p><p>Compute MMD between x T and ∪ K i=1 x S i , and add to L adv Eq. <ref type="formula" target="#formula_12">(5)</ref> 15:</p><p>Update parameters via backpropagating gradients of the total loss L Eq. (7) 16: until converge Entropy regularization In the meta-training process, for each example x in meta-target, we know exactly from which source x is sampled. This provides additional insight that the α distri- bution is skewed, which can be utilized as a soft constraint. Therefore, we propose to regularize the entropy of the α distribution over all the sources, rather than meta-sources: 4</p><formula xml:id="formula_17">H α(x, ·) = − K l=1 α(x, S l ) · log α(x, S l ) R h = K i=1 |S i | j=1 H α(x S i j , ·)<label>(6)</label></formula><p>Joint learning Our final objective is the weighted combination of each individual compo- nent loss:</p><formula xml:id="formula_18">L = λ · L moe + (1 − λ) · L mtl + γ · L adv + η · R h<label>(7)</label></formula><p>where λ controls the balance of the MoE loss and MTL loss. γ is set to 0 in non-adversarial set- ting when unlabeled data from the target domain <ref type="bibr">4</ref> Alternatively, we can directly exploit this supervision and minimize the KL divergence of the α distribution and its ground truth one-hot distribution. In practice, however, we found it beneficial to allow examples from one domain to be attended to different sources. This observation may be attributed to the fact that each domain indeed consists of mul- tiple latent sub-domains.</p><p>is not provided. Additionally, it would be straight- forward to add an MoE loss for labeled data in the target domain if they are available, thus extending our framework to a setting where we have few- shot target annotations. The training process is shown in Algorithm 1. 1. In CHEN12, each domain has 2,000 labeled examples for training (1,000 positive and 1,000 negative), and the target test set has 3,000 to 6,000 examples. <ref type="bibr">5</ref> 2. In ZISER17, each domain also has 2,000 labeled examples (1,000 positive and 1,000 negative), sampled differently from CHEN12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>For each dataset, we conduct experiments by se- lecting the target domain in a round-robin fash-ion. Following the protocol in previous work, we use cross-validation over source domains for hyper-parameters selection for each adaptation task ( <ref type="bibr" target="#b35">Zhao et al., 2018)</ref>. When training with an adversary, we use the 2,000 examples training set of the target domain as the unlabeled data in both the settings. In ZISER17, the same data is also used for test, resulting in a transductive setting.</p><p>Part-of-Speech tagging We further consider a sequence tagging task, where the metric is com- puted over the token-level encodings and multi- class predictions are made at the token (word) level. We use the SANCL dataset ) which contains part-of-speech (POS) tagging annotations in 5 web domains: Emails, Weblogs, Answers, Newsgroups, and Re- views. Among these, Newsgroups, Reviews, and Answers have both a validation and a test set, and are used as target domains. The test set from We- blogs and Emails are used as individual source do- mains. The tagging is performed using the Univer- sal POS tagset ( ). We also use Twitter ( <ref type="bibr" target="#b19">Liu et al., 2018)</ref> as an additional training source. Since it differs substantially from other sources and the target domain, we can assess our model's ability to handle negative transfer. We consider 750 sentences from each SANCL source domain for training, and up to 2,250 sentences from the Twitter dataset to magnify the negative transfer. The validation set in the standard split of each target domain is used for hyper-parameters selection and early-stopping in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We verify the efficacy of our approach (MoE) in non-adversarial and adversarial settings respec- tively. In both settings, we compare our approach against the following two baselines:</p><p>• best-SS: the best single-source adaptation model among all the sources.</p><p>• uni-MS: the unified multi-source adaptation model, which is trained using the combina- tion of all the source domain data with single- source transfer methods. uni-MS is a com- mon and strong baseline for multi-source do- main adaptation ( <ref type="bibr" target="#b35">Zhao et al., 2018</ref>).</p><p>For the rest of the paper, we name the adversar- ial counterpart of the models as * -A.</p><p>In the adversarial setting on CHEN12, in addi- tion to best-SS and uni-MS with adversarial loss, we further compare with the following two sys- tems that also utilize unlabeled data from target domain.</p><p>• mSDA: the marginalized stacked denoising autoencoder ( <ref type="bibr" target="#b6">Chen et al., 2012</ref>). mSDA outperforms prior deep learning and shallow learning approaches such as structural corre- spondence learning ) and denoising autoencoder <ref type="bibr" target="#b12">(Glorot et al., 2011</ref>).</p><p>• MDAN: the multi-source domain adversarial network ( <ref type="bibr" target="#b35">Zhao et al., 2018)</ref>. MDAN gives the state-of-the-art performance for multi-source domain adaptation on CHEN12. It general- izes the domain adversarial network to multi- ple source domain adaptation by selectively backpropagating the domain discrimination loss according to domain classification error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>For CHEN12, since the dataset is in TF-IDF for- mat and the word ordering information is not available, we use a multilayer perceptron (MLP) with an input layer of 5,000 dimensions and one hidden layer of 500 dimensions as our encoder. For ZISER17, we instead use a convolutional neu- ral network (CNN) encoder with a combination of kernel widths 3 and 5 <ref type="bibr" target="#b16">(Kim, 2014)</ref>, each with one hidden layer of size 150, which are then concate- nated to a 300 dimension representation. <ref type="bibr">6</ref> For the POS tagging encoder, we use a hier- archical bidirectional LSTM (BiLSTM) network, which contains a character-level BiLSTM for gen- erating individual word representations, followed by a word-level BiLSTM that generates contextu- alized word representations.</p><p>For MMD, we follow Bousmalis et al. (2016) and use 19 RBF kernels with the standard devia- tion parameters ranging from 10 −6 to 10 6 . <ref type="bibr">7</ref> All the models were trained using Adam with weight decay. Learning rate is set to 10 −4 for CHEN12 and 10 −3 for ZISER17 and POS tagging. We use mini-batches of 32 samples from each do- main. We tune the coefficients λ, η for each adap- tation task. γ is set to 1 for all experiments.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sentiment Analysis on Amazon Reviews</head><p>We report our results on the Amazon reviews datasets in <ref type="table" target="#tab_1">Table 1 (CHEN12) and Table 2</ref> (ZISER17). Our approach (MoE) consistently achieves the best performance across different set- tings and tasks. The results clearly demonstrate the value of us- ing multiple sources. In most cases, even a uni- fied model performs better than the oracle best sin- gle source. By smartly combining all the sources, our model outperforms the unified model signif- icantly. One exception is the task of "B,D,K- E" in CHEN12, where the unified multi-source model doesn't improve over the best single source model, constituting a negative transfer scenario. However, even in this scenario, our approach still performs significantly better, demonstrating its ro- bustness in handling negative transfer.</p><p>Impact of adversarial adaptation We achieve consistent improvements over the baseline sys- tems with the addition of the adversarial loss. In most cases, MoE also achieves additional im- provement (e.g., 79.42% vs. 80.87% in "D,E,K- B"). We notice that in some cases, e.g., "B,D,K- E" in CHEN12 and "B,E,K-D" in ZISER17, the adversarial loss doesn't help MoE. This might be attributed to the fact that by aligning the target dis- tribution with the source domains, the representa- tion space becomes more compact, thus making it more difficult to capture source domain-specific characteristics and increasing the difficulty of met- ric learning in MoE.</p><p>Analysis on the metric (α) <ref type="figure" target="#fig_3">Figure 2</ref> visualizes the distribution of α values, learned by our model in different tasks, across the source domains. The visualization is based on 200 examples for each domain randomly sampled from the correspond- ing test set. From the heatmap we can see that for a specific target domain, different examples may have different α distributions. Moreover, for most examples, the α distribution is skewed, indicating that our model draws on a few most informative source domains. <ref type="figure" target="#fig_4">Figure 3</ref> exemplifies the above point. For in- stance, the first review about "charger" and "bat- tery" is closer to the Electronics source domain. This relation is successfully captured by the α dis- tribution produced by our model.</p><p>We further investigate the impact of entropy regularization over α.  • … i did n't keep it on the charger for 24 hours ( not realizing that this is a problem ) and now the battery only works for 10 minutes at a time … • best 2 quart pot in the world . … with the glorious one pot meals cookbook . it has wonderful recipies , and the pot works wonderful .</p><formula xml:id="formula_19">η = 0) K E D B Chen12 B D E K Ziser17 B D E K 0.</formula><p>• great kit however the book that comes with the kit needs some work . the photos in the book are not accurate with the descriptions . …  on CHEN12 and ZISER17. It shows that entropy regularization benefits our model under both non- adversarial and adversarial settings. <ref type="table" target="#tab_6">Table 4</ref> summarizes our results on POS tagging. Again, our approach consistently achieves the best performance across different settings and tasks. Adding Twitter as a source leads to a drop in per- formance for the unified model, as a result of neg- ative transfer. Our method, however, robustly han- dles negative transfer and manages to even benefit from this additional source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Part-of-Speech Tagging</head><p>Impact of negative transfer <ref type="table" target="#tab_7">Table 5</ref> presents the α distribution learned by the metric, on aver- age for all tokens of the target domain. As we can see, our model (MoE-A) effectively learns to de- crease the weights on Twitter, demonstrating again its ability to alleviate negative transfer. We further study the impact of this outlier source by varying the amount of Twitter data used during training. We gradually increase the number of Twitter instances by 750. As shown in <ref type="table" target="#tab_8">Table 6</ref>, the increase of the Twitter data does not benefit the unified multi-source model (uni-MS-A), and even amplifies negative transfer for the Answers and Re- views domains. However, the performance of our MoE (MoE-A) model stays stable, consistently in- creasing with more Twitter, showing robustness in handling negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel mixture-of- experts (MoE) approach for unsupervised do- main adaptation from multiple diverse source do- mains. We model the domain relations through a point-to-set distance metric, and introduce a meta-training mechanism to learn this metric. Ex- perimental results on sentiment classification and part-of-speech tagging demonstrate that our ap- proach consistently outperforms various baselines and can robustly handle negative transfer. The ef- fectiveness of our approach suggests its potential application to a broader range of domain adapta- tion tasks in NLP and other areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TARGET</head><p>NON-ADVERSARIAL ADVERSARIAL best-SS uni-MS uni-MS † MoE best-SS-A uni-MS-A uni-MS-A † MoE-A    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>H</head><label></label><figDesc>measures the discrepancy between D S and D T based on Reproducing Kernel Hilbert Space (RKHS). φ(·) is the feature map induced by a uni- versal kernel. We follow Bousmalis et al. (2016) and use a linear combination of multiple RBF ker- nels:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 1</head><label>1</label><figDesc>Task and Dataset Sentiment classification We use the multi- domain Amazon reviews dataset (Blitzer et al., 2007), one of the standard benchmark datasets for domain adaptation. It contains reviews on four do- mains: Books (B), DVDs (D), Electronics (E), and Kitchen appliances (K). We follow the specific experiment settings pro- posed by Chen et al. (2012) (CHEN12) and Ziser and Reichart (2017) (ZISER17).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>SETTING</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: α distributions across source domains for randomly selected 200 examples in each target domain of CHEN12 (left) and ZISER17 (right). Columns represent target domains and rows represent sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of Kitchen (K) reviews in ZISER17 and their α distribution over Books (B), DVDs (D) and Electronics (E). The manually highlighted words indicate the specific Kitchen products described in the reviews.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>NON-ADVERSARIAL ADVERSARIAL best-SS uni-MS MoE mSDA † MDAN best-SS-A uni-MS-A MoE-A</head><label></label><figDesc></figDesc><table>D,E,K-B 
75.43 
78.43 79.42 
76.98 
78.63 
80.07 
80.25 
80.87 
B,E,K-D 
81.23 
82.49 83.35 
78.61 
80.65 
82.68 
83.30 
83.99 
B,D,K-E 
85.51 
84.79  *  86.62 
81.98 
85.34 
86.32 
85.96  *  
86.38 
B,D,E-K 
86.83 
87.00 87.96 
84.33 
86.26 
87.05 
87.55 
88.06 

Average 
82.25 
83.18 84.34 
80.48 
82.72 
84.03 
84.27 
84.83 

Table 1: Multi-Source domain adaptation accuracy on Amazon dataset of CHEN12.  *  indicates negative transfer, 
i.e., the unified multi-source model underperforms the best single-source model. mSDA  † is not an adversarial 
approach, but utilizes unlabeled data from target domain. 

SETTING 
NON-ADVERSARIAL 

ADVERSARIAL 

best-SS uni-MS MoE best-SS-A uni-MS-A MoE-A 

D,E,K-B 
85.35 
87.00 87.55 
86.85 
87.55 
87.85 
B,E,K-D 
85.25 
86.80 87.85 
86.00 
87.40 
87.65 
B,D,K-E 
86.80 
88.30 89.20 
88.90 
89.35 
89.50 
B,D,E-K 
88.90 
89.65 90.45 
89.95 
90.35 
90.45 

Average 
86.58 
87.94 88.76 
87.93 
88.66 
88.86 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Multi-Source domain adaptation accuracy on Amazon dataset of ZISER17.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 summarizes</head><label>3</label><figDesc>the ab- lation test results of entropy regularization (</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Ablation test of entropy regularizer on 
CHEN12 and ZISER17 (decrease in accuracy). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>POS tagging results on SANCL data. Source domains include Web, Emails, Twitter.  † indicates the unified 
multi-source model trained without Twitter, thus can be considered as the oracle performance (upper-bound) of 
uni-MS. 

TARGET 
SOURCE 

Twitter Emails 
Web 

Answers 
0.0527 0.5941 0.3531 
Reviews 
0.0640 0.5250 0.4100 
Newsgroup 0.0538 0.4960 0.4490 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Distribution of the metric values α on average for all tokens in the SANCL test set.</figDesc><table>TARGET 
MODEL 
( * -A) 

# Twitter instances 

750 1,500 2,250 

Answers 
uni-MS 89.04 89.04 86.93 
MoE 
89.80 91.22 90.90 

Reviews 
uni-MS 87.90 87.45 87.68 
MoE 
89.40 90.23 91.14 

Newsgroup 
uni-MS 90.20 90.10 90.21 
MoE 
91.13 91.32 91.82 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>POS tagging accuracy with varying amounts of Twitter data in training.</figDesc><table></table></figure>

			<note place="foot" n="1"> Our code and data are available at https://github. com/jiangfeng1124/transfer.</note>

			<note place="foot" n="2"> In traditional MoE frameworks (Jacobs et al., 1991a,b; Shazeer et al., 2017), α is commonly realized as a &quot;gating network&quot;, which produces a normalized weight vector that determines the combination of experts depending solely on the input example x. However, simple gating networks do not yield promising results in our scenario. We hypothesize that both the input instance and the underlying domain distribution should be captured for determining the credit assignment.</note>

			<note place="foot" n="3"> This actually makes it a multi-class classification problem with respect to every token of a sequence.</note>

			<note place="foot" n="5"> This dataset has been processed by the author to TF-IDF representations, using the 5,000 most frequent unigram and bigram features, thus word order information is not available.</note>

			<note place="foot" n="6"> Note that with a more extensive architecture search, we are likely to achieve better results. This, however, is not the main focus of this work. 7 Detailed values are presented in the supplementary material in Bousmalis et al. (2016).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank MIT NLP group and the anonymous re-viewers for their helpful comments. We also thank Shiyu Chang and Mo Yu for insightful discussions on metric learning. This work is supported by the MIT-IBM Watson AI Lab. Any opinions, find-ings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding orga-nizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-domain text classification with multiple domains and disparate label sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjira</forename><surname>Himanshu Sharad Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1641" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th ACL</title>
		<meeting>the 45th ACL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1627" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multinomial adversarial networks for multi-domain text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05694</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning from multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1757" to="1774" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ivor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning</title>
		<meeting>the 26th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="250" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Steven J Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. Doha</title>
		<meeting>EMNLP. Doha<address><addrLine>Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain attention with an ensemble of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACL</title>
		<meeting>the 55th ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multidomain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th ACL</title>
		<meeting>the 46th ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="257" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parsing tweets into universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer learning from multiple source domains via consensus regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
	<note>Yuhong Xiong, and Qing He</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Overview of the 2012 shared task on parsing the web. In Notes of the first workshop on syntactic analysis of noncanonical language (sancl)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zvika</forename><surname>Michael T Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas G</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005 workshop on transfer learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<title level="m">Deep domain confusion: Maximizing for domain invariance</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia. ACM</title>
		<meeting>the 15th ACM international conference on Multimedia. ACM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Diverse few-shot text classification with multiple metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Potdar</forename><surname>Saloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aspect-augmented adversarial networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="515" to="528" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multiple source domain adaptation with adversarial learning. ICLR Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural structural correspondence learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yftah</forename><surname>Ziser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canada</forename><surname>Vancouver</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="400" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
