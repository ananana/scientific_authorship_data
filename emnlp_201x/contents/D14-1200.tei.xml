<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Joint Entity and Relation Extraction with Table Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<addrLine>2-12-1 Hisakata, Tempaku-ku</addrLine>
									<postCode>468-8511</postCode>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute</orgName>
								<address>
									<addrLine>2-12-1 Hisakata, Tempaku-ku</addrLine>
									<postCode>468-8511</postCode>
									<settlement>Nagoya</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Joint Entity and Relation Extraction with Table Representation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1858" to="1869"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a history-based struc-tured learning approach that jointly extracts entities and relations in a sentence. We introduce a novel simple and flexible table representation of entities and relations. We investigate several feature settings , search orders, and learning methods with inexact search on the table. The experimental results demonstrate that a joint learning approach significantly out-performs a pipeline approach by incorporating global features and by selecting appropriate learning methods and search orders .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extraction of entities and relations from texts has been traditionally treated as a pipeline of two sep- arate subtasks: entity recognition and relation ex- traction. This separation makes the task easy to deal with, but it ignores underlying dependencies between and within subtasks. First, since entity recognition is not affected by relation extraction, errors in entity recognition are propagated to re- lation extraction. Second, relation extraction is often treated as a multi-class classification prob- lem on pairs of entities, so dependencies between pairs are ignored. Examples of these dependen- cies are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. For dependencies between subtasks, a Live in relation requires PER and LOC entities, and vice versa. For in-subtask dependencies, the Live in relation between "Mrs. Tsutayama" and "Japan" can be inferred from the two other relations.</p><p>Figure 1 also shows that the task has a flexible graph structure. This structure usually does not cover all the words in a sentence differently from other natural language processing (NLP) tasks such as part-of-speech (POS) tagging and depen- Mrs. Tsuruyama is from Kumamoto Prefecture in Japan . dency parsing, so local constraints are considered to be more important in the task. Joint learning approaches <ref type="bibr" target="#b25">(Yang and Cardie, 2013;</ref><ref type="bibr" target="#b22">Singh et al., 2013</ref>) incorporate these de- pendencies and local constraints in their models; however most approaches are time-consuming and employ complex structures consisting of multi- ple models. <ref type="bibr" target="#b11">Li and Ji (2014)</ref> recently proposed a history-based structured learning approach that is simpler and more computationally efficient than other approaches. While this approach is promis- ing, it still has a complexity in search and restricts the search order partly due to its semi-Markov rep- resentation, and thus the potential of the history- based learning is not fully investigated.</p><p>In this paper, we introduce an entity and relation table to address the difficulty in representing the task. We propose a joint extraction of entities and relations using a history-based structured learning on the table. This table representation simplifies the task into a table-filling problem, and makes the task flexible enough to incorporate several en- hancements that have not been addressed in the previous history-based approach, such as search orders in decoding, global features from relations to entities, and several learning methods with in- exact search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we first introduce an entity and re- lation table that is utilized to represent the whole entity and relation structures in a sentence. We then overview our model on the table. We finally explain the decoding, learning, search order, and features in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Entity and relation table</head><p>The task we address in this work is the extraction of entities and their relations from a sentence. En- tities are typed and may span multiple words. Re- lations are typed and directed.</p><p>We use words to represent entities and relations. We assume entities do not overlap. We employ a BILOU (Begin, Inside, Last, Outside, Unit) en- coding scheme that has been shown to outperform the traditional BIO scheme <ref type="bibr" target="#b18">(Ratinov and Roth, 2009)</ref>, and we will show that this scheme induces several label dependencies between words and be- tween words and relations in §2.3.2. A label is assigned to a word according to the relative posi- tion to its corresponding entity and the type of the entity. Relations are represented with their types and directions. ⊥ denotes a non-relation pair, and → and ← denote left-to-right and right-to-left re- lations, respectively. Relations are defined on not entities but words, since entities are not always given when relations are extracted. Relations on entities are mapped to relations on the last words of the entities.</p><p>Based on this representation, we propose an en- tity and relation table that jointly represents en- tities and relations in a sentence. <ref type="figure">Figure 2</ref> illus- trates an entity and relation table corresponding to an example in <ref type="figure" target="#fig_0">Figure 1</ref>. We use only the lower tri- angular part because the table is symmetric, so the number of cells is n(n + 1)/2 when there are n words in a sentence. With this entity and relation table representation, the joint extraction problem can be mapped to a table-filling problem in that labels are assigned to cells in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model</head><p>We tackle the table-filling problem by a history- based structured learning approach that assigns la- bels to cells one by one. This is mostly the same as the traditional history-based model <ref type="bibr" target="#b2">(Collins, 2002</ref>) except for the table representation.</p><p>Let x be an input table, Y(x) be all possible assignments to the table, and s(x, y) be a scoring function that assesses the assignment of y ∈ Y(x) to x. With these definitions, we define our model to predict the most probable assignment as fol- lows:</p><formula xml:id="formula_0">y * = arg max y∈Y(x) s(x, y)<label>(1)</label></formula><p>This scoring function is a decomposable function, and each decomposed function assesses the as- signment of a label to a cell in the table.</p><formula xml:id="formula_1">s(x, y) = |x| ∑ i=1 s(x, y, 1, i)<label>(2)</label></formula><p>Here, i represents an index of a cell in the table, which will be explained in §2.3.1. The decom- posed function s(x, y, 1, i) corresponds to the i-th cell. The decomposed function is represented as a linear model, i.e., an inner product of features and their corresponding weights.</p><formula xml:id="formula_2">s(x, y, 1, i) = w·f (x, y, 1, i)<label>(3)</label></formula><p>The scoring function are further divided into two functions as follows:</p><formula xml:id="formula_3">s(x, y, 1, i) = s local (x, y, i) + s global (x, y, 1, i)<label>(4)</label></formula><p>Here, s local (x, y, i) is a local scoring func- tion that assesses the assignment to the i-th cell without considering other assignments, and s global (x, y, 1, i) is a global scoring function that assesses the assignment in the context of 1st to (i − 1)-th assignments. This global scoring func- tion represents the dependencies between entities, between relations, and between entities and rela- tions. Similarly, features f are divided into local features f local and global features f global , and they are defined on its target cell and surrounding con- texts. The features will be explained in §2.5. The weights w can also be divided, but they are tuned jointly in learning as shown in §2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Decoding</head><p>The scoring function s(x, y, 1, i) in Equation <ref type="formula" target="#formula_1">(2)</ref> uses all the preceding assignments and does not rely on the Markov assumption, so we cannot em- ploy dynamic programming.</p><p>We instead employ a beam search to find the best assignment with the highest score ( <ref type="bibr" target="#b1">Collins and Roark, 2004</ref>). The beam search assigns la- bels to cells one by one with keeping the top K best assignments when moving from a cell to the next cell, and it returns the best assignment when labels are assigned to all the cells. The pseudo code for decoding with the beam search is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mrs.</head><p>Tsutayama is from Kumamoto Prefecture in Japan . <ref type="figure">Figure 2</ref>: The entity and relation table for the example in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><formula xml:id="formula_4">Mrs. B-PER Tsutayama ⊥ L-PER is ⊥ ⊥ O from ⊥ ⊥ ⊥ O Kumamoto ⊥ ⊥ ⊥ ⊥ B-LOC Prefecture ⊥ Live in→ ⊥ ⊥ ⊥ L-LOC in ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ O Japan ⊥ Live in→ ⊥ ⊥ ⊥ Located in→ ⊥ U-LOC . ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥ ⊥</formula><p>INPUT: x: input table with no assignment, K: beam size OUTPUT: best assignment y * for</p><formula xml:id="formula_5">x 1: b ← [x] 2: for i = 1 to |x| do 3:</formula><p>T ← ∅</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for k = 1 to |b| do 5:</p><formula xml:id="formula_6">for a ∈ A(i, b[k]) do 6: T ← T ∪ append(a, b[k]) 7:</formula><p>end for  We explain how to map the table to a sequence (line 2 in <ref type="figure" target="#fig_2">Figure 3</ref>), and how to calculate possible assignments (line 6 in <ref type="figure" target="#fig_2">Figure 3</ref>) in the following subsections.</p><p>2.3.1 <ref type="table">Table-</ref>to-sequence mapping Cells in an input table are originally indexed in two dimensions. To apply our model in §2.2 to the cells, we need to map the two-dimensional table to a one-dimensional sequence. This is equivalent to defining a search order in the table, so we will use the terms "mapping" and "search order" inter- changeably.</p><p>Since it is infeasible to try all possible map- pings, we define six promising static mappings (search orders) as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. Note that the "left" and "right" directions in the captions cor- respond to not word orders, but tables. We de-  ) with the highest priority on the "up to down" order, which checks a sentence forwardly (from the beginning of a sentence). Similarly, we also define two map- pings (Figures 4(c) and 4(d)) with the highest pri- ority on the "right to left" order, which check a sentence backwardly (from the end of a sentence).  <ref type="table">Table 1</ref>: Label dependencies from relations to en- tities. * indicates any type. We also investigate dynamic mappings (search orders) with an easy-first policy ( <ref type="bibr" target="#b7">Goldberg and Elhadad, 2010)</ref>. Dynamic mappings are different from the static mappings above, since we reorder the cells before each decoding 1 . We evaluate the cells using the local scoring function, and assign indices to the cells so that the cells with higher scores have higher priorities. In addition to this na¨ıvena¨ıve easy-first policy, we define two other dy- namic mappings that restricts the reordering by combining the easy-first policy with one of the fol- lowing two policies: entity-first (all entities are de- tected before relations) and close-first (closer cells are detected before distant cells) policies.</p><note type="other">Condition Possible labels on w</note><formula xml:id="formula_7">i Relation(s) on w i−1 B-*, O, U-* Relation(s) on w i L-*, U-*</formula><formula xml:id="formula_8">Label on w i Relations from/to w i B-*, I-*, O ⊥ L-*, U-* * Label on w i+1 Relations from/to w i I-*, L-* ⊥ B-*, U-*, O *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Label dependencies</head><p>To avoid illegal assignments to a table, we have to restrict the possible assignments to the cells ac- cording to the preceding assignments. This restric- tion can also reduce the computational costs.</p><p>We consider all the dependencies between cells to allow the assignments of labels to the cells in an arbitrary order. Our representation of entities and relations in §2.1 induces the dependencies be- tween entities and between entities and relations. Tables 1-3 summarize these dependencies on the i- th word w i in a sentence. We can further utilize de- pendencies between entity types and relation types if some entity types are involved in a limited num-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label on w i−2</head><p>Possible labels on</p><formula xml:id="formula_9">w i B-TYPE B-*, I-TYPE, L-TYPE, O, U-* I-TYPE B-*, I-TYPE, L-TYPE, O, U-* L-TYPE B-*, I-*, L-*, O, U-* O B-*, I-*, L-*, O, U-* U-TYPE B-*, I-*, L-*, O, U-* O/S B-*, I-*, L-*, O, U-* Label on w i−1 Possible labels on w i B-TYPE I-TYPE, L-TYPE I-TYPE I-TYPE, L-TYPE L-TYPE B-*, O, U-* O B-*, O, U-* U-TYPE B-*, O, U-* O/S B-*, O, U-* Label on w i+1 Possible labels on w i B-TYPE L-*, O, U-* I-TYPE B-TYPE, I-TYPE L-TYPE B-TYPE, I-TYPE O L-*, O, U-* U-TYPE L-*, O, U-* O/S L-*, O, U-* Label on w i+2</formula><p>Possible labels on <ref type="table">Table 3</ref>: Label dependencies between entities. TYPE represents an entity type, and O/S means the word is outside of a sentence.</p><formula xml:id="formula_10">w i B-TYPE B-*, I-*, L-*, O, U-* I-TYPE B-TYPE, I-TYPE, L-*, O, U-* L-TYPE B-TYPE, I-TYPE, L-*, O, U-* O B-*, I-*, L-*, O, U-* U-TYPE B-*, I-*, L-*, O, U-* O/S B-*, I-*, L-*, O, U-*</formula><p>ber of relation types or vice versa. We note that the dependencies between entity types and rela- tion types include not only words participating in relations but also their surrounding words. For ex- ample, the label on w i−1 can restrict the types of relations involving w i . We employ these type de- pendencies in the evaluation, but we omit these de- pendencies here since these dependencies are de- pendent on the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learning</head><p>The goal of learning is to minimize errors between predicted assignments y * and gold assignments y gold by tuning the weights w in the scoring func- tion in Equation 3. We employ a margin-based structured learning approach to tune the weights w. The pseudo code is shown in <ref type="figure" target="#fig_6">Figure 5</ref>. This ap- proach enhances the traditional structured percep-</p><formula xml:id="formula_11">INPUT: training sets D = {(x i , y i )} N i=1 , T: iterations OUTPUT: weights w 1: w ← 0 2: for t = 1 to T do 3: for x, y ∈ D do 4:</formula><p>y * ← best assignment for x using decod- ing in <ref type="figure" target="#fig_2">Figure 3</ref> with s ′ in Equation <ref type="formula" target="#formula_13">(5)</ref> 5:</p><formula xml:id="formula_12">if y * ̸ = y gold then 6: m ← arg max i {s ′ (x, y gold , 1, i)− s ′ (x, y * , 1, i)} 7: w ← update(w, f (x, y gold , 1, m), f (x, y * , 1, m)) 8: end if 9:</formula><p>end for 10: end for 11: return w Margin-based structured learn- ing approach with a max-violation update. update(w, f (x, y gold , 1, m), f (x, y * , 1, m)) depends on employed learning methods.</p><p>tron <ref type="bibr" target="#b2">(Collins, 2002</ref>) in the following ways. Firstly, we incorporate a margin ∆ into the scoring func- tion as follows so that wrong assignments with small differences from gold assignments are pe- nalized (lines 4 and 6 in <ref type="figure" target="#fig_6">Figure 5</ref>) <ref type="bibr" target="#b6">(Freund and Schapire, 1999</ref>).</p><formula xml:id="formula_13">s ′ (x, y) = s(x, y) + ∆(y, y gold )<label>(5)</label></formula><p>Similarly to the scoring function s, the margin ∆ is defined as a decomposable function using 0-1 loss as follows:</p><formula xml:id="formula_14">∆(y, y gold ) = |x| ∑ i=1 ∆(y i , y gold i ), ∆(y i , y gold i ) = { 0 if y i = y gold i 1 otherwise<label>(6)</label></formula><p>Secondly, we update the weights w based on a max-violation update rule following Huang et al. ) We employ parame- ter averaging except for DCD-SSVM. AROW and AdaGrad store additional information for covari- ance and feature counts respectively, and DCD- SSVM keeps a working set and performs addi- tional updates in each iteration. Due to space limi- tations, we refer to the papers for the details of the learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Features</head><p>Here, we explain the local features f local and the global features f global introduced in §2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Local features</head><p>Our focus is not to exploit useful local features for entities and relations, so we incorporate several features from existing work to realize a reasonable baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Global features</head><p>We design global features to represent dependen- cies among entities and relations. <ref type="table" target="#tab_3">Table 5</ref> summa- rizes the global features 2 . These global features are activated when all the information is available during decoding. We incorporate label dependency features like traditional sequential labeling for entities. Al- though our model can include other non-local fea- tures between entities (Ratinov and Roth, 2009), we do not include them expecting that global fea- tures on entities and relations can cover them. We design three types of global features for relations. These features are activated when all the partic- ipating relations are not ⊥ (non-relations). Fea- tures except for the "Crossing" category are simi- lar to global relation features in <ref type="bibr" target="#b11">Li and Ji (2014)</ref>. We further incorporate global features for both en- tities and relations. These features are activated when the relation label is not ⊥. These features can act as a bridge between entities and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section, we first introduce the corpus and evaluation metrics that we employed for evalua- tion. We then show the performance on the train- ing data set with explaining the parameters used <ref type="table">Target   Category  Features  Word  Lexical</ref> Character n-grams (n=2,3,4) (Entity)</p><p>Attributes by parsers (base form, POS) Word types (all-capitalized, initial-capitalized, all-digits, all-puncts, all- digits-or-puncts) Contextual</p><p>Word n-grams (n=1,2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>,3) within a context window size of 2 Word pair Entity Entity lexical features of each word (Relation) Contextual</head><p>Word n-grams (n=1,2,3) within a context window size of 2 Shortest path Walk features (word-dependency-word or dependency-word- dependency) on the shortest paths in parsers' outputs n-grams (n=2,3) of words and dependencies on the paths n-grams (n=1,2) of token modifier-modifiee pairs on the paths The length of the paths  for the test set evaluation, and show the perfor- mance on the test data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation settings</head><p>We used an entity and relation recognition corpus by <ref type="bibr" target="#b19">Roth and Yih (2004)</ref>  <ref type="bibr">3</ref> . The corpus defines four named entity types Location, Organization, Per- son, and Other and five relation types Kill, Live In, Located In, OrgBased In and Work For. All the entities were words in the original cor- pus because all the spaces in entities were replaced with slashes. Previous systems <ref type="bibr" target="#b20">(Roth and Yih, 2007;</ref><ref type="bibr" target="#b9">Kate and Mooney, 2010</ref>) used these word boundaries as they were, treated the boundaries as given, and focused the entity classification prob- lem alone. Differently from such systems, we re- covered these spaces by replacing these slashes with spaces to evaluate the entity boundary detec- tion performance on this corpus. Due to this re- placement and the inclusion of the boundary de- tection problem, our task is more challenging than the original task, and our results are not compara- ble with those by the previous systems.</p><p>The corpus contains 1,441 sentences that con- tain at least one relation. Instead of 5-fold cross validation on the entire corpus by the previous sys- tems, we split the data set into training (1,153 sen- tences) and blind test (288 sentences) data sets and developed the system on the training data set. We tuned the hyper-parameters using a 5-fold cross validation on the training data set, and evaluated the performance on the test set.</p><p>We prepared a pipeline approach as a baseline. We first trained an entity recognition model using the local and global features, and then trained a relation extraction model using the local features and global features without global "Relation" fea- tures in <ref type="table" target="#tab_3">Table 5</ref>. We did not employ the global "Relation" features in this baseline since it is com- mon to treat relation extraction as a multi-class classification problem.</p><p>We extracted features using the results from two syntactic parsers Enju ( <ref type="bibr" target="#b15">Miyao and Tsujii, 2008)</ref> and LRDEP <ref type="bibr" target="#b21">(Sagae and Tsujii, 2007)</ref>. We em- ployed feature hashing <ref type="bibr" target="#b24">(Weinberger et al., 2009)</ref> and limited the feature space to 2 24 . The num- bers of features greatly varied for categories and targets. They also caused biased predictions that prefer entities to relations in our preliminary ex- periments. We thus chose to re-scale the features as follows. We normalized local features for each feature category and then for each target. We also normalized global features for each feature cate- gory, but we did not normalize them for each target since normalization was impossible during decod- ing. We instead scaled the global features, and the scaling factor was tuned by using the same 5-fold cross validation above.</p><p>We used the F1 score on relations with entities as our primary evaluation measure and used it for tuning parameters. In this measure, a relation with two entities is considered correct when the offsets and types of the entities and the type of the relation are all correct. We also evaluated the F1 scores for entities and relations individually on the test data set by checking their corresponding cells. An en- tity is correct when the offset and type are correct, and a relation is correct when the type is correct and the last words of two entities are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance on Training Data Set</head><p>It is infeasible to investigate all the combinations of the parameters, so we greedily searched for a default parameter setting by using the evaluated results on the training data set. The default pa- rameter setting was the best setting except for the beam size. We show learning curves on the train- ing data set in <ref type="figure">Figure 6</ref> when we varied each pa- rameter from the default parameter setting. We employed 5-fold cross validation. The default pa- rameter setting used DCD-SSVM as the learning method, entity-first, easy-first as the search order, local and global features, and 8 as the beam size. This section discusses how these parameters affect the performance on the training data set and ex- plains how the parameter setting was selected for the test set. <ref type="figure">Figure 6</ref>(a) compares the learning methods in- troduced in §2.4. DCD-SSVM and AdaGrad per- formed slightly better than perceptron, which has often been employed in history-based structured learning. AROW did not show comparable per- formance to the others. We ran 100 iterations to find the number of iterations that saturates learn- ing curves. The large number of iterations took time and the performance of DCD-SSVM almost converged after 30 iterations, so we employed 50 iterations for other evaluation on the training data set. AdaGrad got its highest performance more quickly than other learning methods and AROW converged slower than other methods, so we em- ployed 10 for AdaGrad, 90 for AROW, and 50 it- erations for other settings on the test data set.</p><p>The performance was improved by widening the beam as in <ref type="figure">Figure 6(b)</ref>, but the improvement was gradually diminished as the beam size in- creased. Since the wider beam requires more train- ing and test time, we chose 8 for the beam size. <ref type="figure">Figure 6</ref>(c) shows the effects of joint learning as well as features explained in §2.5. We show the performance of the pipeline approach (Pipeline) introduced in §3.1, and the performance with lo- cal features alone (Local), local and global fea- tures without global "Relation" features in <ref type="table" target="#tab_3">Table 5</ref> (Local+global (−relation)) and all local and global features (Local+global). We note that Pipeline shows the learning curve of relation extraction in the pipeline approach. Features in "Local+global (−relation)" are the same as the features in the pipeline approach, and the result shows that the joint learning approach performed slightly better than the pipeline approach. The incorporation of global "Entity" and "Entity+Relation" features improved the performance as is common with the existing pipeline approaches, and relation-related features further improved the performance.</p><p>Static search orders in §2.3.1 also affected the performance as shown in <ref type="figure">Figure 6</ref> between the performances with the best order and worst order was about 0.04 in an F1 score, which is statistically significant, and the performance can be worse than the pipeline approach in <ref type="figure">Figure 6</ref>(c). This means improvement by joint learning can be easily cancelled out if we do not carefully con- sider search order. It is also surprising that the sec- ond worst order <ref type="figure" target="#fig_4">(Figure 4(b)</ref>) is the most intuitive "left-to-right" order, which is closest to the order in Li and Ji (2014) among the six search orders. <ref type="figure">Figure 6</ref>(e) shows the performance with dy- namic search orders. Unfortunately, the easy-first policy did not work well on this entity and relation task, but, with the two enhancements, dynamic or- ders performed as well as the best static order in <ref type="figure">Figure 6(d)</ref>. This shows that entities should be de-tected earlier than relations on this data set. <ref type="table">Table 6</ref> summarizes the performance on the test data set. We employed the default parameter set- ting explained in §3.2, and compared parameters by changing the parameters shown in the first col- umn. We performed a statistical test using the ap- proximate randomization method <ref type="bibr" target="#b17">(Noreen, 1989)</ref> on our primary measure ("Entity+Relation"). The results are almost consistent with the results on the training data set with a few exceptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance on Test Data Set</head><p>Differently from the results on the training data set, AdaGrad and AROW performed significantly worse than perceptron and DCD-SSVM and they performed slightly worse than the pipeline ap- proach. This result shows that DCD-SSVM per- forms well with inexact search and the selection of learning methods can significantly affect the entity and relation extraction performance.</p><p>The joint learning approach showed a signifi- cant improvement over the pipeline approach with relation-related global features, although the joint learning approach alone did not show a signif- icant improvement over the pipeline approach. Unfortunately, no joint learning approach outper- formed the pipeline approach in entity recognition. This may be partly because hyper-parameters were tuned to the primary measure. The results on the pipeline approach also indicate that the better per- formance on entity recognition does not necessar- ily improve the relation extraction performance.</p><p>Search orders also affected the performance, and the worst order (right to left, down to up) and best order (close-first, left to right) were signifi- cantly different. The performance of the worst or- der was worse than that of the pipeline approach, although the difference was not significant. These results show that it is necessary to carefully select the search order for the joint entity and relation extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with Other Systems</head><p>To compare our model with the other sys- tems ( <ref type="bibr" target="#b20">Roth and Yih, 2007;</ref><ref type="bibr" target="#b9">Kate and Mooney, 2010)</ref>, we evaluated the performance of our model when the entity boundaries were given. Differ- ently from our setting in §3.1, we used the gold entity boundaries encoded in the BILOU scheme and assigned entity labels to the boundaries. We performed 5-fold cross validation on the data set following <ref type="bibr" target="#b20">Roth and Yih (2007)</ref> although the split was different from theirs since their splits were not available. We employed the default parameter set- ting in §3.2 for this comparison. <ref type="table">Table 7</ref> shows the evaluation results. Although we cannot directly compare the results, our model performs better than the other models. Compared to <ref type="table">Table 6, Table 7</ref> also shows that the inclusion of entity boundary detection degrades the perfor- mance about 0.09 in F-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Search order in structured learning has been stud- ied in several NLP tasks. Left-to-right and right- to-left orderings have been often investigated in sequential labeling tasks ( <ref type="bibr" target="#b10">Kudo and Matsumoto, 2001</ref>). Easy-first policy was firstly introduced by <ref type="bibr" target="#b7">Goldberg and Elhadad (2010)</ref> for dependency parsing, and it was successfully employed in sev- eral tasks, such as joint POS tagging and depen- dency parsing ( <ref type="bibr" target="#b12">Ma et al., 2012</ref>) and co-reference resolution <ref type="bibr" target="#b23">(Stoyanov and Eisner, 2012)</ref>. Search order, however, has not been focused in relation extraction tasks.</p><p>Named entity recognition ( <ref type="bibr" target="#b5">Florian et al., 2003;</ref><ref type="bibr" target="#b16">Nadeau and Sekine, 2007)</ref> and relation extrac- tion ( <ref type="bibr" target="#b27">Zelenko et al., 2003;</ref><ref type="bibr" target="#b14">Miwa et al., 2009</ref>) have often been treated as separate tasks, but there are some previous studies that treat enti- ties and relations jointly in learning. Most stud- ies built joint learning models upon individual models for subtasks, such as Integer Linear Pro- gramming (ILP) <ref type="bibr" target="#b20">(Roth and Yih, 2007;</ref><ref type="bibr" target="#b25">Cardie, 2013) and</ref><ref type="bibr">Card-Pyramid Parsing (Kate and</ref><ref type="bibr" target="#b9">Mooney, 2010)</ref>. Our approach does not re- quire such individual models, and it also can de- tect entity boundaries that these approaches except for <ref type="bibr" target="#b25">Yang and Cardie (2013)</ref> did not treat. Other studies ( <ref type="bibr" target="#b26">Yu and Lam, 2010;</ref><ref type="bibr" target="#b22">Singh et al., 2013</ref>) built global probabilistic graphical models. They need to compute distributions over variables, but our approach does not. <ref type="bibr" target="#b11">Li and Ji (2014)</ref> proposed an approach to jointly find entities and relations. They incorporated a semi-Markov chain in repre- senting entities and they defined two actions dur- ing search, but our approach does not employ such representation and actions, and thus it is more sim- ple and flexible to investigate search orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a history-based struc- tured learning approach that jointly detects enti- Local + global † 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610 ⋆ (a) Up to down, left to right 0.824 / 0.801 / 0.813 0.821 / 0.433 / 0.567 0.787 / 0.415 / 0.543 (b) Up to down, right to left 0.828 / 0.808 / 0.818 0.850 / 0.461 / 0.597 0.822 / 0.445 / 0.578 (c) Right to left, up to down 0.823 / 0.799 / 0.811 0.826 / 0.448 / 0.581 0.789 / 0.427 / 0.554 (d) Right to left, down to up 0.811 / 0.784 / 0.797 0.774 / 0.445 / 0.565 0.739 / 0.425 / 0.540 (e) Close-first, left to right 0.821 / 0.806 / 0.813 0.807 / 0.522 / 0.634 0.780 / 0.504 / 0.612 ⋆ (f) Close-first, right to left 0.817 / 0.801 / 0.809 0.832 / 0.491 / 0.618 0.797 / 0.471 / 0.592 Easy-first 0.811 / 0.790 / 0.801 0.862 / 0.415 / 0.560 0.831 / 0.399 / 0.540 Entity-first, easy-first † 0.812 / 0.802 / 0.807 0.783 / 0.524 / 0.628 0.760 / 0.509 / 0.610 ⋆ Close-first, easy-first 0.816 / 0.803 / 0.810 0.796 / 0.486 / 0.603 0.767 / 0.468 / 0.581 <ref type="table">Table 6</ref>: Performance of entity and relation extraction on the test data set (precision / recall / F1 score). The † denotes the default parameter setting in §3.2 and ⋆ represents a significant improvement over the underlined "Pipeline" baseline (p&lt;0.05). Labels (a)-(f) correspond to those in <ref type="figure" target="#fig_4">Figure 4</ref>.  <ref type="table">Table 7</ref>: Results of entity classification and relation extraction on the data set using the 5-fold cross validation (precision / recall / F1 score).</p><p>ties and relations. We introduced a novel entity and relation table that jointly represents entities and relations, and showed how the entity and re- lation extraction task can be mapped to a simple table-filling problem. We also investigated search orders and learning methods that have been fixed in previous research. Experimental results showed that the joint learning approach outperforms the pipeline approach and the appropriate selection of learning methods and search orders is crucial to produce a high performance on this task. As future work, we plan to apply this approach to other relation extraction tasks and explore more suitable search orders for relation extraction tasks.</p><p>We also plan to investigate the potential of this ta- ble representation in other tasks such as semantic parsing and co-reference resolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>b</head><label></label><figDesc>← top K tables from T using the scoring function in Equation (2) 10: end for 11: return b[0]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Decoding with the beam search. A(i, t) returns possible assignments for i-th cell of a table t, and append(a, t) returns a table t updated with an assignment a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Static search orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>From another point of view, entities are detected before relations in Figures 4(b) and 4(c) whereas the order in a sentence is prioritized in Figures 4(a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Margin-based structured learning approach with a max-violation update. update(w, f (x, y gold , 1, m), f (x, y * , 1, m)) depends on employed learning methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 2012 )</head><label>2012</label><figDesc>(lines 6-7 in Figure 5). Finally, we em- ploy not only perceptron (Collins, 2002) but also AROW (Mejer and Crammer, 2010; Crammer et al., 2013), AdaGrad (Duchi et al., 2011), and DCD-SSVM (Chang and Yih, 2013) for learning methods (line 7 in Figure 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 6: Learning curves of entity and relation extraction on the training data set using 5-fold cross validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Label dependencies from entities to rela-
tions. 

and 4(d). We further define two close-first map-
pings (Figures 4(e) and 4(f)) since entities are 
easier to find than relations and close relations are 
easier to find than distant relations. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 summarizes</head><label>4</label><figDesc></figDesc><table>the local features. 
Local features for entities (or words) are similar 
to the features used by Florian et al. (2003), but 
some features are generalized and extended, and 
gazetteer features are excluded. For relations (or 
pairs of words), we employ and extend features in 
Miwa et al. (2009). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 : Local features.</head><label>4</label><figDesc></figDesc><table>Target 
Category 
Details 
Entity 
Bigram 
Bigrams of labels 
Combinations of two labels and their corresponding POS tags 
Combinations of two labels and their corresponding words 
Trigram 
Trigrams of labels 
Combinations of three labels and each of their corresponding POS tags 
Combinations of three labels and each of their corresponding words 
Entity 
Combinations of a label and its corresponding entity 
Relation 
Entity-
sharing 
Combinations of two relation labels that share a word (i.e., relations in 
same columns or same rows in a table) 
Combinations of two relation labels and the shared word 
Relation shortest path features between non-shared words, augmented by 
a combination of relation labels and the shared word 
Cyclic 
Combinations of three relation labels that make a cycle 
Crossing 
Combinations of two relation labels that cross each other 
Entity + 
Entity-
Relation label and the label of its participating entity 
Relation 
relation 
Relation label and the label and word of its participating entity 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 : Global features.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> It is also possible to reorder the cells during decoding, but it greatly increases the computational costs.</note>

			<note place="foot" n="2"> We tried other &quot;Entity+Relation&quot; features to represent a relation and both its participating entities, but they slightly degraded the performance in our preliminary experiments.</note>

			<note place="foot" n="3"> conll04.corp at http://cogcomp.cs.illinois. edu/page/resource_view/43</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dual coordinate descent algorithms for efficient large margin structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive regularization of weight vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="187" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<editor>Walter Daelemans and Miles Osborne</editor>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="168" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large margin classification using the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="296" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Elhadad</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="742" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured perceptron with inexact search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction using card-pyramid parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chunking with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies</title>
		<meeting>the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Easy-first Chinese POS tagging and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiliang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India, December</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1731" to="1746" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Confidence in structured-prediction using confidence-weighted models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avihai</forename><surname>Mejer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="971" to="981" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A rich feature vector for protein-protein interaction extraction from multiple corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rune</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature forest models for probabilistic HPSG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A survey of named entity recognition and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lingvisticae Investigationes</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Computer-Intensive Methods for Testing Hypotheses : An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-04" />
			<publisher>WileyInterscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004)</title>
		<editor>Hwee Tou Ng and Ellen Riloff</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Global Inference for Entity and Relation Identification via a Linear Programming Formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dependency parsing and domain adaptation with LR models and parser ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>the CoNLL Shared Task Session of EMNLP-CoNLL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1044" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint inference of entities, relations, and coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 workshop on Automated knowledge base construction</title>
		<meeting>the 2013 workshop on Automated knowledge base construction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Easy-first coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="2519" to="2534" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
