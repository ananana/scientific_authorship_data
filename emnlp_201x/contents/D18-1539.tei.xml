<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Domain Adjacent Instances for Semantic Parsers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ferguson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Google Inc</orgName>
								<orgName type="institution" key="instit3">Google Inc</orgName>
								<orgName type="institution" key="instit4">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Google Inc</orgName>
								<orgName type="institution" key="instit3">Google Inc</orgName>
								<orgName type="institution" key="instit4">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Google Inc</orgName>
								<orgName type="institution" key="instit3">Google Inc</orgName>
								<orgName type="institution" key="instit4">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Gonzàlez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Google Inc</orgName>
								<orgName type="institution" key="instit3">Google Inc</orgName>
								<orgName type="institution" key="instit4">Google Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Domain Adjacent Instances for Semantic Parsers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4964" to="4969"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4964</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When the semantics of a sentence are not representable in a semantic parser&apos;s output schema, parsing will inevitably fail. Detection of these instances is commonly treated as an out-of-domain classification problem. However , there is also a more subtle scenario in which the test data is drawn from the same domain. In addition to formalizing this problem of domain-adjacency, we present a comparison of various baselines that could be used to solve it. We also propose a new simple sentence representation that emphasizes words which are unexpected. This approach improves the performance of a downstream semantic parser run on in-domain and domain-adjacent instances.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsers map text to logical forms, which can then be used by downstream components to fulfill an action. Consider, for example, a system for booking air travel, in which a user provides natural language input, and a downstream subsys- tem is able to make or cancel flight reservations. Users of the system typically have a general under- standing of its purpose, so the input will revolve around the correct topic of air travel. However, they are unlikely to know the limits of the system's functionality, and may provide inputs for which the expected action is beyond its capabilities, such as asking to change seats on a flight reservation. Because the logical schema is designed with ful- fillment in mind, no logical form can capture the semantics of these sentences, making it impossi- ble for the parser to generate a correct parse. Any output the parser generates will cause unintended actions to be executed downstream. For example, asking to change seats might be misparsed and executed as changing flights. Instead, the parser should identify that this input is beyond its scope so the condition can be handled. <ref type="bibr">1</ref> In this paper, we formalize this pervasive problem, which we call domain-adjacent instance identification. While this task is similar to that of identifying out-of-domain input instances (e.g., banking with respect to air travel), it is much more subtle -the instances come from roughly the same domain as the parser's training examples, and thus use very similar language. Domain adjacency is a property with respect to the parser's output schema, inde- pendent of the data used to train it.</p><p>In this paper, we formalize this task, and pro- pose a simple approach for representing sen- tences in which words are weighted by how likely they are to differentiate between in-domain and domain-adjacent instances. Note that while this approach can also be applied to out-of-domain instances, in this paper we are interested in its performance on domain-adjacent instances. We describe an evaluation framework for this new task and, finally, evaluate our proposed method against a set of baselines, comparing performance on the domain-adjacent classification problem and a downstream semantic parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setting</head><p>A semantic parser can be seen as a function ϕ that maps sentences x in a natural language L to log- ical forms y ∈ Y. Assuming the existence of an oracle parserˆϕparserˆ parserˆϕ, the problem we propose in this pa- per is that of determining, for a given test instance x, whether it belongs to the domain Φ ofˆϕofˆ ofˆϕ, i.e., if its semantics can be encoded in the schema Y.</p><p>In real-world usage, the input sentences x will be generated by a human user, who associates the capabilities of the parser to a particular topic (e.g., air travel). Thus most of the x ∈ L \ Φ will share topic with thê x ∈ Φ. Because of the similarity between these x andˆxandˆ andˆx, we call this task identifica- tion of domain-adjacent instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our goal is to identify input instances whose se- mantics are not representable in the parser's out- put schema, and we assume only an in-domain dataset is available at training time. Our approach is based on determining similarity to these training instances. We split the task in two parts: 1) encode the sentences to a compact representation that pre- serves the needed information, and 2) given these representations, identify which sentences are so dissimilar that they are unlikely to be parseable with any schema that covers the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Representation</head><p>Among recent work in distributional semantics, averaging the word vectors to represent a sentence ( <ref type="bibr" target="#b17">Wieting et al., 2016;</ref><ref type="bibr" target="#b0">Adi et al., 2017</ref>) has proven to be a simple and robust approach. However, we have an intuition that words which are unexpected in their context given the training data may be a strong signal that an instance is domain-adjacent. To incorporate this signal, we propose a weighted average, in which the weight corresponds to how unexpected the word is in its context. For exam- ple, given in-domain predicates from <ref type="figure">Figure 1</ref>, in the domain-adjacent sentence "Upgrade my flight to SFO with my miles", upgrade should receive a much higher weight than flight or SFO.</p><p>Our weighting scheme is as follows: We use the cosine distance between the expected (¯ v i ) and the actual (ˆ v i ) domain-specific word embedding at a given position (i) in a sentence to compute its weight:</p><formula xml:id="formula_0">w i = 1 − cos(¯ v i , ˆ v i ).</formula><p>The expected word embedding is computed using the context embed- dings,</p><formula xml:id="formula_1">¯ v i = i+c j=i−c,j =î v j , wherê</formula><p>v j is a domain- specific word embedding, in a window of size c around position i. Intuitively, w i represents how surprising the word is in the context.</p><p>Since our training set is too small to di- rectly learn domain-specific embeddings, we learn a mapping from general pre-trained em- beddings. We train a continuous bag-of-words model ( <ref type="bibr" target="#b12">Mikolov et al., 2013</ref>) in which we pass pre-trained embeddings (v i ) instead of 1-hot vec- tors, as input to the embedding layer. The layer thus learns a mapping from pre-trained to domain- specific embeddings (ˆ v i ). We use this mapping to compute new embeddings for words that are miss- ing from the training set. Only words that do not have pre-trained embeddings are ignored.</p><p>Finally, for a sentence with n words, we take the weighted average of the pre-trained embeddings of the words in the sentence, using the weights from above:</p><formula xml:id="formula_2">S = ( n i=1 w i v i ) / ( n i=1 w i ).</formula><p>This approach assigns high weight to words that differ significantly from what is expected based on the training data. By combining these weights with the pre-trained word embeddings, we allow the model to incorporate external information, im- proving generalization beyond the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Domain-Adjacent Model</head><p>A number of techniques can be applied to pre- dict whether a sentence is domain-adjacent from its continuous representation. Of the methods we tried, we found k-nearest neighbors <ref type="bibr" target="#b1">(Angiulli and Pizzuti, 2002</ref>) to perform best: to classify a sen- tence, we calculate the average cosine distance be- tween its embedding and its k nearest neighbors in the training data, and label it domain-adjacent if this value is greater than some threshold. This simpler model relies more heavily on the external information brought in by pre-trained word em- beddings, while more complex models seem to overfit to the training data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section, we introduce an evaluation frame- work for this new task. We consider training and test sets from a single domain, with only the lat- ter containing domain-adjacent instances. Test in- stances are classified individually, and we measure performance on in-domain/domain-adjacent clas- sification and semantic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Semantic Parser</head><p>We simulate this setting by adapting the OVER- NIGHT dataset ( <ref type="bibr" target="#b16">Wang et al., 2015)</ref>. This dataset is composed of queries drawn from eight do- mains, each having a set of seven to eighteen distinct semantic predicates. Queries consist of a crowd-sourced textual sentence and its corre- sponding logical form containing one or more of these domain-specific semantic predicates. For each domain, we select a set of predicates to exclude from the logical schema (see <ref type="table" target="#tab_2">Table 1</ref>), and remove all instances containing these predicates from the training set (since they are now domain- adjacent). We then train a domain-adjacent model and semantic parser on the remaining training data and attempt to identify the domain-adjacent exam- ples in the test data. We use the train/test splits from <ref type="bibr" target="#b16">Wang et al. (2015)</ref>. In all experiments, we use the SEMPRE parser <ref type="figure">(Berant et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Because this is a novel task, and results are not comparable to previous work, we report results from a variety of baseline systems. The first baseline, CONFIDENCE, identifies instances as domain-adjacent if the semantic parser's confi- dence in its predictions is below some threshold.</p><p>The remaining baselines follow the two-part ap- proach from Section 3. AUTOENCODER is inspi- red by <ref type="bibr" target="#b14">Ryu et al. (2017)</ref>'s work on identifying out-of-domain examples. For the sentence repre- sentation, this method uses a bi-LSTM with self- attention, trained to predict the semantic predi- cates, and concatenates the final hidden state from each direction as the sentence representation. An autoencoder is used as the domain-adjacent classi- fier.</p><p>The remaining methods use the nearest neigh- bor model discussed in Section 3.2. For sentence representations, we include baselines drawn from different neural approaches. In CBOW, we sim- ply average the pre-trained word embeddings in the sentence. In CNN, we train a two-layer CNN with a final softmax layer to predict the seman- tic predicates for a sentence. We concatenate the mean pooling of each layer as the sentence rep- resentation. In LSTM, we use the same sen- tence representation as in AUTOENCODER, with the nearest neighbor domain-adjacent model. Fi- nally, SURPRISE is the approach presented in Sec- tion 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Direct Evaluation</head><p>We first directly evaluate the identification of domain-adjacent instances: <ref type="table" target="#tab_4">Table 2</ref> reports the area under a receiver operating characteristic curve (AUC) for the considered models <ref type="bibr" target="#b5">(Fawcett, 2006</ref>). SURPRISE generally performs the best on this evaluation; and, in general, the simpler models tend to perform better, suggesting that more com- plex approaches tune too much to the training data.</p><p>Qualitatively, for domains where the SURPRISE model performs better, it places higher weight on words we would consider important for distin- guishing domain-adjacent sentences. For exam- ple in "show me recipes with longer preparation times than rice pudding" from Recipes, "longer" and "preparation" have the highest weights. In Social, there are two in-domain predicates (em- ploymentStartDate and educationEndDate) which use very similar wording to those that are domain- adjacent, making it difficult to isolate surprising words. The weights in this domain seem to instead emphasize unusual wordings such as "soonest" in "employees with the soonest finish date".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Analysis</head><p>In order to determine the contribution of each one of the components of SURPRISE, we per- formed an ablation analysis comparing the fol- lowing modifications of the method: CBOW, as described above, using an unweighted average of pre-trained embeddings; FREQUENCY, using a   weighted average of pre-trained embeddings, with weights based on inverse document frequency in the training set; PRETRAINED, using the surprise schema but with weights determined using pre- trained embeddings; and the full SUPRISE as pre- sented above. Each approach adds one component (weighting, surprise-based weights, and domain- specific embeddings) with respect to the previous one.</p><p>The results of the experiment are shown in Ta- ble 3. We can see that FREQUENCY performs slightly worse than CBOW and PRETRAINED performs even worse than that. We can conclude that the combination of the weighting schema and the tuned vectors is what makes SUPRISE effec- tive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Downstream Task Evaluation</head><p>We next evaluate how including the domain- adjacent predictions affects the performance of a semantic parser. In a real setting, when the se- mantic parser is presented with domain-adjacent input that is beyond its scope, the correct behav- ior is to label it as such so that it can be han- dled properly by downstream components. To simulate this behavior, we set the gold parse for domain-adjacent instances to be an empty parse, and automatically assign an empty parse to any instance that is identified as domain-adjacent. We report accuracy of the semantic parser with 20% domain-adjacent test data. We include two addi- tional models: NOFILTER, in which nothing is la- beled domain-adjacent, and ORACLE, in which all the domain-adjacent instances are correctly iden- tified. For each baseline requiring a threshold, we set it such that 3% of the instances in the dev set would be marked as domain-adjacent (intuitively, this represents the error-tolerance of the system). <ref type="table" target="#tab_7">Table 4</ref> shows the results for this experiment. In general, the relative performance is similar to that in the direct evaluation (e.g. SURPRISE tends to do well on most domains, but performs poorly on BASKETBALL and SOCIAL in particular). How- ever, in this evaluation, misclassifying an instance as domain-adjacent if the semantic parser would have accurately parsed it is worse than misclassi- fying the instance if the semantic parser could not have accurately parsed it. For example, in SOCIAL we can thus infer that SURPRISE is marking some instances as domain-adjacent that would otherwise be accurately parsed as the performance there is actually worse than for NOFILTER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Domain-adjacency identification is a new task, but relatively little effort has been devoted to even the related task of identifying out-of-domain in- stances (i.e., from completely separate domains) for semantic parsers. <ref type="bibr" target="#b6">Hakkani-Tur et al. (2015)</ref> approached the problem by clustering sentences based on shared subgraphs in their general seman- tic parses; <ref type="bibr" target="#b14">Ryu et al. (2017)</ref> classify sentences with autoencoder reconstruction error.</p><p>Prior distributional semantics work to create compact sentential representations generated spe- cific embeddings for downstream tasks <ref type="bibr" target="#b8">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b10">Kim, 2014;</ref><ref type="bibr" target="#b15">Socher et al., 2013</ref>   Another relatively sparse area of related work is handling the domain-adjacent instances once they have been identified. The simplest thing to do is to return a generic error. For user-facing applica- tions, one such message might state that the sys- tem can't handle that specific query. <ref type="bibr" target="#b3">Azaria et al. (2016)</ref> approach this problem by having the user break down the domain-adjacent instance into a sequence of simpler textual instructions and then attempting to map those to known logical forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Identifying domain-adjacent instances is a prac- tical issue that can improve downstream seman- tic parsing precision, and thus provide a smoother and more reliable user experience. In this pa- per, we formalize this task, and introduce a novel sentence embedding approach which outperforms baselines. Future work includes exploring alter- native ways of incorporating information outside of the given training set and experimenting with various combinations of semantic parsers and up- stream domain-adjacency models. Another area of future research is how the underlying system should recover when domain-adjacent instances are detected.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>independent embeddings, learned without down- stream task supervision. Kiros et al. (2015), Hill et al. (2016), and Kenter et al. (2016) learn rep- resentations by predicting the surrounding sen- tences. Wieting et al. (2016) use paraphrases as supervision. Mu et al. (2017) represent sentences by the low-rank subspace spanned by the embed- dings of the words in them; Arora et al. (2017) use a weighted average of word embeddings, with their projection onto the first principal component across all sentences in the corpus removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Predicates excluded from training and consid-
ered domain-adjacent. Domains have 5-20 predicates. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>AUC for domain-adjacent instance identification, using KNN as the domain-adjacent model. 

Basketball Blocks Calendar Housing Publications Recipes Restaurants Social 
CBOW 
0.743 
0.782 
0.662 
0.910 
0.884 
0.670 
0.911 
0.675 
FREQUENCY 
0.656 
0.703 
0.771 
0.884 
0.887 
0.667 
0.834 
0.591 
PRETRAINED 
0.612 
0.636 
0.512 
0.819 
0.842 
0.526 
0.858 
0.538 
SURPRISE 
0.755 
0.827 
0.817 
0.933 
0.978 
0.758 
0.941 
0.545 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : AUC for domain-adjacent instance identification, using ablated versions of SURPRISE with KNN.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>). Recently, work has focused on domain-</head><label></label><figDesc></figDesc><table>Basketball Blocks Calendar Housing Publications Recipes Restaurants Social 
NOFILTER 
0.358 
0.294 
0.617 
0.461 
0.511 
0.570 
0.626 
0.355 
ORACLE 
0.558 
0.494 
0.817 
0.661 
0.711 
0.770 
0.826 
0.555 
AUTOENCODER 
0.413 
0.268 
0.581 
0.447 
0.463 
0.530 
0.543 
0.417 
CONFIDENCE 
0.389 
0.306 
0.644 
0.472 
0.525 
0.568 
0.665 
0.360 
CBOW 
0.344 
0.295 
0.634 
0.515 
0.621 
0.575 
0.722 
0.358 
CNN 
0.452 
0.324 
0.674 
0.488 
0.573 
0.570 
0.605 
0.446 
LSTM 
0.385 
0.314 
0.622 
0.495 
0.581 
0.547 
0.612 
0.363 
SURPRISE 
0.356 
0.371 
0.679 
0.570 
0.668 
0.554 
0.764 
0.345 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy for a semantic parser evaluated on a test set in which 20% is domain adjacent. 

</table></figure>

			<note place="foot" n="1"> In the final page of the paper, we suggest a few immediate downstream system behaviors when a domain-adjacent instance is identified, but others have investigated the related problem of teaching the system new behavior (Azaria et al., 2016).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast outlier detection in high dimensional spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Angiulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Pizzuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="15" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instructable intelligent personal agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2016</title>
		<meeting>AAAI 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Fawcett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clustering novel intents in a conversational interaction system with semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Cheng</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH 2015</title>
		<meeting>INTERSPEECH 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Siamese CBOW: Optimizing word embeddings for sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2013</title>
		<meeting>ICLR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representing sentences as low-rank subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural sentence embedding using only in-domain sentences for out-of-domain sentence detection in dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghan</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwi</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="26" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
