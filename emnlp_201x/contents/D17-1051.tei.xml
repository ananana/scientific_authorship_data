<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Humor in Reviews using Background Text Sources</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Morales</surname></persName>
							<email>amorale4@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Humor in Reviews using Background Text Sources</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="492" to="501"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of automatically identifying humorous text from a new kind of text data, i.e., online reviews. We propose a generative language model, based on the theory of incongruity, to model humorous text, which allows us to leverage background text sources, such as Wikipedia entry descriptions, and enables construction of multiple features for identifying humorous reviews. Evaluation of these features using supervised learning for classifying reviews into humorous and non-humorous reviews shows that the features constructed based on the proposed generative model are much more effective than the major features proposed in the existing literature, allowing us to achieve almost 86% accuracy. These humorous review predictions can also supply good indicators for identifying helpful reviews.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The growth of online feedback systems, such as online reviews in which users can write about their preferences and opinions, has allowed for creativ- ity in the written communication of user ideas. As such, these feedback systems have become ubiq- uitous, and it's not difficult to imagine a future with smart systems reacting to user's behaviour in a human-like manner <ref type="bibr" target="#b15">(Nijholt, 2014</ref>). An es- sential component for personal communication is the expression of humor. Although many peo- ple have studied the theory of humor, it still re- mains loosely defined <ref type="bibr" target="#b21">(Ritchie, 2009)</ref>, this leads to difficulties in modelling humor. While the task for identifying humor in text has been previously studied, most approaches have focused on shorter text such as Twitter data <ref type="bibr" target="#b14">(Mihalcea and Strapparava, 2006;</ref><ref type="bibr" target="#b20">Reyes et al., 2012</ref><ref type="bibr" target="#b19">Reyes et al., , 2010</ref>) (see Section 6 for a more complete review of related work). In this paper, we study the problem of automatically identifying humorous text from a new kind of text data, i.e., online reviews. In order to quantitatively test whether the review is humorous, we devise a novel approach, using the theory of incongruity, to model the reviewer's humorous intent when writ- ing the review. The theory of incongruity states that we laugh because there is something incon- gruous <ref type="bibr" target="#b0">(Attardo, 1994)</ref>, in other words, there is a change from our expectation.</p><p>Specifically, we propose a general generative language model to model the generation of humor- ous text. The proposed model is a mixture model with multinomial distributions as component mod- els (i.e., models of topics), similar to Probabilis- tic Latent Semantic Analysis <ref type="bibr" target="#b4">(Hofmann, 1999)</ref>. However, the main difference is that the compo- nent word distributions (i.e., component language models) are all assumed to be known in our model, and they are designed to model the two types of language used in a humorous text, including 1) the general background model estimated using all the reviews, and 2) the reference language models of all the topical aspects covered in the review that capture the typical words used when each of the covered aspects is discussed. Thus the model only has the parameters indicating the relative cover- age of these component language models. The idea here is to use these parameters to assess how well a review can be explained by collectively by the reference language models corresponding to all the topical aspects covered in the review, which are estimated using an external text source (e.g., Wikipedia).</p><p>We construct multiple features based on the generative model and evaluate them using super- vised learning for classifying reviews into humor- ous and non-humorous reviews. Experiment re-sults on a Yelp 1 review data set show that the fea- tures constructed based on the proposed generative model are much more effective than the major fea- tures proposed in the existing literature, allowing us to achieve almost 86% accuracy. We also exper- imented with using the results of humorous review prediction to further predict helpful reviews, and the results show that humorous review prediction can supply good indicators for identifying helpful reviews for consumers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Referential Humor and Incongruity</head><p>In this section we describe some observations in our data that have motivated our approach to solv- ing the problem. In particular, we show that hu- morous reviews tend to reference aspects which deviate from what is expected. That is, in funny re- views, the authors tend to use referential humor, in which specific concepts or entities are referenced to produce comedic effects, which we call aspects. Here we define referential humor to be a humor- ous piece of text which references aspects outside of the typical context, in our case restaurant re- views. For the rest of the paper we use humorous and funny interchangeably.</p><p>Our study uses review data from Yelp. Yelp has become a popular resource for identifying high quality restaurants. A Yelp user is able to submit reviews rating the overall experience of the restau- rants. The reviews submitted to Yelp tend to have similar context, in particular they mention several aspects rating the quality of the restaurant such as food, price, service and so on. This information is expected from the reviewer in their review, how- ever it is not always the case since there is no re- quirement for writing the review. Yelp users are able to vote for a review in several criterion, such as funny, cool, and useful. This gives the users an incentive for not only creating informative reviews but possibly entertaining reviews.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we show a humorous review, ran- domly sampled by using our classifier with a high probability of being funny, where the reviewer as- serts that the food has extreme medicinal prop- erties. The reviewer refers to "Nyquil" a com- mon cold medicine to express the food's incredible ability to cure ailments. This appears almost sur- prising since it would not normally be mentioned in restaurants reviews. To identify the intended humor, we can use the references the reviewer 1 www.yelp.com makes, e.g. Nyquil, as clues to what she is empha- sising, e.g. the savory soondubu, by making such comparisons, e.g. the heavenly taste and amazing price. Yelp users seem to consider funny reviews which tended to deviate from what was expected into things which would seem out of place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language Models as a Proxy for Incongruity</head><p>Motivated by the observations discussed in the previous section (i.e., reviewers tend to reference some entities which seem unexpected in the con- text of the topic of the review), we propose a gen- erative language model based on the theory of in- congruity to model the generation of potentially humorous reviews. Following previous work on humor, we use the definition of incongruity in hu- mor as "what people find unexpected" <ref type="bibr" target="#b14">(Mihalcea and Strapparava, 2006</ref>), where "unexpected" con- cepts are those concepts which people do not con- sider to be the norm in some domain, later we for- malize unexpectedness using our model. We now describe the proposed model in more detail. Suppose we observe the following refer-</p><formula xml:id="formula_0">ences to K d topical aspects A d = {r 1 , r 2 , ..., r K d } in a review R d = [w 1 , w 2 , ..., w N d ],</formula><p>where each r i corresponds to an aspect reference (i.e. NyQuil in our running example), and w i ∈ V , where V is the vocabulary set. The model generates a word, for some review, at a time, which talks about a specific aspect or is related to the language used in Yelp more broadly; we call the latter the background language model. Thus a word is generated from a mixture model, and its probability is an interpola- These aspects provide some context to the un- derling meaning of a review; the reviewers use these aspects for creative writing when describ- ing their dining experience. These aspects allow us to use external information as the context, thus we develop measures for incongruity addressing the juxtaposition of the aspect's context and the review. The review construction process is repre- sented in a generative model, see <ref type="figure" target="#fig_1">Figure 2</ref>, where the shaded nodes represent our observations, we have observed the words as well as the referenced aspects which the reviewer has mentioned in their review. The light nodes are the labels for the as- pect which has generated the corresponding word.</p><p>Since the background language model, denoted by θ B , is review independent, we can simplify the generative model by copying the background lan- guage model for each review, thus we can focus on the parameter estimation for each review in paral- lel.</p><p>A key component to the success of our fea- tures is the mesh of background text from external sources, or background text sources, and the re- views. In our example, <ref type="figure" target="#fig_0">Figure 1</ref>, Nyquil is a criti- cal component for understanding the humor. How- ever it is difficult to understand some references a reviewer makes without any prior knowledge. To do so, we incorporate external background knowl- edge in the form of language models for the refer- enced aspect present in the reviews. If the reviewer has made K d references to different aspects A d in review R d , then for each r i there is a correspond- ing language model θ r i w = P (w|θ r i ) over the vo- cabulary w ∈ V . For simplicity, we describe the model for each document, and use the notation θ i w and θ i for the corresponding language model of r i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Incorporating Background Text Sources</head><p>As described before, some features we will use to describe incongruity correspond to the weights of the mixture model used to generate the words in the review, which take into account the language of the references she will make or allude, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The probability that an author will generate a word w, for the dth review given corre-</p><formula xml:id="formula_1">sponding aspects Θ = {θ B , θ 1 , ..., θ K d }, is P (w, d, Θ) = K d z=0 P (w, z, d, Θ) = K d z=0 P (w|z, Θ)P (z|d) = λθ B w + (1 − λ) K d i=1 π i θ i w Note K d</formula><p>indicates the different aspects the re- viewer will mention in a review, R d , and hence it can vary between reviews. θ B w = P (w|z = 0, Θ) is the probability that the word will appear when writing a review (e.g. background language model) and θ i w can be interpreted as word distri- butions over aspect i. Here λ = P (z = 0|d) is the weight for the background language model and</p><formula xml:id="formula_2">π i = P (z = i|d) 1 − P (z = 0|d)</formula><p>denotes the relative weights of the referenced aspect's language models used in the review. We denote our parameters for re-</p><formula xml:id="formula_3">view R d as Λ R d = {π 1 , ..., π K d , λ}.</formula><p>Note that the parameter set varies depending on how many references the review makes. In order to estimate P (w|θ i ), we first need to find the aspects that the user is mentioning in their reviews. In general as- pects can be defined as any topics explicitly de- fined in external background text data; in our ex- periments we define aspects as Wikipedia entities. In subsection 5.1, we describe one way of obtain- ing these aspects, but first we describe the estima- tion methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Estimation</head><p>To estimate our parameters Λ R d , we would like to maximize the likelihood of P (R d ), which is the same as maximizing the log-likelihood of P (R d ).</p><p>That isˆΛ</p><formula xml:id="formula_4">isˆ isˆΛ = argmax Λ log P (R d |Λ) = argmax Λ w∈V c(w, R d ) log (P (w, d, Θ))</formula><p>Here c(w, R d ) represents the number of occur- rences of the word w in R d . In order to maxi- mize the log-likelihood we use the EM algorithm <ref type="bibr" target="#b3">(Dempster et al., 1977)</ref>, to compute the update rules for the parameters λ and π 1 , ...π K d . For the E-Step, at the n + 1th iteration we have</p><formula xml:id="formula_5">P (z w = 0) = θ B w λ (n) K d l=1 θ l w π (n) l (1 − λ (n) ) + θ B w λ (n) P (z w = j) = θ j w π (n) j K d l=1 θ l w π (n) l</formula><p>Where z w is a hidden variable indicating whether we have selected any of the aspect language mod- els, or the background language model, when gen- erating the word w. The update rules for the M-</p><p>Step are as follows:</p><formula xml:id="formula_6">λ (n) = w∈V c(w, R d )P (z w = 0) n , π (n) j = w∈V c(w, R d )P (z w = j)(1 − P (z w = 0)) K d l=1 w∈V c(w, R d )P (z w = l)(1 − P (z w = 0))</formula><p>We ran EM until the parameters converged or a small threshold was reached. Note there is some similarity to other topic modelling approaches like PLSA <ref type="bibr" target="#b4">(Hofmann, 1999)</ref>. PLSA is a way to soft cluster the documents into several topics, in doing so a word distribution for each topic is learned. In our work we make the assumption that the "topics" are fixed, namely they are the aspects which the reviewer mentions in their review. Note that, we can similarly derive update rules for an different topic model such as LDA ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, how- ever prior work, (Lu et al., 2011), shows that LDA does not show superior performance over PLSA empirically for a number of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features construction</head><p>Since we are interested in studying discriminative features for humorous and non-humorous reviews, we set up a classification problem to classify a re- view into either humorous or non-humorous. In classification problems the data plays a critical role; here the labels are obtained from the funny votes in our Yelp dataset, and we describe how we created the ground-truth in Section 5. Here in this section, we discuss the new features we can con- struct based on the proposed language model and estimated parameter values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Incongruity features</head><p>A natural feature in our incongruity model is the estimated background weight, λ, since it indicates how much emphasis the reviewer puts in their re- view to describe the referenced aspects, we de- note this feature by A1. Another feature is based on the relative weights for the referenced aspect's language models. There tends to be more 'sur- prise' in a review when the reviewer talks about multiple aspects equally, this is because the more topics the reviewer writes about the more intricate the review becomes. We use the entropy of the weights</p><formula xml:id="formula_7">H(R d ) = − K d i=1 π i log π i</formula><p>as another in- congruity score and label this feature as A2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unexpectedness features</head><p>Humor often relies on introducing concepts which seem out of place to produce a comedic ef- fect. Thus we want to measure this divergence from the references and the language expected in the reviews. Hence a natural measure is the KL-divergence measure the distance between the background language model and the aspect lan- guage models. We use the largest deviation, max i {D KL (θ i ||θ B )} as feature D2. For this fea- ture we tried different combinations such as a weighted average, but both features seemed to per- form equally so we only describe one of them.</p><p>By considering the context of the references in the reviews we can distinguish which statements should be considered as humorous, thus we also use the relative weight for each aspect to mea- sure unexpectedness. Formally we have U j = π j D KL (θ j ||θ B ), lastly we will denote max i {U i } these set of features as U2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline features from previous work</head><p>For completeness, we also include a description of all the baseline features used in our experiments; they represent the state of the art in defining fea- tures for this task. These features described be- low do not use any external text sources (leverag- ing external text sources is a novel aspect of our work), and they are more contextual and syntacti- cal based features. We describe some of the most promising features, which have previously shown to be useful in identifying humor in text. Context features: Due to the popular success of context features by <ref type="bibr" target="#b13">Mihalcea and Pulman (2007)</ref> we tried the following features content related fea- tures: C1: the uni-grams in the review.</p><note type="other">2 C2: length of the review. C3: average word length. C4: the ratio of uppercase and lowercase charac- ters to other characters in the review text. Alliteration: Inspired by the success that Mihal- cea and Strapparava (2006) had using the presence and absence of alliteration in jokes, we developed a similar feature for identifying funny reviews. We used CMU's pronunciation dictionary 3 to extract the pronunciation to identify alliteration chains, and rhyme chains in sentences. A chain is a con- secutive set of words which have similar pronun- ciation, for</note><p>example if the words words "scenery" and "greenery" are consecutive they would form a rhyme chain. Similarly, "vini, vidi, visa" also forms another chain this time an alliteration chain. We used the review's total number of alliteration chains and rhyme chains and denote it by E1. Note that there could be different lengths of chains, we experimented with some variations but they per- formed roughly the same, for simplicity we did not describe them here. Ambiguity: Ambiguity in word interpretation has also been found to be useful in finding jokes. The reasoning is that if a word has multiple interpreta- tion it is possible that the author intended another interpretation of the word instead of the more com- mon one. We restricted the words in the reviews to only nouns and used Wordnet 4 to extract the synsets for these words. Then we counted the av- erage number of synsets for each of these words, finally we took the mean score for all the words in the reviews. We call these features lexical ambi- guity and denote it by E2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>For our experiments we obtained the reviews from the Yelp Dataset Challenge 5 , this dataset con- tains over 1.6 million reviews from 10 different cities. We also crawled reviews from Yelp in the Los Angeles area which is not included in the <ref type="bibr">[</ref>  Yelp Dataset Challenge. This dataset was particu- larly interesting since the readers are able to vote whether a review is considered cool, funny, and/or helpful. It also allows the flexibility for the review- ers to write longer pieces of text to express their overall rating of a restaurant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Identifying Aspects in Reviews</head><p>We use recent advancements in Wikification, which aims to connect important entities and con- cepts in text to Wikipedia, it is also known as dis- ambiguation to Wikipedia. In particular we use the work of <ref type="bibr" target="#b18">Ratinov et al. (2011)</ref>, in order to obtain the Wikipedia pages of the entities in the reviews, we call these aspects of the review. Using the Wikipedia description of the aspects we can com- pute the language models for each aspect. Using mitlm, the MIT language modeling toolkit by <ref type="bibr" target="#b5">Hsu and Glass (2008)</ref>, we apply Modified Kneser-Ney smoothing to obtain the language models from the Wikipedia pages obtained from review's aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Preliminaries and Groundtruth Construction</head><p>In <ref type="figure" target="#fig_2">Figure 3</ref> we give an account of data statistics based on a random sample of 500,000 reviews, fo- cusing on the funny voting judgements and the star rating distributions. In <ref type="figure" target="#fig_2">Figure 3a</ref>, we no- tice that on average the highly rated restaurants tend to have more reviews. Since users would   <ref type="table">Table 1</ref>: Classification accuracies, using 5-fold cross validation, the 95% confidence is given inside the parenthesis.</p><p>prefer to dine in a restaurant expecting to get a better overall experience, they create a feedback on the reviews for those highly rated restaurants. This "rich-get-richer" effect has been also been re- cently observed in other social networks ( <ref type="bibr" target="#b24">Su et al., 2016)</ref> and a more detailed analysis is out of scope of this paper. We observe that most of the re- views receive a low number of funny votes in <ref type="figure" target="#fig_2">Fig- ure 3b</ref>, with µ = 0.55, where µ is the average funny rating. Computing the restaurant's average funny votes, then taking the mean by the star rat- ings for each category range, see <ref type="figure" target="#fig_2">Figure 3c</ref>, which seems to be consistently increasing across the dif- ferent star ratings. Note that this also includes the restaurants with zero funny votes, by excluding these we found that the ratings were more con- sistently stable on about 2.1 votes. Thus regard- less of restaurant rating, the funny reviews dis- tribution are stable on average. Considering the prevalence of noise in the voting process, we also analysed those reviews with more than one funny vote (µ = 3.90), and with more than two votes (µ = 5.54).</p><p>To construct our ground-truth data, we took all of the reviews at least five funny votes, which indi- cates the review was collectively funny, and con- sidered those as humorous reviews, we consid- ered all the reviews with zero funny votes as non- humorous reviews. We obtained 17,769 humorous reviews and 856,202 non-humorous, from which we sampled 12,000 reviews from each category, and another 5,000 reviews was left for a develop- ment dataset, to obtain a corpus with 34,000 re- views total. In total we collected 2,747 wikipedia pages with an average of about 247 sentences per page. In our work we focused on identifying dis- tinguishing features and relative improvement in a balanced dataset and while the true distribution may be skewed, we leave the unbalance distribu- tion study for future work.</p><p>Finally we use five-fold cross validation to eval- uate all the methods. Due to the success of linear classifiers in text classification tasks we were in- terested in studying the Perceptron and Adaboost algorithms, we also used a Naive Bayes classi- fier which has been shown to perform relatively well in humor recognition tasks <ref type="bibr" target="#b14">(Mihalcea and Strapparava, 2006</ref>). We used the Learning Based Java (LBJava) toolkit by <ref type="bibr" target="#b22">Rizzolo and Roth (2010)</ref> for the implementation of all the classifiers and used their recommended parameter settings. For the Averaged Perceptron implementation, we used a learning rate of 0.05 and thickness of 5. In Adaboost, we choose BinaryMIRA as our weak learner to do our boosting on. We also considered SparseWinnow and SparseConfidenceWeighted to be our weak learner as well, but the boosting per- formance for those two learners is marginal on the development set. <ref type="bibr">6</ref> All experiments were run on an Intel Core i5-4200U CPU with 1.60GHz running Ubuntu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Predicting Funny Reviews</head><p>We report the results of the features in <ref type="table">Table 1</ref>. First we can compare the accuracies of the indi- vidual features. For the content related features we see that the best features is C1, which is con- sistent to what others have found in humor recog- nition research <ref type="bibr" target="#b13">(Mihalcea and Pulman, 2007)</ref>. The other content related features are based on some popular features for detecting useful reviews, how- ever we notice that in the humor context it is not very effective. The performance of the contextual features could indicate that humor is not specific to a particular context and thus comparing differ- ent context between humorous and non-humorous text will not always work.</p><p>For the alliteration and ambiguity features which were reported to be very useful in short text, such as one-liners and on Twitter, are not as useful in detecting humours reviews. The reason is pretty clear since when writing a funny review, the re- viewer does not worry about the limitation of text and thus their humor does not rush to a punch-line. Instead the reviewer is able to write a longer more creative piece, adhering to less structure. The fea- tures based on incongruity and unexpectedness, do really well in distinguishing the funny and non- funny reviews. For incongruity the best feature is A2, achieving about the same accuracy as unex- pectedness features of about 83% accuracy.</p><p>The best feature was D2 achieving an accuracy of around 84% accuracy. The features seem to be consistent over all of our classifiers. This indi- cates that incorporating background text sources to identify humor in reviews is crucial, and our features we can indirectly capture some common knowledge, e.g. prior knowledge. In particu- lar it provides evidence that humor in online re- views can be better categorized as referential hu- mor (Ritchie, 2009) rather then shorter jokes. The results also suggest that we can use these features to help predict the style of humorous text.</p><p>Exploring this would be an interesting venue for future work. When we combine our features for the classification task and find that the best com- bination is the incongruity features with the diver- gence features. We do not report the results for features E1, E2 and other context features, C2, C3, C4, since their performance when combined with other features did not add to the accuracy of the more discriminant feature. The divergence fea- ture D2 plays a big role in the accuracy perfor- mance. This is in line with our hypothesis that the more uncommon language used the more it is pos- sible to be for a humorous purpose.</p><p>It is interesting to see that AdaBoost performed the best out of all three classifiers achieving about 86% accuracy, especially when more features were added, the classifier was able to use this in- formation for improvement. While Naive Bayes and the Perceptron algorithm did not make such improvement achieving about 85% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ranking Funny Reviews</head><p>From the data we noticed that funny reviews tend to be voted highly useful,in particular we noticed a correlation coefficient of 0.77. Although it would have been easy to use the useful votes as a fea- ture to determine whether the review is funny/not funny, these scores are only available after people have been exposed to these reviews. To test how well the features worked when identifying help- ful reviews, in a more realistic setting, we formu- lated a retrieval problem. Given a set of reviews, D = {R 1 , R 2 , ..., R m } and relevant scores based on usefulness, U = {u 1 , u 2 , ..., u m }, is it possible to develop a scoring function such that we rank the useful reviews higher? For this task we used the classification output of Naive Bayes, P (funny|R i ) where i is the current example under considera- tion, for our scoring function and trained with the best performing features in the original dataset. We used a with-held dataset crawled from restau- rants in Yelp in the Los Angeles area containing about 1,360 reviews with 260 reviews labelled as helpful and the other reviews labelled as not help- ful. To obtain the ground truth we used the useful votes in Yelp similar to how we constructed the funny labels, using a threshold of 5 votes mini- mum to be considered helpful. This experiment reveals two things about our features for detect- ing humorous reviews. First we see that the preci-</p><formula xml:id="formula_8">K Precision @ K 1</formula><p>1.00 10 0.50 25 0.48 50 0.44 100 0.45 200 0.54 <ref type="table">Table 2</ref>: Precision of useful reviews.</p><p>sion is around 50%, see <ref type="table">Table 2</ref>, this is more than two times better than random guess which is about 19% and second that our features can be used to filter out some useful reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Although there has been much work in the the- ory of humor by many linguists, philosophers and mathematicians <ref type="bibr" target="#b17">(Paulos, 2008)</ref>, the definition of humor is still a debated topic of research <ref type="bibr" target="#b0">(Attardo, 1994)</ref>. There have been many applications from computational humor research; for instance, cre- ating embodied agents using humor, such as chat bots, which could allow for more engaging inter- actions and can impact many domains in education ( <ref type="bibr" target="#b1">Binsted et al., 2006</ref>). Existing work on computa- tional humor research can typically be divided into humor recognition and humor generation. In humor generation, some systems have suc- cessfully generated jokes and puns by exploiting some lexical structure in the pun/joke ( <ref type="bibr" target="#b8">Lessard and Levison, 1992;</ref><ref type="bibr" target="#b10">Manurung et al., 2008;</ref><ref type="bibr" target="#b12">McKay, 2002</ref>). The HAHAcronym project was able to take user inputs and output humorous acronyms and it achieves comical effects by exploiting incongruity <ref type="bibr" target="#b23">(Stock and Strapparava, 2002</ref>). Work in automatic generation of humor is limited to particular do- mains, usually only generating short funny texts.</p><p>One of the earliest work on humor recognition in text data is the work of <ref type="bibr">Mihalcea and Strapparave (2006)</ref>, trying to identify one-liners, short sentences with a humorous effect. They frame the problems as a classification problem and develop surface features (alliteration, antonym, and adult slang) as well as context related features. They ul- timately proposed that additional knowledge such as, irony, ambiguity, incongruity, and common sense knowledge among other things would be beneficial in humor recognition, but they do not further pursue these avenues. Although they are able to distinguish between humorous and non- humorous one liners, in longer of texts such as re- views it is not so clear that these features suffice. Instead we make use of the creative writing struc- ture of the reviewers by looking at the referenced entities in their reviews.</p><p>Although verbal irony can be humorous, and an active topic of research <ref type="bibr" target="#b26">(Wallace, 2013)</ref>, it is of- ten defined as the "opposite to what the speaker means", and combining features for identifying both humor and irony has been studied (see, e.g., <ref type="bibr" target="#b20">Reyes et al. (2012)</ref>).</p><p>In the work by <ref type="bibr" target="#b20">Reyes et al. (2012)</ref>, the authors defined the unexpected- ness feature as semantic relatedness of concepts in Wordnet and assuming that the less the semantic relatedness of concepts the funnier the text. In our work we use a similar definition but applying it to the "topical" relatedness of the referenced aspects and the background language model. The authors demonstrate that irony and humor share some sim- ilar characteristics and thus we can potentially use similar features to discriminate them. There has been some early work in identifying humor fea- tures in web comments ( <ref type="bibr" target="#b19">Reyes et al., 2010)</ref>, in these comments the users are able to create humor through dialogue thus making the problem more complex. More recently there was a workshop in SemEval-2017 <ref type="bibr">7</ref> , which focus is on identifying humorous tweets which are related, typically as a punchline, to a particular hashtag. <ref type="bibr" target="#b7">Kiddon and Brun (2011)</ref> aimed to understand "That's what she said" (TWSS) jokes, which they classify as double entendres. They frame the prob- lem as metaphor identification and notice that the source nouns are euphemisms for sexually explicit nouns. They also make use of the common struc- ture of the TWSS jokes to the erotic domains to improve 12% in precision over word-based fea- tures. In our work we try to explicitly model the incongruity of the reviewer, by doing so we are able to distinguish the separate language used by the user when introducing humorous concepts. Recently there has been work in consumer re- search, to identify the prevalence of humor in so- cial media <ref type="bibr" target="#b11">(McGraw et al., 2015</ref>). The main focus was to examine the benign violation theory, which "suggest that things are humorous when people perceive something as wrong yet okay". One of their finding suggests that humor is more preva- lent in complaints than in praise, thus motivating the usage of automatic humor identification meth- ods for restaurants regardless of its popularity.</p><p>While there is a breadth of work in identifying helpful reviews and opinion spam in reviews <ref type="bibr" target="#b6">(Jindal and Liu, 2008)</ref> as well as deceptive opinion spam <ref type="bibr" target="#b16">(Ott et al., 2011)</ref>, and synthetic opinion spam <ref type="bibr" target="#b25">(Sun et al., 2013)</ref>; we show that humour can also be used to identify helpful reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have studied humorous text identification in a novel setting involving online reviews. This task has not been studied in the previous work and is different than detecting humorous jokes or one- liners, this allows for creative and expressive writ- ing since the reviewer is not limited in text. In this problem we cannot directly apply the ideas that others have developed in order to identify the hu- morous reviews. Instead features that are based on the theory of incongruity are shown to outperform previous features and are effective in the classi- fication task. Our model introduces a novel and way to incorporate external text sources for humor identification task, and which can be applied to any natural language provided there is a reference database, i.e. news articles or Wikipedia pages, in that language. We also show that the features developed can also be used to identify helpful re- views. This is very useful in the online review set- ting since there tends to be a cumulative advan- tage, that is the "rich get richer" effect which lim- its the exposure that the users get to other helpful reviews. Thus identifying these types of review early can potentially diversify the types of reviews that the users read.</p><p>Although we used a background language model on the entire corpus to capture a sense of expectation, there could be other ways to do this. For example, we could develop neural network embeddings to capture the entities descriptions in the reviews. Another direction would be to use topic models and see whether reviewers are more inclined to compare different types of references when talking about certain aspects of restaurants or other products. A different approach to identi- fying helpful reviews would be to create entertain- ing and informative summaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A funny review (left), with K d = 3, aspect topics (right) contain words in their corresponding language model, probabilities removed for clarity, the colored (bracketed) word correspond to a different aspect assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Generation model for reviews, where the dth review has K d aspects in the review. The shaded nodes here are the observed data and the light node z are the latent variables corresponding to aspect assignments. tion of the background language and the language of the references as shown in Figure 2. These aspects provide some context to the underling meaning of a review; the reviewers use these aspects for creative writing when describing their dining experience. These aspects allow us to use external information as the context, thus we develop measures for incongruity addressing the juxtaposition of the aspect's context and the review. The review construction process is represented in a generative model, see Figure 2, where the shaded nodes represent our observations, we have observed the words as well as the referenced aspects which the reviewer has mentioned in their review. The light nodes are the labels for the aspect which has generated the corresponding word. Since the background language model, denoted by θ B , is review independent, we can simplify the generative model by copying the background language model for each review, thus we can focus on the parameter estimation for each review in parallel. A key component to the success of our features is the mesh of background text from external sources, or background text sources, and the reviews. In our example, Figure 1, Nyquil is a critical component for understanding the humor. However it is difficult to understand some references a reviewer makes without any prior knowledge. To do so, we incorporate external background knowledge in the form of language models for the referenced aspect present in the reviews. If the reviewer has made K d references to different aspects A d in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Mean average number of reviews for restaurants falling in five different star rating ranges. (b) Log occurrences of funny votes per review. (c) Mean average voting judgements for restaurants in different star ratings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Features</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="2"> We also considered content-based features derived from PLSA topic weights, however the unigram features outperform these features, thus we exclude them for lack of space. 3 www.speech.cs.cmu.edu/cgi-bin/cmudict 4 http://wordnet.princeton.edu/ 5 http://www.yelp.com/dataset_challenge</note>

			<note place="foot" n="6"> Since our main goal is to understand the effectiveness of various features we did not further tune these parameters since they are presumably orthogonal to the question we study.</note>

			<note place="foot" n="7"> http://alt.qcri.org/semeval2017/ task6/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The first author was supported by the University of Illinois, Urbana-Champaign College of Engineer-ing's Support for Underrepresented Groups in En-gineering (SURGE) Fellowship and the Graduate College's Graduate Distinguished Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Linguistic theories of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Attardo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Walter de Gruyter</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computational humor. Intelligent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Binsted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalu</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D O&amp;apos;</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iterative language model estimation: efficient data structure &amp; algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Opinion spam and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Web Search and Data Mining</title>
		<meeting>the 2008 International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">That&apos;s what she said: double entendre identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuriy</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational modelling of linguistic humour: Tom swifties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Lessard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Levison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALLC/ACH Joint Annual Conference</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="175" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Investigating task performance of probabilistic topic models: an empirical study of plsa and lda. Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="178" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The construction of a pun generator for language skills development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruli</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalu</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><forename type="middle">O</forename><surname>Mara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="841" to="869" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Peter Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Humorous complaining. Journal of Consumer Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1171" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generation of idiom-based witticisms to aid second language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Characterizing humour: An exploration of features in humorous texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="337" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to laugh (automatically): Computational models for humor recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="126" to="142" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards humor modelling and facilitation in smart environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Nijholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Affective and Pleasurable Design</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding deceptive opinion spam by any stretch of the imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey T</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mathematics and humor: A study of the logic of humor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulos</forename><surname>John Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local and global algorithms for disambiguation to wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating humour features on web comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From humor recognition to irony detection: The figurative language of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Can computers create humor? AI Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><forename type="middle">Ritchie</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning based java for rapid development of nlp systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hahacronym: Humorous agents for humorous acronyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliviero</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<editor>Carlo Strapparava, and Anton Nijholt</editor>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="125" to="135" />
			<pubPlace>Stock, Oliviero</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The effect of recommendations on network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1157" to="1167" />
		</imprint>
	</monogr>
	<note>ternational World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synthetic review spamming and defense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1088" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Computational irony: A survey and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
