<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Entities and Relations with Joint Minimum Risk Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2256</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
							<email>{changzhisun}@stu.ecnu.edu.cn {ybwu,mlan,slsun}@cs.ecnu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">East China Normal University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Chih</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewen</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Entities and Relations with Joint Minimum Risk Training</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2256" to="2265"/>
							<date type="published">October 31-November 4, 2018. 2018. 2256</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the task of joint entity relation extraction. Unlike prior efforts, we propose a new lightweight joint learning paradigm based on minimum risk training (MRT). Specifically, our algorithm optimizes a global loss function which is flexible and effective to explore interactions between the entity model and the relation model. We implement a strong and simple neural network where the MRT is executed. Experiment results on the benchmark ACE05 and NYT datasets show that our model is able to achieve state-of-the-art joint extraction performances .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Detecting entities and relations is usually the first step towards extracting structured knowledge from plain texts. Its goal is to identify text spans repre- senting typed objects (entities) and semantic rela- tions among those text spans (relations). For ex- ample, in the following sentence, "Associate Press" is an organization entity (ORG), "writer" is a person entity (PER), and the two en- tities have an affiliation relation (ORG-AFF).</p><p>Two types of models have been applied to the extraction task, the pipeline model and the joint model. In the pipeline setting, the task is bro- ken down into independently learned components (an entity model and a relation model). De- spite its flexibility, the pipeline ignores interac- tions between the two models. For example, the entity model doesn't look at relation annotations which are useful for identifying entities (e.g., if an ORG-AFF relation exists, the entity model can only assign ORG and AFF to its entities). The joint setting, on the other hand, extracts entities and relations in a unified model, which can ex- plore shared information and alleviate error prop- agations between models. Here we will focus on joint models.</p><p>One simple joint learning paradigm is through sharing parameters <ref type="bibr" target="#b14">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b9">Katiyar and Cardie, 2017)</ref>. Typically, instead of train- ing two independent models, the entity and rela- tion model can share some input features or in- ternal hidden states. It has an advantage that no additional constraint is required on the two sub- models. But the connections among sub-models are still not fully explored due to independent sub- model decoders. For example, to get signals from relation annotations, the entity model needs to wait for the relation model to update the shared parameters. To further utilize the interaction be- tween decoders, some complex joint decoding al- gorithms (e.g., simultaneously decoding entities and relations in beam search) have been carefully studied ( <ref type="bibr" target="#b12">Li and Ji, 2014;</ref><ref type="bibr" target="#b8">Katiyar and Cardie, 2016;</ref><ref type="bibr" target="#b27">Zhang et al., 2017;</ref><ref type="bibr" target="#b28">Zheng et al., 2017)</ref>. In this paradigm, it is important (and hard) to make a good balance between the exactness of the joint decoding algorithm and capacities of individual sub-models.</p><p>In this work, we propose a joint minimum risk training (MRT) <ref type="bibr" target="#b17">(Och, 2003;</ref><ref type="bibr" target="#b22">Smith and Eisner, 2006</ref>) method for the entity and relation extraction task. It provides a lightweight way to strengthen connections between the entity model and the rela- tion model, and keeps their capacities unaffected. Given an input x and a loss function ∆(ˆ y, y) (measuring the difference between model outputˆy outputˆ outputˆy and the true y), MRT seeks a posterior P (ˆ y|x) to minimize the expected loss E ˆ y∼P (ˆ y|x) ∆(ˆ y, y). Comparing with prior joint decoding algorithms, the MRT-based algorithm is simple and can be ap- plied to a broad range of entity relation extraction models without changing the original sub-models and decoders <ref type="figure" target="#fig_1">(Figure 1)</ref>.</p><p>One advantage of the MRT-based method is that it can explicitly optimize a global sentence-level loss (e.g., F1 score) rather than local token-level losses. Therefore, it may catch more sentence- level information in the training time and match evaluation metrics better in the testing time. Fur- thermore, besides the handcrafted losses, we also try to directly learn a loss function from data dur- ing the joint MRT process. The automatically ob- tained loss would help MRT to calibrate its risk estimation with knowledge from the data distribu- tion. On the other hand, comparing with preivous single task MRT, the joint MRT algorithm here will integrate messages from different sub-models, which is the key step for enhancing decoder in- teractions in the joint learning. As a result, the training of the entity model now can directly ac- knowledge the loss of the relation model (without waiting for shared parameters) and vice versa.</p><p>We compile the proposed joint MRT with a strong neural network-based model which uses re- current neural networks (RNN) in the entity model and convolutional neural networks (CNN) in the relation model. On benchmark ACE05 and NYT datasets, we show that the new RNN+CNN struc- ture outperforms previous neural network-based models. After adding the joint MRT, our model is able to achieve state-of-the-art performances.</p><p>To summarize, our main contributions include 1. proposing a new joint learning paradigm based on minimum risk training for the joint entity relation extraction task.</p><p>2. implementing a strong and simple neural- network-based entity relation extraction model which carries the proposed MRT algorithm. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In many pipelined entity relation extraction sys- tems, one first learns an entity model, then learns a relation model based on entities generated by the entity model ( <ref type="bibr" target="#b15">Miwa et al., 2009;</ref><ref type="bibr" target="#b3">Chan and Roth, 2011;</ref><ref type="bibr" target="#b13">Lin et al., 2016)</ref>. Such systems are often flexible to incorporate different data sources and different learning algorithms. However, they may also suffer from error propagation and data ineffi- ciency. To tackle the problem, many recent studies try to develop joint extraction algorithms. Parameter sharing is a basic strategy in joint learning paradigms. For example, in <ref type="bibr" target="#b14">(Miwa and Bansal, 2016</ref>), the entity model is a sentence-level RNN, and the relation model is a dependency tree path RNN which takes hidden states of the en- tity model as features (i.e., the shared parame- ters). Our basic extraction model is similar to theirs but with a CNN-based relation model. Simi- larly, <ref type="bibr" target="#b9">Katiyar and Cardie (2017)</ref> build a simplified relation model on the entity RNN using the atten- tion mechanism.</p><p>To further explore interactions between the en- tity decoder and the relation decoder, some joint decoding algorithms were studied. For example, <ref type="bibr" target="#b8">Katiyar and Cardie (2016)</ref> propose a CRF-based model which conducts joint decoding with aug- mented transition matrices. <ref type="bibr" target="#b28">Zheng et al. (2017)</ref> propose to directly encode relations in the sequen- tial labelling tag set. Both of them are exact decod- ing algorithms, but they need adding constraints on the relation model (e.g., <ref type="bibr" target="#b28">Zheng et al. (2017)</ref> cannot handle entities which appear in multiple re- lations). On the other side, Li and Ji (2014) de- velop a joint decoding algorithm based on beam search. <ref type="bibr" target="#b27">Zhang et al. (2017)</ref> study a globally nor- malized joint model. They retain capacities of sub- models, while their decoding algorithms are inex- act. Here, we introduce MRT to the task, which is a more lightweight setting of joint learning.</p><p>Minimum risk training is a learning framework which tries to handle models with arbitrary dis- crepancy metrics (i.e., losses of a model output w.r.t. the true answer) <ref type="bibr" target="#b17">(Och, 2003;</ref><ref type="bibr" target="#b22">Smith and Eisner, 2006</ref>; <ref type="bibr" target="#b4">Gimpel and Smith, 2010)</ref>. It has been successfully applied to many NLP tasks. Some recent work include <ref type="bibr" target="#b5">(He and Deng, 2012;</ref>) which apply MRT to (neural) ma-chine translation, ( <ref type="bibr" target="#b25">Xu et al., 2016</ref>) which develops a shift-reduce CCG parser to directly optimize F1, and ( <ref type="bibr" target="#b1">Ayana et al., 2016</ref>) which uses a MRT-based model for summarization. We note that most pre- vious applications of MRT focus on a single job, while the joint entity relation extraction consists of two sub-tasks. Investigating MRT in joint learning scenarios is the main topic of this work.</p><p>Finally, the sampling algorithm of solving MRT is similar to the policy gradient algorithm in rein- forcement learning (RL) <ref type="bibr" target="#b23">(Sutton and Barto, 1998</ref>). Some recent NLP applications which share the key idea of MRT but are described with RL language also show promising results (e.g., dialog systems ( <ref type="bibr" target="#b11">Li et al., 2016)</ref>, machine translation ( <ref type="bibr" target="#b16">Nguyen et al., 2017)</ref>). The idea of learning loss functions from data is similar to inverse reinforcement learn- ing ( <ref type="bibr" target="#b0">Abbeel and Ng, 2004;</ref><ref type="bibr" target="#b18">Ratliff et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Approach</head><p>We define the joint entity and relation extraction task following the setting of <ref type="bibr" target="#b14">(Miwa and Bansal, 2016)</ref>. Given an input sentence s = w 1 , . . . , w |s| (w i is a word), the task is to extract a set of enti- ties E and a set of relations R. An entity e ∈ E is a sequence of words labelling with an entity type (e.g., person (PER), organization (ORG)). Let T e be the set of possible entity types. A relation r is a triple (e 1 , e 2 , l), where e 1 and e 2 are two entities, l is a relation type describing the semantic relation between e 1 and e 2 (e.g., organization affiliation re- lation (ORG-AFF)). Let T r be the set of possible relation types.</p><p>In our joint extraction method <ref type="figure">(Figure 2</ref>), we treat entity detection as a sequence labelling task (Section 3.1) and relation detection as a classifi- cation task (Section 3.2). Models of the two tasks share parameters and are trained jointly. Departing from previous joint learning algorithms <ref type="bibr" target="#b14">(Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b9">Katiyar and Cardie, 2017;</ref><ref type="bibr" target="#b27">Zhang et al., 2017)</ref>, we introduce minimum risk training to the joint extraction model. It optimizes a global loss function and bridges the discrepancy between training and testing (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity Detection</head><p>To represent entities in s, we assign a tag t i to each word w i following the BILOU tagging scheme: t i takes a value in {B- * , I- * , L- * , O, U- * }, where B, I, L and O denote the begin, inside, end and outside of an entity, U denotes a single word en- tity and * ∈ T e represents different entity types. For example, for a person (PER) entity "Patrick McDowell", we assign B-PER to "Patrick" and L-PER to "McDowell". Given an input sentence s, the entity model predicts the tags of wordsˆt wordsˆwordsˆt = ˆ t 1 , ˆ t 2 , . . . , ˆ t |s| by learning from the true tags t = t 1 , t 2 , . . . , t |s| .</p><p>We use a bidirectional long short term memory (bi-LSTM) network (Hochreiter and Schmidhu- ber, 1997) to solve the sequence labelling task. At each sentence position i, a forward LSTM chain computes a hidden state vector ⃗ h i by recursively collecting information from the beginning of s to the current position i. Similarly, a backward LSTM chain collects information ⃗ h i from the end of s to the position i.</p><formula xml:id="formula_0">⃗ h i = LSTM(x i , ⃗ h i−1 ; ⃗ θ), ⃗ h i = LSTM(x i , ⃗ h i+1 ; ⃗ θ).</formula><p>The word representation x i of w i has two parts</p><formula xml:id="formula_1">x i = w i ⊕ c i (⊕ is the vector concatenation). w i</formula><p>is a word embedding of word w i (from an embed- ding look-up table W e ). c i is a character-based representation of w i which is obtained by running a convolution neural network on the character se- quence of w i :</p><formula xml:id="formula_2">c i = CNN(char(w i ); θ c ).</formula><p>To predict the tagˆttagˆ tagˆt i , we combine the forward and the backward hidden vector to h i = ⃗ h i ⊕ ⃗ h i , and apply a softmax function on h i to get the pos- terior ofˆtofˆ ofˆt i ,</p><formula xml:id="formula_3">P ent ( ˆ t i |s; θ E ) = Softmax(W E · h i ), (1)</formula><p>where θ E = {W e , θ c , ⃗ θ, ⃗ θ, W E } are parameters of the entity model. Given an input sentence s and its ground truth tag sequence t, the training objec- tive is to minimize L ent , 2</p><formula xml:id="formula_4">L ent (θ E ) = − 1 |s| |s| ∑ i=1 log P ent ( ˆ t i = t i |s; θ E ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Detection</head><p>Given a set of detected entitiesˆEentitiesˆ entitiesˆE (obtaining from the entity tag sequencê t), we consider all en- tity pairs inˆEinˆ inˆE as candidate relations. The task of relation detection is to predict a relation type l ∈ T r for each pair, <ref type="bibr">3</ref> and output a relation set</p><formula xml:id="formula_5">f e 1 f e 2 f left f right f dist l + h i v i B-PER B-GPE L-GPE L-PER O PER GPE CNN CNN CNN MLP word (w) char (c) bi-LSTM (h) Softmax Patrick McDowell in Kuwait City f middle h i v i L ent + L rel L mrt tags ( ˆ t)</formula><p>Joint Learning with MRT global loss local losses</p><p>Figure 2: Our network structure for the joint entity and relation extraction.</p><formula xml:id="formula_6">ˆ R = {(e 1 , e 2 , l)|e 1 , e 2 ∈ ˆ E, e 1 ̸ = e 2 , l ∈ T r }.</formula><p>To build the relation model, we extract two types of features, namely, features regarding words in e 1 , e 2 and features regarding contexts of the entity pair (e 1 , e 2 ).</p><p>To extract features on words in e 1 , e 2 , we use two convolutional neural networks. Taking e 1 as an example, for each word w i in e 1 , we first col- lect w i 's bi-LSTM hidden vector h i from the en- tity model. Then, we concatenate h i with a one- hot entity tag representation v i ofˆtofˆ ofˆt i . We build a feature vector f e 1 for e 1 by running a CNN (a sin- gle convolution layer with a max-pooling layer) on vectors {h i ⊕ v i |w i ∈ e 1 }. Similarly, we build f e 2 for e 2 with another CNN.</p><p>For context features of the entity pair (e 1 , e 2 ), we build three feature vectors by looking at words between e 1 and e 2 (f middle ), words on the left of the pair (f left ) and words on the right of the pair (f right ). For f middle , we run a CNN on words between e 1 and e 2 like the case of f e 1 , f e 2 . For f left and f right , we use the "LSTM-Minus" method as ( <ref type="bibr" target="#b24">Wang and Chang, 2016;</ref><ref type="bibr" target="#b27">Zhang et al., 2017)</ref>. Assume that the left context of (e 1 , e 2 ) is from sentence position 0</p><formula xml:id="formula_7">to i, then f left = ⃗ h i ⊕ ( ⃗ h 0 − ⃗ h i+1 ). Similarly, if the right context of (e 1 , e 2 ) is from j to |s| − 1, then f right = ( ⃗ h |s|−1 − ⃗ h j−1 ) ⊕ ⃗ h j .</formula><p>We also use a one- hot feature f dist to describe the distance between e 1 and e 2 in the sentence.</p><p>Finally, f e 1 , f e 2 , f middle , f left , f right and f dist are concatenated to a single vector f e 1 ,e 2 . To get the posterior of the relation typê l, we apply a multi- layer perceptron with one hidden layer on f e 1 ,e 2 ,</p><formula xml:id="formula_8">P rel ( ˆ l|s, e 1 , e 2 ; θ R ) = Softmax(W 2 · ReLU(W 1 · f e 1 ,e 2 )), (2)</formula><p>where θ R = {θ e 1 , θ e 2 , θ middle , W 1 , W 2 } contains parameters of the relation model (shared parame- ters with the entity model are omitted).</p><p>Given an input sentence s, the training objective is to minimize</p><formula xml:id="formula_9">L rel (θ R ) = − ∑ e 1 ,e 2 ∈ ˆ E e 1 ̸ =e 2 log P rel ( ˆ l = l|s, e 1 , e 2 ; θ R ) | ˆ E|(| ˆ E| − 1) ,</formula><p>where the true label l of a candidate entity pair (e 1 , e 2 ) can be read from true annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Minimum Risk Training</head><p>To jointly learn the entity model and the relation model, one common strategy is to optimize the combined objective function</p><formula xml:id="formula_10">L = L ent + L rel ,</formula><p>where the joint learning is accomplished by the shared parameters. However, we would think that L optimizes a "local" loss by observing that a) in both L ent and L rel , the loss functions are calcu- lated by only looking at local parts. For example, the loss in L ent is based on the correctness of lo- cal entity tags t i rather than a global measurement (e.g., F1 score of extracted entities), b) both the entity model and the relation model are unaware of the loss from the other side. For example, the entity model needs to wait for the relation model to update the shared parameters rather than get direct supervision from the loss of the relation model. Here we introduce the minimum risk training framework to the joint model. Comparing with optimizing the local loss in L, the joint MRT will optimize a global loss and provide a tighter con- nection between the entity decoder and the rela- tion decoder. To illustrate the algorithm, we first aggregate some notations. Let y ≜ (E, R) contain the ground truth entity tag sequence and relations, ˆ y ≜ ( ˆ E, ˆ R) contain outputs of the joint extraction model and Y(s) be the set of all possible outputs of the input sentence s (y, ˆ y ∈ Y(s)). We define the joint probability,</p><formula xml:id="formula_11">P (ˆ y|s; θ) = P ( ˆ E|s; θ E )P ( ˆ R|s, ˆ E; θ R ) = ∏ i P ent ( ˆ t i |s; θ E ) ∏ e 1 ,e 2 ∈ ˆ E e 1 ̸ =e 2 P rel ( ˆ l|s, e 1 , e 2 ; θ R ),</formula><p>where θ = θ E ∪ θ R is the joint model parameter, and P ent , P rel are in Equation 1 and 2.</p><p>The objective of MRT is to minimize the fol- lowing expected loss (i.e., risk),</p><formula xml:id="formula_12">E ˆ y∼P (ˆ y|s;θ) ∆(ˆ y, y) = ∑ ˆ y∈Y(s) P (ˆ y|s; θ)∆(ˆ y, y),<label>(3)</label></formula><p>where ∆(ˆ y, y) is a (arbitrary) loss function de- scribing the difference betweenˆybetweenˆ betweenˆy and y. In our model, the loss function ∆(ˆ y, y) is the key factor to enhance the joint extraction perfor- mances. First, in ∆(ˆ y, y), we consider sentence- level F1 scores of entity and relation extraction re- sults (denoted by</p><formula xml:id="formula_13">F ent ( ˆ E, E), F rel ( ˆ R, R)). Specif- ically, we use 1 − F ent ( ˆ E, E) and 1 − F rel ( ˆ R, R)</formula><p>as the metric of the entity loss and the relation loss respectively. On the one hand, F1 scores charac- terize the overall performance of the outputs and make the training objective be consistent with the testing time evaluation metric. On the other hand, F1 scores cannot be decomposed onto local pre- dictions ofˆEofˆ ofˆE andˆRandˆ andˆR like the log losses in L ent and L rel , thus we need a different training algorithm.</p><p>Second, different from previous applications of MRT on single tasks ( <ref type="bibr" target="#b25">Xu et al., 2016;</ref>, we have two sources of losses in the joint extraction. By integrating losses of individual tasks in the learning algorithm, the entity model could forecast how plausible a candidate entity is according to the relation model, and the relation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The Sampling Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Entity model θE, relation model θR, sentence s, the sample size K Output: A subset Y ′ (s) of Y(s) 1: Y ′ (s) ← {(E, R)} // add the ground truth 2: while |Y ′ (s)| ≤ K do 3: i ← 1 4:</head><p>while i ≤ |s| do 5:</p><p>with prob. 0.9, sample t ′ i ∼ Pent(·|s; θE) 6:</p><p>with prob. 0.1, sample t ′ i uniformly 7:</p><p>i ← i + 1 8:</p><p>end while 9:</p><formula xml:id="formula_14">E ′ ← t ′ = t ′ 1 , t ′ 2 , · · · , t ′ |s| 10: R ′ ← ∅ 11:</formula><p>for e1, e2 ∈ ˆ E, e1 ̸ = e2 do 12:</p><p>sample l ′ ∼ P rel (·|s, e1, e2; θR) 13:</p><formula xml:id="formula_15">R ′ ← R ′ ∪ {(e1, e2, l ′ )} 14:</formula><p>end for 15:</p><formula xml:id="formula_16">Y ′ (s) ← Y ′ (s) ∪ {(E ′ , R ′ )} 16:</formula><p>end while model could also know the confidence of the en- tity extraction results. Here, we define a global loss by adding losses of the two models,</p><formula xml:id="formula_17">∆ E+R (ˆ y, y) = 1 − 1 2 [F ent ( ˆ E, E) + F rel ( ˆ R, R)].</formula><p>To compare with ∆ E+R , we also try two al- ternatives of ∆(ˆ y, y) in experiments, namely,</p><formula xml:id="formula_18">∆ E (ˆ y, y) = 1 − F ent ( ˆ E, E) and ∆ R (ˆ y, y) = 1 − F rel ( ˆ R, R).</formula><p>They only look one model's loss.</p><p>Third, in addition to handcrafted loss functions, we further ask whether the joint MRT model could benefit from automatic "loss engineering". Specif- ically, let Γ(ˆ y) be the loss learned from the train- ing set, we augment ∆(ˆ y, y) of the MRT objec- tive with Γ(ˆ y), and require the learning process to assign a smaller Γ value (with a margin) to the ground truth output y than otherˆyotherˆ otherˆy ∈ Y \{y},</p><formula xml:id="formula_19">min . ∑ ˆ y∈Y(s) P (ˆ y|s; θ) (∆(ˆ y, y) + Γ(ˆ y)) + ξ s.t. Γ(y * ) − Γ(y) ≥ 1 − ξ, ξ ≥ 0,<label>(4)</label></formula><p>where y * = arg minˆy∈Yminˆ minˆy∈Y (s) Γ(ˆ y). Here, we sim- ply set Γ(ˆ y) = 1 − P (ˆ y|s; θ) <ref type="bibr">4</ref>  </p><p>where [u] + = max(u, 0) is the hinge loss.</p><p>Optimizing the expected loss is hard since the size of Y(s) is exponential. In practice, we could approximate the expectation in Equation 3 by sam- pling a tractable subset Y ′ (s) of Y(s). Specifi- cally, we first obtain an entity set E ′ by sampling (without replacement) an entity tag sequence t ′ from P ent . <ref type="bibr">5</ref> Then based on the sampled entities, we get a relation set R ′ by sampling l ′ from P rel for each entity pairs. Algorithm 1 lists the pseudo code. <ref type="bibr">6</ref> In experiments, we also try a variant of Algorithm 1 which only samples from the entity model, and selects relation labels with the maxi- mum posterior (i.e., doesn't sample relations).</p><p>With the sampled subset Y ′ (s), we consider a revised version of the original MRT objective,</p><formula xml:id="formula_21">L mrt (θ) = ∑ ˆ y∈Y ′ (s) Q(ˆ y|s; θ, µ, α)∆(ˆ y, y),<label>(6)</label></formula><p>where Q(ˆ y|s; θ, µ, α) is a re-normalization of</p><formula xml:id="formula_22">P (ˆ y|s; θ) on the subset Y ′ (s), 7 Q(ˆ y|s; θ, µ, α) = 1 Z [P ( ˆ E|s, θ E ) µ P ( ˆ R|s, ˆ E, θ R ) 1−µ ] α Z = ∑ (E ′ ,R ′ )∈Y ′ (s) [P (E ′ |s, θ E ) µ P (R ′ |s, E ′ , θ R ) 1−µ ] α</formula><p>The hyper-parameter α controls the sharpness of the Q distribution <ref type="bibr" target="#b17">(Och, 2003)</ref>, and µ weights the importance of the entity model and the relation model in Q. Similary, we can rewrite the objec- tive in Equation 5 with Y ′ (s) and Q. Finally, we remark that if we view MRT as a fine tuning step, it can be applied in any joint learning model based on building the joint distri- bution P (ˆ y|s, θ) (e.g., the globally normalized P in ( <ref type="bibr" target="#b27">Zhang et al., 2017)</ref>). Thus, we would think that MRT is a flexible and lightweight framework for the joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>To train the joint extraction model, we first pre- train the model with objective L (i.e., minimize the local loss), then optimize the local loss and the global loss simultaneously with objective L + L mrt . The setting is slightly different from previ- ous work which only optimize L mrt in the second step. We find that adding L in the experiments could make the training more stable. <ref type="bibr">5</ref> To accelerate sampling, we borrow the idea of ε-greedy in reinforcement learning: with probability 0.9, we sample t ′ i from Pent, and with probability 0.1, we sample it uniformly. <ref type="bibr">6</ref> The time complexity is O(K|s|) which is the same to the beam search algorithm with beam size K ( <ref type="bibr" target="#b27">Zhang et al., 2017)</ref>. <ref type="bibr">7</ref> Here we follow the literature of MRT to apply the re- normalization on Y ′ (s). Another formulation is the policy gradient framework which sticks to the original probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When training with</head><p>L in the pre-training step, we apply the scheduled sampling strategy <ref type="bibr" target="#b2">(Bengio et al., 2015</ref>) in the entity model as <ref type="bibr" target="#b14">(Miwa and Bansal, 2016)</ref>. Models are regularized with dropout and trained using Adadelta <ref type="bibr" target="#b26">(Zeiler, 2012)</ref>. We give the full derivation of Equation 6's gradi- ent in the supplementary. <ref type="bibr">8</ref> We select models using development sets: within a fix number of epochs, the model with the best relation extraction performance on the devel- opment set is picked out for testing. <ref type="bibr">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed model on two datasets. ACE05 is a standard corpus for the entity relation extraction task. It is labelled with 7 entity types and 6 relation types. We use the same split of ACE05 documents as previous work (351 train- ing, 80 development, and 80 testing). 10 NYT ( <ref type="bibr" target="#b20">Riedel et al., 2010</ref>) is a larger corpus which is la- belled with 3 entity types and 24 relation types. <ref type="bibr">11</ref> The training set has 353k relation triples which are generated by distant supervision. It also provides another 3880 manually labelled relation triples. Following <ref type="bibr" target="#b19">(Ren et al., 2017;</ref><ref type="bibr" target="#b28">Zheng et al., 2017)</ref>, we exclude the None relation label and randomly select 10% of the labelled data as the develop- ment set. We will mainly discuss the results on ACE05 where many previous joint learning mod- els are available for comparison.</p><p>We list detailed hyper-parameter settings in the supplementary. Note that, except µ, α, K which are introduced in the joint MRT and selected on the development set, <ref type="bibr">12</ref> we don't tune hyper- parameters extensively. For example, we use the same setting in both ACE 05 and NYT rather than tune parameters on each of them.</p><p>As previous work, we evaluate performances <ref type="bibr">8</ref> We remark that the MRT objective (Equation 6) is dif- ferentiable with respect to model parameters ( . The non-decomposability of the F1 score does not make the model non-differentiable. In our implementation, the gradient is automatically calculated using autograd tools. Please see the supplementary for more details. <ref type="bibr">9</ref> We focus on the performance of the ent-to-end relation extraction, so we select models by the relation extraction re- sults. It is also possible to consider both the performances of the entity model and the relation model. We leave the study of advanced model selection algorithms for future work. <ref type="bibr">10</ref> We use the dataset in https://github.com/tticoin/LSTM- ER, which is from (Miwa and Bansal, 2016).</p><p>11 https://github.com/shanzhenren/CoType. <ref type="bibr">12</ref> The default setting is α = 10 −4 , µ = 1.0, K = 3 in systems without self-learned Γ loss and α = 1, µ = 1.0, K = 2 in systems with Γ loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Entity <ref type="table" target="#tab_0">Relation  P  R  F  P  R  F   L&amp;J (2014)</ref> 85.2 76.9 80.8 65.4 39.8 49.5 M&amp;B <ref type="formula" target="#formula_21">(2016)</ref> 82.9 83.9 83.4 57.2 54.0 55.6 Zhang (2017) - - 83.5 - - 57.5 K&amp;C <ref type="formula">(2017)</ref> 84.0 81.3 82.6 55.5 51.8 53.6 NN 84.0 82.9 83.4 59.5 56.3 57.8 MRT 83.9 83.2 83.6 64.9 55.1 59.6  <ref type="bibr" target="#b12">and Ji, 2014</ref>) for details).</p><p>using precision (P), recall (R) and F1 scores. Specifically, an output entity e is correct if its type and the region of its head are correct, and an out- put relation r is correct if its e 1 , e 2 , l are correct (i.e., "exact match").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on ACE05</head><p>We first compare proposed models with previous work <ref type="table" target="#tab_0">(Table 1)</ref>. In general, our plain neural net- work model (NN) is competitive, and after compil- ing with MRT, it achieves non-negligible improve- ment over existing state-of-the-art systems. (both on the entity and the relation extraction). <ref type="bibr">13</ref> We have following two detailed comparisons. Among systems which only rely on shared pa- rameters <ref type="bibr" target="#b14">((Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b9">Katiyar and Cardie, 2017)</ref> and NN), NN gives the best result (we give detailed results on different relation types in the supplement). One possible reason is that the "RNN+CNN" network structure is not fully explored in previous joint learning models. More importantly, it suggests that how to build powerful sub-models and utilize shared parameters are still among the key problems of the task.</p><p>Comparing with the best joint decoding sys- tem which adopts global normalization in train- ing ( <ref type="bibr" target="#b27">Zhang et al., 2017)</ref>, MRT mainly improves the relation extraction results. We think that the improvement may come from the sentence-level loss applied in MRT: both systems consider inter- actions between decoders, and both objectives are approximated by sampling, but MRT optimizes F1 score while <ref type="bibr" target="#b27">Zhang et al. (2017)</ref> optimize label ac- 13 It is worth noting that our models don't access addi- tional linguistic resources such as POS tags and dependency trees. We have tried to add syntactic features in ( <ref type="bibr" target="#b27">Zhang et al., 2017)</ref>, but didn't observe improvements.  curacy. For the joint decoding system in ( <ref type="bibr" target="#b12">Li and Ji, 2014</ref>), although it cannot beat recent neural network-based models, it is interesting to compare MRT with a feature-enriched version of ( <ref type="bibr" target="#b12">Li and Ji, 2014</ref>)'s model in the future work. Next, we evaluate the joint MRT with different loss functions and sampling methods.</p><p>As mentioned in Section 3.3, we have three op- tions (∆ E+R , ∆ E , ∆ R ) for ∆(ˆ y, y) and a self- learned loss function Γ. The first five rows of <ref type="table" target="#tab_2">Ta- ble 2</ref> show their performances on the test data. We have three observations regarding the results.</p><p>1. ∆ R , ∆ E+R have higher relation F1 scores than ∆ E and NN. Thus, adding relation loss in ∆(ˆ y, y) is helpful for relation extraction. We think that knowing the relation loss could bias the entity model to highlight the entities appearing in rela- tions, which provides a better candidate relation set for the relation extraction model. <ref type="bibr">2</ref>. ∆ E has the best entity extraction results, which implies that the sentence-level entity loss alone could benefit entity extraction. While after adding relation loss (∆ E+R ), the entity performance slightly decreases. One reason might be that our model selection strategy only focuses on the re- lation part (footnote 9), thus the model with im- proved entity performances may not be selected.</p><p>3. The learned loss Γ can help to improve perfor- mances, but only using Γ is not as effective as the handcrafted ∆ functions (which are tailored to the evaluation metrics). By combining both the prior knowledge and information from the dataset, Γ + ∆ E+R achieves the best results.</p><p>Regarding the sampling method, we test a vari- ant of Algorithm 1 which samples entities but not relations (the last five rows of <ref type="table" target="#tab_2">Table 2</ref>). Comparing with the default sampling algorithm, it has similar entity extraction performances, but its behaviour on the relation extraction is different. Specifically, adding entity loss in ∆(ˆ y, y) (i.e., ∆ E , ∆ E+R ) now affects relation results negatively. It may sug- gest that when only exploring the output of entity extraction, the entity loss may dominate the rela- tion loss, and trap the joint model to exploit the entity model only. On the other hand, the perfor- mances of self-learned loss Γ are less sensitive to the sampling method. We haven't had a clear un- derstanding of the relationship between sampling algorithms and loss functions, but the above re- sults show that adding data-related loss function could improve the robustness of MRT in practice.</p><p>Thirdly, we present influences of hyper- parameters for MRT with ∆ E+R on the develop- ment set in <ref type="figure" target="#fig_3">Figure 3</ref> and 4 (other settings have sim- ilar results). We find that, for the parameters ex- amined here, it is hard for the entity model and the relation model to agree with each other: parame- ters achieving high relation performances usually get low entity performances, and vise versa. Thus, if we perform the model selection by only look- ing at relation extraction results, the joint model may sacrifice entity extraction performances. For α and µ <ref type="figure" target="#fig_3">(Figure 3)</ref>, we observe that on the ACE05 dataset, the model prefers a small α (which means a sharper Q) and µ at boundary (i.e., Q is either close to the entity model or the relation model). Regarding the sample size K <ref type="figure" target="#fig_4">(Figure 4)</ref>, we don't observe a convergence of performances in a small range of K. Since the computation cost increases rapidly as we increase the sample size (K = 5 is about 2x slower than K = 3 in our implementa- tion), we stick to a small K.</p><p>Finally, due to lack of space, we provide more discussions on model configurations (including re- sults regarding different entity pair distances, ad- ditional experiments on tuning hyper parameters etc.), and detailed error analyses on concrete sam- ples in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on NYT</head><p>We briefly list results on the NYT dataset in Ta- ble 3. The baseline methods are <ref type="bibr" target="#b19">(Ren et al., 2017)</ref> which is based on a joint embedding of entities and relations, and ( <ref type="bibr" target="#b28">Zheng et al., 2017</ref>    baseline results. In particular, comparing with the joint tagging scheme in ( <ref type="bibr" target="#b28">Zheng et al., 2017)</ref>, MRT adds no constraint on the relation extraction model and can explore the large NYT training set more effectively. At the same time, since the training set is automatically generated, the global losses ob- served in MRT are also noisy. Like recent work on bandit structured prediction ( <ref type="bibr" target="#b10">Kreutzer et al., 2017;</ref><ref type="bibr" target="#b16">Nguyen et al., 2017)</ref>, the results here suggest that MRT could be a reasonable choice when the su- pervision of the joint learning is partial and noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced minimum risk training to the task of joint entity and relation extraction. We showed that, with a global loss function, MRT could en- hance the connection between the sub-models. Extensive experiments on benchmark datasets wit- ness the effectiveness of the joint MRT.   <ref type="bibr">et al., 2017)</ref>, we give results under the "exact match" crite- rion as ACE05. To compare with ( <ref type="bibr" target="#b28">Zheng et al., 2017)</ref>, we give results which ignore the entity type in the justification of relations. We use α = 1, µ = 1, K = 2 and ∆E+R + Γ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>Associated Press]ORG [writer]PER [Patrick McDowell]PER in [Kuwait City]GPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Paradigms of joint entity relation extraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>P</head><label></label><figDesc>(ˆ y|s; θ) (∆(ˆ y, y) − P (ˆ y|s; θ)) + [1 − P (y|s; θ) + P (y * |s; θ)] + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MRT with different Q distributions on the development set. Rows are settings of µ, and columns are settings of α. In each cell, we draw F1 scores of the entity extraction (the left gray bar) and the relation extraction (the right dark bar) under the combination of corresponding α and µ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MRT with different sample size K on the development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results on the ACE05 test data. (Miwa and Bansal, 

2016) and (Katiyar and Cardie, 2017) are joint training sys-
tems without joint decoding. (Li and Ji, 2014) and (Zhang 
et al., 2017) are joint decoding algorithms. NN is our neu-
ral network model without minimum risk training. MRT is 
minimum risk training with loss Γ (Equation 5). We omit 
pipeline methods which underperform joint models (see (Li 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>MRT with different loss functions and sampling 

methods. The numbers in subscripts indicate improvements 
over the NN setting in Table 1. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the NYT dataset. To compare with (Ren 

</table></figure>

			<note place="foot" n="1"> Our implementation is available at https:// github.com/changzhisun/entrel-joint-mrt. 3. achieving state-of-the-art results on two benchmark datasets (ACE05 and NYT).</note>

			<note place="foot" n="2"> We have also tried biLSTM-CRF (Huang et al., 2015) as an advanced entity model, but performances are nearly the same in our experiments. 3 We include a NONE relation type in Tr which means that there exists no relation between e1 and e2.</note>

			<note place="foot" n="4"> Further study on different Γ(ˆ y) is left for future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors wish to thank the reviewers for their helpful comments and suggestions. This research is (partially) supported by STCSM (18ZR1411500), NSFC(61673179), Shanghai Knowledge Service Platform Project (ZF1213), and Shanghai Key Laboratory of Trustworthy Computing (07dz22304201604).</p><p>The corre-sponding authors are Yuanbin Wu, Man Lan and Shiliang Sun.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004)</title>
		<meeting><address><addrLine>Banff, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shiqi Shen Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01904</idno>
		<title level="m">Neural headline generation with minimum risk training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Softmaxmargin crfs: Training log-linear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="733" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum expected bleu training of phrase and lexicon translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigating lstms for joint extraction of opinion entities and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="919" to="929" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Going out on a limb: Joint extraction of entity mentions and relations without dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="917" to="928" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bandit structured prediction for neural sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1503" to="1513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A rich feature vector for protein-protein interaction extraction from multiple corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rune</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reinforcement learning for bandit neural machine translation with simulated human feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boydgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1464" to="1474" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Maximum margin planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06-25" />
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cotype: Joint extraction of typed entities and relations with knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqiu</forename><surname>Xiang Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases, European Conference, ECML PKDD 2010</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09-20" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1683" to="1692" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimum risk annealing for training log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</title>
		<meeting>the COLING/ACL 2006 Main Conference Poster Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<title level="m">troduction to Reinforcement Learning</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Expected f-measure training for shift-reduce parsing with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
