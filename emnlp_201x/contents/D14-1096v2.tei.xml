<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Can We Get From 1000 Tokens? A Case Study of Multilingual POS Tagging For Resource-Poor Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
							<email>lduong@student.unimelb.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Victoria Research Laboratory</orgName>
								<orgName type="institution">National ICT Australia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Can We Get From 1000 Tokens? A Case Study of Multilingual POS Tagging For Resource-Poor Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Correction: We unintentionally mis-represented Garrette et al. (2013) in the published version of this paper by stat-ing that they required an external tag dictionary. We have corrected these inaccuracies to reflect their modest data requirements.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we address the problem of multilingual part-of-speech tagging for resource-poor languages. We use parallel data to transfer part-of-speech information from resource-rich to resource-poor languages. Additionally, we use a small amount of annotated data to learn to &quot;correct&quot; errors from projected approach such as tagset mismatch between languages , achieving state-of-the-art performance (91.3%) across 8 languages. Our approach is based on modest data requirements , and uses minimum divergence classification. For situations where no universal tagset mapping is available, we propose an alternate method, resulting in state-of-the-art 85.6% accuracy on the resource-poor language Malagasy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Part-of-speech (POS) tagging is a crucial task for natural language processing (NLP) tasks, provid- ing basic information about syntax. Supervised POS tagging has achieved great success, reach- ing as high as 95% accuracy for many languages <ref type="bibr" target="#b19">(Petrov et al., 2012</ref>). However, supervised tech- niques need manually annotated data, and this is either lacking or limited in most resource- poor languages. Fully unsupervised POS tagging is not yet useful in practice due to low accu- racy ( <ref type="bibr" target="#b4">Christodoulopoulos et al., 2010)</ref>. In this pa- per, we propose a semi-supervised method to nar- row the gap between supervised and unsupervised approaches. We demonstrate that even a small amount of supervised data leads to substantial im- provement.</p><p>Our method is motivated by the availability of parallel data. Thanks to the development of mul- tilingual documents from government projects, book translations, multilingual websites, and so forth, parallel data between resource-rich and resource-poor languages is relatively easy to ac- quire. This parallel data provides the bridge that permits us to transfer POS information from a resource-rich to a resource-poor language.</p><p>Systems that make use of cross-lingual tag projection typically face several issues, includ- ing mismatches between the tagsets used for the languages, artifacts from noisy alignments and cross-lingual syntactic divergence. Our approach compensates for these issues by training on a small amount of annotated data on the target side, demonstrating that only 1k tokens of annotated data is sufficient to improve performance.</p><p>We first tag the resource-rich language using a supervised POS tagger. We then project POS tags from the resource-rich language to the resource- poor language using parallel word alignments. The projected labels are noisy, and so we use various heuristics to select only "good" training examples. We train the model in two stages. First, we build a maximum entropy classifier T on the (noisy) projected data. Next, we train a supervised classifier P on a small amount of annotated data (1,000 tokens) in the target lan- guage, using a minimum divergence technique to incorporate the first model, T . Compared with the state of the art <ref type="bibr">(Täckström et al., 2013)</ref>, we make more-realistic assumptions (e.g. relying on a tiny amount of annotated data rather than a huge crowd-sourced dictionary) and use less parallel data, yet achieve a better overall result. We achieved 91.3% average accuracy over 8 lan- guages, exceeding <ref type="bibr">Täckström et al. (2013)</ref>'s result of 88.8%.</p><p>The test data we employ makes use of map- pings from language-specific POS tag inventories to a universal tagset ( <ref type="bibr" target="#b19">Petrov et al., 2012)</ref>. How- ever, such a mapping might not be available for resource-poor languages. Therefore, we also pro- pose a variant of our method which removes the need for identical tagsets between the projection model T and the correction model P , based on a two-output maximum entropy model over tag pairs. Evaluating on the resource-poor language Malagasy, we achieved 85.6% accuracy, exceed- ing the state-of-the-art of 81.2% ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>There is a wealth of prior work on multilingual POS tagging. The simplest approach takes advan- tage of the typological similarities that exist be- tween languages pairs such as Czech and Russian, or Serbian and Croatian. They build the tagger -or estimate part of the tagger -on one lan- guage and apply it to the other language ( <ref type="bibr" target="#b22">Reddy and</ref><ref type="bibr">Sharoff, 2011, Hana et al., 2004</ref>). <ref type="bibr" target="#b28">Yarowsky and Ngai (2001)</ref> pioneered the use of parallel data for projecting tag information from a resource-rich language to a resource-poor lan- guage. <ref type="bibr" target="#b7">Duong et al. (2013b)</ref> used a similar method on using sentence alignment scores to rank the goodness of sentences. They trained a seed model from a small part of the data, then applied this model to the rest of the data using self-training with revision.</p><p>Das and Petrov (2011) also used parallel data but additionally exploited graph-based label prop- agation to expand the coverage of labelled tokens. Each node in the graph represents a trigram in the target language. Each edge connects two nodes which have similar context. Originally, only some nodes received a label from direct label projection, and then labels were propagated to the rest of the graph. They only extracted the dictionary from the graph because the labels of nodes are noisy. They used the dictionary as the constraints for a feature-based HMM tagger <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2010)</ref>. Both <ref type="bibr" target="#b7">Duong et al. (2013b)</ref> and <ref type="bibr" target="#b5">Das and Petrov (2011)</ref> achieved 83.4% accuracy on the test set of 8 European languages. <ref type="bibr" target="#b11">Goldberg et al. (2008)</ref> pointed out that, with the presence of a dictionary, even an incomplete one, a modest POS tagger can be built using simple methods such as expectation maximization. This is because most of the time, words have a very limited number of possible tags, thus a dictionary that specifies the allowable tags for a word helps to restrict the search space. With a gold-standard dictionary, <ref type="bibr" target="#b5">Das and Petrov (2011)</ref> achieved an ac- curacy of approximately 94% on the same 8 lan- guages. The effectiveness of a gold-standard dic- tionary is undeniable, however it is costly to build one, especially for resource-poor languages. <ref type="bibr" target="#b16">Li et al. (2012)</ref> used the dictionary from Wiktionary, 1 a crowd-sourced dictionary. They scored 84.8% ac- curacy on the same 8 languages. Currently, Wik- tionary covers over 170 languages, but the cov- erage varies substantially between languages and, unsurprisingly, it is poor for resource-poor lan- guages. Therefore, relying on Wiktionary is not effective for building POS taggers for resource- poor languages. <ref type="bibr">Täckström et al. (2013)</ref> combined both token information (from direct projected data) and type constraints (from Wiktionary's dictionary) to form the state-of-the-art multilingual tagger. They built a tag lattice and used these token and type con- straints to prune it. The remaining paths are the training data for a CRF tagger. They achieved 88.8% accuracy on the same 8 languages. <ref type="table">Table 1</ref> summarises the performance of the above models across all 8 languages. Note that these methods vary in their reliance on external resources. <ref type="bibr" target="#b7">Duong et al. (2013b)</ref> use the least, i.e. only the Europarl Corpus ( <ref type="bibr" target="#b14">Koehn, 2005)</ref>. <ref type="bibr" target="#b5">Das and Petrov (2011)</ref> additionally use the United Nation Parallel Corpus. <ref type="bibr" target="#b16">Li et al. (2012)</ref> didn't use any par- allel text but used Wiktionary instead. <ref type="bibr">Täckström et al. (2013)</ref> exploited more parallel data than <ref type="bibr" target="#b5">Das and Petrov (2011)</ref> and also used a dictionary from <ref type="bibr" target="#b16">Li et al. (2012)</ref>.</p><p>Another approach for resource-poor languages is based on the availability of a small amount of annotated data.  built a POS tagger for Kinyarwanda and Malagasy. They didn't use parallel data but instead exploited four hours of manual annotation to build ∼4,000 tokens or ∼3,000 word-types of annotated data. These tokens or word-types were used to build a tag dic- tionary. They employed label propagation for ex- panding the coverage of this dictionary in a similar vein to <ref type="bibr" target="#b5">Das and Petrov (2011)</ref>. They built train- ing examples using this dictionary and then trained the tagger on this data. They achieved 81.9% and 81.2% accuracy for Kinyarwanda and Malagasy respectively.</p><p>The method we propose in this paper is similar in only using a small amount of annotation. How- ever, we directly use the annotated data to train the model rather than using a dictionary. We argue   <ref type="bibr" target="#b3">and Marsi, 2006</ref>).</p><p>that with a proper "guide", we can take advantage of very limited annotated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotated data</head><p>Our annotated data mainly comes from CoNLL shared tasks on dependency parsing <ref type="bibr" target="#b3">(Buchholz and Marsi, 2006</ref>). The language specific tagsets are mapped into the universal tagset. We will use this annotated data mainly for evaluation. <ref type="table" target="#tab_2">Ta- ble 2</ref> shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 lan- guages makes our system comparable with pre- viously proposed methods. Nevertheless, we try to use as few resources as possible, in order to simulate the situation for resource-poor languages. Later in Section 6 we adapt the approach for Mala- gasy, a truly resource-poor language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Universal tagset</head><p>We employ the universal tagset from (Petrov et al., 2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (deter- miner and article), ADP (preposition and post- position), CONJ (conjunctions), NUM (numeri- cal), PRT (particle), PUNC (punctuation) and X (all other categories including foreign words and abbreviations). <ref type="bibr" target="#b19">Petrov et al. (2012)</ref> provide the mapping from each language-specific tagset to the universal tagset. The idea of using the universal tagset is of great use in multilingual applications, enabling compar- ison across languages. However, the mapping is not always straightforward. <ref type="table" target="#tab_2">Table 2</ref> shows the size of the annotated data for each language, the num- ber of tags presented in the data, and the list of tags that are not matched. We can see that only 8 tags are presented in the annotated data for Dan- ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are missing. <ref type="bibr">2</ref> Thus, a classifier using all 12 tags will be heavily penalized in the evaluation. <ref type="bibr" target="#b16">Li et al. (2012)</ref> considered this problem and tried to manually modify the Danish mappings. Moreover, PRT is not really a universal tag since it only appears in 3 out of the 8 languages. <ref type="bibr" target="#b21">Plank et al. (2014)</ref> pointed out that PRT often gets con- fused with ADP even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the map- ping, and other incompatibilities arising from in- correct alignments and syntactic divergence be- tween the source and target languages.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Directly Projected Model (DPM)</head><p>In this section we describe a maximum entropy tagger that only uses information from directly projected data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parallel data</head><p>We first collect Europarl data having English as the source language, an average of 1.85 million parallel sentences for each of the 8 language pairs. In terms of parallel data, we use far less data com- pared with other recent work. <ref type="bibr" target="#b5">Das and Petrov (2011)</ref> used Europarl and the ODS United Na- tion dataset, while <ref type="bibr">Täckström et al. (2013)</ref> addi- tionally used parallel data crawled from the web. The amount of parallel data is crucial for align- ment quality. Since DPM uses alignments to trans- fer tags from source to target language, the per- formance of DPM (and other models that exploit projection) largely depends on the quantity of par- allel data. The "No LP" model of <ref type="bibr" target="#b5">Das and Petrov (2011)</ref>, which only uses directly projected labels (without label propagation), scored 81.3% for 8 languages. However, using the same model but with more parallel data, Täckström et al. <ref type="formula" target="#formula_4">(2013)</ref> scored 84.9% on the same test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label projection</head><p>We use the standard alignment tool <ref type="bibr">Giza++ (Och and Ney, 2003)</ref> to word align the parallel data. We employ the Stanford POS tagger ( <ref type="bibr" target="#b26">Toutanova et al., 2003)</ref> to tag the English side of the parallel data and then project the label to the target side. It has been confirmed in many studies <ref type="bibr" target="#b24">(Täckström et al., 2013</ref><ref type="bibr" target="#b5">, Das and Petrov, 2011</ref><ref type="bibr" target="#b25">, Toutanova and Johnson, 2008</ref>) that directly projected labels are noisy. Thus we need a method to reduce the noise. We employ the strategy of <ref type="bibr" target="#b28">Yarowsky and Ngai (2001)</ref> of ranking sentences using a their alignment scores from IBM model 3. Firstly, we want to know how noisy the pro- jected data is. Thus, we use the test data to build a simple supervised POS tagger using the TnT tagger <ref type="bibr" target="#b2">(Brants, 2000</ref>) which employs a second- order Hidden Markov Model (HMM). We tag the projected data and compare the label from direct projection and from the TnT tagger. The labels from the TnT Tagger are considered as pseudo- gold labels. Column "Without Mapping" from Ta- ble 3 shows the average accuracy for the first n- sentences (n = 60k, 100k, 200k, 500k) for 8 lan- guages according to the ranking. Column "Cov- erage" shows the percentages of projected label (the other tokens are Null aligned). We can see that when we select more data, both coverage and accuracy fall. In other words, using the sentence alignment score, we can rank sentences with high coverage and accuracy first. However, even after ranking, the accuracy of projected labels is less than 80% demonstrating how noisy the projected labels are. <ref type="table" target="#tab_6">Table 3</ref> (column "With Mapping") additionally shows the accuracy using simple tagset mapping, i.e. mapping each tag to the tag it is assigned most frequently in the test data. For example DET, PRT, PUNC, NUM, missing from Danish gold data, will be matched to PRON, X, X, ADJ respectively. This simple matching yields a ∼ 4% (absolute) im- provement in average accuracy. This illustrates the importance of handling tagset mapping carefully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The model</head><p>In this section, we introduce a maximum entropy tagger exploiting the projected data. We select the first 200k sentences from <ref type="table" target="#tab_6">Table 3</ref> for this experi- ment. This number represents a trade-off between size and accuracy. More sentences provide more information but at the cost of noisier data. <ref type="bibr" target="#b7">Duong et al. (2013b)</ref>   We ignore tokens that don't have labels, which arise from null alignments and constitute approxi- mately 14% of the data. The remaining data (∼1.4 million tokens) are used to train a maximum en- tropy (MaxEnt) model. MaxEnt is one of the simplest forms of probabilistic classifier, and is appropriate in this setting due to the incomplete sequence data. While sequence models such as HMMs or CRFs can provide more accurate mod- els of label sequences, they impose a more strin-   <ref type="table">Table 5</ref>: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature set at a time gent training requirement. <ref type="bibr">3</ref> We also experimented with a first-order linear chain CRF trained on con- tiguous sub-sequences but observed ∼ 4% (abso- lute) drop in performance.</p><formula xml:id="formula_0">Data Size (k) Coverage (%) Without</formula><p>The maximum entropy classifier estimates the probability of tag t given a word w as</p><formula xml:id="formula_1">P (t|w) = 1 Z(w) exp D j=1 λ j f j (w, t) ,</formula><p>where Z(w) = t exp D j=1 λ j f j (w, t) is the normalization factor to ensure the probabilities P (t|w) sum to one. Here f j is a feature function and λ j is the weight for this feature, learned as part of training. We use Maximum A Posteriori (MAP) estimation to maximize the log likelihood of the training data, D = {w i , t i } N i=1 , subject to a zero-mean Gaussian regularisation term,</p><formula xml:id="formula_2">L = log P (Λ) N i=1 P (t (i) |w (i) ) = − D j=1 λ 2 j 2δ 2 + N i=1 D j=1 λ j f j (w i , t i ) − log Z(w i )</formula><p>where the regularisation term limits over-fitting, an important concern when using large feature sets. For our experiments we set δ 2 = 1. We use L-BFGS which performs gradient ascent to maxi- mize L. <ref type="table" target="#tab_4">Table 4</ref> shows the features we considered for building the DPM. We use mkcls, an unsu- pervised method for word class induction which is widely used in machine translation <ref type="bibr" target="#b18">(Och, 1999)</ref>. We run mkcls to obtain 100 word classes, using only the target language side of the parallel data. <ref type="table">Table 5</ref> shows the accuracy of the DPM evalu- ated on 8 languages ("All features model"). DPM performs poorly on Danish, probably because of the tagset mapping issue discussed above. The DPM result of 80.2% accuracy is encouraging, particularly because the model had no explicit su- pervision.</p><p>To see what features are meaningful for our model, we remove features in turn and report the result. The result in <ref type="table">Table 5</ref> disagrees with <ref type="bibr">Täckström et al. (2013)</ref> on the word class features. They reported a gain of approximately 3% (ab- solute) using the word class. However, it seems to us that these features are not especially mean- ingful (at least in the present setting). Possible reasons for the discrepancy are that they train the word class model on a massive quantity of exter- nal monolingual data, or their algorithms for word clustering are better <ref type="bibr" target="#b27">(Uszkoreit and Brants, 2008)</ref>. We can see that the most informative features are Capitalization, Number and Punctuation. This makes sense because in languages such as Ger- man, capitalization is a strong indicator of NOUN. Number and punctuation features ensure that we classify NUM and PUNCT tags correctly.</p><p>In this section we incorporate the directly pro- jected model into a second correction model trained on a small supervised sample of 1,000 an- notated tokens. Our DPM model is not very accu- rate; as we have discussed it makes many errors, due to invalid or inconsistent tag mappings, noisy alignments, and cross-linguistic syntactic diver- gence. However, our aim is to see how effectively we can exploit the strengths of the DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated to- kens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or oth- erwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct.  showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time.</p><p>Our correction model makes use of a mini- mum divergence (MD) model <ref type="bibr" target="#b1">(Berger et al., 1996)</ref>, a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation <ref type="bibr" target="#b8">(Foster, 2000</ref>) and parsing <ref type="bibr">van Noord, 2008, Johnson and</ref><ref type="bibr" target="#b13">Riezler, 2000</ref>). These previous approaches have used var- ious sources of reference distribution, e.g., incor- porating information from a simpler model <ref type="bibr" target="#b13">(Johnson and Riezler, 2000</ref>) or combining in-and out- of-domain models <ref type="bibr" target="#b20">(Plank and van Noord, 2008)</ref>. <ref type="bibr" target="#b20">Plank and van Noord (2008)</ref> concluded that this method for adding prior knowledge only works with high quality reference distributions, other- wise performance suffers.</p><p>In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model s o = P (t|w) are both maximum entropy models. In this case we show that the MD setup can be simplified to a regularization term, namely a Gaussian prior with a non-zero mean. We model the classification probability, P (t|w) as the product between a base model and a maximum entropy classifier,</p><formula xml:id="formula_3">P (t|w) ∝ P (t|w) exp D j=1 γ j f j (w, t)</formula><p>where here we use the DPM model as base model P (t|w). Under this setup, where P uses the same features as P , and both are log-linear models, this simplifies to</p><formula xml:id="formula_4">P (t|w) ∝ exp   D j=1 λ j f j (w, t) + D j=1 γ j f j (w, t)   ∝ exp D j=1 (λ j + γ j ) f j (w, t)<label>(1)</label></formula><p>where the constant of proportionality is</p><formula xml:id="formula_5">Z (w) = t exp D j=1 (λ j + γ j ) f j (w, t)</formula><p>. It is clear that Equation (1) also defines a maximum entropy clas- sifier, with parameters α j = λ j + γ j , and conse- quently this might seem to be a pointless exercise. The utility of this approach arises from the prior: MAP training with a zero mean Gaussian prior over γ is equivalent to a Gaussian prior over the aggregate weights, α j ∼ N (λ j , σ 2 ). This prior enforces parameter sharing between the two mod- els by penalising parameter divergence from the underlying DPM model λ. The resulting training objective is</p><formula xml:id="formula_6">L corr = log P (t|w, α) − 1 2σ 2 D j=1 (α j − λ j ) 2</formula><p>which can be easily optimised using standard gradient-based methods, e.g., L-BFGS. The con- tribution of the regulariser is scaled by the constant 1 2σ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regulariser sensitivity</head><p>Careful tuning of the regularisation term σ 2 is crit- ical for the correction model, both to limit over- fitting on the very small training sample of 1,000 tokens, and to control the extent of the influence of the DPM model over the correction model. A larger value of σ 2 lessens the reliance on the DPM and allows for more flexible modelling of the training set, while a small value of σ 2 forces the parameters to be close to the DPM estimates at the expense of data fit. We expect the best value to be somewhere between these extremes, and use line-search to find the optimal value for σ 2 . For this purpose, we hold out 100 tokens from the 1,000 instance training set, for use as our devel- opment set for hyper-parameter selection. From <ref type="figure">Figure 1</ref>, we can see that the model per- forms poorly on small values of σ 2 . This is under- standable because the small σ 2 makes the model   <ref type="figure">Figure 1</ref>: Sensitivity of regularisation parameter σ 2 against the average accuracy measured on 8 languages on the development set too similar to DPM, which is not very accurate (80.2%). At the other extreme, if σ 2 is large, the DPM model is ignored, and the correction model is equivalent with the supervised model (∼ 88% accuracy). We select the value of σ 2 = 70, which maximizes the accuracy on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The model</head><p>Using the value of σ 2 = 70, we retrain the model on the whole 1,000-token training set and evalu- ate the model on the rest of the annotated data. <ref type="table">Table 6</ref> shows the performance of DPM, Super- vised model, Correction model and the state-of- the-art model <ref type="bibr">(Täckström et al., 2013</ref>). The super- vised model trains a maximum entropy tagger us- ing the same features as in <ref type="table" target="#tab_4">Table 4</ref> on this 1000 to- kens. The only difference between the supervised model and the correction model is that in the cor- rection model we additionally incorporate DPM as the prior. The supervised model performs surprisingly well confirming that our features are meaning- ful in distinguishing between tags. This model achieves high accuracy on Danish compared with other languages probably because Danish is eas- ier to learn since it contains only 8 tags. Despite the fact that the DPM is not very accurate, the cor- rection model consistently outperforms the super- vised model on all considered languages, approx- imately 4.3% (absolute) better on average. This shows that our method of incorporating DPM to the model is efficient and robust.</p><p>The correction model performs much bet- ter than the state-of-the-art for 7 languages but  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correction Model Supervised Model</head><p>Figure 2: Learning curve for correction model and supervised model: the x-axis is the size of data (number of tokens); the y-axis is the average ac- curacy measured on 8 languages; the dashed line shows the data condition reported in <ref type="table">Table 6</ref> slightly worse for 1 language. On average we achieve 91.3% accuracy compared with 88.8% for the state-of-the-art, an error rate reduction of 22.3%. This is despite using fewer resources and only modest supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Tagset mismatch In the correction model, we implicitly resolve the mismatched tagset issue. DPM might contain tags that don't appear in the target language or generally are errors in the map- ping. However, when incorporating DPM into the correction model, only the feature weight of tags that appear in the target language are retained. In general, because we don't explicitly do any map- ping between languages, we might have trouble if the tagset size of the target language is bigger than the source language tagset. However, this is not the case for our experiment because we choose En- glish as the source-side and English has the full 12 tags.</p><p>Learning curve We investigate the impact of the number of available annotated tokens on the correction model. <ref type="figure">Figure 2</ref> shows the learning curve of the correction model and the supervised model. We can clearly see the differences be- tween 2 models when the size of training data is small. For example, at 100 tokens, the difference is very large, approximately 18% (absolute), it is also 6% (absolute) better than DPM. This differ- ence diminishes as we add more data. This make sense because when we add more data, the super- vised model become stronger, while the effective- <ref type="table">Model   da  nl  de  el  it  pt  es  sv  Avg  DPM</ref> 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2 Täckström et al. <ref type="formula" target="#formula_4">(2013)</ref> 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8 Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0 Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3 DPM <ref type="table">(with dict)</ref> 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8 Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6 <ref type="table">Table 6</ref>: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the state-of-the-art system <ref type="bibr">(Täckström et al., 2013</ref>). The best performance for each language is shown in bold. The models that are built with a dictionary are provided for reference.</p><p>ness of the DPM prior on the correction model is wearing off. An interesting observation is that the correction model is always better, even when we add massive amounts of annotated data. At 50,000 tokens, when the supervised model reaches 96% accuracy, the correction model is still 0.3% (abso- lute) better, reaching 96.3%. It means that even at that high level of confidence, some informa- tion can still be added from DPM to the correc- tion model. This improvement probably comes from the observation that the ambiguity in one language is explained through the alignment. It also suggests that this method could improve the performance of a supervised POS tagger even for resource-rich languages.</p><p>Our methods are also relevant for annotation projects for resource-poor languages. Assuming that it is very costly to annotate even 100 tokens, applying our methods can save annotation effort but maintain high performance. For example, we just need 100 tokens to match the accuracy of a su- pervised method trained on 700 tokens, or we just need 500 tokens to match the performance with nearly 2,000 tokens of supervised learning.</p><p>Our method is simple, but particularly suitable for resource-poor languages. We need a small amount of annotated data for a high performance POS tagger. For example, we need only around 300 annotated tokens to reach the same accuracy as the state-of-the-art unsupervised POS tagger (88.8%).</p><p>Tag dictionary Although, it is not our objec- tive to rely on the dictionary, we are interested in whether the gains from the correction model still persist when the DPM performance is im- proved. We attempt to improve DPM, following the method of <ref type="bibr" target="#b16">Li et al. (2012)</ref> by building a tag dic- tionary using Wiktionary. This dictionary is then used as a feature which fires for word-tag pairings present in the dictionary. We expect that when we add this additional supervision, the DPM model should perform better. <ref type="table">Table 6</ref> shows the perfor- mance of DPM and the correction model when in- corporating the dictionary. The DPM model only increases 0.6% absolute but the correction model increases 1.3%. Additionally, it shows that our model can improve further by incorporating exter- nal information where available.</p><p>CRF Our approach of using simple classifiers begs the question of whether better results could be obtained using sequence models, such as con- ditional random fields (CRFs). As mentioned pre- viously, a CRF is not well suited for incomplete data. However, as our second 'correction' model is trained on complete sequences, we now con- sider using a CRF in this stage. The training al- gorithm is as follows: first we estimate the DPM feature weights on the incomplete data as before, and next we incorporate the feature weights into a CRF trained on the 1,000 annotated tokens. This is complicated by the different feature sets between the MaxEnt classifier and the CRF, however the classifier uses a strict subset of the CRF features. Thus, we use the minimum divergence prior for the token level features, and a standard zero-mean prior for the sequence features. That is, the ob- jective function of the CRF correction model be- comes:</p><formula xml:id="formula_7">L corr crf = log P (t|w, α) − 1 2δ 2 1 j∈F 1 (α j − λ j ) 2 − 1 2δ 2 2 j∈F 2 α 2 j (2)</formula><p>where F 1 is the set of features referring to only one label as in the DPM maxent model and F 2 is the set of features over label pairs. The union of F = F 1 ∪ F 2 is the set of all features for the CRF. We perform grid search using held out data as before for δ 2 1 and δ 2 2 . The CRF correc- tion model scores 88.1% compared with 86.5% of the supervised CRF model trained on the 1,000 tokens. Clearly, this is beneficial, however, the CRF correction model still performs worse than the MaxEnt correction model (91.3%). We are not sure why but one reason might be overfitting of the CRF, due to its large feature set and tiny train- ing sample. Moreover, this CRF approach is or- thogonal to <ref type="bibr">Täckström et al. (2013)</ref>: we could use their CRF model as the DPM model and train the CRF correction model using the same minimum divergence method, presumably resulting in even higher performance.</p><p>6 Two-output model  also use only a small amount of annotated data, evaluating on two resource-poor languages <ref type="bibr">Kinyarwanda (KIN)</ref> and Malagasy (MLG). As a simple baseline, we trained a maxent supervised classifier on this data, achieving competitive results of 76.4% and 80.0% accuracy compared with their published results of 81.9% and 81.2% for KIN and MLG, respec- tively. Note that the  method is more complicated than this baseline.</p><p>We want to further improve the accuracy of MLG using parallel data. Applying the technique from Section 4 will not work directly, due to the tagset mismatch (the Malagasy tagset contains 24 tags) which results in highly different feature sets. Moreover, we don't have the language expertise to manually map the tagset. Thus, in this section, we propose a method capable of handling tagset mismatch. For data, we use a parallel English- Malagasy corpus of ∼100k sentences, 4 and the POS annotated dataset developed by , which comprises 4230 tokens for training and 5300 tokens for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The model</head><p>Traditionally, MaxEnt classifiers are trained us- ing a single label. <ref type="bibr">5</ref> The method we propose is trained with pairs of output labels: one for the Malagasy tag (t M ) and one for the universal tag 4 http://www.ark.cs.cmu.edu/global-voices/ <ref type="bibr">5</ref> Or else a sequence of labels, in the case of a conditional random field ( <ref type="bibr" target="#b15">Lafferty et al., 2001</ref>). However, even in this case, each token is usually assigned a single label. An excep- tion is the factorial CRF ( <ref type="bibr" target="#b23">Sutton et al., 2007)</ref>, which models several co-dependent sequences. Our approach is equivalent to a factorial CRF without edges between tags for adjacent tokens in the input.</p><p>(t U ), which are both predicted conditioned on a Malagasy word (w M ) in context. Our two-output model is defined as</p><formula xml:id="formula_8">P (t M , t U |w M ) = 1 Z(w M ) exp D j=1 λ j f M j (w, t M ) + E j=1 γ j f U j (w, t U ) + F j=1 α j f B j (w, t M , t U )<label>(3)</label></formula><p>where f M , f U , f B are the feature functions con- sidering t M only, t U only, and over both outputs t M and t U respectively, and Z(w M ) is the parti- tion function. We can think of Eq. <ref type="formula" target="#formula_8">(3)</ref> as the com- bination of 3 models: the Malagasy maxent super- vised model, the DPM model, and the tagset map- ping model. The central idea behind this model is to learn to predict not just the MLG tags, as in a standard supervised model, but also to learn the mapping between MLG and the noisy projected universal tags. Framing this as a two output model allows for information to flow both ways, such that confident taggings in either space can inform the other, and accordingly the mapping weights α are optimised to maximally exploit this effect. One important question is how to obtain la- belled data for training the two-output model, as our small supervised sample of MLG text is only annotated for MLG labels t M . We resolve this by first learning the DPM model on the projected labels, after which we automatically label our correction training set with predicted tags from the DPM model. That is, we augment the an- notated training data from (t M , w M ) to become (t M , t U , w M ). This is then used to train the two- output maxent classifier, optimising a MAP ob- jective using standard gradient descent. Note that it would be possible to apply the same minimum divergence technique for the two-output maxent model. In this case the correction model would include a regularization term over the λ to bias to- wards the DPM parameters, while γ and α would use a zero-mean regularizer. However, we leave this for future work. <ref type="table">Table 7</ref> summarises the performance of the state-of-the-art ( , the super- vised model and the two-output maxent model evaluated on the Malagasy test set. The two-output maxent model performs much better than the su- pervised model, achieving ∼5.3% (absolute) im- provement. An interesting property of this ap- Model Accuracy (%)  81.2 MaxEnt Supervised 80.0 2-output MaxEnt <ref type="table">(Universal tagset)</ref> 85.3 2-output MaxEnt <ref type="table">(Penn tagset)</ref> 85.6 <ref type="table">Table 7</ref>: The performance of different models for Malagasy.</p><p>proach is that we can use different tagsets for the DPM. We also tried the original Penn treebank tagset which is much larger than the universal tagset (48 vs. 12 tags). We observed a small im- provement reaching 85.6%, suggesting that some pertinent information is lost in the universal tagset. All in all, this is a substantial improvement over the state-of-the-art result of 81.2% (  and an error reduction of 23.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we thoroughly review the work on multilingual POS tagging of the past decade. We propose a simple method for building a POS tag- ger for resource-poor languages by taking advan- tage of parallel data and a small amount of anno- tated data. Our method also efficiently resolves the tagset mismatch issue identified for some lan- guage pairs. We carefully choose and tune the model. Comparing with the state-of-the-art, we are using the more realistic assumption that a small amount of labelled data can be made avail- able rather than requiring a crowd-sourced dic- tionary. We use less parallel data which as we pointed out in section 3.1, could have been a huge disadvantage for us. Moreover, we did not exploit any external monolingual data. Importantly, our method is simpler but performs better than previ- ously proposed methods. With only 1,000 anno- tated tokens, less than 1% of the test data, we can achieve an average accuracy of 91.3% compared with 88.8% of the state-of-the-art (error reduction rate ∼22%). Across the 8 languages we are sub- stantially better at 7 and slightly worse at one. Our method is reliable and could even be used to im- prove the performance of a supervised POS tagger. Currently, we are building the tagger and eval- uating through several layers of mapping. Each layer might introduce some noise which accumu- lates and leads to a biased model. Moreover, the tagset mappings are not available for many resource-poor languages. We therefore also pro- posed a method to automatically match between tagsets based on a two-output maximum entropy model. On the resource-poor language Mala- gasy, we achieved the accuracy of 85.6% com- pared with the state-of-the-art of 81.2% ( . Unlike their method, we additionally use a small amount of parallel data.</p><p>In future work, we would like to improve the performance of DPM by collecting more parallel data. <ref type="bibr" target="#b6">Duong et al. (2013a)</ref> pointed out that using a different source language can greatly alter the performance of the target language POS tagger. We would like to experiment with different source languages other than English. We assume that we have 1,000 tokens for each language. Thus, for the 8 languages we considered we will have 8,000 an- notated tokens. Currently, we treat each language independently, however, it might also be interest- ing to find some way to incorporate information from multiple languages simultaneously to build the tagger for a single target language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>da</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The size of annotated data from 
CoNLL (Buchholz and Marsi, 2006), and the 
number of tags included and missing for 8 lan-
guages. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>also used sentence alignment scores to rank sentences. Their model stabilizes after us- ing 200k sentences. We conclude that 200k sen- tences is enough and capture most information from the parallel data.</figDesc><table>Features 
Descriptions 
W@-1 
Previous word 
W@+1 
Next word 
W@0 
Current word 
CAP 
First character is capitalized 
NUMBER Is number 
PUNCT 
Is punctuation 
SUFFIX@k Suffix up to length 3 (k &lt;= 3) 
WC 
Word class 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Feature template for a maximum entropy 
tagger 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The coverage, and POS tagging accuracy with and without tagset mapping of directly projected 
labels, averaged over 8 languages for different data sizes 

Model 
da 
nl 
de 
el 
it 
pt 
es 
sv 
Avg 
All features 
64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2 
-Word Class 
64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9 
-Suffix 
64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6 
-Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6 
-Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8 

</table></figure>

			<note place="foot" n="1"> http://www.wiktionary.org/</note>

			<note place="foot" n="2"> Many of these are mistakes in the mapping, however, they are indicative of the kinds of issues expected in lowresource languages.</note>

			<note place="foot" n="3"> Täckström et al. (2013) train a CRF on incomplete data, using a tag dictionary heuristic to define a &apos;gold standard&apos; lattice over label sequences.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Dan Garreette, Jason Baldridge and Noah Smith for Malagasy and Kin-yarwanda datasets. This work was supported by the University of Melbourne and National ICT Australia (NICTA). NICTA is funded by the Aus-tralian Federal and Victoria State Governments, and the Australian Research Council through the ICT Centre of Excellence program. Dr Cohn is the recipient of an Australian Research Council Fu-ture Fellowship (project number FT130101105).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of HLT-NAACL</title>
		<meeting>eeding of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="39" to="71" />
		</imprint>
		<respStmt>
			<orgName>COMPUTATIONAL LINGUISTICS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TnT: A statistical part-ofspeech tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Applied Natural Language Processing (ANLP &apos;00)</title>
		<meeting>the Sixth Conference on Applied Natural Language Processing (ANLP &apos;00)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two decades of unsupervised pos induction: How far have we come?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with bilingual graph-based projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="600" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Increasing the quality and quantity of source language data for Unsupervised CrossLingual POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1243" to="1249" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simpler unsupervised POS tagging with bilingual projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="634" to="639" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A maximum entropy/minimum divergence translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning a part-of-speech tagger from two hours of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Real-world semi-supervised learning of postaggers for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Em can find pretty good hmm pos-taggers (when given a good start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meni</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="746" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A resource-light approach to Russian morphology: Tagging Russian using Czech resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Hana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;04)</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting auxiliary distributions in stochastic unificationbased grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference</title>
		<meeting>the 1st North American Chapter of the Association for Computational Linguistics Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Machine Translation Summit (MT Summit X)</title>
		<meeting>the Tenth Machine Translation Summit (MT Summit X)<address><addrLine>Phuket, Thailand. AAMT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wiki-ly supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1389" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An efficient method for determining bilingual word classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL &apos;99</title>
		<meeting>the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL &apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Turkey, may</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gertjan Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning part-of-speech taggers with inter-annotator agreement loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross language POS taggers (and other tools) for Indian languages: An experiment with Kannada using Telugu resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Sharoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP workshop on Cross Lingual Information Access: Computational Linguistics and the Information Need of Multilingual Societies. (CLIA 2011 at IJNCLP 2011), Chiang Mai</title>
		<meeting>IJCNLP workshop on Cross Lingual Information Access: Computational Linguistics and the Information Need of Multilingual Societies. (CLIA 2011 at IJNCLP 2011), Chiang Mai<address><addrLine>Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="693" to="723" />
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A bayesian lda-based model for semi-supervised partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J.C. Platt, D. Koller, and Y. Singer a nd S.T. Roweis</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1 (NAACL &apos;03)</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1 (NAACL &apos;03)<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed word clustering for large scale class-based language modeling in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL International Conference Proceedings</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL &apos;01</title>
		<meeting>the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, NAACL &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
