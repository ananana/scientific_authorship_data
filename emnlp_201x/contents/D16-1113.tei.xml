<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Gram Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taygun</forename><surname>Kekeç</surname></persName>
							<email>taygunkekec@gmail.com,D.M.J.Tax@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Bioinformatics Laboratory</orgName>
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<postCode>2628CD</postCode>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Pattern Recognition and Bioinformatics Laboratory</orgName>
								<orgName type="institution">Delft University of Technology Delft</orgName>
								<address>
									<postCode>2628CD</postCode>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Gram Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1060" to="1065"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embedding models learn vectorial word representations that can be used in a variety of NLP applications. When training data is scarce, these models risk losing their generalization abilities due to the complexity of the models and the overfitting to finite data. We propose a regularized embedding formulation , called Robust Gram (RG), which penalizes overfitting by suppressing the disparity between target and context embeddings. Our experimental analysis shows that the RG model trained on small datasets generalizes better compared to alternatives, is more robust to variations in the training set, and correlates well to human similarities in a set of word similarity tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings represent each word as a unique vector in a linear vector space, encoding particular semantic and syntactic structure of the natural lan- guage ( <ref type="bibr" target="#b2">Arora et al., 2016)</ref>. In various lingual tasks, these sequence prediction models shown superior re- sults over the traditional count-based models <ref type="bibr" target="#b3">(Baroni et al., 2014</ref>). Tasks such as sentiment analysis <ref type="bibr" target="#b13">(Maas et al., 2011</ref>) and sarcasm detection ( <ref type="bibr" target="#b6">Ghosh et al., 2015</ref>) enjoys the merits of these features.</p><p>These word embeddings optimize features and predictors simultaneously, which can be interpreted as a factorization of the word cooccurence matrix C. In most realistic scenarios these models have to be learned from a small training set. Furthermore, word distributions are often skewed, and optimiz- ing the reconstruction ofˆCofˆ ofˆC puts too much empha- sis on the high frequency pairs ( <ref type="bibr" target="#b11">Levy and Goldberg, 2014</ref>). On the other hand, by having an unlucky and scarce data sample, the estimatedˆCestimatedˆ estimatedˆC rapidly deviates from the underlying true cooccurence, in particu- lar for low-frequency pairs ( <ref type="bibr" target="#b10">Lemaire and Denhire, 2008)</ref>. Finally, noise (caused by stemming, removal of high frequency pairs, typographical errors, etc.) can increase the estimation error heavily ( <ref type="bibr" target="#b1">Arora et al., 2015)</ref>.</p><p>It is challenging to derive a computationally tractable algorithm that solves all these problems. Spectral factorization approaches usually employ Laplace smoothing or a type of SVD weighting to alleviate the effect of the noise <ref type="bibr" target="#b19">(Turney and Pantel, 2010)</ref>. Alternatively, iteratively optimized embed- dings such as Skip Gram (SG) model <ref type="bibr" target="#b15">(Mikolov et al., 2013b</ref>) developed various mechanisms such as undersampling of highly frequent hub words apriori, and throwing rare words out of the training.</p><p>Here we propose a fast, effective and general- izable embedding approach, called Robust Gram, that penalizes complexity arising from the factorized embedding spaces. This design alleviates the need from tuning the aforementioned pseudo-priors and the preprocessing procedures. Experimental results show that our regularized model 1) generalizes bet- ter given a small set of samples while other methods yield insufficient generalization 2) is more robust to arbitrary perturbations in the sample set and alterna- tions in the preprocessing specifications 3) achieves much better performance on word similarity task, especially when similarity pairs contains unique and hardly observed words in the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Robust Gram Embeddings</head><p>Let |y| = V the vocabulary size and N be the total number of training samples. Denote x, y to be V × 1 discrete word indicators for the context and target: corresponding to the context and word indicators c, w in word embedding literature. Define Ψ d×V and Φ d×V as word and context embedding matri- ces. The projection on the matrix column space, Φx, gives us the embedding x ∈ R d . We use Φx and Φ x interchangeably. Using a very general formulation for the regularized optimization of a (embedding) model, the following objective is minimized:</p><formula xml:id="formula_0">J = N i L(Ψ, Φ, x i , y i ) + g(Ψ, Φ)<label>(1)</label></formula><p>where L(Ψ, Φ, x i , y i ) is the loss incurred by embed- ding example target y i using context x i and embed- ding parameters Ψ, Φ, and where g(Ψ, Φ) is a reg- ularization of the embedding parameters. Different embedding methods differ in the form of specified loss function and regularization. For instance, the Skip Gram likelihood aims to maximize the follow- ing conditional:</p><formula xml:id="formula_1">L(Ψ, Φ, x, y) = − log p(y|x, Φ, Ψ) = − log exp(Ψ T y Φ x ) y exp(Ψ T y Φ x )<label>(2)</label></formula><p>This can be interpreted as a generalization of Multinomial Logistic Regression (MLR). Rewriting</p><formula xml:id="formula_2">(Ψy) T (Φx) = (y T Ψ T Φx) = y T W x = W y x</formula><p>shows that the combination of Φ and Ψ become the weights in the MLR. In the regression the input x is trans- formed to directly predict y. The Skip Gram model, however, transforms both the context x and the tar- get y, and can therefore be seen as a generalization of the MLR. It is also possible to penalize the quadratic loss between embeddings ( <ref type="bibr" target="#b7">Globerson et al., 2007)</ref>:</p><formula xml:id="formula_3">L(.) = − log exp(−||Ψ y − Φ x || 2 ) y exp(−||Ψ y − Φ x || 2 )<label>(3)</label></formula><p>Since these formulations predefine a particular embedding dimensionality d, they impose a low rank constraint on the factorization W = Ψ T Φ. This means that g(Ψ, Φ) contains λrank(Φ T Ψ) with a sufficiently large λ. The optimization with an explicit rank constraint is NP hard. Instead, approximate rank constraints are utilized with a Trace Norm ( <ref type="bibr" target="#b5">Fazel et al., 2001</ref>) or Max Norm <ref type="bibr" target="#b18">(Srebro and Shraibman, 2005</ref>). However, adding such constraints usually requires semidefinite programs which quickly becomes computationally prohibitive even with a moderate vocabulary size.</p><p>Do these formulations penalize the complexity? Embeddings under quadratic loss are already reg- ularized and avoids trivial solutions thanks to the second term. They also incorporate a bit weighted data-dependent 2 norm. Nevertheless, choosing a log-sigmoid loss for Equation 1 brings us to the Skip Gram model and in that case, p regularization is not stated. Without such regularization, unbounded op- timization of 2V d parameters has potential to con- verge to solutions that does not generalize well.</p><p>To avoid this overfitting, in our formulation we choose g 1 as follows:</p><formula xml:id="formula_4">g 1 = V v λ 1 ||Ψ v || 2 2 + ||Φ v || 2 2 (4)</formula><p>where Ψ v is the row vector of words. Moreover, an appropriate regularization can also penalize the deviance between low rank matrices Ψ and Φ. Although there are words in the language that may have different context and target represen- tations, such as the 1 , it is natural to expect that a large proportion of the words have a shared repre- sentation in their context and target mappings. To this end, we introduce the following regularization:</p><formula xml:id="formula_5">g 2 = λ 2 ||Ψ − Φ|| 2 F (5)</formula><p>where F is the Frobenius matrix norm. This as- sumption reduces learning complexity significantly while a good representation is still retained, opti- mization under this constraint for large vocabular- ies is going to be much easier because we limit the degrees of freedom.</p><p>The Robust Gram objective therefore becomes:</p><formula xml:id="formula_6">LL+λ 1 V v ||Ψ v || 2 2 + ||Φ v || 2 2 +λ 2 ||Ψ−Φ|| 2 F (6) where LL = N i L(p(y i |x i , Ψ, Φ))</formula><p>is the data log- likelihood, p(y i |x i , Ψ, Φ) is the loglinear prediction model, and L the cross entropy loss. Since we are in the pursuit of preserving/restoring low masses inˆCinˆ inˆC, norms such as 2 allow each element to still possess a small probability mass and encourage smoothness in the factorized Ψ T Φ matrix. As L is picked as the cross entropy, Robust Gram can be interpreted as a more principled and robust counterpart of Skip Gram objective.</p><p>One may ask what particular factorization Equa- tion 6 induces. The objective searches for Ψ, Φ ma- trices that have similar eigenvectors in the vector space. A spectral PCA embedding obtains an asym- metric decomposition W = U ΣV T with Ψ = U and Φ = ΣV , albeit a convincing reason for embed- ding matrices to be orthonormal lacks. In the Skip Gram model, this decomposition is more symmet- ric since neither Ψ nor Φ are orthonormal and di- agonal weights are distributed across the factorized embeddings. A symmetric factorization would be: <ref type="bibr" target="#b11">Levy and Goldberg, 2014</ref>). The objective in Eq. 6 converges to a more symmetric decomposition since ||Ψ − Φ|| is penal- ized. Still some eigenvectors across context and tar- get maps are allowed to differ if they pay the cost. In this sense our work is related to power SVD ap- proaches <ref type="bibr" target="#b4">(Caron, 2000</ref>) in which one searches an a to minimize ||W − U Σ a Σ 1−a V T ||. In our formula- tion, if we enforce a solution by applying a strong constraint on ||Ψ − Φ|| 2 F , then our objective will gradually converge to a symmetric powered decom- position such that U ≈ V .</p><formula xml:id="formula_7">Ψ = U Σ 0.5 ,Φ = Σ 0.5 V T as in (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>The experiments are performed on a subset of the Wikipedia corpus containing approximately 15M words. For a systematic comparison, we use the same symmetric window size adopted in <ref type="bibr" target="#b17">(Pennington et al., 2014</ref>), 10. Stochastic gradient learning rate is set to 0.05. Embedding dimensionality is set to 100 for model selection and sensitivity anal- ysis. Unless otherwise is stated, we discard the most frequent 20 hub words to yield a final vocabulary of 26k words. To understand the relative merit of our approach 2 , Skip Gram model is picked as the baseline. To retain the learning speed, and avoid inctractability of maximum likelihood learning, we learn our embeddings with Noise Contrastive Es- timation using a negative sample ( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Selection</head><p>For model selection, we are going to illustrate the log likelihood of different model instances. How- ever, exact computation of the LL is computation- ally difficult since a full pass over the validation likelihood is time-consuming with millions of sam- ples. Hence, we compute a stochastic likelihood with a few approximation steps. We first subsam- ple a million samples rather than a full evaluation set, and then sample few words to predict in the window context similar to the approach followed in ( <ref type="bibr" target="#b11">Levy and Goldberg, 2014</ref>). Lastly, we approximate the normalization factor with one negative sample for each prediction score (Mnih and Kavukcuoglu, 2013)( <ref type="bibr" target="#b8">Gutmann and Hyvärinen, 2012)</ref>. Such an approximation works fine and gives smooth error curves. The reported likelihoods are computed by averaging over 5-fold cross validation sets.</p><p>Results. <ref type="figure" target="#fig_0">Figure 1</ref> shows the likelihood LL ob- tained by varying {λ 1 , λ 2 }. The plot shows that there exits a unique minimum and both constraints contribute to achieve a better likelihood compared to their unregularized counterparts (for which λ 1 = λ 2 = 0). In particular, the regularization imposed by the differential of context and target embeddings g 2 contributes more than the regularization on the em-beddings Ψ and Φ separately. This is to be expected as g 2 also incorporates an amount of norm bound on the vectors. The region where both constraints are employed gives the best results. Observe that we can simply enhance the effect of g 2 by adding a small amount of bounded norm g 1 constraint in a stable manner. Doing this with pure g 2 is risky be- cause it is much more sensitive to the selection of λ 2 . These results suggest that the convex combina- tion of stable nature of g 1 with potent regularizer of g 2 , finally yields comparably better regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sensitivity Analysis</head><p>In order to test the sensitivity of our model and base- line Skip Gram to variations in the training set, we perform two sensitivity analyses. First, we simu- late a missing data effect by randomly dropping out γ ∈ <ref type="bibr">[0,</ref><ref type="bibr">20]</ref> percent of the training set. Under such a setting, robust models are expected to be effected less from the inherent variation. As an addition, we inspect the effect of varying the minimum cut- off parameter to measure the sensitivity. In this ex- periment, from a classification problem perspective, each instance is a sub-task with different number of classes (words) to predict. Instances with small cut-off introduces classification tasks with very few training samples. This cut-off choice varies in differ- ent studies <ref type="bibr" target="#b17">(Pennington et al., 2014;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013b)</ref>, and it is usually chosen based on heuristic and storage considerations. Results. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the likelihood of the Robust and Skip Gram model by varying the dropout ratio on the training set. As the training set shrinks, both models get lower LL. Nevertheless, likelihood decay of Skip Gram is relatively faster. When 20% drop is applied, the LL drops to 74% in the SG model. On the other hand the RG model not only starts with a much higher LL, the drop is also to 75.5%, suggesting that RG objective is more resis- tant to random variations in the training data. <ref type="figure" target="#fig_2">Figure 3</ref> shows the results of varying the rare- words cut-off threshold. We observe that the like- lihood obtained by the Skip Gram is consistently lower than that of the Robust Gram. The graph shows that throwing out these rare words helps the objective of SG slightly. But for the Robust Gram re- moving the rare words actually means a significant decrease in useful information, and the performance starts to degrade towards the SG performance. RG avoids the overfitting occurring in SG, but still ex- tracts useful information to improve the generaliza- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Similarity Performance</head><p>The work of ( <ref type="bibr">Schnabel et al., 2015)</ref> demonstrates that intrinsic tasks are a better proxy for measuring the generic quality than extrinsic evaluations. Mo- tivated by this observation, we follow the experi- mental setup of ( <ref type="bibr">Schnabel et al., 2015;</ref><ref type="bibr" target="#b0">Agirre et al., 2009)</ref>, and compare word correlation estimates of each model to human estimated similarities with Spearman's correlation coefficient. The evaluation is performed on several publicly available word sim- ilarity datasets having different sizes. For datasets having multiple subjects annotating the word simi- larity, we compute the average similarity score from all subjects.</p><p>We compare our approach to set of techniques on the horizon of spectral to window based approaches. A fully spectral approach, HPCA, (Lebret and Le-bret, 2013) extracts word embeddings by running a Hellinger PCA on the cooccurrence matrix. For this method, context vocabulary upper and lower bound parameters are set to {1, 10 −5 }, as promoted by its author. <ref type="bibr">GLoVe (Pennington et al., 2014</ref>) approach formulates a weighted least squares problem to com- bine global statistics of cooccurence and efficiency of window-based approaches. Its objective can be interpreted as an alternative to the cross-entropy loss of Robust Gram. The x max , α values of the GLoVe objective is by default set to 100, 3/4. Finally, we also compare to shallow representation learning net- works such as Skip Gram and Continuous Bag of Words (CBoW) ( <ref type="bibr" target="#b14">Mikolov et al., 2013a</ref>), competitive state of the art window based baselines.</p><p>We set equal window size for all these models, and iterate three epochs over the training set. To yield more generality, all results obtained with 300 dimensional embeddings and subsampling parame- ters are set to 0. For Robust Gram approach, we have set λ 1 , λ 2 = {0.3, 0.3}. To obtain the similarity re- sults, we use the final Φ context embeddings.</p><p>Results. <ref type="table" target="#tab_1">Table 1</ref> depicts the results. The first ob- servation is that in this setting, obtaining word sim- ilarity using HPCA and GLoVe methods are subop- timal. Frankly, we can conjecture that this scarce data regime is not in the favor of the spectral meth- ods such as HPCA. Its poor performance can be at- tributed to its pure geometric reconstruction formu- lation, which runs into difficulties by the amount of inherent noise. Compared to these, CBoW's perfor- mance is moderate except in the RW dataset where it performs the worst. Secondly, the performance of the SG is relatively better compared to these ap- proaches. Surprisingly, under this small data setting, RG outperforms all of its competitors in all datasets except for RG65, a tiny dataset of 63 words con- taining very common words. It is admissible that RG sacrifices a bit in order to generalize to a large variety of words. Note that it especially wins by a margin in MEN and Rare Words (RW) datasets, having the largest number of similarity query sam- ples. As the number of query samples increases, RG embeddings' similarity modeling accuracy be- comes clearly perceptible. The promising result Ro- bust Gram achieves in RW dataset also sheds light on why CBoW performed worst on RW: CBOW overfits rapidly confirming the recent studies on the  stability of CBoW ( <ref type="bibr" target="#b12">Luo et al., 2014</ref>). Finally, these word similarity results suggest that RG embeddings can yield much more generality under data scarcity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents a regularized word embedding approach, called Robust Gram. In this approach, the model complexity is penalized by suppressing de- viations between the embedding spaces of the tar- get and context words. Various experimental results show that RG maintains a robust behaviour under small sample size situations, sample perturbations and it reaches a higher word similarity performance compared to its competitors. The gain from Robust Gram increases notably as diverse test sets are used to measure the word similarity performance. In future work, by taking advantage of the promis- ing results of Robust Gram, we intend to explore the model's behaviour in various settings. In particu- lar, we plan to model various corpora, i.e. predictive modeling of sequentially arriving network packages. Another future direction might be encoding avail- able domain knowledge by additional regularization terms, for instance, knowledge on synonyms can be used to reduce the degrees of freedom of the opti- mization. We also plan to enhance the underlying optimization by designing Elastic constraints ( <ref type="bibr" target="#b20">Zou and Hastie, 2005</ref>) specialized for word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The LL objective for varying λ 1 , λ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training dropouts effect on LL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LL w.r.t the cut-off parameter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Spearman's ρ coefficient. Higher is better. 

</table></figure>

			<note place="foot" n="1"> Consider prediction of Suleiman from the, and the from oasis. We expect the to have different vectorial representations.</note>

			<note place="foot" n="2"> Our implementation can be downloaded from github.com/taygunk/robust gram embeddings</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors acknowledge funding by the Dutch Organization for Scientific Research (NWO; grant 612.001.301). We also would like to thank Hamdi Dibeklioglu and Mustafa Unel for their kind support during this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno>abs/1502.03520</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Linear algebraic structure of word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>with applications to polysemy. CoRR, abs/1601.03764</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experiments with lsa scoring: Optimal rank and basis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIAM Computational Information Retrieval Workshop</title>
		<meeting>of SIAM Computational Information Retrieval Workshop</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A rank minimization heuristic with application to minimum order system approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitham</forename><surname>Hindi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 American Control Conference</title>
		<meeting>the 2001 American Control Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="4734" to="4739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sarcastic or not: Word embeddings to predict the literal or sarcastic meaning of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Euclidean embedding of cooccurrence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2265" to="2295" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Word emdeddings through hellinger PCA. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebret</surname></persName>
		</author>
		<idno>abs/1312.5542</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Effects of highorder co-occurrences on word semantic similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benot</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Denhire</surname></persName>
		</author>
		<idno>abs/0804.0143</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study on the cbow model&apos;s overfitting and stability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Web-scale Knowledge Representation Retrieval &amp;#38; Reasoning, Web-KR &apos;14</title>
		<meeting>the 5th International Workshop on Web-scale Knowledge Representation Retrieval &amp;#38; Reasoning, Web-KR &apos;14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="9" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1310.4546</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar. Tobias Schnabel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rank, tracenorm and max-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Shraibman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3559</biblScope>
			<biblScope unit="page" from="545" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
