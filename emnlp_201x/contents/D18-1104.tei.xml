<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compact Personalized Models for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lilt, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Simianer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lilt, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lilt, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compact Personalized Models for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="881" to="886"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>881</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose and compare methods for gradient-based domain adaptation of self-attentive neu-ral machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture-combining a state-of-the-art self-attentive model with compact domain adaptation-provides high quality personalized machine translation that is both space and time efficient.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Professional translators typically translate a collec- tion of related documents drawn from a domain for which they have a set of previously translated examples. Domain adaptation is critical to provid- ing high quality suggestions for interactive machine translation and post-editing interfaces. When many translators use the same shared service, the system must train and apply a personalized adapted model for each user. We describe a system architecture and training method that achieve high space efficiency, time efficiency, and translation performance by en- couraging structured sparsity in the set of offset tensors stored for each user.</p><p>Effective model personalization requires both batch adaptation to an in-domain training set, as well as incremental adaptation to the test set. Batch adaptation is applied when a user uploads relevant translated documents before starting to work. Incre- mental adaptation is applied when a user provides a correct translation of each segment just after receiv- ing machine translation suggestions, and the system is able to train on that correction before generating suggestions for the next segment. This is referred to as a posteriori adaptation by <ref type="bibr" target="#b21">Turchi et al. (2017)</ref>. Our experiments compare both types of adaptation. There are cases for which incremental adaptation achieves better performance using fewer examples, as examples drawn directly from the test set are often highly relevant to subsequent parts of that test set. There are also cases for which the gains from both types of domain adaptation are additive.</p><p>The time required to translate and to adapt both must be minimal in a personalized translation ser- vice. Interactive translation requires suggestions to be generated at typing speed, and incremental adap- tation must occur within a few hundred milliseconds to keep up with a translator's typical workflow. The service can be expected to store models for a large number of users and dynamically load and adapt models for many active users concurrently. There- fore, minimizing the number of parameters stored for each user's personalized model is important both for reducing storage requirements and latency. We achieve space and time efficiency by representing each user's model as an offset from the unadapted baseline parameters and encouraging most offset tensors to be zero during adaptation.</p><p>We show that group lasso regularization can be applied to a self-attentive Transformer model to freeze up to 75% of the parameters with minimal or no loss of adapted translation quality across ex- periments on four English→German data sets. We confirm these findings for six additional language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is extensive work on incremental adapta- tion from human post edits or simulated post edits, both for statistical machine translation ( <ref type="bibr" target="#b6">Green et al., 2013;</ref><ref type="bibr">Denkowski et al., 2014a,b;</ref><ref type="bibr" target="#b23">Wuebker et al., 2015)</ref> and neural machine translation ( <ref type="bibr" target="#b14">Peris et al., 2017;</ref><ref type="bibr" target="#b21">Turchi et al., 2017;</ref><ref type="bibr" target="#b8">Karimova et al., 2017)</ref>. Both <ref type="bibr" target="#b21">Turchi et al. (2017)</ref> and <ref type="bibr" target="#b8">Karimova et al. (2017)</ref> apply vanilla fine-tuning algorithms. In addition to fine-tuning towards user corrections, the former applies a priori adaptation to retrieved data that is similar to the incoming source sentences. <ref type="bibr" target="#b14">Peris et al. (2017)</ref> propose a variant of fine-tuning with passive-aggressive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models.</p><p>Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: <ref type="bibr" target="#b5">Duh et al. (2010)</ref> presented an application of multi-task learning via 1 // 2 regularization for feature selection in an N - best reranking task. A similar approach, employ- ing 1 // 2 regularization for feature selection and multi-task learning, was developed by <ref type="bibr" target="#b17">Simianer et al. (2012)</ref> and <ref type="bibr" target="#b17">Simianer and Riezler (2013)</ref> for tuning of SMT systems. Both works report improve- ments from regularization.</p><p>Techniques for enforcing sparse models using 1 regularization during stochastic gradient descent optimization were previously developed for linear models ( <ref type="bibr" target="#b20">Tsuruoka et al., 2009</ref>).</p><p>An extremely space efficient method for person- alized model adaptation is presented by <ref type="bibr" target="#b12">Michel and Neubig (2018)</ref>. Here, adaptation is performed solely on the output vocabulary bias vector. An- other notable approach for creating compact mod- els is student-teacher-training or knowledge distil- lation <ref type="bibr" target="#b9">(Kim and Rush, 2016)</ref>. To the best of our knowledge, this has not been applied in a domain adaptation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Self-Attentive Translation Model</head><p>The neural machine translation systems used in this work are based on the Transformer model intro- duced by <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref>, which uses self- attention rather than recurrent or convolutional lay- ers to aggregate information across words. In addi- tion to its superior performance, its main practical advantage over recurrent models is faster training.</p><p>The Transformer follows the encoder-decoder paradigm. Source word vectors x 1 , . . . , x m are chosen from an embedding matrix X e . A series of stacked encoder layers generate intermediate representations z 1 , . . . , z m . Each layer of the en- coder consists of two sub-layers: a multi-head self- attention layer that uses scaled dot-product atten- tion over all source positions, followed by a feed- forward filter layer. Layer normalization ( <ref type="bibr">Ba et al., 2016</ref>), dropout ( <ref type="bibr" target="#b18">Srivastava et al., 2014)</ref>, and resid- ual connections <ref type="bibr" target="#b7">(He et al., 2016</ref>) are applied to each sub-layer.</p><p>A series of stacked decoder layers produces a sequence of target word vectors y 1 , . . . , y n . Each decoder layer has three sub-layers: self-attention, encoder-attention, and a filter. For target position j, the self-attention layer can attend to any previous target position j ∈ <ref type="bibr">[1, j]</ref>, with target words offset by one so that representations at j can observe word j −1, but not word j. The encoder-attention layer can attend to the final encoder state z i for any source position i ∈ <ref type="bibr">[1, m]</ref>. Observed target word vectors are chosen from an embedding matrix Y e , and target word j is predicted from y j via a soft-max layer parameterized by an output projection matrix Y o .</p><p>The encoders in this work have six layers that have a self-attention sub-layer size of 256 and a filter sub-layer size of 512. Each filter performs two linear transformations and a ReLU activation:</p><formula xml:id="formula_0">f (x) = max(0, xW 1 + b 1 )W 2 + b 2 .</formula><p>The decoders in this work have three layers, and all sub-layer sizes are 256. The decoder sub- layers are simplified versions of those described in <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref>: The filter sub-layers perform only a single linear transformation, and layer nor- malization is only applied once per decoder layer after the filter sub-layer.</p><p>Unlike in <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref>, none of X e , Y e , or Y o share parameters in our TensorFlow 1 imple- mentation. Baseline models are optimized with Adam ( <ref type="bibr" target="#b10">Kingma and Ba, 2015)</ref>. stochastic gradient descent (SGD) is the most effec- tive optimizer for fine tuning ( <ref type="bibr" target="#b21">Turchi et al., 2017)</ref>. In our experiments, batch adaptation uses a batch size of 7000 words for 10 Epochs and a fixed learn- ing rate of 0.1, dropout of 0.1, and label smoothing with ls = 0.1 ( <ref type="bibr" target="#b19">Szegedy et al., 2016)</ref>.</p><p>Incremental adaptation uses a batch size of one and a learning rate of 0.01. To ensure a strong adaptation effect within a single document, we set dropout and label smoothing to zero and perform up to three SGD updates on each segment. After each update, we measure the model perplexity on the current training example and continue with another update if the perplexity is still above 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Offset Tensors</head><p>In a personalized translation service, adapted mod- els need to be loaded quickly, so a space-efficient representation is critical for time efficiency as well. Production speed requirements using contemporary cloud hardware limit model sizes to roughly 10M parameters per user, while a high-quality baseline Transformer model typically requires 35M parame- ters or more. We propose to store the parameters of an adapted model as an offset from the baseline model. Each tensor is a sum W = W b +W u , where W b is from the baseline model and is shared across all adapted models, while the offset W u is specific to an individual user domain. Space efficiency is achieved by only storing W u for a subset of tensors and setting the rest of the offset tensors to zero.</p><p>One approach to achieving model sparsity is to manually partition the network into a small number of regions and systematically evaluate translation performance when storing offsets for only one re- gion. We define five distinct regions, which are evaluated in isolation: Outer layers (the first and last layers of both encoder and decoder), inner lay- ers (all the remaining layers), the two embedding matrices X e and Y e , and the output projection ma- trix Y o . The latter three are each stored as a single matrix and each contributes 10.3M parameters to the full model size in English→German. During adaptation, the embedding matrices are only up- dated for vocabulary present in the training exam- ples, and so the offsets can be stored efficiently as a sparse collection of columns. The same principle can be applied to the output projection matrix by only updating parameters corresponding to vocabu- lary items that appears in the adaptation examples (denoted Sparse Output Proj. in <ref type="table" target="#tab_1">Table 1</ref>).</p><p>A second approach to achieving model sparsity is to use a procedure to select the subset of offset tensors that are stored. For example, we evaluate a simple policy that stores an offset for all tensors whose average change in parameter values is higher than a threshold. This set is selected on a develop- ment domain and held fixed for all other domains. We refer to this method as fixed adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tensor Selection via Group Lasso</head><p>A group sparse regularization penalty such as group lasso can be applied to the offset tensors for simul- taneous regularization and tensor selection. This penalty drives entire offset tensors to zero, so that they do not need to be stored or loaded. We add the following regularization term to the loss function (Scardapane et al., 2017):</p><formula xml:id="formula_1">R 1,2 (T ) = T ∈T |T ||∆T 2<label>(1)</label></formula><formula xml:id="formula_2">∆T 2 = τ ∈T ∆τ 2<label>(2)</label></formula><p>Here, each tensor corresponds to one group. T denotes the set of all tensors in the model, τ ∈ T the set of all weights within a single tensor and ∆τ the size of the offset for τ . Note that we are regu- larizing the difference between the parameters of the adapted model and the baseline model, rather than regularizing the full network parameters di- rectly. In this way, we maintain the expressive power of the full network while minimizing the size of the adapted models. Group lasso regular- ization is equivalent to 1 regularization when the group size is 1. Sparsity among groups is encour- aged because the 1 norm serves as a convex proxy for the 0 norm, which would explicitly penalize the number of non-zero elements <ref type="bibr" target="#b24">(Yuan and Lin, 2006</ref>). To facilitate tensor selection, we define a threshold ϑ to clip offset tensors ∆T with aver- age weight 1 |T | τ ∈T ∆τ &lt; ϑ to zero. Both the threshold ϑ and the regularization weight λ were manually tuned on a development domain and set to ϑ = 10 −4 and λ = 10 −6 . We apply clipping to all tensors except the embedding and output pro- jection matrices X e , Y e and Y o . As our production constraints allow us to retain only one of the three, we pre-select the sparse output projection as part of the model and exclude the embedding matrices from adaptation. This method will be denoted as Lasso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User1</head><p>User2 Autodesk IWSLT  <ref type="table" target="#tab_1">Table 1</ref>: Experimental results in BBBB (%) on the English→German data. We evaluate changes to each region of the network separately. In combination with sparse output projection, we also evaluate a fixed selection of parameters chosen by thresholding and a set selected dynamically for each data set using group lasso. The two bottom rows show repetition rates in % for the source and target sides of the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We first evaluate all techniques on an English→German Transformer network trained on 98M parallel sentence pairs. We apply byte pair encoding ( <ref type="bibr" target="#b16">Sennrich et al., 2016</ref>) separately to each language and obtain vocabularies with 40K unique tokens each. We refer to the unadapted model as Baseline. We evaluate on four domains. For development, we use a data set labeled User1 that was gathered from a user of the browser-based CAT (computer-aided translation) tool Lilt 2 and contains documents from the financial domain with 48K segments for batch adaptation and 1790 segments for testing and incremental adaptation. We further evaluate on a second user test set User2 (technical support, 31k batch adaptation, 1000 test segments); the public Autodesk corpus 3 , where we select the first 20k segments for batch adaptation and the next 1000 segments for testing; and the IWSLT corpus 4 (semi-technical talks), where we use all provided 206K sentences for batch adaptation and the dev2010 set (888 sentences) for testing. The overall best performing compact adaptation technique, group lasso regularization, is further evaluated on six other language pairs trained using production data sets collected from Lilt's user base: English↔French, English↔Russian and English↔Chinese. Adaptation is performed on user data from various domains (technical manuals, finance, legal), each with 8k-10k segments for batch adaptation and 2000 segments for testing and incremental adaptation. Translation quality is evaluated using the cased BBBB ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>) measure.  Experimental results in BBBB (%) on six production language pairs. We compare the unadapted baseline model with a full model and the model with sparse output projection and group lasso, both with application of batch and incremental adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>ished by storing a sparse variant that is restricted only to observed vocabulary. In addition, we evaluate two methods of choosing a subset of tensors procedurally. We first experi- ment with a fixed subset of tensor offsets that was chosen by selecting all tensors for which parame- ters were offset by more than 0.002 on average after batch adaptation on the User1 data set. This sim- ple procedure approaches the performance of full model adaptation, but stores only 27% of its param- eters. Dynamically selecting tensor offsets for each data set using group lasso regularization improves performance on 6 out of 8 data conditions. The combination of batch and incremental adap- tation yields further improvements, with the excep- tion of the User2 and IWSLT tasks, where incre- mental adaptation overall performs not as well as batch adaptation. For these tasks, both tests sets exhibit lower repetition rates <ref type="bibr">5 (Cettolo et al., 2014</ref>) than the test sets for the two other tasks (see the two bottom lines in <ref type="table" target="#tab_1">Table 1</ref>). The User2 test set is fur- thermore a random sample of non-consecutive text from a translation memory, which is suboptimal for incremental learning.</p><p>Altogether, we are able to achieve translation performance similar to full model adaptation with 25% of the total network parameters. Note that due to the selection of entire tensors with groupwise regularization, there is nearly zero space overhead incurred by storing a sparse set of offset tensors. <ref type="table" target="#tab_2">Table 2</ref> confirms our main findings on six other language pairs. We observe average improvements of 14.3 BBBB with our final compact model, which compares to 15.5 BBBB for full model adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We describe an efficient approach to personalized machine translation that stores a sparse set of ten- <ref type="bibr">5</ref> Repetition rates have been confirmed to be a suitable in- dicator for gains through incremental adaptation in numerous works ( <ref type="bibr" target="#b23">Wuebker et al., 2015;</ref>).</p><p>sor offsets for each user domain. Group lasso regu- larization applied to the offsets during adaptation achieves high space and time efficiency while yield- ing translation performance close to a full adapted model, for both batch and incremental adaptation and their combination.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>English→German results. Full 
model adaptation, where all offsets are stored, im-
proves over the baseline in all cases to various de-
grees. This full model contains only 25.8M param-
eters, as offsets for both embedding matrices are 
stored as sparse collections of columns for the vo-
cabulary present in the adaptation data. Next, we 
evaluate the impact of storing offsets only for one 
region at a time. We observe that among the three 
vocabulary matrices, the output projection Y o has 
the strongest impact on quality, which is not dimin-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="4"> Compact Adaptation 4.1 Fine Tuning Personalized machine translation requires batch adaptation to a domain-relevant bitext (such as a user provided translation memory) as well as incremental adaptation to the test set. We apply finetuning, which involves continuing to train model parameters with a gradient-based method on domainrelevant data, as a simple and effective method for neural translation domain adaptation (Luong and Manning, 2015). The fine-tuned model without regularization and clipping is denoted as the Full Model. Confirming previous work, we found that 1 https://www.tensorflow.org/</note>

			<note place="foot" n="2"> https://lilt.com 3 https://autodesk.app.box.com/ Autodesk-PostEditing 4 http://workshop2017.iwslt.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Online adaptation to post-edits for phrase-based statistical machine translation. Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Simianer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Wäschle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="309" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The repetition rate of text as a predictor of the effectiveness of machine translation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Association for Machine Translation in the Americas (AMTA)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="166" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning from post-editing: Online model adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real time adaptive machine translation for post-editing with cdec and transcenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Lacruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Humans and Computer-assisted Translation</title>
		<meeting>the EACL 2014 Workshop on Humans and Computer-assisted Translation<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">N-best reranking by multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The efficacy of human postediting for language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI Conference on Human Factors in Computing Systems</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A user-study on online adaptation of neural machine translation to human post-edits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sariya</forename><surname>Karimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Simianer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<idno>abs/1712.04853</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequencelevel knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<meeting><address><addrLine>Da Nang, Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extreme adaptation for personalized neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="312" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Online learning for neural machine translation post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Álvaro</forename><surname>Peris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cebrián</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Casacuberta</surname></persName>
		</author>
		<idno>abs/1706.03196</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Group sparse regularization for deep neural networks. Neurocomput</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Comminiello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="81" to="89" />
		</imprint>
	</monogr>
	<note>Amir Hussain, and Aurelio Uncini</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Simianer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan Riezler ;</forename><surname>Sofia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Patrick</forename><surname>Bulgaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Simianer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
	<note>Proceedings of the Eighth Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Jun&amp;apos;ichi Tsujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Continuous learning from human post-edits for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Amin Farajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="233" to="244" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical incremental adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1059" to="1065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="49" to="67" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
