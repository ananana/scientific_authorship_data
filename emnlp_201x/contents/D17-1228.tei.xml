<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Steering Output Style and Topic in Neural Response Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
							<email>diwang@cs.cmu.edu, jojic@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Steering Output Style and Topic in Neural Response Generation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2140" to="2150"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications , including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to to restrict style and topic without degrading output quality in conversational tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural encoder-decoder models have demon- strated great promise in many sequence gener- ation tasks, including neural machine transla- tion ( <ref type="bibr" target="#b28">Sutskever et al., 2014;</ref><ref type="bibr" target="#b18">Luong et al., 2015;</ref><ref type="bibr">Wu et al., 2016)</ref>, image captioning ( <ref type="bibr">Xu et al., 2015</ref>), summarization ( <ref type="bibr" target="#b23">Rush et al., 2015;</ref><ref type="bibr" target="#b7">Gu et al., 2016;</ref><ref type="bibr" target="#b10">Kikuchi et al., 2016)</ref>, and conversation gen- eration ( <ref type="bibr" target="#b29">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b26">Sordoni et al., 2015;</ref><ref type="bibr" target="#b24">Shang et al., 2015;</ref><ref type="bibr" target="#b25">Shao et al., 2017;</ref><ref type="bibr" target="#b24">Li et al., 2015</ref>). These encouraging early successes have motivated research interest in training more natural-sounding conversational systems based on large volumes of open-domain human-to-human interactions. In order to create more human-like chat context Where are you?</p><p>Twitter bot i'm on my way to london now. Star Wars bot we're heading to the dark side of jabba's palace. Hillary bot i'm here in philadelphia, pennsylvania Trump bot i went to the white house. we were in brooklyn. Kennedy bot i am on the question of the united states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>chat context What are you doing?</head><p>Twitter bot i'm just chilling with my dad, i'm just sitting in the kitchen and listening to music Star Wars bot i'm doing my magic solo.</p><p>Hillary bot i'm running for president, i'm going to be talking about some of these things Trump bot i'm building a wall, right now Kennedy bot today, i am asking the congress for a new program to make a new effort to increase the tax privileges and to stimulate patterns of conversation, the agents need to have recognizable (and tunable) style, just as individ- ual humans do, and also need to accept guidance from separate information processing modules in order to increase quality of responses. In an ex- treme case, an agent may be micro-managed by a human user who uses the neural model to en- force grammar and style (e.g., a level of polite- ness, or a type of humor), while driving the con- tent directly (e.g., by expressing In this manner, the neural model becomes an authoring tool, rather than an independent chat-bot. On the other hand, in fully automated agent systems, the agent may be influenced by a knowledge database, or some other artificial information system, while running in a pre-set style or a style deemed best based on the course of the conversation. One obstacle to achieving this with neural lan- guage generation models is that the sentence rep- resentation is distributed across all coordinates of the embedding vector in a way that is hard to dis- entangle, and thus control. In order to gain insight into the full distribution of what a decoder might produce given the prompt sentence as input, the model has to be heavily (and sometimes cleverly) sampled. The second problem is that neural mod- els only become highly functional after training with very large amounts of data, while the strongly recognizable style usually must be defined by a relatively tiny corpus of examples (e.g., all Sein- feld episodes, or all popular song lyrics).</p><p>In this paper, we address the challenge of how to enforce the decoder models to mimic a specific language style with only thousands of target sen- tences, as well as generating specific content in that style. We developed and experimented with several training and decoding procedures to allow the model to adapt to target language style and follow additional content guidance. Our experi- ments, conducted on an open-domain corpus of Twitter conversations and small persona corpora, show that our methods are capable of responding to queries in a transferred style without significant loss of relevance, and can respond within a specific topic as restricted by a human. Some examples of 'scenting' the base conversation model with par- ticular styles are shown in <ref type="table" target="#tab_0">Table 1</ref>. More can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recurrent neural network based encoder-decoder models have been applied to machine translation and quickly achieved state-of-the-art results <ref type="bibr" target="#b18">Luong et al., 2015)</ref>. As an ex- tension, the attention mechanism enables the de- coder to revisit the input sequence's hidden states and dynamically collects information needed for each decoding step. Specifically, our conversa- tion model is established based on a combination of the models of ( ) and ( <ref type="bibr" target="#b18">Luong et al., 2015</ref>) that we found to be effective. In section 3, we describe the attention-based neural encoder-decoder model we used in detail.</p><p>This work follows the line of research initiated by <ref type="bibr" target="#b22">(Ritter et al., 2011</ref>) and ( <ref type="bibr" target="#b29">Vinyals and Le, 2015)</ref> who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. <ref type="bibr" target="#b26">Sordoni et al. (2015)</ref> extended <ref type="bibr" target="#b22">(Ritter et al., 2011</ref>) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history. Recently, researchers have used neural encoder-decoder models to directly generate responses in an end-to-end fashion with- out relying on SMT phrase tables( <ref type="bibr" target="#b29">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b26">Sordoni et al., 2015;</ref><ref type="bibr" target="#b24">Shang et al., 2015;</ref><ref type="bibr" target="#b25">Shao et al., 2017;</ref><ref type="bibr" target="#b24">Li et al., 2015)</ref>.  defined a "persona" as the char- acter that an artificial agent, as actor, plays or per- forms during conversational interactions. Their dataset requires user identification for all speak- ers in the training set, while our methods treat the base data (millions of twitter conversations) as un- labeled, and the target persona is defined simply by a relatively small sample of their speech. In this sense, the persona can be any set of text data. In our experiments, for example, we used a generic Star Wars character that was based on the entire set of Star Wars scripts (in addition to 46 million base conversations from Twitter). This provides us with a system that can talk about almost anything, being able to respond to most prompts, but in a recognizable Star Wars style. Other possibilities include training (styling) on famous personalities, or certain types of poetry, or song lyrics, or even mixing styles by providing two or more datasets for styling. Thus our targets are highly recogniz- able styles, and use of these for emphasis (or cari- cature) by human puppeteers who can choose from multiple options and guide neural models in a di- rection they like. We expect that these tools might not only be useful in conversational systems, but could also be popular in social media for text au- thoring that goes well beyond spelling/grammar auto correction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Encoder-Decoder Background</head><p>In general, neural encoder-decoder models aim at generating a target sequence Y = y 1 , . . . , y Ty given a source sequence X = (x 1 , . . . , x Tx ). Each word in both source and target sentences, x t or y t , belongs to the source vocabulary V x , and the target vocabulary V y respectively. First, an encoder converts the source se- quence X into a set of context vectors C = {h 1 , h 2 , . . . , h Tx }, whose size varies with regard to the length of the source passage. This context representation is generated using a multi-layered recurrent neural network (RNN). The encoder RNN reads the source passage from the first token until the last one, where</p><formula xml:id="formula_0">h i = Ψ (h i−1 , E x [x t ]) .</formula><p>Here E x ∈ R |Vx|×d is an embedding matrix con- taining vector representations of words, and Ψ is a recurrent activation unit that we employ in the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>The decoder, which is also implemented as an RNN, generates one word at a time, based on the context vector set returned by the encoder. The decoder's hidden state ¯ h t is a fixed-length con- tinuous vector that is updated in the same way as encoder. At each time step t in the decoder, a time-dependent attentional context vector c t is computed based on the current hidden state of the decoder ¯ h t and the whole context set C. Decoding starts by computing the content- based score of each context vector as: e t,i = ¯ h t W a h i . This relevance score measures how helpful the i-th context vector of the source se- quence is in predicting next word based on the de- coder's current hidden state ¯ h t . Relevance scores are further normalized by the softmax function:</p><formula xml:id="formula_1">α t,i = exp(e t,i ) Tx j=1 exp(e t,j )</formula><p>, and we call α t,i the at- tention weight. The time-dependent context vec- tor c t is then the weighted sum of the context vectors with their attention weights from above:</p><formula xml:id="formula_2">c t = Tx i=1 α t,i h i .</formula><p>With the context vector c t and the hidden state h t , we then combine the information from both vectors to produce an attentional hidden state as follow: z t = tanh(W c [c t ; h t ]). The probability distribution for the next target symbol is computed by p(y t = k|˜yk|˜y &lt;t , X) ∝ exp(W s z t + b t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding with Selective Sampling</head><p>The standard objective function for neural encoder-decoder models is the log-likelihood of target T given source S, which at test time yields the statistical decision problem:</p><formula xml:id="formula_3">ˆ T = arg max T log p(T |S)}.<label>(1)</label></formula><p>However, as discussed in ( <ref type="bibr" target="#b24">Li et al., 2015;</ref><ref type="bibr" target="#b25">Shao et al., 2017)</ref>, simply conducting beam search over the above objective will tend to generate generic and safe responses that lack diversity, such as "I am not sure". In section 7.3, we present a ranking experiment in which we verify that an RNN-based neural decoder provides a poor approximation of the above conditional probability, and instead bi- ases towards the target language model p(T ). For- tunately, the backward model p(S|T ) empirically perform much better than p(T |S) on the relevance ranking task. Therefore, we directly apply Bayes' rule to Equation 1, as in statistical machine trans- lation ( <ref type="bibr" target="#b2">Brown et al., 1993)</ref>, and use:</p><formula xml:id="formula_4">ˆ T = arg max T log p(S|T ) + log p(T )}. (2)</formula><p>Since p(T |S) is empirically biased towards p(T ), in practice, this objective also resembles the Max- imum Mutual Information (MMI) objective func- tion in ( <ref type="bibr" target="#b24">Li et al., 2015)</ref>. The challenge now is to develop an effective search algorithm for a target words sequence that maximize the product in Equation 2. Here, we follow a similar process as in <ref type="bibr" target="#b32">(Wen et al., 2015)</ref> which generates multiple target hypotheses with stochastic sampling based on p(T |S), and then ranks them with the objective function 2 above. However, as also observed by <ref type="bibr" target="#b25">(Shao et al., 2017)</ref>, step-by-step naive sampling can accumulate errors as the sequence gets longer.</p><p>To reduce language errors of stochastic sam- pling, we introduce a sample selector to choose the next token among N stochastically sampled tokens based on the predicted output word dis- tributions. The sample selector, which is a multilayer perceptron in our experiments, takes the following features: 1) the log-probability of current sample word in p(w t |S); 2) the entropy of current predicted word distribution, wt P (w t |S) log P (w t |S) for all w t in the vo- cabulary; 3) the log-probability of current sample word in p(w t |∅), which we found effective in rank- ing task. The selector outputs a binary variable that indicates whether the current sample should be accepted or rejected.</p><p>At test time, if none of the N sampled tokens are above the classification threshold, we choose the highest scored token. If there are more than 1 ac- ceptable samples among N stochastically sampled tokens, we randomly choose one among them. Ideally, this permits us to safely inject diversity while maintaining language fluency. We also use the sample acceptor's probabilities as the language model score P (T ) for objective in equation 2.</p><p>As regards directly integrating beam-search, we found (a) that beam-search often produces a set of similar top-N candidates, and (b) that decoding with only the objective p(Y |X) can easily lead to irrelevant candidates. (See section 7.3) Therefore, we use the selective-sampling method to generate candidates for all our experiments; this (a) sam- ples stochastically then (b) selects using a learned objective from data. The sample-then-select ap-proach encourages more diversity (v.s. MMI's beam-search) while still maintain language flu- ency (v.s. naive-sampling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Output style restriction using a small 'scenting' dataset</head><p>In this section, we propose three simple yet effec- tive methods of influencing the language style of the output in the neural encoder-decoder frame- work. Our language style restricting setup as- sumes that there is a large open-domain parallel corpus that provides training for context-response relevance, and a smaller monologue speaker cor- pus that reflects the language characteristics of the target speaker. We will refer to this smaller set as a 'scenting' dataset, since it hints at, or insinuates, the characteristics of the target speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Rank: Search in the Target Corpus</head><p>Our first approach to scenting is to simply use the all sentences in the target speaker's corpus as generation candidates, ranked by the objective (2) for a given prompt. Since these sentences are naturally-occurring instead of generated word- by-word, we can safely assume p(T ) is constant (and high), and so the objective only requires sort- ing the sentences based on the backward model p(S|T ). RNN-based ranking methods are among the most effective methods for retrieving relevant re- sponses ( <ref type="bibr">Nyberg, 2015, 2016)</ref>. Thus this approach is a very strong baseline. Its lim- itation is also obvious: by limiting all possible responses to a fixed finite set of sentences, this method cannot provide a good response if such a response is not already in the scenting dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">M ultiply: Mixing the base model and the target language model during generation</head><p>In our second method we use both the vanilla encoder-decoder model trained on open-domain corpus and the target domain language model trained on the corpus while decoding output sen- tence. The idea is to use a speaker's language model, which is also RNN-based in our ex- periments, to restrict the open-domain encoder- decoder model's step-by-step word prediction. Similar ideas have been tested in domain adap- tation for statistical machine translation ( <ref type="bibr" target="#b12">Koehn and Schroeder, 2007)</ref>, where both in-domain and open-domain translation tables were used as can- didates for generating target sentence. Because open-domain encoder-decoder models are trained with various kinds of language patterns and top- ics, choosing a sequence that satisfies both mod- els may produce relevant responses that are also in the target language style. We found that a straightforward way of achieving this is to multi- ply the two models' distributions p 1 (t|S) λ 1 p 2 (t) λ 2 at each point and then re-normalize before sam- pling. The weights can be tuned either by the per- plexity on the validation set, or through manually controlling the trade-off between style restriction and answer accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">F inetune: Over-training on Target Corpus with Pseudo Context</head><p>Fine-tuning is a widely used in the neural network community to achieve transfer learning. This strat- egy permits us to train the neural encoder-decoder on a larger general parallel corpus, and then use the learned parameters to initialize the training of a styled model. Most of the time, however, the target speaker's corpus will lack training data in parallel form. For example, if we train on song lyrics or movie scripts, or political speeches, the data will not be in a question-answer form. To make encoder-decoder overtraining possible, we treat every sentence in the scenting corpus as a tar- get sentence T generated a pseudo context from the backward model p(S|T ) trained on the open- domain corpus. Over-training on such pairs im- parts the scenting dataset's language characteris- tics, while retaining the generality of the original model. We also found that the previous sentence in the styled corpus (i.e., previous sentence in the speech) provides helpful context for the current sentence, analogous with a question-answer link. Thus we use both pseudo context and the previ- ous sentence as possible sources S to fine-tune the in-domain decoder. To avoid overfitting, we stop overtraining when the perplexity on the in-domain validation set starts to increase. A corresponding sample acceptor is also trained for the fine-tuned model: we found it helpful to initialize this from the open-domain model's sample acceptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Restricting the Output Topic</head><p>We further introduce a topic restricting method for neural decoders based on the Counting Grid (Jo- jic and Perina, 2011) model, by treating language guidance as a topic embedding. Our model exten-sion provides information about the output topic in the form of an additional topic embedding vector to the neural net at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">CG: Counting Grids</head><p>The basic counting grid π k is a set of distribu- tions on the d-dimensional toroidal discrete grid E indexed by k. The grids in this paper are bi- dimensional and typically from (E x = 32) × (E y = 32) to (E x = 64) × (E y = 64) in size.</p><p>The index z indexes a particular word in the vo-</p><formula xml:id="formula_5">cabulary z = [1 . . . Z].</formula><p>Thus, π i (z) is the proba- bility of the word z at the d-dimensional discrete location i, and z π i (z) = 1 at every location on the grid. The model generates bags of words, each represented by a list of words w = {w n } N n=1 with each word w n taking an integer value between 1 and Z. The modeling assumption in the basic CG model is that each bag is generated from the dis- tributions in a single window W of a preset size, e.g., (W x = 5) × (W y = 5). A bag can be gener- ated by first picking a window at a d-dimensional location , denoted as W , then generating each of the N words by sampling a location k n for a par- ticular micro-topic π kn uniformly within the win- dow, and sampling from that micro-topic.</p><p>Because the conditional distribution p(k n |) is a preset uniform distribution over the grid locations inside the window placed at location , the variable k n can be summed out (Jojic and Perina, 2011), and the generation can directly use the grouped histograms</p><formula xml:id="formula_6">h (z) = 1 |W| j∈W π j (z),<label>(3)</label></formula><p>where |W| is the area of the window, e.g. 25 when 5×5 windows are used. In other words, the posi- tion of the window in the grid is a latent variable given which we can write the probability of the bag as</p><formula xml:id="formula_7">P (w|) = wn∈w h (wn) = wn∈w 1 |W| · j∈W π j (wn)<label>(4)</label></formula><p>As the grid is toroidal, a window can start at any position and there is as many h distributions as there are π distributions. The former will have a considerably higher entropy as they are averages of many π distributions. Although the basic CG model is essentially a simple mixture assuming the existence of a single source (one window) for all the features in one bag, it can have a very large number of (highly related) choices h to choose from. Topic models ( <ref type="bibr" target="#b1">Blei et al., 2003;</ref><ref type="bibr" target="#b14">Lafferty and Blei, 2006</ref>), on the other hand, are admixtures that capture word co-occurrence statistics by using a much smaller number of topics that can be more freely combined to explain a single document (and this makes it harder to visualize the topics and pin- point the right combination of topics to use in in- fluencing the output).</p><p>In a well-fit CG model, each data point tends to have a rather peaky posterior location distribution because the model is a mixture. The CG model can be learned efficiently using the EM algorithm because the inference of the hidden variables, as well as updates of π and h can be performed us- ing summed area tables <ref type="bibr" target="#b5">(Crow, 1984)</ref>, and are thus considerably faster than most of the sophisticated sampling procedures used to train other topic mod- els. The use of overlapping windows helps both in controlling the capacity of the model and in orga- nizing topics on the grid automatically: Two over- lapping windows have only slighly different h dis- tributions, making CGs especially useful in visu- alization applications where the grid is shown in terms of the most likely words in the component distributions π ( <ref type="bibr" target="#b21">Perina et al., 2014</ref>). <ref type="bibr">1</ref> Having trained the grid on some corpus (in our case a sample of the base model's corpus), the mapping of either a source S and/or target T sen- tence can be obtained by treating the sentences as bags of words. By appending one or both of these mappings to the decoder's embedding of the target T , the end-to-end encoder-decoder learning can be performed in a scenario where the decoder is ex- pected to get an additional hint through a CG map- ping. In our experiments, we only used the embed- ding of the target T as the decoder hint, and we ap- pended the full posterior distribution over CG lo- cations to the encoder's embedding. At test time, we only have the S and need to generate T without knowing where it may map in the counting grid. We considered two ways of providing a mapping:</p><p>• The user provides a hint sentence H (could be just a few words in any order), and the CG mapping of the user's hint, i.e. the full poste- rior distribution p(|H), is used in the decod- ing. The posterior probabilities over 32 × 32 grid locations are unwrapped into a vector with a size of |L| = 1024, and then concate- nated with the word embedding as the input at each time-step. That acts to expand the user's hint into a sentence with similar content (and style if the model is also styled).</p><p>• The CG is scanned and a variety of mappings are tested as inputs to provide a diverse set of possible answers. In our experiments, instead of scanning over all 1024 possible locations in the grid, we retrieved several possible an- swers using information retrieval (ranking of the data samples in the training set based on the source S and picking the top ten). Then the CG mapping p(|H) of these retrieved hints is used to decode several samples from each.</p><p>As an example, <ref type="figure" target="#fig_0">Figure 1</ref> shows a portion of a CG trained on randomly chosen 800k tweets from the twitter corpus. In each cell of the grid, we show the top words in the distribution π j (z) over words (z) in that location (j). (Each cell has a distribu- tion over the entire vocabulary). As a response to "I am hungry," using two highlighted areas as hints, we can generate either a set of empathic re- sponses, such as 'Me too,' or food suggestions, such as 'Let's have cake.' It will also be evident that some areas of the grid may produce less sen- sical answers. These can later be pruned by likeli- hood criteria or by user selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Datasets</head><p>Yahoo! Answer Dataset. We use the Compre- hensive Questions and Answers dataset 2 to train and validate the performances of different decod- ing setups with ranking experiments described in section 7.3. This dataset contains 4.4 million Ya- hoo! Answers questions and the user-selected best answers. Unlike the conversational datasets, such as the Twitter dataset described below, it con- tains more relevant and specific responses for each question, which leads to less ambiguity in ranking.</p><p>Twitter Conversation Dataset. We trained our base encoder-decoder models on the Twitter Con- versation Triple Dataset described in ( <ref type="bibr" target="#b26">Sordoni et al., 2015)</ref>, which consists of 23 million conver- sational snippets randomly selected from a collec- tion of 129M context-message-response triples ex- tracted from the Twitter Firehose over the 3-month period from June through August 2012. For the purposes of our experiments, we split the triples into context-message and message-response pairs yielding 46M source-target pairs. For tuning and evaluation, we used the development dataset of size 200K conversation pairs and the test dataset of 5K examples. The corpus is preprocessed using a Twitter specific tokenizer (O' <ref type="bibr">Connor et al., 2010)</ref>. The vocabulary size is limited to 50,000 exclud- ing the special boundary symbol and the unknown word tag.</p><p>Scenting datasets. A variety of persona charac- ters have been trained and tested, including Hillary Clinton, Donald Trump, John F. Kennedy, Richard Nixon, singer-songwriters, stand-up comedians, and a generic Star Wars character. In experiments, we evaluated on a diverse set of representative tar- get speakers: JFK. We mainly tested our models on John F. Kennedy's speeches collected from American Presidency Project 3 , which contains 6474 training and 719 validation sentences.</p><p>Star Wars. Movie subtitles of three Star Wars movies are also tested 4 . They are extracted from Cornell Movie-Dialogs Corpus (Danescu- Niculescu-Mizil and Lee, 2011), and have 495 training and 54 validation sentences.</p><p>Singer-Songwriter. We also evaluated our ap- proach on a lyric corpus from a collective of singers: Coldplay, Linkin Park, and Green Day. The lyric dataset is collected from mldb.org and has 9182 training and 1020 validation lines.</p><p>Debate Chat Contexts. We designed testing questionnaires with 64 chat contexts spanning a range of topics in politic, science, and technology: the sort of questions we might ask in an entertain- ing political debate. <ref type="bibr">5</ref> To test the model's ability to control output topic in section 7.4.3, we also cre- ated one hint per question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Network Setup and Implementation</head><p>Our encoder and decoder RNNs contains two- layer stacked LSTMs. Each LSTM layer has a memory size of 500. The network weights are randomly initialized using a uniform distri- bution (−0.08, 0.08), and are trained with the ADAM optimizer ( <ref type="bibr" target="#b11">Kingma and Ba, 2014)</ref>, with an initial learning rate of 0.002. Gradients were clipped so their norm does not exceed 5. Each mini-batch contains 200 answers and their ques- tions. The words of input sentences were first con- verted to 300-dimensional vector representations learned from the RNN based language modeling tool word2vec <ref type="figure" target="#fig_0">(Mikolov et al., 2013)</ref>. The begin- ning and end of each passage are also padded with a special boundary symbol. During decoding, our model generates 500 candidate samples in parallel, then ranks them. As these are processed in batches on GPU, generation is very efficient. We also ex- perimented incorporating an information retrieval (IR) module to automatically collect topic hints for CG-based decoder. Specifically, a full-text index of twitter corpus is built using solr <ref type="bibr">6</ref> , and the top 10 searched results based on the source sentence are be used to generate posterior CG distributions as hints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Validating the Decoding Setup with Ranking</head><p>We performed a ranking evaluation applying dif- ferent decoding setups on the Yahoo! Answers dataset. Here we wanted to test the relevance judg- ment capacities of different setups, and validate the necessity of the new decoding method dis- cussed in section 4. Yahoo! Answers question is used as source S, and its answer is treated as tar- get T . Each test question is associated with one true answer and 19 random answers from the test set. MRR (Mean Reciprocal Rank) and P@1 (pre- cision of top1) were then used as evaluation met- rics. <ref type="table">Table 2</ref> shows the answer ranking evaluation results: the forward model P (T |S), by itself is close to the performance of random selection in distinguishing true answer from wrong answers. This implies that a naive beam search over only the forward model may generate irrelevant out- puts. One hypothesis was that P (T |S) is bi- ased toward P (T ), and performance indeed im- proves after normalizing by P (T ). However, it is difficult to directly decode with objective P (T |S)/P (T |∅), because this objective removes the influence of the target-side language model. Decoding only according to this function will thus result in only low-frequency words and un- grammatical sentences, behavior also noted by <ref type="bibr" target="#b25">Shao et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRR P@1</head><p>P rnn (T |S) 0.224 0.075 P rnn (T |S)/P rnn (T |∅) 0.652 0.524 P rnn (S|T ) 0.687 0.556 <ref type="table">Table 2</ref>: Ranking the true target answer among random answers on Yahoo! Answers test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Human Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Systems</head><p>We tested 10 different system configurations to evaluate the overall output quality, and their abili- ties of influencing output language style and topic:</p><p>• vanilla-sampling each word in the target.</p><p>• selective-sampling as described in section 4; all the following systems are using it as well.</p><p>• cg-ir uses IR results to create counting grid topic hints (sections 6.1 and 7.2).</p><p>• rank uses proposals from the full JFK corpus as in section 5.1.</p><p>• multiply with a JFK language model as in section 5.2.</p><p>• finetune with JFK dataset as in section 5.3.</p><p>• finetune-cg-ir uses IR results as topic hints for fine-tuned JFK.</p><p>• finetune-cg-topic forced to use the given topic hint for fine-tuned JFK.</p><p>• singer-songwriter fine-tuned cg-topic.</p><p>• starwars fine-tuned cg-topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Evaluation Setup</head><p>Owing to the low consistency between automatic metrics and human perception on conversational tasks ( <ref type="bibr" target="#b17">Liu et al., 2016;</ref><ref type="bibr" target="#b27">Stent et al., 2005</ref>) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT). Workers were selected based on their AMT prior approval rate (&gt;95%). Each questionnaire was presented to 3 different workers. We evaluated our proposed models on the 64 debate chat contexts. Each of the evalu- ated methods generated 3 samples for every chat context. To ensure calibrated ratings between sys- tems, we show the human judges all system out- puts (randomly ordered) for each particular test case at the same time. For each chat context, we conducted three kinds of assessments:</p><p>Quality Assessment Workers were provided with the following guidelines: "Given the chat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Quality (MOS) Style vanilla-sampling 2.286 ± 0.046 - selective-sampling 2.681 ± 0.049 10.42% cg-ir 2.566 ± 0.048 10.24% rank 2.477 ± 0.048 21.88% multiply 2.627 ± 0.048 13.54% finetune 2.597 ± 0.046 20.83% finetune-cg-ir 2.627 ± 0.049 20.31% finetune-cg-topic 2.667 ± 0.045 21.09% singer-songwriter 2.373 ± 0.045 - starwars 2.677 ± 0.048 - <ref type="table">Table 3</ref>: Results of quality assessments with 5- scale mean opinion scores (MOS) and JFK style assessments with binary ratings. Style results are statistically significant compared to the selective- sampling by paired t-tests (p &lt; 0.5%).</p><p>context, a chat-bot needs to continue the conver- sation. Rate the potential answers based on your own preference on a scale of 1 to 5 (the highest):"</p><p>• 5-Excellent: "Very appropriate response, and coherent with the chat context." In this test, the outputs of all 10 systems evaluated are then provided to worker together for a total of 30 responses. In total, we gathered 64 · 30 · 3 = 5760 ratings for quality assessments, and 47 dif- ferent workers participated.</p><p>Style Assessment. We provided following in- structions: "Which candidate responses are likely to have come from or are related to [Persona Name]?". Checkboxes were provided for the re- sponses from style-influenced systems and from selective-sampling as a baseline.</p><p>Topic Assessment. The instruction was: "Which candidate answers to the chat context above are similar or related to the following answer: '[a hint topic provided by us]'?". This was also a checkbox questionnaire. Candidates are from both style-and topic-influenced systems (fine-tuned cg-topic), and from selective-sampling as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persona</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style</head><p>Topic Ours Base Ours Base John F. Kennedy 21% 10% 33% 22% Star Wars 27% 3% 14% 8% Singer-Songwriter 31% 23% 17% 9% <ref type="table">Table 4</ref>: The style and topic assessments (both bi- nary) of three models with different personas and with restriction of specific target topic for each chat context. All style and topic results are statis- tically significant compared to the Base (selective- sampling) by paired t-tests with p &lt; 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3">Results</head><p>Overall Quality. We conducted mean opinion score (MOS) tests for overall quality assessment of generated responses with questionnaires de- scribed above. <ref type="table">Table 3</ref> shows the MOS results with standard error. It can be seen that all the systems based on selective sampling are signifi- cantly better than vanilla sampling baseline. When restricting output's style and/or topic, the MOS score results of most systems do not decline signif- icantly except singer-songwriter, which attempts to generate lyrics-like outputs in response to to political debate questions, resulting in uninter- pretable strings.</p><p>Our rank method uses p(S|T ) to pick the an- swer from the original persona corpus, and is thus as good at styling as the person themselves. Be- cause most of our testing questionnaire is po- litical, the rank was indeed often able to find related answers in the dataset (JFK). Also, un- like generation-based approaches, rank has oracle- level language fluency and it is expected to have quality score of at least 2 ("Interpretable, but not related"). Overall, however, the quality score of rank is still lower than other approaches. Note that a hybrid system can actually chose between rank and the decoder's outputs based on likelihood, as shown in the example of bJFk-bNixon debate in the supplemental material.</p><p>Influencing the Style. Table 3 also shows the likelihood of being labeled as JFK for different methods. It is encouraging that finetune based ap- proaches have similar chances as the rank system which retrieves sentences directly from JFK cor- pus, and are significantly better than the selective- sampling baseline.</p><p>Influencing both Style and Topic. <ref type="table">Table 4 sum- marizes the results in terms of style (the fraction of  answers labeled as in-style for the target persona)</ref>, and topic (the percentage of answers picked as re- lated to the human-provided topic hint text). We used the last three of the ten listed systems, which are both styled and use specific topic hints to gen- erate answers. These results demonstrate that it is indeed possible to provide simple prompts to a styled model and drive their answers in a de- sired direction while picking up the style of the persona. It also shows that the style of some char- acters is harder to recreate than others. For exam- ple, workers are more likely to label baseline re- sults as lyrics from a singer-songwriter than lines from Star Wars movies, which might be because lyrics often take significant freedom with struc- ture and grammar. We also found that it is harder for Star Wars and Singer-Songwriter bots to fol- low topic hints than it is for the John F. Kennedy model, largely because the political debate ques- tions we used overlap less with the topics found in the scenting datasets for those two personas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this study we investigated the possibility of steering the style and content in the output of a neural encoder-decoder model <ref type="bibr">7</ref> . We showed that acquisition of highly recognizable styles of fa- mous personalities, characters, or professionals, is achievable, and that it is even possible to allow users to influence the topic direction of conver- sations. The tools described in the paper are not only useful in conversational systems (e.g., chat- bots), but can also be useful as authoring tools in social media. In the latter case, the social media users might use neural models as consultants to help with crafting responses to any post the user is reading. The AMT tests show that these models do indeed provide increased recognizability of the style, without sacrificing quality or relevance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A part of a Counting Grid trained on Twitter data and its use in providing topical hints in decoding. For the source sentence at the top, the decoder may produce the two target samples on the right, if the circled locations are used as a hint, or the two sentences at the bottom if the locations in the lower right are picked.</figDesc><graphic url="image-1.png" coords="6,72.00,62.81,453.50,255.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>• 4 -</head><label>4</label><figDesc>Good: "Coherent with the chat context." • 3-Fair: "Interpretable and related. It is OK for you to receive this chat response." • 2-Poor: "Interpretable, but not related." • 1-Bad: "Not interpretable."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Example responses from our Star Wars, 
Hillary, Trump, and Kennedy bots with scented 
conversation models. 

</table></figure>

			<note place="foot" n="1"> (Chen et al., 2017) have recently proposed using LDA for topic modeling in Sequence-To-Sequence response generation models. We believe that the CG embedding used here will prove easier to apply and interpret through visualization.</note>

			<note place="foot" n="2"> http://webscope.sandbox.yahoo.com/ catalog.php?datatype=l</note>

			<note place="foot" n="3"> http://www.presidency.ucsb.edu/ 4 Koncel-Kedziorski et al. (2016) also uses Star Wars scripts to test theme rewriting of algebra word problems. 5 See the Supplementary material.</note>

			<note place="foot" n="6"> https://lucene.apache.org/solr/</note>

			<note place="foot" n="7"> The code and testing data are available at https://github.com/digo/ steering-response-style-and-topic</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Shashank Srivastava, Donald Brinkman, Michel Galley, and Bill Dolan for useful discus-sions and encouragement. Di Wang is supported by the Tencent Fellowship and Yahoo! Fellowship, to which we gratefully acknowledge.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet Allocation. J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Mathematics of Statistical Machine Translation: Parameter Estimation</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma ; California</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Summed-area Tables for Texture Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 11th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics</title>
		<meeting>the Workshop on Cognitive Modeling and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multidimensional Counting Grids: Inferring Word Order from Disordered Bags of Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence<address><addrLine>Arlington, Virginia; United States</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="547" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Controlling Output Length in Neural Encoder-Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experiments in Domain Adaptation for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A themerewriting approach for generating algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR abs/1610.06210</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Correlated Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y Weiss, P B Schölkopf, and J C Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">A Persona-Based Neural Conversation Model</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">A DiversityPromoting Objective Function for Neural Conversation Models. Arxiv pages</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno>CoRR abs/1603.0</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TweetMotif : Exploratory search and topic summarization for Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Skim-reading thousands of documents in one minute: Data indexing and visualization for multifarious search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Turski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interactive Data Exploration and Analytics (IDEA&apos;14) at KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-driven Response Generation in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural Responding Machine for Short-Text Conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR abs/1503.0</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generating Long and Diverse Responses with Neural Conversation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Neural Network Approach to ContextSensitive Generation of Conversational Responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naacl-2015. Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACLHLT 2015)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating Evaluation Methods for Generation in the Presence of Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Singhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing</title>
		<meeting>the 6th International Conference on Computational Linguistics and Intelligent Text Processing<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="341" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orioi</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">A Neural Conversational Model. ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="707" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CMU OAQA at TREC 2016 LiveQA: An Attentional Neural Encoder-Decoder Approach for Answer Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016</title>
		<meeting>The Twenty-Fifth Text REtrieval Conference, TREC 2016<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
