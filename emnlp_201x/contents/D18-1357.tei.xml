<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Zheng</surname></persName>
							<email>zheng@renj.me</email>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research</orgName>
								<address>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Research</orgName>
								<address>
									<settlement>Sunnyvale</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3188" to="3197"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3188</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural text generation, including neural machine translation, image captioning, and sum-marization, has been quite successful recently. However, during training time, typically only one reference is considered for each example , even though there are often multiple references available, e.g., 4 references in NIST MT evaluations, and 5 references in image cap-tioning data. We first investigate several different ways of utilizing multiple human references during training. But more importantly , we then propose an algorithm to generate exponentially many pseudo-references by first compressing existing human references into lattices and then traversing them to generate new pseudo-references. These approaches lead to substantial improvements over strong baselines in both machine translation (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural text generation has attracted much atten- tion in recent years thanks to its impressive gener- ation accuracy and wide applicability. In addition to demonstrating compelling results for machine translation (MT) <ref type="bibr" target="#b15">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), by simple adaptation, practically very same or similar models have also proven to be suc- cessful for summarization <ref type="bibr" target="#b13">(Rush et al., 2015;</ref><ref type="bibr" target="#b11">Nallapati et al., 2016</ref>) and image or video captioning ( <ref type="bibr" target="#b18">Venugopalan et al., 2015;</ref><ref type="bibr">Xu et al., 2015a</ref>).</p><p>The most common neural text generation model is based on the encoder-decoder frame- work ( <ref type="bibr" target="#b15">Sutskever et al., 2014</ref>) which generates a variable-length output sequence using an RNN- based decoder with attention mechanisms <ref type="bibr" target="#b0">(Bahdanau et al., 2014;</ref><ref type="bibr" target="#b20">Xu et al., 2015b</ref>). There are many recent efforts in improving the generation accuracy, e.g., <ref type="bibr">ConvS2S (Gehring et al., 2017</ref>) and Transformer ( <ref type="bibr" target="#b16">Vaswani et al., 2017)</ref>. However, all these efforts are limited to training with a single reference even when multiple references are avail- able.</p><p>Multiple references are essential for evaluation due to the non-uniqueness of translation and gen- eration unlike classification tasks. In MT, even though the training sets are usually with sin- gle reference (bitext), the evaluation sets often come with multiple references. For example, the NIST Chinese-to-English and Arabic-to-English MT evaluation datasets <ref type="bibr">(2003)</ref><ref type="bibr">(2004)</ref><ref type="bibr">(2005)</ref><ref type="bibr">(2006)</ref><ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref> have in total around 10,000 Chinese sentences and 10,000 Ara- bic sentences each with 4 different English trans- lations. On the other hand, for image caption- ing datasets, multiple references are more com- mon not only for evaluation, but also for train- ing, e.g., the MSCOCO ( <ref type="bibr" target="#b9">Lin et al., 2014</ref>) dataset provides 5 references per image and PASCAL- 50S and ABSTRACT-50S ( <ref type="bibr" target="#b17">Vedantam et al., 2015)</ref> even provide 50 references per image. Can we use the extra references during training? How much can we benefit from training with multiple refer- ences?</p><p>We therefore first investigate several different ways of utilizing existing human-annotated refer- ences, which include Sample One ( <ref type="bibr" target="#b8">Karpathy and Fei-Fei, 2015)</ref>, Uniform, and Shuffle methods (ex- plained in Sec. 2). Although Sample One has been explored in image captioning, to the best of our knowledge, this is the first time that an MT system is trained with multiple references.</p><p>Actually, four or five references still cover only a tiny fraction of the exponentially large space of potential references <ref type="bibr" target="#b3">(Dreyer and Marcu, 2012)</ref>. More importantly, encouraged by the success of training with multiple human references, we fur- ther propose a framework to generate many more pseudo-references automatically. In particular, we design a neural multiple-sequence alignment algo-rithm to compress all existing human references into a lattice by merging similar words across dif- ferent references (see examples in <ref type="figure" target="#fig_0">Fig. 1)</ref>; this can be viewed as a modern, neural version of paraphrasing with multiple-sequence alignment ( <ref type="bibr">Lee, 2003, 2002</ref>). We can then gen- erate theoretically exponentially more references from the lattice.</p><p>We make the following main contributions:</p><p>• Firstly, we investigate three different methods for multi-reference training on both MT and image captioning tasks (Section 2).</p><p>• Secondly, we propose a novel neural network-based multiple sequence alignment model to compress the existing references into lattices. By traversing these lattices, we generate exponentially many new pseudo- references (Section 3).</p><p>• We report substantial improvements over strong baselines in both MT (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr) by training on the newly generated pseudo-references (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Using Multiple References</head><p>In order to make the multiple reference training easy to adapt to any frameworks, we do not change anything from the existing models itself. Our mul- tiple reference training is achieved by converting a multiple reference dataset to a single reference dataset without losing any information. Considering a multiple reference dataset D, where the i th training example, (x i , Y i ), includes one source input x i , which is a source sentence in MT or image vector in image captioning, and a ref- erence set and D shuffle are ordered sets): Sample One: The most straightforward way is to use a different reference in different epochs dur- ing training to explore the variances between refer- ences. For each example, we randomly pick one of the K references in each training epoch (note that the random function will be used in each epoch). This method is commonly used in existing image captioning literatures, such as ( <ref type="bibr">Karpathy and FeiFei, 2015</ref>), but never used in MT. This approach can be formalized as:</p><formula xml:id="formula_0">Y i = {y 1 i , y 2 i , ...y K i } of K references.</formula><formula xml:id="formula_1">D sample one = |D| i=1 {(x i , y k i i )}, k i = rand(1, ..., K)</formula><p>Uniform: Although all references are accessible by using Sample One, it is not guaranteed that all references are used during training. So we intro- duce Uniform which basically copies x i training example K times and each time with a different reference. This approach can be formalized as:</p><formula xml:id="formula_2">D uniform = |D| i=1 K k=1 {(x i , y k i )}</formula><p>Shuffle is based on Uniform, but shuffles all the source and reference pairs in random order before each epoch. So, formally it is:</p><formula xml:id="formula_3">D shuffle = Shuffle(D uniform ) Sample</formula><p>One is supervised by different training signals in different epochs while both Uniform and Shuffle include all the references at one time. Note that we use mini-batch during training. When we set the batch size equal to the entire training set size in both Uniform and Shuffle, they become equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pseudo-References Generation</head><p>In text generation tasks, the given multiple refer- ences are only a small portion in the whole space of potential references. To cover a larger number of references during training, we want to generate more pseudo-references which is similar to exist- ing ones.</p><p>Our basic idea is to compress different refer- ences y 0 , y 1 , ..., y K into a lattice. We achieve this by merging similar words in the references. Finally, we generate more pseudo-references by simply traversing the compressed lattice and se- lect those with high quality according to its BLEU score.</p><p>Take the following three references from the NIST Chinese-to-English machine translation dataset as an example:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Naive Idea: Hard Word Alignment</head><p>The simplest way to compress different references into a lattice is to do pairwise reference compres- sion iteratively. At each time, we select two refer- ences and merge the same words in them.</p><p>Considering the previous example, we can de- rive an initial lattice from the three references as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Assume that we first do a pairwise reference compression on first two ref- erences, we can merge at four sharing words: Indonesia, its, opposition and foreign, and the lattice will turn to <ref type="figure" target="#fig_0">Fig. 1(b)</ref>. If we further com- press the first and third references, we can merge at Indonesia, opposition, to and foreign, which gives the lattice <ref type="figure" target="#fig_0">Fig. 1</ref>  lattice can align these words, we can gener- ate the lattice shown in <ref type="figure" target="#fig_0">Fig. 1</ref> Following the previously described algorithm, we can merge the two references at "two elephants", at "to" and at "a". However, "to" in the two references are very different (it is a prepo- sition in the first reference and an infinitive in the second) and should not be merged. Thus, the lat- tice in <ref type="figure" target="#fig_1">Fig. 2(b)</ref> will generate the following wrong pseudo-references:</p><p>1. Two elephants try to a small entry 2. Two elephants in an enclosure next to fit through a brick building Therefore, we need to investigate a better method to compress the lattice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Measuring Word Similarity in Context</head><p>To tackle the above listed two problems of hard alignment, we need to identify synonyms and words with similar meanings. <ref type="bibr" target="#b1">Barzilay and Lee (2002)</ref> utilize an external synonyms dictionary to get the similarity score between words. How- ever, this method ignores the given context of each word. For example, in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, there are two Indonesia's in the second path of reference. If we use a synonyms dictionary, both Indonesia to- kens will be aligned to the Indonesia in the first or third sentence with the same score. This incor- rect alignment would lead to meaningless lattice.</p><p>Thus, we introduce the semantic substitution matrix which measures the semantic similarity of each word pairs in context. Formally, given a sen- tence pair y i and y j , we build a semantic substi- tution matrix M = R |y i |×|y j | , whose cell M u,v represents the similarity score between word y i,u and word y j,v .</p><p>We propose a new neural network-based mul- tiple sequence alignment algorithm to take con- text into consideration. We first build a language model (LM) to obtain the semantic representation of each word, then these word representations are used to construct the semantic substitution matrix between sentences. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the architecture of the bidirectional LM ( <ref type="bibr" target="#b10">Mousa and Schuller, 2017)</ref>. The optimiza- tion goal of our LM is to minimize the i th word's prediction error given the surrounding word's hid- den state:</p><formula xml:id="formula_4">p(w i | − − → h i−1 ⊕ ← − − h i+1 )<label>(1)</label></formula><p>For any new given sentences, we concatenate both forward and backward hidden states to rep- resent each word y i,u in a sentence y i . We then calculate the normalized cosine similarity score of word y i,u and y j,v as:  <ref type="figure">Fig. 4</ref> shows an example of the semantic sub- stitution matrix of first two sentences in example references of <ref type="figure" target="#fig_0">Fig. 1(a)</ref>.</p><formula xml:id="formula_5">M u,v = cosine( − → h u ⊕ ← − h u , − → h v ⊕ ← − h v )<label>(2)</label></formula><formula xml:id="formula_6">w 1 w 2 w 3 ˆ w 2 ! h 1 ! h 0 ! h 2 ! h 3 h 3 h 4 h 2 h 1 &lt; s &gt; &lt; /s &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Iterative Pairwise Word Alignment using Dynamic Programming</head><p>With the help of semantic substitution matrix M u,v which measures pairwise word similarity, we need to find the optimal word alignment to compress references into a lattice. Unfortunately, this computation is exponential in the number of sequences. Thus, we use iter- ative pairwise alignment which greedily merges sentence pairs ( <ref type="bibr" target="#b4">Durbin et al., 1998</ref>).</p><p>Based on pairwise substitution matrix we can define an optimal pairwise sequence alignment as an optimal path from M 0,0 to M |y i |,|y j | . This is a dynamic programming problem with the state transition function described in Equation (3). <ref type="figure" target="#fig_3">Fig. 5</ref> shows the optimal path according to the se- mantic substitution matrix in <ref type="figure">Fig. 4</ref>. There is a gap if the continuous step goes vertical or horizontal, and an alignment if it goes diagonal.</p><formula xml:id="formula_7">opt(u, v) =      opt(u−1, v−1)+M u,v opt(u−1, v) opt(u, v−1)<label>(3)</label></formula><p>What order should we follow to do the iter- ative pairwise word alignment? Intuitively, we need to compress the most similar reference pair first, since this compression will lead to more aligned words. Following this intuition, we order reference pairs by the maximum alignment score opt(|y i |, |y j |) (i.e. the score of bottom-right cell in <ref type="figure" target="#fig_3">Fig. 5</ref>) which is the sum of all aligned words. Using this order, we can iteratively merge each sentence pair in descending order, unless both the sentences have already been merged (this will pre- vent generating a cyclic lattice).</p><p>Since the semantic substitution matrix M u,v , defined as a normalized cosine similarity, scales in (0, 1), it's very likely for the DP algorithm to align unrelated words. To tackle this problem, we deduct a global penalty p from each cell of M u,v . With the global penalty p, the DP algorithm will not align a word pair (y i,u , y i,v ) unless M u,v ≥ p.</p><p>After the pairwise references alignment, we merge those aligned words. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, after we generate an initial lattice as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, we then calculate the maximum align- ment score of all sentence pairs. After that, the lattice turns into <ref type="figure" target="#fig_0">Fig. 1(d)</ref> by merging the first two references (assuming they have the highest score) according to pairwise alignment shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Then we pick the sentence pair with next highest alignment score (assuming it's the last two sen- tences). Similar to the previous step, we find align- ments according to the dynamic programming and merge to the final lattice (see <ref type="figure" target="#fig_0">Fig. 1(e)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Traverse Lattice and Pseudo-References Selection by BLEU</head><p>We generate pseudo-references by simply travers- ing the generated lattice. For example, if we tra- verse the final lattice shown in <ref type="figure" target="#fig_0">Fig. 1(e)</ref>, we can generate 213 pseudo-refrences in total. Then, we can put those generated pseudo- references to expand the training dataset. To bal- ance the number of generated pseudo-references for each example, we force the total number of pseudo-references from each example to be 20 40 60  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Avg. Length of Original References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To investigate the empirical performances of our proposed algorithm, we conduct experiments on machine translation and image captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine Translation</head><p>We evaluate our approach on NIST Chinese-to- English translation dataset which consists of 1M pairs of single reference data and 5974 pairs of 4 reference data <ref type="bibr">(NIST 2002</ref><ref type="bibr">(NIST , 2003</ref><ref type="bibr">(NIST , 2004</ref><ref type="bibr">(NIST , 2005</ref><ref type="bibr">(NIST , 2006</ref><ref type="bibr">(NIST , 2008</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows the statistics of this dataset. We first pre-train our model on a 1M pairs single reference dataset and then train on the <ref type="bibr">NIST 2002</ref><ref type="bibr">NIST , 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref>. We use the NIST 2006 dataset as validation set and NIST 2008 as test sets. <ref type="figure" target="#fig_4">Fig. 6(a)</ref> analyzes the number and quality of generated references using our proposed ap- proach. We set the global penalty as 0.9 and only calculate the top 50 generated references for the average BLEU analysis. From the figure, we can see that when the sentence length grows, the num- ber of generated references grows exponentially. To generate enough references for the following experiments, we set an initial global penalty as 0.9 and gradually decrease it by 0.05 until we collect no less than 100 references. We train a bidirec- tional language model on the pre-training dataset and training dataset with Glove ( <ref type="bibr" target="#b12">Pennington et al., 2014</ref>) word embedding size of 300 dimension, for 20 epochs to minimize the perplexity We employ byte-pair encoding (BPE) <ref type="bibr" target="#b14">(Sennrich et al., 2015</ref>) which reduces the source and target language vocabulary sizes to 18k and 10k. We adopt length reward ( <ref type="bibr" target="#b7">Huang et al., 2017</ref>) to find optimal sentence length. We use a two layer bidi- rectional LSTM as the encoder and a two layer LSTM as the decoder. We perform pre-training for 20 epochs to minimize perplexity on the 1M dataset, with a batch size of 64, word embedding size of 500, beam size of 15, learning rate of 0.1, learning rate decay of 0.5 and dropout rate of 0.3. We then train the model in 30 epochs and use the best batch size among 100, 200, 400 for each up- date method. These batch sizes are multiple of the number of references used in experiments, so it is guaranteed that all the references of one single ex- ample are in one batch for the Uniform method. The learning rate is set as 0.01 and learning rate decay as 0.75. We do each experiment three times and report the average result. <ref type="table" target="#tab_6">Table 2</ref> shows the translation quality on the dev- set of machine translation task. Besides the orig- inal 4 references in the training set, we gener- ate another four dataset with 10, 20, 50 and 100 references including pseudo-references using hard word alignment and soft word alignment. We compare the three update methods (Sample One, Uniform, Shuffle) with always using the first ref- erence (First). All results of soft word alignment are better than corresponding hard word alignment results and the best result is achieved with 50 ref- erences using Uniform and soft word alignment. According to <ref type="table" target="#tab_7">Table 3</ref>, Shuffle with original 4 refer- ences has +0.7 BLEU improvement and Uniform <ref type="table" target="#tab_1">Task  Pre-training Training Validation Testing   Machine Translation  # of examples  1,000,000  4,667  616  691  # of refs per example  1  4  4  4</ref> Image Captioning # of examples - 113,287 5,000 5,000 # of refs per example - 5 5 5   with 50 references has +1.5 BLEU improvement. From <ref type="figure">Fig. 7(b)</ref>, we can see that using the Sam- ple One method, the translation quality drops dra- matically with more than 10 references. This may be due to the higher variance of used reference in each epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image Captioning</head><p>For the image captioning task, we use the widely- used MSCOCO image captioning dataset. Follow- ing prior work, we use the Kapathy split <ref type="bibr" target="#b8">(Karpathy and Fei-Fei, 2015</ref>   coder. We train every model for 100 epochs and calculate the BLEU score on validation set and se- lect the best model. For every update method, we find the optimal batch size among 50, 250, 500, 1000 and we use a beam size of 5. <ref type="figure" target="#fig_4">Fig. 6(b)</ref> analyzes the correlation between aver- age references length with the number and quality of generated references. We set global penalty as 0.6 (which is also adopted for the generated ref- erences in the following experiments) and calcu- late the top 50 generated references for the aver- age BLEU analysis. Since the length of original references is much shorter than the previous ma- chine translation dataset, it has worse quality and fewer generated references. <ref type="table" target="#tab_9">Table 4</ref> shows that the best result is achieved with 20 references using Shuffle. This result is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Original <ref type="table">References   a gray tabby cat is curled in a red bowl that sits on a table near a window   a brown and black cat is sleeping in a bowl on a table   a grey tiger cat sleeping in a brown bowl on a table   an image of a cat sitting inside of a bowl on the kitchen table   a</ref>    different from the result of machine translation task where Uniform method is the best. This may be because the references in image caption- ing dataset are much more diverse than those in machine translation dataset. Different captions of one image could even talk about different as- pects. When using the Uniform method, the high variance of references in one batch may harm the model and lead to worse text generation quality. <ref type="table" target="#tab_10">Table 5</ref> shows that it outperforms Sample One with 4 original references, which is adopted in previous work <ref type="bibr" target="#b8">(Karpathy and Fei-Fei, 2015</ref>), +3.1 BLEU score and +11.7 CIDEr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>Fig <ref type="figure" target="#fig_4">. 6</ref> shows a training example in the COCO dataset and its corresponding generated lattice and pseudo-references which is sorted according to its BLEU score. Our proposed algorithm generates 73724 pseudo-references in total. All the top 50 pseudo-references' BLEU scores are above 97.1 and the top three even achieve 100.0 BLEU score though they are not identical to any original refer- ences. Although the BLEU of last two sentences is 0.0, they are still valid to describe this picture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">4 10 20 50</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduce several multiple-reference training methods and a neural-based lattice compression framework, which can generate more training ref- erences based on existing ones. Our proposed framework outperforms the baseline models on both MT and image captioning tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lattice construction with word alignment. (b-c) is hard word alignment and 33 pseudo-references can be generated. (d-e) is soft word alignment, 213 pseudo-references can be generated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mistakes from hard word alignment by merging at "to".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Bidirectional Language Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Dynamic Programming on Semantic Substitution Matrix</figDesc><graphic url="image-3.png" coords="5,368.55,66.46,145.07,108.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Analysis of generated references</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 7: Translation quality of machine translation task on dev-set with soft alignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Text generation quality of image captioning task on validation set with soft alignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1 .</head><label>1</label><figDesc></figDesc><table>Indonesia reiterated its opposition 
to foreign military presence 

2. Indonesia repeats its opposition 
against station of foreign troops in 
Indonesia 

3. Indonesia reiterates opposition to 

garrisoning foreign armies (a) 
Indonesia 

I n d o n e s ia 

I n d o n e s ia 

reiterated 
its 
opposition 
to 
foreign 
military 
p re se n c e 

repeats 
its opposition against 
station 
of 
foreign troops 
in 
Indonesia 

reiterates 
opposition 
to 
garrisoning 
foreign 

a rm ie s 

(b) 

In d o n e s ia 

In d o n e s ia 

to 
mil ita ry pre sen ce 

repeats 

reiterated 
its 
opposition 

aga ins t 
station 
of 

foreign 
tro ops in Ind one sia 

reiterates 
opposition 
to 
garrisoning 
foreign 
a r m ie s 

(c) 
Indonesia 

to 
military presence 
repeats 

reiterated 

its 
opposition 

against 
station 
of 

foreign 

troops in Indonesia 
reiterates 

garrisoning 

armies 

(d) 

In d o n e s ia 

In d o n e s ia 

to 
military 
pr es en ce 

repeats 

reiterated 
its 
opposition 

aga ins t 
station 
of 

foreign 

troops 
in 
Indo nesi a 

reiterates 
opposition 
to 
garrisoning 
foreign 
a r m ie s 

(e) 
Indonesia 

to 
military 
presence 

repeats 

reiterated 
its 

opposition 
against 
station 
of 
foreign troops 
in 
Indonesia 

reiterates 

garrisoning 


armies 


</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 1 : Statistics of datasets used in following experiments.</head><label>1</label><figDesc></figDesc><table># of Refs Method 
BLEU 
0 
Pre-train 
37.44 
1 
First  *  
38.64 

4 

Sample One 
38.81 
Uniform 
38.78 
Shuffle 
38.87 
Includes Pseudo-Refs 
Hard Align Soft Align 

10 

Sample One 
37.48 
39.41 
Uniform 
39.20 
39.35 
Shuffle 
39.13 
39.53 

20 

Sample One 
37.27 
38.70 
Uniform 
39.14 
39.46 
Shuffle 
39.12 
39.42 

50 

Sample One 
37.42 
37.62 
Uniform 
39.30 
39.65 
Shuffle 
38.98 
39.08 

100 

Sample One 
37.54 
37.63 
Uniform 
39.23 
39.46 
Shuffle 
38.88 
39.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU on the MT validation set.  *  Baseline 

# of Refs Method BLEU 
0 
Pre-train 33.58 
1 
First  *  
34.49 
4 
Shuffle 35.20 (+0.7) 
 † 50 
Uniform 35.98 (+1.5) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU on the MT test set.  † Includes pseudo-
references generated by soft word alignment algorithm. 
 *  Baseline. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>BLEU/CIDEr on the image captioning valida-
tion set.  *  Baseline. 

# of Refs Method 
BLEU 
CIDEr 
1 
First 
26.70 
80.70 
5 
Sample One  *  28.67 
85.41 
5 
Shuffle 
30.94 (+2.3) 94.10 (+8.7) 
 † 20 
Shuffle 
31.79 (+3.1) 97.10 (+11.7) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>BLEU/CIDEr on the image captioning test set 
with soft.  † Includes pseudo-references generated by 
soft word alignment algorithm.  *  Baseline. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>cat asleep in a fruit bowl on a dining room table</head><label></label><figDesc></figDesc><table>Generated Lattice using Soft Alignment 

an image of 

a 

gray 

brown 

grey 

tiger 

and 

black 

tabby 


cat 

sitting 
is 

curled 
sleeping 

asleep 

inside of 

in 
a 

red 

brown 

fruit 


bowl 

that sits 

on 

a 

the 

kitchen 


dining room 

table 

near a window 

ID Pseudo-references 
BLEU 
1 a grey tiger cat sleeping in a brown bowl on a table near a window 100.0 
2 a grey tiger cat sleeping in a brown bowl on a dining room table 
100.0 
3 a brown and black cat is sleeping in a bowl on the kitchen table 
100.0 
... ... 
... 
48 a grey tiger cat sleeping in a fruit bowl on a table 
97.1 
49 a cat asleep in a red bowl that sits on a table 
97.1 
50 a gray tabby cat is sleeping in a bowl on a table 
97.1 
... ... 
... 
73723 a grey and tabby cat inside of a red bowl on the dining room table 
0.0 
73724 a grey and tabby cat inside of a red bowl on a kitchen table 
0.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Training example that generates maximum number of pseudo-references (73724). The selected 8 pseudo-
references are sorted according to their BLEU score. 

</table></figure>

			<note place="foot" n="3">. Indonesia reiterates opposition to foreign troops in Indonesia 4. ... However, this simple hard alignment method (only identical words can be aligned) suffers from two problems: 1. Different words may have similar meanings and need to be merged together. For example, in the previous example, reiterated, repeats and reiterates should be merged together. Similarly, military, troops and armies also have similar meanings. If the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by DARPA grant N66001-17-2-4030, and NSF grants IIS-1817231 and IIS-1656051. We thank the anony-mous reviewers for suggestions and Juneki Hong for proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bootstrapping lexical choice via multiple-sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to paraphrase: An unsupervised approach using multiple-sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyter: Meaning-equivalent semantics for translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Biological sequence analysis: probabilistic models of proteins and nucleic acids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Mitchison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">When to finish? optimal beam search for neural text generation (modulo beam size)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft COCO: common objects in context</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Classify or select: Neural architectures for extractive document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. 2015a. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
