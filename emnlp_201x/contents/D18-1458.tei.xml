<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongbo</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics and Philology</orgName>
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><forename type="middle">M ¨</forename><surname>Uller</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">4263</biblScope>
							<biblScope unit="page" from="4263" to="4272"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, non-recurrent architectures (convo-lutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Different architectures have been shown to be effective for neural machine translation (NMT), ranging from recurrent architectures <ref type="bibr" target="#b13">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b24">Sutskever et al., 2014;</ref><ref type="bibr" target="#b16">Luong et al., 2015</ref>) to convolutional <ref type="bibr" target="#b13">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b8">Gehring et al., 2017</ref>) and, most recently, fully self- attentional (Transformer) models ( <ref type="bibr">Vaswani et al., 2017)</ref>. Since comparisons ( <ref type="bibr" target="#b8">Gehring et al., 2017;</ref><ref type="bibr">Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Hieber et al., 2017</ref>) are mainly carried out via BLEU (Papineni et al., * Work carried out during a visit to the machine transla- tion group at the University of <ref type="bibr">Edinburgh. 2002)</ref>, it is inherently difficult to attribute gains in BLEU to architectural properties.</p><p>Recurrent neural networks (RNNs) <ref type="bibr" target="#b7">(Elman, 1990)</ref> can easily deal with variable-length input sentences and thus are a natural choice for the encoder and decoder of NMT systems. Mod- ern variants of RNNs, such as GRUs ( <ref type="bibr" target="#b5">Cho et al., 2014</ref>) and LSTMs <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref>, address the difficulty of training recurrent networks with long-range dependencies. <ref type="bibr" target="#b8">Gehring et al. (2017)</ref> introduce a neural architecture where both the encoder and decoder are based on CNNs, and report better BLEU scores than RNN-based NMT models. Moreover, the computation over all tokens can be fully parallelized during training, which increases efficiency. <ref type="bibr">Vaswani et al. (2017)</ref> propose Transformer models, which are built en- tirely with attention layers, without convolution or recurrence. They report new state-of-art BLEU scores for EN→DE and EN→FR. Yet, the BLEU metric is quite coarse-grained, and offers no in- sight as to which aspects of translation are im- proved by different architectures.</p><p>To explain the observed improvements in BLEU, previous work has drawn on theoretical ar- guments. Both <ref type="bibr" target="#b8">Gehring et al. (2017)</ref> and <ref type="bibr">Vaswani et al. (2017)</ref> argue that the length of the paths in neural networks between co-dependent elements affects the ability to learn these dependencies: the shorter the path, the easier the model learns such dependencies. The papers argue that Transformers and CNNs are better suited than RNNs to capture long-range dependencies.</p><p>However, this claim is based on a theoreti- cal argument and has not been empirically tested. We argue other abilities of non-recurrent networks could be responsible for their strong performance. Specifically, we hypothesize that the improve- ments in BLEU are due to CNNs and Transform- ers being strong semantic feature extractors.</p><p>In this paper, we evaluate all three popular NMT architectures: models based on RNNs (re- ferred to as RNNS2S in the remainder of the pa- per), based on CNNs (referred to as ConvS2S) and self-attentional models (referred to as Transform- ers). Motivated by the aforementioned theoreti- cal claims regarding path length and semantic fea- ture extraction, we evaluate their performance on a subject-verb agreement task (that requires mod- eling long-range dependencies) and a word sense disambiguation (WSD) task (that requires extract- ing semantic features). Both tasks build on test sets of contrastive translation pairs, Lingeval97 (Sennrich, 2017) and ContraWSD ( <ref type="bibr" target="#b20">Rios et al., 2017)</ref>.</p><p>The main contributions of this paper can be summarized as follows:</p><p>• We test the theoretical claims that architec- tures with shorter paths through networks are better at capturing long-range dependen- cies. Our experimental results on modeling subject-verb agreement over long distances do not show any evidence that Transformers or CNNs are superior to RNNs in this regard.</p><p>• We empirically show that the number of at- tention heads in Transformers impacts their ability to capture long-distance dependen- cies. Specifically, many-headed multi-head attention is essential for modeling long- distance phenomena with only self-attention.</p><p>• We empirically show that Transformers excel at WSD, indicating that they are strong se- mantic feature extractors. <ref type="bibr">Yin et al. (2017)</ref> are the first to compare CNNs, LSTMs and GRUs on several NLP tasks. They find that CNNs are better at tasks related to se- mantics, while RNNs are better at syntax-related tasks, especially for longer sentences. Based on the work of <ref type="bibr" target="#b15">Linzen et al. (2016)</ref>, <ref type="bibr" target="#b2">Bernardy and Lappin (2017)</ref> find that RNNs per- form better than CNNs on a subject-verb agree- ment task, which is a good proxy for how well long-range dependencies are captured. <ref type="bibr">Tran et al. (2018)</ref> find that a Transformer language model performs worse than an RNN language model on a subject-verb agreement task. They, too, note that this is especially true as the distance between sub- ject and verb grows, even if RNNs resulted in a higher perplexity on the validation set. This result of <ref type="bibr">Tran et al. (2018)</ref> is clearly in contrast to the general finding that Transformers are better than RNNs for NMT tasks. <ref type="bibr" target="#b1">Bai et al. (2018)</ref> evaluate CNNs and LSTMs on several sequence modeling tasks. They con- clude that CNNs are better than RNNs for se- quence modeling. However, their CNN models perform much worse than the state-of-art LSTM models on some sequence modeling tasks, as they themselves state in the appendix. <ref type="bibr" target="#b25">Tang et al. (2018)</ref> evaluate different RNN ar- chitectures and Transformer models on the task of historical spelling normalization which translates a historical spelling into its modern form. They find that Transformer models surpass RNN mod- els only in high-resource conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In contrast to previous studies, we focus on the machine translation task, where architecture com- parisons so far are mostly based on BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NMT Architectures</head><p>We evaluate three different NMT architectures: RNN-based models, CNN-based models, and Transformer-based models. All of them have a bi- partite structure in the sense that they consist of an encoder and a decoder. The encoder and the decoder interact via a soft-attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015)</ref>, with one or multiple attention layers.</p><p>In the following sections, h l i is the hidden state at step i of layer l, h l i−1 represents the hidden state at the previous step of layer l, h l−1 i means the hid- den state at i of l − 1 layer, E x i represents the embedding of x i , and e pos,i denotes the positional embedding at position i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">RNN-based NMT</head><p>RNNs are stateful networks that change as new in- puts are fed to them, and each state has a direct connection only to the previous state. Thus, the path length of any two tokens with a distance of n in RNNs is exactly n. <ref type="figure" target="#fig_0">Figure 1 (a)</ref> shows an illustration of RNNs.</p><formula xml:id="formula_0">h l i = h l−1 i + f rnn (h l−1 i , h l i−1 )<label>(1)</label></formula><p>In deep architectures, two adjacent layers are com- monly connected with residual connections. In the lth encoder layer, h l i is generated by Equation 1, where f rnn is the RNN (GRU or LSTM) function.</p><formula xml:id="formula_1">x 1 x 2 x 3 x 4 x 5 (a) RNN x 1 x 2 x 3 x 4 x 5 padding padding (b) CNN x 1 x 2 x 3 x 4 x 5 (c) Self-attention</formula><p>In the first layer,</p><formula xml:id="formula_2">h 0 i = f rnn (E x i , h 0 i−1 )</formula><p>. In addition to the connection between the en- coder and decoder via attention, the initial state of the decoder is usually initialized with the average of the hidden states or the last hidden state of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">CNN-based NMT</head><p>CNNs are hierarchical networks, in that convolu- tion layers capture local correlations. The local context size depends on the size of the kernel and the number of layers. In order to keep the out- put the same length as the input, CNN models add padding symbols to input sequences. Given an L- layer CNN with a kernel size k, the largest context size is L(k −1). For any two tokens in a local con- text with a distance of n, the path between them is only n/(k − 1).</p><p>As <ref type="figure" target="#fig_0">Figure 1</ref> (b) shows, a 2-layer CNN with ker- nel size 3 "sees" an effective local context of 5 to- kens. The path between the first token and the fifth token is only 2 convolutions. 1 Since CNNs do not have a means to infer the position of elements in a sequence, positional embeddings are introduced.</p><formula xml:id="formula_3">h l i = h l−1 i + f cnn (W l [h l−1 i−−k/2 ; ...; h l−1 i+k/2 ] + b l ) (2)</formula><p>The hidden state h l i shown in Equation 2 is related to the hidden states in the same convolution and the hidden state h l−1 i from the previous layer. k denotes the kernel size of CNNs and f cnn is a non- linearity. ConvS2S chooses Gated Linear Units (GLU) which can be viewed as a gated variation of ReLUs. W l are called convolutional filters. In the input layer, h 0 i = E x i + e pos,i .</p><p>1 Note that the decoder employs masking to avoid condi- tioning the model on future information, which reduces the effective context size to L k−1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Transformer-based NMT</head><p>Transformers rely heavily on self-attention net- works. Each token is connected to any other token in the same sentence directly via self- attention. Moreover, Transformers feature at- tention networks with multiple attention heads. Multi-head attention is more fine-grained, com- pared to conventional 1-head attention mecha- nisms. <ref type="figure" target="#fig_0">Figure 1</ref> (c) illustrates that any two to- kens are connected directly: the path length be- tween the first and the fifth tokens is 1. Similar to CNNs, positional information is also preserved in positional embeddings.</p><p>The hidden state in the Transformer encoder is calculated from all hidden states of the previous layer. The hidden state h l i in a self-attention net- work is computed as in Equation 3.</p><formula xml:id="formula_4">h l i = h l−1 i + f (self-attention(h l−1 i ))<label>(3)</label></formula><p>where f represents a feedforward network with ReLU as the activation function and layer normal- ization. In the input layer, h 0 i = E x i + e pos,i . The decoder additionally has a multi-head atten- tion over the encoder hidden states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Evaluation of Machine Translation</head><p>Since we evaluate different NMT architectures explicitly on subject-verb agreement and WSD (both happen implicitly during machine transla- tion), BLEU as a measure of overall translation quality is not helpful. In order to conduct these targeted evaluations, we use contrastive test sets. Sets of contrastive translations can be used to analyze specific types of errors. Human refer- ence translations are paired with one or more con- trastive variants, where a specific type of error is introduced automatically.</p><p>The evaluation procedure then exploits the fact that NMT models are conditional language mod- els. By virtue of this, given any source sentence S and target sentence T , any NMT model can assign to them a probability P (T |S). If a model assigns a higher score to the correct target sentence than to a contrastive variant that contains an error, we consider it a correct decision. The accuracy of a model on such a test set is simply the percentage of cases where the correct target sentence is scored higher than all contrastive variants.</p><p>Contrastive evaluation tests the sensitivity of NMT models to specific translation errors. The contrastive examples are designed to capture spe- cific translation errors rather than evaluating the global quality of NMT models. Although they do not replace metrics such as BLEU, they give fur- ther insights into the performance of models, on specific linguistic phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Lingeval97</head><p>Lingeval97 has over 97,000 English→German contrastive translation pairs featuring different lin- guistic phenomena, including subject-verb agree- ment, noun phrase agreement, separable verb- particle constructions, transliterations and polar- ity. In this paper, we are interested in evaluat- ing the performance on long-range dependencies. Thus, we focus on the subject-verb agreement cat- egory which consists of 35,105 instances.</p><p>In German, verbs must agree with their subjects in both grammatical number and person. There- fore, in a contrastive translation, the grammatical number of a verb is swapped.  <ref type="table" target="#tab_0">Table 1</ref>: An example of a contrastive pair in the subject-verb agreement category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">ContraWSD</head><p>In ContraWSD, given an ambiguous word in the source sentence, the correct translation is replaced by another meaning of the ambiguous word which is incorrect. For example, in a case where the En- glish word line is the correct translation of the Ger- man source word Schlange, ContraWSD replaces line with the other translations of Schlange, such as snake, serpent, to generate contrastive transla- tions.</p><p>For German→English, ContraWSD contains 84 different German word senses. It has <ref type="bibr">7,</ref><ref type="bibr">200</ref> German→English lexical ambiguities, each lexi- cal ambiguity instance has 3.5 contrastive transla- tions on average. For German→French, it consists of 71 different German word senses. There are 6,700 German→French lexical ambiguities, with an average of 2.2 contrastive translations each lex- ical ambiguity instance. All the ambiguous words are nouns so that the disambiguation is not possi- ble simply based on syntactic context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Subject-verb Agreement</head><p>The subject-verb agreement task is the most pop- ular choice for evaluating the ability to capture long-range dependencies and has been used in many studies ( <ref type="bibr" target="#b15">Linzen et al., 2016;</ref><ref type="bibr" target="#b2">Bernardy and Lappin, 2017;</ref><ref type="bibr" target="#b21">Sennrich, 2017;</ref><ref type="bibr">Tran et al., 2018</ref>). Thus, we also use this task to evaluate different NMT architectures on long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Different architectures are hard to compare fairly because many factors affect performance. We aim to create a level playing field for the comparison by training with the same toolkit, Sockeye (Hieber et al., 2017) which is based on MXNet ( <ref type="bibr" target="#b4">Chen et al., 2015)</ref>. In addition, different hyperparameters and training techniques (such as label smoothing or layer normalization) have been found to affect the performance ( <ref type="bibr" target="#b3">Chen et al., 2018</ref>). We apply the same hyperparameters and techniques for all ar- chitectures except the parameters of each specific architecture. Since the best hyperparameters for different architectures may be diverse, we verify our hyperparameter choice by comparing our re- sults to those published previously. Our models achieve similar performance to that reported by <ref type="bibr" target="#b9">Hieber et al. (2017)</ref> with the best available set- tings. In addition, we extend Sockeye with an interface that enables scoring of existing transla- tions, which is required for contrastive evaluation.</p><p>All the models are trained with 2 GPUs. Dur- ing training, each mini-batch contains 4096 to- kens. A model checkpoint is saved every 4,000 updates. We use Adam ( <ref type="bibr" target="#b14">Kingma and Ba, 2015)</ref> as the optimizer. The initial learning rate is set to 0.0002. If the performance on the validation set has not improved for 8 checkpoints, the learn- ing rate is multiplied by 0.7. We set the early stopping patience to 32 checkpoints. All the neu- ral networks have 8 layers. For RNNS2S, the en- coder has 1 bi-directional LSTM and 6 stacked uni-directional LSTMs, and the decoder is a stack of 8 uni-directional LSTMs. The size of embed- dings and hidden states is 512. We apply layer nor- malization and label smoothing (0.1) in all mod- els. We tie the source and target embeddings. The dropout rate of embeddings and Transformer blocks is set to 0.</p><note type="other">1. The dropout rate of RNNs and CNNs is 0.2. The kernel size of CNNs is 3. Trans- formers have an 8-head attention mechanism.</note><p>To test the robustness of our findings, we also test a different style of RNN architecture, from a different toolkit. We evaluate bi-deep transi- tional RNNs (Miceli ) which are state-of-art RNNs in machine translation. We use the bi-deep RNN-based model (RNN-bideep) implemented in Marian (Junczys- <ref type="bibr">Dowmunt et al., 2018)</ref>. Different from the previous settings, we use the Adam optimizer with β 1 = 0.9, β 2 = 0.98, = 10 −9 . The initial learning rate is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0003.">We tie target embeddings and output em- beddings. Both the encoder and decoder have 4 layers of LSTM units, only the encoder layers are bi-directional. LSTM units consist of several cells (deep transition): 4 in the first layer of the decoder, 2 cells everywhere else.</head><p>We use training data from the WMT17 shared task. <ref type="bibr">2</ref> We use newstest2013 as the validation set, and use newstest2014 and newstest2017 as the test sets. All BLEU scores are computed with Sacre- BLEU <ref type="bibr" target="#b19">(Post, 2018)</ref>. There are about 5.9 million sentence pairs in the training set after preprocess- ing with Moses scripts. We learn a joint BPE model with 32,000 subword units ( <ref type="bibr" target="#b23">Sennrich et al., 2016)</ref>. We employ the model that has the best per- plexity on the validation set for the evaluation. <ref type="table">Table 2</ref> reports the BLEU scores on newstest2014 and newstest2017, the perplexity on the valida- tion set, and the accuracy on long-range depen- dencies. <ref type="bibr">3</ref> Transformer achieves the highest accu- racy on this task and the highest BLEU scores on both newstest2014 and newstest2017. Compared to RNNS2S, ConvS2S has slightly better results re- garding BLEU scores, but a much lower accuracy on long-range dependencies. The RNN-bideep model achieves distinctly better BLEU scores and a higher accuracy on long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Results</head><p>However, it still cannot outperform Transformers on any of the tasks.  <ref type="table">Table 2</ref>: The results of different NMT models, in- cluding the BLEU scores on newstest2014 and new- stest2017, the perplexity on the validation set, and the accuracy of long-range dependencies. <ref type="figure" target="#fig_1">Figure 2</ref> shows the performance of different ar- chitectures on the subject-verb agreement task. It is evident that Transformer, RNNS2S, and RNN- bideep perform much better than ConvS2S on long-range dependencies. However, Transformer, RNNS2S, and RNN-bideep are all robust over long distances. Transformer outperforms RNN-bideep for distances 11-12, but RNN-bideep performs equally or better for distance 13 or higher. Thus, we cannot conclude that Transformer models are particularly stronger than RNN models for long distances, despite achieving higher average accu- racy on distances above 10. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">CNNs</head><p>Theoretically, the performance of CNNs will drop when the distance between the subject and the verb exceeds the local context size. However, ConvS2S is also clearly worse than RNNS2S for subject-verb agreement within the local context size. In order to explore how the ability of ConvS2S to capture long-range dependencies depends on the local context size, we train additional systems, varying the number of layers and kernel size. <ref type="table" target="#tab_3">Ta- ble 3</ref> shows the performance of different ConvS2S models. <ref type="figure" target="#fig_2">Figure 3</ref> displays the performance of two 8-layer CNNs with kernel size 3 and 7, a 6-layer CNN with kernel size 3, and RNNS2S. The results indicate that the accuracy increases when the local context size becomes larger, but the BLEU score does not. Moreover, ConvS2S is still not as good as RNNS2S for subject-verb agreement.</p><p>Layer   Regarding the explanation for the poor perfor- mance of ConvS2S, we identify the limited context size as a major problem. One assumption to ex- plain the remaining difference is that, scale invari- ance of CNNs is relatively poor ( <ref type="bibr">Xu et al., 2014</ref>). Scale-invariance is important in NLP, where the distance between arguments is flexible, and cur- rent recurrent or attentional architectures are better suited to handle this variance.</p><p>Our empirical results do not confirm the theoret- ical arguments in <ref type="bibr" target="#b8">Gehring et al. (2017)</ref> that CNNs can capture long-range dependencies better with a shorter path. The BLEU score does not corre- late well with the targeted evaluation of long-range distance interactions. This is due to the locality of BLEU, which only measures on the level of n- grams, but it may also indicate that there are other trade-offs between the modeling of different phe- nomena depending on hyperparameters. If we aim to get better performance on long-range dependen- cies, we can take this into account when optimiz- ing hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">RNNs vs. Transformer</head><p>Even though Transformer achieves much better BLEU scores than RNNS2S and RNN-bideep, the accuracies of these architectures on long-range de- pendencies are close to each other in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Our experimental result contrasts with the result from <ref type="bibr">Tran et al. (2018)</ref>. They find that Transform- ers perform worse than LSTMs on the subject- verb agreement task, especially when the distance between the subject and the verb becomes longer.</p><p>We perform several experiments to analyze this discrepancy with <ref type="bibr">Tran et al. (2018)</ref>.</p><p>A first hypothesis is that this is caused by the amount of training data, since we used much larger datasets than <ref type="bibr">Tran et al. (2018)</ref>. We retrain all the models with a small amount of training data simi- lar to the amount used by <ref type="bibr">Tran et al. (2018)</ref>, about 135K sentence pairs. The other training settings are the same as in Section 4.1. We do not see the expected degradation of Transformer-s, compared to RNNS2S-s (see <ref type="figure" target="#fig_4">Figure 4)</ref>. In <ref type="table">Table 4</ref>, the perfor- mance of RNNS2S-s and Transformer-s is similar, including the BLEU scores on newstest2014, new- stest2017, the perplexity on the validation set, and the accuracy on the long-range dependencies.  A second hypothesis is that the experimental set- tings lead to the different results. In order to inves- tigate this, we do not only use a small training set, but also replicate the experimental settings of <ref type="bibr">Tran et al. (2018)</ref>. The main changes are neural network layers (8→4); embedding size (512→128); multi- head size (8→2); dropout rate (0.1→0.2); check- point save frequency (4,000→1,000), and initial learning rate (0.0002→0.001).  <ref type="table">Table 4</ref>: The results of different models with small training data and replicate settings. Trans is short for Transformer. Models with the suffix "-s" are models trained with small data set. Models with the suffix "-re" are models trained with replicate settings. "h2, h4, h8" indicates the number of attention heads for Transformer models. Trans-re-h2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Trans-re-h4</p><p>Trans-re-h8</p><p>Figure 5: Results of the models with replicate settings, varying the number of attention heads for the Trans- former models.</p><p>In the end, we get a result that is similar to <ref type="bibr">Tran et al. (2018)</ref>. In <ref type="figure">Figure 5</ref>, Transformer-re-h2 performs clearly worse than RNNS2S-re on long- range dependencies. By increasing the number of heads in multi-head attention, subject-verb ac- curacy over long distances can be improved sub- stantially, even though it remains below that of RNNS2S-re. Also, the effect on BLEU is small. Our results suggest that the importance of multi- head attention with a large number of heads is larger than BLEU would suggest, especially for the modeling of long-distance phenomena, since multi-head attention provides a way for the model to attend to both local and distant context, whereas distant context may be overshadowed by local context in an attention mechanism with a single or few heads.</p><p>Although our study is not a replication of <ref type="bibr">Tran et al. (2018)</ref>, who work on a different task and a different test set, our results do suggest an al- ternative interpretation of their findings, namely that the poor performance of the Transformer in their experiments is due to hyperparameter choice. Rather than concluding that RNNs are superior to Transformers for the modeling of long-range de- pendency phenomena, we find that the number of heads in multi-head attention affects the ability of Transformers to model long-range dependencies in subject-verb agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">WSD</head><p>Our experimental results on the subject-verb agreement task demonstrate that CNNs and Trans- former are not better at capturing long-range de- pendencies compared to RNNs, even though the paths in CNNs and Transformers are shorter. This finding is not in accord with the theoretical argu- ment in both <ref type="bibr" target="#b8">Gehring et al. (2017)</ref> and <ref type="bibr">Vaswani et al. (2017)</ref>. However, these architectures per- form well empirically according to BLEU. Thus, we further evaluate these architectures on WSD, to test our hypothesis that non-recurrent architec- tures are better at extracting semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental settings</head><p>We evaluate all architectures on ContraWSD on both DE→EN and DE→FR. We reuse the param- eter settings in Section 4.1, except that: the initial learning rate of ConvS2S is reduced from 0.0003 to 0.0002 in DE→EN; the checkpoint saving fre- quency is changed from 4,000 to 1,000 in DE→FR because of the training data size.</p><p>For DE→EN, the training set, validation set, and test set are the same as the other direction EN→DE. For DE→FR, we use around 2.1 million sentence pairs from Europarl (v7) <ref type="bibr">(Tiedemann, 2012)</ref>  <ref type="bibr">4</ref> and News Commentary (v11) cleaned by <ref type="bibr" target="#b20">Rios et al. (2017)</ref>  <ref type="bibr">5</ref> as our training set. We use newstest2013 as the evaluation set, and use new- stest2012 as the test set. All the data is prepro- cessed with Moses scripts.  In addition, we also compare to the best result reported for DE→EN, achieved by uedin-wmt17 ( , which is an ensemble of 4 different models and reranked with right-to-left models. 6 uedin-wmt17 is based on the bi-deep RNNs ) that we men- tioned before. To the original 5.9 million sentence pairs in the training set, they add 10 million syn- thetic pairs with back-translation. <ref type="table" target="#tab_6">Table 5</ref> gives the performance of all the architec- tures, including the perplexity on validation sets, the BLEU scores on newstest, and the accuracy on ContraWSD. Transformers distinctly outper- form RNNS2S and ConvS2S models on DE→EN and DE→FR. Moreover, the Transformer model on DE→EN also achieves higher accuracy than uedin-wmt17, although the BLEU score on new- stest2017 is 1.4 lower than uedin-wmt17. We at- tribute this discrepancy between BLEU and WSD performance to the use of synthetic news training data in uedin-wmt17, which causes a large boost in BLEU due to better domain adaptation to newstest, but which is less helpful for ContraWSD, whose test set is drawn from a variety of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Results</head><p>For DE→EN, RNNS2S and ConvS2S have the same BLEU score on newstest2014, ConvS2S has a higher score on newstest2017. However, the WSD accuracy of ConvS2S is 1.7% lower than RNNS2S. For DE→FR, ConvS2S achieves slightly better results on both BLEU scores and accuracy than RNNS2S.</p><p>The Transformer model strongly outperforms the other architectures on this WSD task, with a gap of 4-8 percentage points. This affirms our hypothesis that Transformers are strong semantic features extractors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hybrid Encoder-Decoder Model</head><p>In recent work, <ref type="bibr" target="#b3">Chen et al. (2018)</ref> find that hybrid architectures with a Transformer encoder and an RNN decoder can outperform a pure Transformer model. They speculate that the Transformer en- coder is better at encoding or extracting features than the RNN encoder, whereas the RNN is better at conditional language modeling.</p><p>For WSD, it is unclear whether the most im- portant component is the encoder, the decoder, or both. Following the hypothesis that Transformer encoders excel as semantic feature extractors, we train a hybrid encoder-decoder model (TransRNN) with a Transformer encoder and an RNN decoder.</p><p>The results (in <ref type="table" target="#tab_6">Table 5</ref>) show that TransRNN performs better than RNNS2S, but worse than the pure Transformer, both in terms of BLEU and WSD accuracy. This indicates that WSD is not only done in the encoder, but that the decoder also affects WSD performance. We note that <ref type="bibr" target="#b3">Chen et al. (2018)</ref>; <ref type="bibr" target="#b6">Domhan (2018)</ref> introduce the tech- niques in Transformers into RNN-based models, with reportedly higher BLEU. Thus, it would be interesting to see if the same result holds true with their architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we evaluate three popular NMT ar- chitectures, RNNS2S, ConvS2S, and Transformers, on subject-verb agreement and WSD by scoring contrastive translation pairs.</p><p>We test the theoretical claims that shorter path lengths make models better capture long-range de- pendencies. Our experimental results show that:</p><p>• There is no evidence that CNNs and Trans- formers, which have shorter paths through networks, are empirically superior to RNNs in modeling subject-verb agreement over long distances.</p><p>• The number of heads in multi-head attention affects the ability of a Transformer to model long-range dependencies in the subject-verb agreement task.</p><p>• Transformer models excel at another task, WSD, compared to the CNN and RNN archi- tectures we tested.</p><p>Lastly, our findings suggest that assessing the per- formance of NMT architectures means finding their inherent trade-offs, rather than simply com- puting their overall BLEU score. A clear under- standing of those strengths and weaknesses is im- portant to guide further work. Specifically, given the idiosyncratic limitations of recurrent and self- attentional models, combining them is an exciting line of research. The apparent weakness of CNN architectures on long-distance phenomena is also a problem worth tackling, and we can find inspi- ration from related work in computer vision ( <ref type="bibr">Xu et al., 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architectures of different neural networks in NMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of different NMT models on the subject-verb agreement task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of ConvS2S models and the RNNS2S model at different distances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of a Transformer and RNNS2S model trained on a small dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 gives</head><label>1</label><figDesc></figDesc><table>an 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The performance of ConvS2S with different settings. K means the kernel size. The ctx column is the theoretical largest local context size in the masked decoder.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The results of different architectures on newstest sets and ContraWSD. PPL is the perplexity on the 
validation set. Acc means accuracy on the test set. 

</table></figure>

			<note place="foot" n="2"> http://www.statmt.org/wmt17/ translation-task.html 3 We report average accuracy on instances where the distance between subject and verb is longer than 10 words.</note>

			<note place="foot" n="4"> http://opus.nlpl.eu/Europarl.php 5 http://data.statmt.org/ContraWSD/</note>

			<note place="foot" n="6"> https://github.com/a-rios/ContraWSD/ tree/master/baselines</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the anonymous reviews and Joakim Nivre who give a lot of valuable and insight-ful comments. We appreciate the grants pro-vided by Erasmus+ Programme and Anna Maria Lundin's scholarship committee. GT is funded by the Chinese Scholarship Council (grant num-ber 201607110016). MM, AR and RS have re-ceived funding from the Swiss National Science Foundation (grant number 105212 169888).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using Deep Neural Networks to Learn Syntactic Agreement. LiLT (Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bernardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Mia Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How much attention do you need? a granular analysis of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1799" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05690</idno>
		<title level="m">Sockeye: A Toolkit for Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<editor>Alham Fikri Aji, Nikolay Bogoychev, André F. T</editor>
		<imprint>
			<pubPlace>Tom Neckermann, Frank Seide, Ulrich Germann</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00344</idno>
		<title level="m">Marian: Fast Neural Machine Translation in C++</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep architectures for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<title level="m">A call for clarity in reporting bleu scores</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Mascarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How Grammatical is Characterlevel Neural Machine Translation? Assessing MT Quality with Contrastive Translation Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="376" to="382" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The University of Edinburgh&apos;s Neural MT Systems for WMT17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="389" to="399" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An evaluation of neural machine translation models on historical spelling normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongbo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Cap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1320" to="1331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
