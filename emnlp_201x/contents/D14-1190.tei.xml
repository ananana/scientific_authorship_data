<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Emotion Analysis via Multi-task Gaussian Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<email>t.cohn@unimelb.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Computing and Information Systems</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Emotion Analysis via Multi-task Gaussian Processes</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1798" to="1803"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a model for jointly predicting multiple emotions in natural language sentences. Our model is based on a low-rank coregionalisation approach, which combines a vector-valued Gaussian Process with a rich parameterisation scheme. We show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset. The proposed model outperforms both single-task baselines and other multi-task approaches .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task learning <ref type="bibr" target="#b6">(Caruana, 1997</ref>) has been widely used in Natural Language Processing. Most of these learning methods are aimed for Do- main Adaptation <ref type="bibr" target="#b8">(Daumé III, 2007;</ref><ref type="bibr" target="#b9">Finkel and Manning, 2009)</ref>, where we hypothesize that we can learn from multiple domains by assuming sim- ilarities between them. A more recent use of multi-task learning is to model annotator bias and noise for datasets labelled by multiple annotators .</p><p>The settings mentioned above have one aspect in common: they assume some degree of posi- tive correlation between tasks. In Domain Adap- tation, we assume that some "general", domain- independent knowledge exists in the data. For an- notator noise modelling, we assume that a "ground truth" exists and that annotations are some noisy deviations from this truth. However, for some set- tings these assumptions do not necessarily hold and often tasks can be anti-correlated. For these cases, we need to employ multi-task methods that are able to learn these relations from data and correctly employ them when making predictions, avoiding negative knowledge transfer.</p><p>An example of a problem that shows this be- haviour is Emotion Analysis, where the goal is to automatically detect emotions in a text <ref type="bibr" target="#b21">(Strapparava and Mihalcea, 2008;</ref><ref type="bibr" target="#b12">Mihalcea and Strapparava, 2012</ref>). This problem is closely related to Opinion Mining ( <ref type="bibr" target="#b13">Pang and Lee, 2008)</ref>, with sim- ilar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels (one for each emotion) instead of a single label. While we expect some emotions to have some degree of correlation, this is usually not the case for all possible emotions. For instance, we expect sadness and joy to be anti-correlated.</p><p>We propose a multi-task setting for Emotion Analysis based on a vector-valued Gaussian Pro- cess (GP) approach known as coregionalisation ( ´ <ref type="bibr">Alvarez et al., 2012</ref>). The idea is to combine a GP with a low-rank matrix which encodes task corre- lations. Our motivation to employ this model is three-fold:</p><p>• Datasets for this task are scarce and small so we hypothesize that a multi-task approach will results in better models by allowing a task to borrow statistical strength from other tasks;</p><p>• The annotation scheme is subjective and very fine-grained, and is therefore heavily prone to bias and noise, both which can be modelled easily using GPs;</p><p>• Finally, we also have the goal to learn a model that shows sound and interpretable correlations between emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-task Gaussian Process Regression</head><p>Gaussian Processes (GPs) <ref type="bibr" target="#b16">(Rasmussen and Williams, 2006</ref>) are a Bayesian kernelised framework considered the state-of-the-art for regression. They have been recently used success- fully for translation quality prediction <ref type="bibr" target="#b3">Beck et al., 2013;</ref> and modelling text periodicities <ref type="bibr" target="#b15">(Preotiuc-Pietro and Cohn, 2013</ref>). In the following we give a brief description on how GPs are applied in a regression setting. Given an input x, the GP regression assumes that its output y is a noise corrupted version of a latent function evaluation, y = f (x) + η, where η ∼ N (0, σ 2 n ) is the added white noise and the function f is drawn from a GP prior:</p><formula xml:id="formula_0">f (x) ∼ GP(µ(x), k(x, x )),<label>(1)</label></formula><p>where µ(x) is the mean function, which is usually the 0 constant, and k(x, x ) is the kernel or co- variance function, which describes the covariance between values of f at locations x and x . To predict the value for an unseen input x * , we compute the Bayesian posterior, which can be cal- culated analytically, resulting in a Gaussian distri- bution over the output y * : 1</p><formula xml:id="formula_1">y * ∼ N (k * (K + σ n I) −1 y T ,<label>(2)</label></formula><formula xml:id="formula_2">k(x * , x * ) − k T * (K + σ n I) −1 k * ),</formula><p>where K is the Gram matrix corre- sponding to the covariance kernel evalu- ated at every pair of training inputs and</p><formula xml:id="formula_3">k * = [x 1 , x * , x 2 , x * , . . . , x n , x * ]</formula><p>is the vector of kernel evaluations between the test input and each training input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Intrinsic Coregionalisation Model</head><p>By extending the GP regression framework to vector-valued outputs we obtain the so-called coregionalisation models. Specifically, we employ a separable vector-valued kernel known as Intrin- sic Coregionalisation Model (ICM) <ref type="bibr">( ´ Alvarez et al., 2012)</ref>. Considering a set of D tasks, we define the corresponding vector-valued kernel as:</p><formula xml:id="formula_4">k((x, d), (x , d )) = k data (x, x ) × B d,d , (3)</formula><p>where k data is a kernel on the input points (here a Radial Basis Function, RBF), d and d are task or metadata information for each input and B ∈ R D×D is the coregionalisation matrix, which en- codes task covariances and is symmetric and posi- tive semi-definite.</p><p>A key advantage of GP-based modelling is its ability to learn hyperparameters directly from data <ref type="bibr">1</ref> We refer the reader to <ref type="bibr">Rasmussen and Williams (2006, Chap.</ref> 2) for an in-depth explanation of GP regression.</p><p>by maximising the marginal likelihood:</p><formula xml:id="formula_5">p(y|X, θ) = f p(y|X, θ, f )p(f ).<label>(4)</label></formula><p>This process is usually performed to learn the noise variance and kernel hyperparameters, in- cluding the coregionalisation matrix. In order to do this, we need to consider how B is parame- terised.  treat the diagonal val- ues of B as hyperparameters, and as a conse- quence are able to leverage the inter-task trans- fer between each independent task and the global "pooled" task. They however fix non-diagonal val- ues to 1, which in practice is equivalent to assum- ing equal correlation across tasks. This can be lim- iting, in that this formulation cannot model anti- correlations between tasks.</p><p>In this work we lift this restriction by adopting a different parameterisation of B that allows the learning of all task correlations. A straightforward way to do that would be to consider every corre- lation as an hyperparameter, but this can result in a matrix which is not positive semi-definite (and therefore, not a valid covariance matrix). To en- sure this property, we follow the method proposed by <ref type="bibr" target="#b5">Bonilla et al. (2008)</ref>, which decomposes B us- ing Probabilistic Principal Component Analysis:</p><formula xml:id="formula_6">B = UΛU T + diag(α),<label>(5)</label></formula><p>where U is an D × R matrix containing the R principal eigenvectors and Λ is a R × R diago- nal matrix containing the corresponding eigenval- ues. The choice of R defines the rank of UΛU T , which can be understood as the capacity of the manifold with which we model the D tasks. The vector α allows for each task to behave more or less independently with respect to the global task. The final rank of B depends on both terms in Equation 5.</p><p>For numerical stability, we use the incomplete- Cholesky decomposition over the matrix UΛU T , resulting in the following parameterisation for B:</p><formula xml:id="formula_7">B = ˜ L ˜ L T + diag(α),<label>(6)</label></formula><p>where˜Lwhere˜ where˜L is a D × R matrix. In this setting, we treat all elements of˜Lof˜ of˜L as hyperparameters. Set- ting a larger rank allows more flexibility in mod- elling task correlations. However, a higher number of hyperparameters may lead to overfitting prob- lems or otherwise cause issues in optimisation due to additional non-convexities in the log likelihood objective. In our experiments we evaluate this be- haviour empirically by testing a range of ranks for each setting.</p><p>The low-rank model can subsume the ones pro- posed by  by fixing and tying some of the hyperparameters:</p><p>Independent: fixing˜Lfixing˜ fixing˜L = 0 and α = 1;</p><p>Pooled: fixing˜Lfixing˜ fixing˜L = 1 and α = 0;</p><p>Combined: fixing˜Lfixing˜ fixing˜L = 1 and tying all compo- nents of α;</p><p>Combined+: fixing˜Lfixing˜ fixing˜L = 1.</p><p>These formulations allow us to easily replicate their modelling approach, which we evaluate as competitive baselines in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>To address the feasibility of our approach, we pro- pose a set of experiments with three goals in mind:</p><p>• To find our whether the ICM is able to learn sensible emotion correlations;</p><p>• To check if these correlations are able to im- prove predictions for unseen texts;</p><p>• To investigate the behaviour of the ICM model as we increase the training set size.</p><p>Dataset We use the dataset provided by the "Af- fective Text" shared task in <ref type="bibr">SemEval-2007 (Strapparava and</ref><ref type="bibr" target="#b19">Mihalcea, 2007)</ref>, which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Surprise. For each emotion, a score between 0 and 100 is given, 0 meaning total lack of emotion and 100 maximum emotional load. We use 100 sen- tences for training and the remaining 900 for test- ing.</p><p>Model For all experiments, we use a Radial Ba- sis Function (RBF) data kernel over a bag-of- words feature representation. Words were down- cased and lemmatized using the WordNet lemma- tizer in the NLTK 2 toolkit ( <ref type="bibr" target="#b4">Bird et al., 2009</ref>). We then use the GPy toolkit 3 to combine this kernel with a coregionalisation model over the six emo- tions, comparing a number of low-rank approxi- mations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines and Evaluation</head><p>We compare predic- tion results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via cross- validation and a single-task GP, optimised via like- lihood maximisation. The SVM models were trained using the Scikit-learn toolkit <ref type="bibr">4 (Pedregosa et al., 2011</ref>). We also compare our results against the ones obtained by employing the "Combined" and "Combined+" models proposed by . Following previous work in this area, we use Pearson's correlation coefficient as evaluation metric. <ref type="figure" target="#fig_0">Figure 1</ref> shows the learned coregionalisation ma- trix setting the initial rank as 1, reordering the emotions to emphasize the learned structure. We can see that the matrix follows a block structure, clustering some of the emotions. This picture shows two interesting behaviours:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learned Task Correlations</head><p>• Sadness and fear are highly correlated. Anger and disgust also correlate with them, al- though to a lesser extent, and could be con- sidered as belonging to the same cluster. We can also see correlation between surprise and joy. These are intuitively sound clusters based on the polarity of these emotions.</p><p>• In addition to correlations, the model learns anti-correlations, especially between joy/surprise and the other emotions. We also note that joy has the highest diagonal value, meaning that it gives preference to indepen- dent modelling (instead of pooling over the remaining tasks).</p><p>Inspecting the eigenvalues of the learned ma- trix allows us to empirically determine its result- ing rank. In this case we find that the model has learned a matrix of rank 3, which indicates that our initial assumption of a rank 1 coregionalisa- tion matrix may be too small in terms of modelling capacity <ref type="bibr">5</ref> .</p><p>This suggests that a higher rank is justified, although care must be taken due to the local optima and overfitting issues cited in §2.1. show the best performing model for each emotion. The scores for the "All" column were calculated over the predictions for all emotions concatenated (instead of just averaging over the scores for each emotion).  <ref type="table">Table 1</ref> shows the Pearson's scores obtained in our experiments. The low-rank models outper- formed the baselines for the full task (predicting all emotions) and for fear, joy and surprise sub- tasks. The rank 5 models were also able to out- perform all GP baselines for the remaining emo- tions, but could not beat the SVM baseline. As expected, the "Combined" and "Combined+" per- formed worse than the low-rank models, probably due to their inability to model anti-correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anger Disgust</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prediction Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error analysis</head><p>To check why SVM performs better than GPs for some emotions, we analysed their gold-standard score distributions. <ref type="figure">Figure 2</ref> shows the smoothed distributions for disgust and fear, comparing the gold-standard scores to predictions from the SVM and GP models. The distributions for the training set follow similar shapes. We can see that GP obtains better matching score distributions in the case when the gold- <ref type="figure">Figure 2</ref>: Test score distributions for disgust and fear. For clarity, only scores between 0 and 50 are shown. SVM performs better on disgust, while GP performs better on fear.</p><p>standard scores are more spread over the full sup- port of response values, i.e., <ref type="bibr">[0,</ref><ref type="bibr">100]</ref>. Since our GP model employs a Gaussian likelihood, it is effec- tively minimising a squared-error loss. The SVM model, on the other hand, uses hinge loss, which is linear beyond the margin envelope constraints. This affects the treatment of outlier points, which attract quadratic cf. linear penalties for the GP and SVM respectively. Therefore, when train- ing scores are more uniformly distributed (which is the case for fear), the GP model has to take the high scores into account, resulting in broader cov- erage of the full support. For disgust, the scores are much more peaked near zero, favouring the more narrow coverage of the SVM.</p><p>More importantly, <ref type="figure">Figure 2</ref> also shows that both SVM and GP predictions tend to exhibit a Gaus- sian shape, while the true scores show an expo- nential behaviour. This suggests that both mod- els are making wrong prior assumptions about the underlying score distribution. For SVMs, this is a non-trivial issue to address, although it is much easier for GPs, where we can use a different like- lihood distribution, e.g., a Beta distribution to re- flect that the outputs are only valid over a bounded range. Note that non-Gaussian likelihoods mean that exact inference is no longer tractable, due to the lack of conjugacy between the prior and likeli- hood. However a number of approximate infer- ence methods are appropriate which are already widely used in the GP literature for use with non- Gaussian likelihoods, including expectation prop- agation <ref type="bibr" target="#b11">(Jylänki et al., 2011</ref>), the Laplace approx- imation (Williams and Barber, 1998) and Markov Chain Monte Carlo sampling ( <ref type="bibr" target="#b0">Adams et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Set Influence</head><p>We expect multi-task models to perform better for smaller datasets, when compared to single-task models. This stems from the fact that with small datasets often there is more uncertainty associated with each task, a problem which can be alleviated using statistics from the other tasks. To measure this behaviour, we performed an additional exper- iment varying the size of the training sets, while using 100 sentences for testing. <ref type="figure" target="#fig_1">Figure 3</ref> shows the scores obtained. As ex- pected, for smaller datasets the single-task mod- els are outperformed by ICM, but their perfor- mance become equivalent as the training set size increases. SVM performance tends to be slightly worse for most sizes. To study why we obtained an outlier for the single-task model with 200 sen- tences, we inspected the prediction values. We found that, in this case, predictions for joy, sur- prise and disgust were all around the same value. <ref type="bibr">6</ref> For larger datasets, this effect disappears and the single-task models yield good predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>This paper proposed an multi-task approach for Emotion Analysis that is able to learn correlations <ref type="bibr">6</ref> Looking at the predictions for smaller datasets, we found the same behaviour, but because the values found were near the mean they did not hurt the Pearson's score as much. and anti-correlations between emotions. Our for- mulation is based on a combination of a Gaussian Process and a low-rank coregionalisation model, using a richer parameterisation that allows the learning of fine-grained task similarities. The pro- posed model outperformed strong baselines when applied to a news headline dataset.</p><p>As it was discussed in Section 4.3, we plan to further explore the possibility of using non- Gaussian likelihoods with the GP models. An- other research avenue we intend to explore is to employ multiple layers of metadata, similar to the model proposed by . An example is to incorporate the dataset provided by <ref type="bibr" target="#b18">Snow et al. (2008)</ref>, which provides multiple non- expert emotion annotations for each sentence, ob- tained via crowdsourcing. Finally, another possi- ble extension comes from more advanced vector- valued GP models, such as the linear model of coregionalisation <ref type="bibr">( ´ Alvarez et al., 2012</ref>) or hierar- chical kernels <ref type="bibr" target="#b10">(Hensman et al., 2013</ref>). These mod- els can be specially useful when we want to em- ploy multiple kernels to explain the relation be- tween the input data and the labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Heatmap showing a learned coregionalisation matrix over the emotions.</figDesc><graphic url="image-1.png" coords="4,72.00,226.63,233.86,176.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pearson's correlation score according to training set size (in number of sentences).</figDesc><graphic url="image-3.png" coords="5,307.28,62.81,219.46,147.46" type="bitmap" /></figure>

			<note place="foot" n="2"> http://www.nltk.org 3 http://github.com/SheffieldML/GPy</note>

			<note place="foot" n="4"> http://scikit-learn.org 5 The eigenvalues were 592, 62, 86, 4, 3 × 10 −3 and 9 × 10 −5 .</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tractable Nonparametric Bayesian Inference in Poisson Processes with Gaussian Process Intensities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan Prescott</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Kernels for Vector-Valued Functions: a Review. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SHEF-Lite : When Less is More for Translation Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT13</title>
		<meeting>WMT13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="337" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Multi-task Gaussian Process Prediction. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<title level="m">Multitask Learning. Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian modelling of gene expression time series across irregularly sampled replicates and clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Neil D Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rattray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">252</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust Gaussian Process Regression with a Studentt Likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasi</forename><surname>Jylänki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarno</forename><surname>Vanhatalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aki</forename><surname>Vehtari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3227" to="3257" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lyrics, Music, and Emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Duborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A temporal model of text periodicities using Gaussian Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preotiuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Pietro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Investigation on the Effectiveness of Features for Translation Quality Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit XIV</title>
		<meeting>MT Summit XIV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cheap and Fast-But is it Good?: Evaluating Non-Expert Annotations for Natural Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Task 14 : Affective Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Semeval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SEMEVAL</title>
		<meeting>SEMEVAL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to identify emotions in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM Symposium on Applied Computing</title>
		<meeting>the 2008 ACM Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bayesian Classification with Gaussian Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1342" to="1351" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
