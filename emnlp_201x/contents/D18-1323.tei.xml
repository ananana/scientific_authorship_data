<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to represent a word and predict it, too: Improving tied architectures for language modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2936</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Aina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra Barcelona</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How to represent a word and predict it, too: Improving tied architectures for language modelling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2936" to="2941"/>
							<date type="published">October 31-November 4, 2018. 2018. 2936</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent state-of-the-art neural language models share the representations of words given by the input and output mappings. We propose a simple modification to these architectures that decouples the hidden state from the word embedding prediction. Our architecture leads to comparable or better results compared to previous tied models and models without tying, with a much smaller number of parameters. We also extend our proposal to word2vec models , showing that tying is appropriate for general word prediction tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In neural models, reusing representations of the same type of data (e.g., sentences or words) in dif- ferent parts of the architecture can be a powerful way to aid learning: it reduces parameters, en- abling more compact models and faster learning. Recurrent neural network (RNN) language models ( <ref type="bibr">Mikolov et al., 2010;</ref><ref type="bibr">Sundermeyer et al., 2012;</ref><ref type="bibr">Zaremba et al., 2014</ref>) have two word mappings: From the input word onto its embedding repre- sentation, and from the internal representation of the network (the hidden layer) to the weights for the prediction of the next word. In standard mod- els, these representations are different. Recently, <ref type="bibr">Inan et al. (2017)</ref> and <ref type="bibr">Press and Wolf (2017)</ref> pro- posed to instead use a single word representation, tying the input and output mappings. Intuitively, both are representations of the same type of data (words), and information learnt when observing a word as input can be reused when predicting this word as output. Tied language models obtained better perplexity and better word similarity scores of embedding matrices while reducing the number of parameters. The models that achieve the latest state-of-the art results incorporate this technique (see, e.g., <ref type="bibr">Merity et al., 2018</ref>).</p><p>However, note that, by tying the output map- ping to the input mapping, the hidden layer of the network is optimised to match the representation of the predicted word. We suggest that this intro- duces a constraint that conflicts with the function of the hidden layer in language models to repre- sent the previous context and transmit information to the next timestep. In this paper, we propose a minimal modification to tied LM architectures to address this issue: we add a linear transformation between the hidden layer and the word embedding prediction, partially decoupling the two. This has an important advantage. Standard tied architec- tures require the hidden layer to have the same dimensionality as the word embeddings. We lift this constraint in our architecture: by separating the hidden layer and the word mapping, we can choose a large hidden layer dimensionality while keeping the embedding dimensionality and, con- sequently, the size of the embedding matrix small.</p><p>In a set of experiments on LM, we show that our tied models achieve results similar to or better than models with standard or no tying, with much smaller embedding sizes and a reduction of 30- 60% in the overall number of parameters. Notably, the word embeddings obtained with the modified model have a higher quality than double-sized em- beddings obtained with standard tied models, as measured on word similarity.</p><p>We further extend this idea to word represen- tation learning models (in particular, word2vec), which have a similar architecture and objective function to language models. For instance, the standard skipgram model ( <ref type="bibr">Mikolov et al., 2013</ref>) has two mappings, one for the context words and one for the target word. While tying these two ma- trices directly constraints learning too strongly, an additional linear mapping adds the sufficient ca- pacity to learn embeddings of the same quality as the standard model using only half the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tied LM architectures 2.1 Previous work</head><p>Equations <ref type="formula">(1)</ref> through (4) define a standard RNN language model ( <ref type="bibr">Mikolov et al., 2010)</ref>:</p><formula xml:id="formula_0">ˆ x t = x t E (1) h t = LST M (ˆ x t , h t−1 ) (2) o = h t W T (3) p = sof tmax(o),<label>(4)</label></formula><p>where x t is the one-hot encoding of the input word at time t, ˆ x t is its embedded representation and E is the embedding matrix (input mapping). We fol- low the majority of previous work in adopting an LSTM <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997)</ref> as the recurrent unit, as it was shown to outperform other recurrent architectures for LM ( <ref type="bibr">Jozefowicz et al., 2015</ref>). The hidden vector h t is used as input to the LSTM for the next time step and also as input to a linear transformation W (output mapping) which produces the output weights. These weights are normalised into probability scores using the soft- max function.</p><p>The tied models proposed by <ref type="bibr">Inan et al. (2017)</ref> and <ref type="bibr">Press and Wolf (2017)</ref> set W to be equal to E <ref type="figure" target="#fig_0">(Figure 1b</ref>). Note that since E is of size |V | × m, where m is the embedding size, and W is of size |V |×n, where n is the hidden state size, the hidden and embedding dimensions must be equal, that is, m = n. <ref type="bibr">Press and Wolf (2017)</ref> observe that the output matrix W represents an embedding matrix since two similar words with indices i and j are learnt to receive similar probabilities given a context and hence the rows W i and W j should also be similar. Indeed, they show that on some word similarity evaluation tasks the output matrix W outperforms significantly the input matrix E. Tying E and W makes the model share the representations for the input and output vocabularies.</p><p>Inan et al. <ref type="formula">(2017)</ref> suggest a theoretical moti- vation for the tying technique and derive it as an instance of a more general approach of augment- ing the cross-entropy loss. They show that a loss that takes into account not only the target word (i.e., log p i for cross-entropy) but the scores for all words in the vocabulary according to their similar- ity to the target (computed as dot-product of em- beddings in E) improves performance on LM. One important practical advantage of tying in- put and output matrices is the reduction in num- ber of parameters with respect to a standard model with the same hidden and embedding dimensions, since instead of two matrices E and W of size |V | × m we have only one matrix of size |V | × m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed modification</head><p>One potential problem with the tied model, where E = W , is that the hidden state h t is optimised to be close to the embedding of the target word. To see this, consider that o j = e j · h t , ∀j. The cross-entropy objective is to maximise log p i for the target word with index i and consequently to minimise log p j ∀j = i. Hence, o i = e i · h t will be increased by the gradient descent update and h t will be aligned closer with e i (for each dimension k of the two vectors, h tk · e ik will be increased). This association between h t and word embedding space could prevent efficient retention of LSTM history in h t , which is used as input for the fol- lowing time step.</p><p>To address this issue, we propose a simple mod- ification to the standard tied model, replacing the output transformation (3) as follows:</p><formula xml:id="formula_1">ˆ h t = h t L o = ˆ h t E T .<label>(5)</label></formula><p>L is an additional linear transformation that de- couples the hidden state h t , which is passed to the next time step and represents the previous, possi- bly long-term, linguistic context, fromˆhfromˆ fromˆh t , which is instead optimised to match the embedding of the output word. As <ref type="figure" target="#fig_0">Figure 1</ref> illustrates, an important advantage of the additional transformation L is that a model can have different dimensions for the hidden vec- tor (n) and the embedding vectors (m). The em- bedding matrix is the largest part of the model when the vocabulary is large (|V | n). Re- ducing the size of the embedding m leads to a significant reduction in the number of parameters, proportional to |V |, and the acceleration of soft- max computation. On the other hand, the size of the additional matrix L is only n × m and con- tributes very little to the overall size of the model. We test empirically how reducing the embedding size affects the performance of language models by varying hidden and embedding sizes in our ex- periments and evaluating embedding matrices on word similarity tasks.</p><p>Standardly used LM models often have two lay- ers of LSTM cells. Thus, the issue we identifed might be mitigated in practice, since the hidden state of the first layer is not directly affected by ty- ing the output and input transformation matrices. Moreover, an LSTM cell carries over information both through the hidden state and a memory state; the latter is affected by tying only indirectly (see <ref type="bibr" target="#b3">Hochreiter and Schmidhuber (1997)</ref> for details on LSTM architectures). However, our experiments show that, in practice, two-layer LSTM LMs are still affected by tying despite these caveats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Extending the tied technique to word2vec</head><p>The tied technique, as formulated above, can be in principle applied to any model which has the same general objective of LM: predicting a target word given context words. The CBOW word2vec model for word representation learning ( <ref type="bibr">Mikolov et al., 2013</ref>) is the primary candidate for testing the ap- plicability of the tied technique beyond LM, since we can see it as substituting the LSTM function (2) with a simple sum of context word embeddings h = i ˆ x i (where x i are words in the context win- dow, e.g., of size 5). Similarly then, the equations in (5) describe the tied version of CBOW model. The linear step here provides the capacity to learn a transformation from the sum of embeddings to the predicted embeddingˆhembeddingˆ embeddingˆh. Without such trans- formation, the tying model would assume that the sum function is always a good approximation of the output embedding. The skipgram word2vec model ( <ref type="bibr">Mikolov et al., 2013)</ref>  <ref type="bibr">Press and Wolf (2017)</ref> apply di- rect tying to this architecture and report that the quality of the obtained embeddings is below the quality of non-tied skipgram embeddings. Unlike CBOW or LSTM, the input-to-hidden state func- tion of the skipgram model is identity, reducing the tying model objective tô x i = ˆ x j for every pair of input-output words i, j. It is thus not surprising that enforcing the tying constraint leads to poor empirical results. We test whether adding an addi- tional linear transformation improves performance of the tied technique also for a skipgram model.</p><note type="other">employs a variant of the LM objective: It is trained to predict context words given a word, instead of the opposite. As in CBOW and neural language models, words are both inputs and tar- gets, making the use of tying an option also for this architecture.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation data</head><p>We use two corpora for the evaluation of lan- guage models. First, we employ a medium- sized corpus of approximately 100M tokens with a relatively large vocabulary, 50K words, created from a Wikipedia dump (henceforth, Wiki). 1 To allow comparison with previous work, we also evaluate on the Penn Treebank (PTB), which is small but has been used as a benchmark for LM since <ref type="bibr">Mikolov et al. (2011</ref> lation between the cosine similarity of word pairs and human judgments. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training setup</head><p>As our base language models, we adopt the ones proposed in <ref type="bibr">Zaremba et al. (2014)</ref>. We use 2-layer LSTMs with dropout applied to the input embed- ding, to the output of the first LSTM layer and to the output of the second layer. We used the PyTorch implementation <ref type="bibr">3</ref> and modified it to in- clude the additional linear layer for our tied mod- els. We report the best model after the hyperpa- rameter search for dropout and learning rate (see the details in Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language modelling results</head><p>We present the LM results for the standard non- tied model, the tied model as in <ref type="bibr">Inan et al. (2017)</ref> and <ref type="bibr">Press and Wolf (2017)</ref>, and our tied model with an additional linear transformation (tied+L) in <ref type="table">Tables 1 (PTB)</ref> and 2 (Wiki). <ref type="table">Table 1</ref> confirms that tying generally brings gains with respect to not tying. This is also true for the cases when the hidden and embedding sizes are different (e.g. 400/200 and 600/400), where our tied+L model outperforms the non-tied model by 5 to 6.4 points having around 40% less param- eters. Furthermore, our decoupled model slightly but consistently improves results with respect to standard tying, confirming our intuition that the coupling of the hidden state to the embedding rep- resentation is a limiting constraint. Smaller tied+L models perform well compared to larger tied mod- els. In particular, the tied+L model with 600/400 units has perplexity of 76.0, compared to 76.1 of the tied 600/600 model, with 55% the number of parametres. Note that our results are comparable to previously reported perplexity values on PTB for similar models. Our best results of 75.5 test perplexity is only 1.2 points behind the large tied model with 1500 units reported in <ref type="bibr">Press and Wolf (2017)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and is only 1.6 points behind the medium tied model with 650 units and variational dropout (Gal and Ghahramani, 2016) reported in Inan et al. (2017).</head><p>On the Wiki corpus with larger vocabulary (Ta- ble 2), we find that tied models achieve slightly lower perplexity than non-tied models with half the number of parameters, and our proposed tied+L model achieves lower perplexity than the tied model. The most relevant result of the present experiment, however, is that the tied+L model with 300 embedding units is actually better than the tied model with 600 units (38.5 vs 39.7 points; the tied+L model has 20M parameters compared to 36M of the tied model) -that is, a smaller model outperforms a larger model. Thus, our decoupling mechanism not only allows models to have better perplexity, but also more compact word embeddings, which are of a higher quality also as measured on word similarity: .42/.61/.68 for the tied+L embeddings of size 300, compared to .39/.55/.64 for the tied embeddings of size 600. <ref type="table" target="#tab_3">Table 3</ref> presents the evaluation of word2vec mod- els on the three word similarity datasets. We ran the experiments only on the Wiki corpus due to its higher coverage (50K vocabulary), and used em- beddings of size 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiments on word2vec models</head><p>Our results on CBOW show that the tied+L ar- chitecture obtains comparable results to the non-   tied architecture with almost half the parameters (15.1M vs 30M). This confirms that tying with an additional linear transformation is appropriate not only for language models but for word learning models more generally.</p><p>The skipgram algorithm shows a small degra- dation of performance for the tied+L architecture with respect to the non-tied one; note that, as ex- plained in Section 2.3, tying makes the most sense for CBOW. However, the fact that standard tying obtains much worse results (similarly to the re- sults of <ref type="bibr">Press and Wolf, 2017)</ref> shows that the linear mapping substantially relaxes the tying constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Overall, our simple modification to tied language modelling architectures generalises previous work by allowing tying without imposing constraints on the number of hidden and embedding dimen- sions. This leads to flexible architectures with a more efficient use of both hidden states and em- beddings. For word representation learning mod- els, having an additional linear transformation re- duces the number of parameters while maintain- ing learning capacity. In general, reducing model size without harming performance is a desirable feature in practice, for example in the case of lan- guage models running on mobile devices, and it is also desirable on theoretical grounds, since it is a better use of the learning capacity of neural net- works. <ref type="bibr">Hakan Inan, Khashayar Khosravi, and Richard Socher. 2017</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effect of the three architectures on mapping sizes. Note that the actual difference between vocabulary size |V | and n, m is of 2 to 3 orders of magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The PTB has approx- imately 1M tokens and is preprocessed to have 10K vocabulary words; we use the standard train- validation-test split. Furthermore, to evaluate the quality of the em- beddings induced by the language models, as well as for the word representation experiments in Sec- tion 3.4, we use three standard word similarity datasets: SimLex-999 (Hill et al., 2015; SimLex), MEN (Bruni et al., 2014), and RareWords (Lu- ong et al., 2013; RW). The performance on these datasets is evaluated in terms of Spearman corre-</figDesc><table>Hid Emb Model Valid Test ∆ 
Size 

200 200 non-tied 95.0 91.1 
4.7M 
tied 
90.8 86.6 -4.5 2.7M 
tied+L 
89.8 85.8 -5.3 2.7M 

400 200 non-tied 89.4 85.3 
8.3M 
tied+L 
83.4 80.3 -5.0 4.3M 

400 non-tied 87.2 83.5 
10.6M 
tied 
82.0 78.2 -5.3 6.6M 
tied+L 
81.9 78.0 -5.5 6.7M 

600 400 non-tied 85.8 82.4 
15.3M 
tied+L 
79.0 76.0 -6.4 9.5M 

600 non-tied 84.3 81.3 
17.8M 
tied 
79.7 76.1 -5.2 11.8M 
tied+L 
78.7 75.5 -5.8 12.1M 

Inan2017 VD tied 650 77.1 73.9 
-
Zaremba2014 1500 
82.2 78.4 
66M 
P&amp;W2016 tied 1500 
77.7 74.3 
51M 

Table 1: LM perplexity results on PTB. ∆: differ-
ence in test perplexity of the tied models with re-
spect to the non-tied model with the same number 
of hidden units. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on Wiki on LM (perplexity; lower is better) and word similarity (Spearman's ρ; for 
non-tied models, results for input and output matrix are reported). 

Model 
Type 
SimLex RW MEN 

CBOW non-tied 
.38 .51 
.63 
tied+L 
.38 .50 
.65 
skipgram non-tied 
.39 .52 
.74 
tied 
.18 .25 
.50 
tied+L 
.35 .51 
.72 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results (ρ) for word2vec models. 

</table></figure>

			<note place="foot" n="1"> https://dumps.wikimedia.org/enwiki/ 20180301/</note>

			<note place="foot" n="2"> We computed correlation on the word pairs covered by the Wiki corpus, namely 98%, 88% and 31% (973, 2648 and 623 datapoints) for SimLex, MEN, and RW, respectively. 3 https://github.com/pytorch/examples/ tree/master/word_language_model</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Germán Kruszewski and the AMORE team for the helpful discussions. This project has received funding from the European Research Council (ERC) under the European Union's Hori-zon 2020 research and innovation programme (grant agreement No 715154), and from the Ramón y Cajal programme (grant RYC-2015-18907) and the Catalan government <ref type="bibr">(SGR 2017</ref><ref type="bibr">(SGR 1575</ref>. We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research. This paper reflects the au-thors' view only, and the EU is not responsible for any use that may be made of the information it contains.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating Semantic Models with Genuine Similarity Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="80" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
