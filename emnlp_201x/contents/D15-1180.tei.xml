<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Molding CNNs for text: non-linear, non-consecutive convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Molding CNNs for text: non-linear, non-consecutive convolutions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The success of deep learning often derives from well-chosen operational building blocks. In this work, we revise the temporal convolution operation in CNNs to better adapt it to text processing. Instead of concatenating word representations , we appeal to tensor algebra and use low-rank n-gram tensors to directly exploit interactions between words already at the convolution stage. Moreover, we extend the n-gram convolution to non-consecutive words to recognize patterns with intervening words. Through a combination of low-rank tensors, and pattern weighting, we can efficiently evaluate the resulting con-volution operation via dynamic programming. We test the resulting architecture on standard sentiment classification and news categorization tasks. Our model achieves state-of-the-art performance both in terms of accuracy and training speed. For instance , we obtain 51.2% accuracy on the fine-grained sentiment classification task. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning methods and convolutional neural networks (CNNs) among them have become de facto top performing techniques across a range of NLP tasks such as sentiment classification, question-answering, and semantic parsing. As methods, they require only limited domain knowl- edge to reach respectable performance with in- creasing data and computation, yet permit easy architectural and operational variations so as to fine tune them to specific applications to reach top performance. Indeed, their success is often con- tingent on specific architectural and operational choices.</p><p>CNNs for text applications make use of tem- poral convolution operators or filters. Similar to image processing, they are applied at multi- ple resolutions, interspersed with non-linearities and pooling. The convolution operation itself is a linear mapping over "n-gram vectors" obtained by concatenating consecutive word (or character) representations. We argue that this basic build- ing block can be improved in two important re- spects. First, the power of n-grams derives pre- cisely from multi-way interactions and these are clearly missed (initially) with linear operations on stacked n-gram vectors. Non-linear interactions within a local context have been shown to improve empirical performance in various tasks <ref type="bibr">(Mitchell and Lapata, 2008;</ref><ref type="bibr">Kartsaklis et al., 2012;</ref><ref type="bibr" target="#b13">Socher et al., 2013)</ref>. Second, many useful patterns are expressed as non-consecutive phrases, such as se- mantically close multi-word expressions (e.g.,"not that good", "not nearly as good"). In typical CNNs, such expressions would have to come to- gether and emerge as useful patterns after several layers of processing.</p><p>We propose to use a feature mapping operation based on tensor products instead of linear opera- tions on stacked vectors. This enables us to di- rectly tap into non-linear interactions between ad- jacent word feature vectors <ref type="bibr" target="#b13">(Socher et al., 2013;</ref><ref type="bibr">Lei et al., 2014</ref>). To offset the accompanying parametric explosion we maintain a low-rank rep- resentation of the tensor parameters. Moreover, we show that this feature mapping can be applied to all possible non-consecutive n-grams in the se- quence with an exponentially decaying weight de- pending on the length of the span. Owing to the low rank representation of the tensor, this oper- ation can be performed efficiently in linear time with respect to the sequence length via dynamic programming. Similar to traditional convolution operations, our non-linear feature mapping can be applied successively at multiple levels.</p><p>We evaluate the proposed architecture in the context of sentence sentiment classification and news categorization. On the Stanford Sentiment Treebank dataset, our model obtains state-of-the- art performance among a variety of neural net- works in terms of both accuracy and training cost. Our model achieves 51.2% accuracy on fine- grained classification and 88.6% on binary clas- sification, outperforming the best published num- bers obtained by a deep recursive model <ref type="bibr" target="#b15">(Tai et al., 2015</ref>) and a convolutional model <ref type="bibr">(Kim, 2014</ref>). On the Chinese news categorization task, our model achieves 80.0% accuracy, while the closest base- line achieves 79.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Deep neural networks have recently brought about significant advancements in various natural lan- guage processing tasks, such as language model- ing ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr">Mikolov et al., 2010)</ref>, sentiment analysis ( <ref type="bibr" target="#b13">Socher et al., 2013;</ref><ref type="bibr">Iyyer et al., 2015;</ref><ref type="bibr">Le and Zuidema, 2015)</ref>, syntactic parsing <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b11">Socher et al., 2011a;</ref><ref type="bibr" target="#b3">Chen and Manning, 2014</ref>) and ma- chine translation ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b6">Devlin et al., 2014;</ref><ref type="bibr" target="#b14">Sutskever et al., 2014</ref>). Models applied in these tasks exhibit significant archi- tectural differences, ranging from recurrent neu- ral networks <ref type="bibr">(Mikolov et al., 2010;</ref><ref type="bibr">Kalchbrenner and Blunsom, 2013</ref>) to recursive models <ref type="bibr" target="#b9">(Pollack, 1990;</ref><ref type="bibr">KÃ¼chler and Goller, 1996)</ref>, and including convolutional neural nets <ref type="bibr" target="#b4">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b5">Collobert et al., 2011;</ref><ref type="bibr" target="#b17">Yih et al., 2014;</ref><ref type="bibr">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b18">Zhang and LeCun, 2015)</ref>.</p><p>Our model most closely relates to the latter. Since these models have originally been developed for computer vision ( <ref type="bibr">LeCun et al., 1998)</ref>, their application to NLP tasks introduced a number of modifications. For instance, <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> use the max-over-time pooling operation to aggre- gate the features over the input sequence. This variant has been successfully applied to seman- tic parsing <ref type="bibr" target="#b17">(Yih et al., 2014</ref>) and information re- trieval ( ). <ref type="bibr">Kalchbrenner et al. (2014)</ref> instead propose (dynamic) k-max pooling operation for modeling sentences. In addition, <ref type="bibr">Kim (2014)</ref> combines CNNs of dif- ferent filter widths and either static or fine-tuned word vectors. In contrast to the traditional CNN models, our method considers non-consecutive n- grams thereby expanding the representation ca- pacity of the model. Moreover, our model cap- tures non-linear interactions within n-gram snip- pets through the use of tensors, moving beyond direct linear projection operator used in standard CNNs. As our experiments demonstrate these ad- vancements result in improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Let x â R LÃd be the input sequence such as a document or sentence. Here L is the length of the sequence and each x i â R d is a vector represent- ing the i th word. The (consecutive) n-gram vector ending at position j is obtained by simply concate- nating the corresponding word vectors</p><formula xml:id="formula_0">v j = [x jân+1 ; x jân+2 ; Â· Â· Â· ; x j ]</formula><p>Out-of-index words are simply set to all zeros.</p><p>The traditional convolution operator is parame- terized by filter matrix m â R ndÃh which can be thought of as n smaller filter matrices applied to each x i in vector v j . The operator maps each n- gram vector v j in the input sequence to m v j â R h so that the input sequence x is transformed into a sequence of feature representations,</p><formula xml:id="formula_1">m v 1 , Â· Â· Â· , m v L â R LÃh</formula><p>The resulting feature values are often passed through non-linearities such as the hyper-tangent (element-wise) as well as aggregated or reduced by "sum-over" or "max-pooling" operations for later (similar stages) of processing. The overall architecture can be easily modified by replacing the basic n-gram vectors and the con- volution operation with other feature mappings. Indeed, we appeal to tensor algebra to introduce a non-linear feature mapping that operates on non- consecutive n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>N-gram tensor Typical nâgram feature map- pings where concatenated word vectors are mapped linearly to feature coordinates may be in- sufficient to directly capture relevant information in the nâgram. As a remedy, we replace concate- nation with a tensor product. Consider a 3-gram (x 1 , x 2 , x 3 ) and the corresponding tensor product</p><formula xml:id="formula_2">x 1 â x 2 â x 3 .</formula><p>The tensor product is a 3-way ar- ray of coordinate interactions such that each ijk entry of the tensor is given by the product of the corresponding coordinates of the word vectors</p><formula xml:id="formula_3">(x 1 â x 2 â x 3 ) ijk = x 1i Â· x 2j Â· x 3k</formula><p>Here â denotes the tensor product operator. The tensor product of a 2-gram analogously gives a two-way array or matrix x 1 â x 2 â R dÃd . The n- gram tensor can be seen as a direct generalization of the typical concatenated vector 2 .</p><p>Tensor-based feature mapping Since each n- gram in the sequence is now expanded into a high-dimensional tensor using tensor products, the set of filters are analogously maintained as high- order tensors. In other words, our filters are linear mappings over the higher dimensional interaction terms rather than the original word coordinates.</p><p>Consider again mapping a 3-gram (x 1 , x 2 , x 3 ) into a feature representation. Each filter is a 3-way tensor with dimensions d Ã d Ã d. The set of h fil- ters, denoted as T , is a 4-way tensor of dimension </p><formula xml:id="formula_4">d Ã d Ã d Ã h,</formula><formula xml:id="formula_5">z l = ijk T ijkl Â· (x 1 â x 2 â x 3 ) ijk = ijk T ijkl Â· x 1i Â· x 2j Â· x 3k (1)</formula><p>The formula is equivalent to summing over all the third-order polynomial interaction terms where tensor T stores the coefficients. Directly maintaining the filters as full tensors leads to parametric explosion. Indeed, the size of the tensor T (i.e. h Ã d n ) would be too large even for typical low-dimensional word vectors where, e.g., d = 300. To this end, we assume a low-rank factorization of the tensor T, represented in the Kruskal form. Specifically, T is decomposed into a sum of h rank-1 tensors</p><formula xml:id="formula_6">T = h i=1 P i â Q i â R i â O i</formula><p>2 To see this, consider word vectors with a "bias" term xi = [xi; 1]. The tensor product of n such vectors includes the concatenated vector as a subset of tensor entries but, in addition, contains all up to n th -order interaction terms.</p><p>where P, Q, R â R hÃd and O â R hÃh are four smaller parameter matrices. P i (similarly Q i , R i and O i ) denotes the i th row of the matrix. Note that, for simplicity, we have assumed that the num- ber of rank-1 components in the decomposition is equal to the feature dimension h. Plugging the low-rank factorization into Eq.(1), the feature- mapping can be rewritten in a vector form as</p><formula xml:id="formula_7">z = O (Px 1 Qx 2 Rx 3 ) (2)</formula><p>where is the element-wise product such that,</p><formula xml:id="formula_8">e.g., (a b) k = a k Ã b k for a, b â R m .</formula><p>Note that while Px 1 (similarly Qx 2 and Rx 3 ) is a lin- ear mapping from each word x 1 (similarly x 2 and x 3 ) into a h-dimensional feature space, higher or- der terms arise from the element-wise products.</p><p>Non-consecutive n-gram features Traditional convolution uses consecutive n-grams in the fea- ture map. Non-consecutive n-grams may nev- ertheless be helpful since phrases such as "not good", "not so good" and "not nearly as good" ex- press similar sentiments but involve variable spac- ings between the key words. Variable spacings are not effectively captured by fixed n-grams. We apply the feature-mapping in a weighted manner to all n-grams thereby gaining access to patterns such as "not ... good". Let z[i, j, k] â R h denote the feature representation corresponding to a 3-gram (x i , x j , x k ) of words in positions i, j, and k along the sequence. This vector is calcu- lated analogously to Eq.(2),</p><formula xml:id="formula_9">z[i, j, k] = O (Px i Qx j Rx k )</formula><p>We will aggregate these vectors into an hâdimensional feature representation at each position in the sequence. The idea is similar to neural bag-of-words models where the feature representation for a document or sentence is obtained by averaging (or summing) of all the word vectors. In our case, we define the aggregate representation z 3 <ref type="bibr">[k]</ref> in position k as the weighted sum of all 3-gram feature representations ending at position k, i.e.,</p><formula xml:id="formula_10">z 3 [k] = i&lt;j&lt;k z[i, j, k] Â· Î» (kâjâ1)+(jâiâ1) = i&lt;j&lt;k z[i, j, k] Â· Î» kâiâ2 (3)</formula><p>where Î» â [0, 1) is a decay factor that down- weights 3-grams with longer spans (i.e., 3-grams that skip more in-between words). As Î» â 0 all non-consecutive 3-grams are omitted, <ref type="bibr">k]</ref>, and the model acts like a traditional model with only consecutive n-grams.</p><formula xml:id="formula_11">z 3 [k] = z[k â 2, k â 1,</formula><formula xml:id="formula_12">When Î» &gt; 0, however, z 3 [k]</formula><p>is a weighted aver- age of many 3-grams with variable spans. </p><formula xml:id="formula_13">z 3 [k] = O f 3 [k] or, equivalently, f 3 [k] = i&lt;j&lt;k Î» kâiâ2 Â· (Px i Qx j Rx k )</formula><p>We can analogously define f 1 [i] and f 2 [j] for 1- grams and 2-grams,</p><formula xml:id="formula_14">f 1 [i] = Px i f 2 [j] = i&lt;j Î» jâiâ1 Â· (Px i Qx j )</formula><p>These dynamic programming tables can be calcu- lated recursively according to the following for- mulas:</p><formula xml:id="formula_15">f 1 [i] = Px i s 1 [i] = Î» Â· s 1 [i â 1] + f 1 [i] f 2 [j] = s 1 [j â 1] Qx j s 2 [j] = Î» Â· s 2 [j â 1] + f 2 [j] f 3 [k] = s 2 [k â 1] Rx k z[k] = O (f 1 [k] + f 2 [k] + f 3 [k])</formula><p>where s 1 [Â·] and s 2 [Â·] are two auxiliary tables. The resulting z <ref type="bibr">[Â·]</ref> is the sum of 1, 2, and 3-gram fea- tures. We found that aggregating the 1,2 and 3- gram features in this manner works better than us- ing 3-gram features alone. Overall, the n-gram feature aggregation can be performed in O(Ln) matrix multiplication/addition operations, and re- mains linear in the sequence length.</p><p>The overall architecture The dynamic pro- gramming algorithm described above maps the original input sequence to a sequence of feature representations z = z[1 : L] â R LÃh . As in standard convolutional architectures, the resulting sequence can be used in multiple ways. One can directly aggregate it to a classifier or expose it to non-linear element-wise transformations and use it as an input to another sequence-to-sequence fea- ture mapping.</p><p>The simplest strategy (adopted in neural bag- of-words models) would be to average the fea- ture representations and pass the resulting aver- aged vector directly to a softmax output unit</p><formula xml:id="formula_16">Â¯ z = 1 L L i=1 z[i] Ë y = softmax W Â¯ z</formula><p>Our architecture, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, in- cludes two additional refinements. First, we add a non-linear activation function after each feature representation, i.e. z = ReLU (z + b), where b is a bias vector and ReLU is the rectified linear unit function. Second, we stack multiple tensor- based feature mapping layers. That is, the input sequence x is first processed into a feature se- quence and passed through the non-linear trans- formation to obtain z (1) . The resulting feature sequence z (1) is then analogously processed by another layer, parameterized by a different set of feature-mapping matrices P, Â· Â· Â· , O, to obtain a higher-level feature sequence z <ref type="bibr">(2)</ref> , and so on. The output feature representations of all these layers are averaged within each layer and concatenated as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. The final prediction is there- fore obtained on the basis of features across the levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning the Model</head><p>Following standard practices, we train our model by minimizing the cross-entropy error on a given training set. For a single training sequence x and the corresponding gold label y â [0, 1] m , the error is defined as,</p><formula xml:id="formula_17">loss (x, y) = m l=1 y l log (Ë y l )</formula><p>where m is the number of possible output label. The set of model parameters (e.g. P, Â· Â· Â· , O in each layer) are updated via stochastic gradient  Initialization We initialize matrices P, Q, R from uniform distribution</p><formula xml:id="formula_18">â 3/d, 3/d and similarly O â¼ U â 3/h, 3/h</formula><p>. In this way, each row of the matrices is an unit vector in expec- tation, and each rank-1 filter slice has unit variance as well,</p><formula xml:id="formula_19">E P i â Q i â R i â O i 2 = 1</formula><p>In addition, the parameter matrix W in the soft- max output layer is initialized as zeros, and the bias vectors b for ReLU activation units are ini- tialized to a small positive constant 0.01.</p><p>Regularization We apply two common tech- niques to avoid overfitting during training. First, we add L2 regularization to all parameter values with the same regularization weight. In addition, we randomly dropout ( <ref type="bibr">Hinton et al., 2012</ref>) units on the output feature representations z (i) at each level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>Datasets We evaluate our model on sentence sentiment classification task and news categoriza- tion task. For sentiment classification, we use the Stanford Sentiment Treebank benchmark <ref type="bibr" target="#b13">(Socher et al., 2013</ref>). The dataset consists of 11855 parsed English sentences annotated at both the root (i.e. sentence) level and the phrase level us- ing 5-class fine-grained labels. We use the stan- dard 8544/1101/2210 split for training, develop- ment and testing respectively. Following previ- ous work, we also evaluate our model on the bi- nary classification variant of this benchmark, ig- noring all neutral sentences. The binary version has 6920/872/1821 sentences for training, devel- opment and testing. For the news categorization task, we evaluate on Sogou Chinese news corpora. <ref type="bibr">3</ref> The dataset con- tains 10 different news categories in total, includ- ing Finance, Sports, Technology and Automobile etc. We use 79520 documents for training, 9940 for development and 9940 for testing. To obtain Chinese word boundaries, we use LTP-Cloud <ref type="bibr">4</ref> , an open-source Chinese NLP platform.</p><p>Baselines We implement the standard SVM method and the neural bag-of-words model NBoW as baseline methods in both tasks. To as- sess the proposed tensor-based feature map, we also implement a convolutional neural network model CNN by replacing our filter with traditional linear filter. The rest of the framework (such as feature averaging and concatenation) remains the same.</p><p>In addition, we compare our model with a wide range of top-performing models on the sentence sentiment classification task. Most of these mod- els fall into either the category of recursive neural networks (RNNs) or the category of convolutional neural networks (CNNs). The recursive neural   <ref type="bibr">and Cardie, 2014</ref>) and the most recent recursive model using long-short-term-memory units RLSTM ( <ref type="bibr" target="#b15">Tai et al., 2015)</ref>. These recursive models assume the in- put sentences are represented as parse trees. As a benefit, they can readily utilize annotations at the phrase level. In contrast, convolutional neu- ral networks are trained on sequence-level, taking the original sequence and its label as training in- put. Such convolutional baselines include the dy- namic CNN with k-max pooling DCNN ( <ref type="bibr">Kalchbrenner et al., 2014</ref>) and the convolutional model with multi-channel CNN-MC by <ref type="bibr">Kim (2014)</ref>. To leverage the phrase-level annotations in the Stan- ford Sentiment Treebank, all phrases and the cor- responding labels are added as separate instances when training the sequence models. We follow this strategy and report results with and without phrase annotations.</p><p>Word vectors The word vectors are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data . In particu- lar, for the English sentiment classification task, we use the publicly available 300-dimensional GloVe word vectors trained on the Common Crawl with 840B tokens ( <ref type="bibr">Pennington et al., 2014</ref>). This choice of word vectors follows most recent work, such as DAN ( <ref type="bibr">Iyyer et al., 2015) and</ref><ref type="bibr">RLSTM (Tai et al., 2015</ref>). For Chinese news categorization, there is no widely-used publicly available word vectors. Therefore, we run word2vec ( <ref type="bibr">Mikolov et al., 2013</ref>) to train 200-dimensional word vec- tors on the 1.6 million Chinese news articles. Both word vectors are normalized to unit norm (i.e. w 2 2 = 1) and are fixed in the experiments with- out fine-tuning.</p><p>Hyperparameter setting We perform an exten- sive search on the hyperparameters of our full model, our implementation of the CNN model (with linear filters), and the SVM baseline. For our model and the CNN model, the initial learn- ing rate of AdaGrad is fixed to 0.01 for sentiment classification and 0.1 for news categorization, and the L2 regularization weight is fixed to 1e â 5 and 1e â 6 respectively based on preliminary runs. The rest of the hyperparameters are randomly cho- sen as follows: number of feature-mapping lay- ers â {1, 2, 3}, n-gram order n â {2, 3}, hidden feature dimension h â {50, 100, 200}, dropout probability â {0.0, 0.1, 0.3, 0.5}, and length de-cay Î» â {0.0, 0.3, 0.5}. We run each config- uration 3 times to explore different random ini- tializations. For the SVM baseline, we tune L2 regularization weight C â {0.01, 0.1, 1.0, 10.0}, word cut-off frequency â {1, 2, 3, 5} (i.e. pruning words appearing less than this times) and n-gram feature order n â {1, 2, 3}.</p><p>Implementation details The source code is implemented in Python using the Theano li- brary ( <ref type="bibr" target="#b2">Bergstra et al., 2010</ref>), a flexible lin- ear algebra compiler that can optimize user- specified computations (models) with efficient automatic low-level implementations, including (back-propagated) gradient calculation. <ref type="table">Table 1</ref> presents the performance of our model and other baseline methods on Stanford Sentiment Treebank benchmark. Our full model obtains the highest accuracy on both the development and test sets. Specifically, it achieves 51.2% and 88.6% test accuracies on fine-grained and binary tasks re- spectively 5 . As shown in <ref type="table" target="#tab_3">Table 2</ref>, our model per- formance is relatively stable -it remains high ac- curacies with around 0.5% standard deviation un- der different initializations and dropout rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Overall Performance</head><p>Our full model is also several times faster than other top-performing models. For example, the convolutional model with multi-channel (CNN- MC) runs over 2400 seconds per training epoch. In contrast, our full model (with 3 feature layers) runs on average 28 seconds with only root labels and on average 445 seconds with all labels.</p><p>Our results also show that the CNN model, where our feature map is replaced with traditional linear map, performs worse than our full model. This observation confirms the importance of the proposed non-linear, tensor-based feature map- ping. The CNN model also lags behind the DCNN and CNN-MC baselines, since the latter two pro- pose several advancements over standard CNN. <ref type="table" target="#tab_4">Table 3</ref> reports the results of SVM, NBoW and our model on the news categorization task. Since the dataset is much larger compared to the senti- ment dataset (80K documents vs. 8.5K sentences), the SVM method is a competitive baseline. It achieves 78.5% accuracy compared to 74.4% and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Accuracy</head><p>Fine-grained Dev 52.5 (Â±0.5) % Test 51.4 (Â±0.6) % Binary Dev 88.4 (Â±0.3) % Test 88.4 (Â±0.5) %  79.2% obtained by the neural bag-of-words model and CNN model. In contrast, our model obtains 80.0% accuracy on both the development and test sets, outperforming the three baselines by a 0.8% absolute margin. The best hyperparameter con- figuration in this task uses less feature layers and lower n-gram order (specifically, 2 layers and n = 2) compared to the sentiment classification task. We hypothesize that the difference is due to the nature of the two tasks: the document classifica- tion task requires to handle less compositions or context interactions than sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Hyperparameter Analysis</head><p>We next investigate the impact of hyperparame- ters in our model performance. We use the mod- els trained on fine-grained sentiment classification task with only root labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of layers</head><p>We plot the fine-grained sen- timent classification accuracies obtained during hyperparameter grid search. <ref type="figure" target="#fig_13">Figure 2</ref> illustrates how the number of feature layers impacts the model performance. As shown in the figure, adding higher-level features clearly improves the classification accuracy across various hyperpa- rameter settings and initializations.</p><p>Non-consecutive n-gram features We also an- alyze the effect of modeling non-consecutive n-            <ref type="formula">(5)</ref> are synthetic inputs, (6) and <ref type="formula">(7)</ref> are two real inputs from the test set. Our model successfully identifies negation, double negation and phrases with different sentiment in one sentence.  grams. <ref type="figure" target="#fig_15">Figure 3</ref> splits the model accuracies ac- cording to the choice of span decaying factor Î». Note when Î» = 0, the model applies feature ex- tractions to consecutive n-grams only. As shown in <ref type="figure" target="#fig_15">Figure 3</ref>, this setting leads to consistent perfor- mance drop. This result confirms the importance of handling non-consecutive n-gram patterns.</p><note type="other">-1 0 1 the movie i -2 -1 0 1 the movie is bad -2 -1 0 1 the movie is good -2 -1 0 1 okay but not good</note><note type="other">-1 0 1 the movie is -2 -1 0 1 the movie is bad -2 -1 0 1 the movie is good -2 -1 0 1 okay but not</note><p>Non-linear activation Finally, we verify the ef- fectiveness of rectified linear unit activation func-     <ref type="figure">Figure 4</ref>: Applying ReLU activation on top of tensor-based feature mapping leads to better per- formance in sentiment classification task. Runs with no activation are plotted as blue circles. <ref type="figure" target="#fig_12">Figure 5</ref> gives examples of input sentences and the corresponding predictions of our model in fine-grained sentiment classification. To see how our model captures the sentiment at different lo- cal context, we apply the learned softmax ac- tivation to the extracted features at each posi- tion without taking the average. That is, for each index i, we obtain the local sentiment p = softmax</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Example Predictions</head><formula xml:id="formula_20">W z (1) [i] â z (2) [i] â z (3) [i]</formula><p>. We plot the expected sentiment scores 2 s=â2 s Â· p(s), where a score of 2 means "very positive", 0 means "neutral" and -2 means "very negative". As shown in the figure, our model successfully learns nega- tion and double negation. The model also iden- tifies positive and negative segments appearing in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed a feature mapping operator for con- volutional neural networks by modeling n-gram interactions based on tensor product and evaluat- ing all non-consecutive n-gram vectors. The as- sociated parameters are maintained as a low-rank tensor, which leads to efficient feature extraction via dynamic programming. The model achieves top performance on standard sentiment classifica- tion and document categorization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</head><p>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing co- adaptation of feature detectors. arXiv preprint arXiv:1207.0580. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where each d 3 slice of T repre- sents a single filter and h is the number of such filters, i.e., the feature dimension. The resulting hâdimensional feature representation z â R h for the 3-gram (x 1 , x 2 , x 3 ) is obtained by multiplying the filter T and the 3-gram tensor as follows. The l th coordinate of z is given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the model architecture. The input is represented as a matrix where each row is a d-dimensional word vector. Several feature map layers (as described in Section 4) are stacked, mapping the input into different levels of feature representations. The features are averaged within each layer and then concatenated. Finally a softmax layer is applied to obtain the prediction output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example sentences and their sentiments predicted by our model trained with root labels. The predicted sentiment scores at each word position are plotted. Examples (1)-(5) are synthetic inputs, (6) and (7) are two real inputs from the test set. Our model successfully identifies negation, double negation and phrases with different sentiment in one sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dev accuracy (x-axis) and test accuracy (y-axis) of independent runs of our model on finegrained sentiment classification task. Deeper architectures achieve better accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of our model variations in sentiment classification task when considering consecutive n-grams only (decaying factor Î» = 0) and when considering non-consecutive n-grams (Î» &gt; 0). Modeling non-consecutive n-gram features leads to better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Analysis of average accuracy and stan-
dard deviation of our model on sentiment classifi-
cation task. 

Model 
Dev Acc. Test Acc. 
SVM (1-gram) 
77.5 
77.4 
SVM (2-gram) 
78.2 
78.0 
SVM (3-gram) 
78.2 
78.5 
NBoW 
74.4 
74.4 
CNN 
79.5 
79.2 
Ours 
80.0 
80.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of various methods on Chi-
nese news categorization task. Our model obtains 
better results than the SVM, NBoW and traditional 
CNN baselines. 

</table></figure>

			<note place="foot" n="1"> Our code and data are available at https://github. com/taolei87/text_convnet</note>

			<note place="foot" n="3"> http://www.sogou.com/labs/dl/c.html 4 http://www.ltp-cloud.com/intro/en/ https://github.com/HIT-SCIR/ltp</note>

			<note place="foot" n="5"> Best hyperparameter configuration based on dev accuracy: 3 layers, 3-gram tensors (n=3), feature dimension d = 200 and length decay Î» = 0.5</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Kai Sheng Tai, Mohit Iyyer and Jordan Boyd-Graber for answering questions about their paper. We also thank Yoon Kim, the MIT NLP group and the reviewers for their comments. We acknowledge the support of the U.S. Army Re-search Office under grant number W911NF-10-1-0533. The work is developed in collaboration with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI). Any opinions, findings, conclusions, or recommenda-tions expressed in this paper are those of the au-thors, and do not necessarily reflect the views of the funding organizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FrÃ©dÃ©ric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<editor>Jeffrey Pennington, Richard Socher, and Christopher D Manning</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Glove: Global vectors for word representation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GrÃ©goire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the companion publication of the 23rd international conference on World wide web companion</title>
		<meeting>the companion publication of the 23rd international conference on World wide web companion</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
