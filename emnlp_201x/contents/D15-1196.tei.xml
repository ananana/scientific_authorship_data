<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Learning of Interpretable Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Learning of Interpretable Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings encode semantic meanings of words into low-dimension word vectors. In most word embeddings, one cannot interpret the meanings of specific dimensions of those word vectors. Non-negative matrix factorization (NMF) has been proposed to learn interpretable word embeddings via non-negative constraints. However, NMF methods suffer from scale and memory issue because they have to maintain a global matrix for learning. To alleviate this challenge, we propose on-line learning of interpretable word embed-dings from streaming text data. Experiments show that our model consistently outperforms the state-of-the-art word embedding methods in both representation ability and interpretability. The source code of this paper can be obtained from http: //github.com/skTim/OIWE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings ( <ref type="bibr" target="#b18">Turian et al., 2010</ref>) aim to encode semantic meanings of words into low- dimensional dense vectors. As compared with tra- ditional one-hot representation and distributional representation, word embeddings can better ad- dress the sparsity issue and have achieved success in many NLP applications recent years.</p><p>There are two typical approaches for word em- beddings. The neural-network (NN) approach ( <ref type="bibr" target="#b1">Bengio et al., 2006</ref>) employs neural-based tech- niques to learn word embeddings. The matrix fac- torization (MF) approach <ref type="bibr" target="#b15">(Pennington et al., 2014)</ref> builds word embeddings by factorizing word- context co-occurrence matrices. The MF approach requires a global statistical matrix, while the N- N approach can flexibly perform learning from * Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) streaming text data, which is efficient in both com- putation and memory. For example, two recen- t NN methods, Skip-Gram and Continuous Bag- of-Word Model (CBOW) ( <ref type="bibr" target="#b12">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>), have achieved impressive impact due to their simplicity and efficiency.</p><p>For most word embedding methods, a critical issue is that, we are unaware of what each dimen- sion represent in word embeddings. Hence, the latent dimension for which a word has its largest value is difficult to interpret. This makes word em- beddings like a black-box, and prevents them from being human-readable and further manipulation.</p><p>People have proposed non-negative matrix fac- torization (NMF) for word representation, denoted as non-negative sparse embedding (NNSE) <ref type="bibr" target="#b14">(Murphy et al., 2012)</ref>. NNSE realizes interpretable word embeddings by applying non-negative con- straints for word embeddings. Although NNSE learns word embeddings with good interpret- abilities, like other MF methods, it also requires a global matrix for learning, thus suffers from heavy memory usage and cannot well deal with stream- ing text data.</p><p>Inspired by the characteristics of NMF meth- ods ( <ref type="bibr" target="#b8">Lee and Seung, 1999</ref>), we note that, non- negative constraints only allow additive combi- nations instead of subtractive combinations, and lead to a parts-based representation. Hence, the non-negative constraints derive interpretabilities of word embeddings. In this paper, we aim to de- sign an online NN method to efficiently learn in- terpretable word embeddings. In order to achieve the goal of interpretable embeddings, we design projected gradient descent <ref type="bibr" target="#b10">(Lin, 2007)</ref> for opti- mization so as to apply non-negative constraints on NN methods such as Skip-Gram. We also em- ploy adaptive gradient descent ( <ref type="bibr" target="#b17">Sun et al., 2012)</ref> to speedup learning convergence. We name the proposed models as online interpretable word em- beddings (OIWE).</p><p>For experiments, we implement OIWE based on Skip-Gram. We evaluate the representation performance of word embedding methods on the word similarity computation task. Experiment re- sults show that, our OIWE models are signifi- cantly superior to other baselines including Skip- Gram, RNN and NNSE. We also evaluate the in- terpretability performance on the word intrusion detection task. The results demonstrate the effec- tiveness of OIWE as compared to NNSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Model</head><p>In this section, we first introduce Skip-Gram and then introduce the proposed online interpretable word embeddings based on Skip-Gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skip-Gram</head><p>Skip-Gram ( <ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>) is simple and effective to learn word embeddings. The objec- tive of Skip-Gram is to make word vectors good at predicting its context words. More specifically, given a word sequence {w 1 , w 2 , . . . , w T }, Skip- Gram aims to maximize the average log probabil- ity</p><formula xml:id="formula_0">1 T T 1 −k≤j≤k,j =0 log Pr(w t+j |w t ) ,<label>(1)</label></formula><p>where k is the context window size, and Pr(w t+j |w t ) indicates the probability of seeing w t+j in the context of w t , which are measured with softmax function</p><formula xml:id="formula_1">Pr(w t+j |w t ) = exp w t+j · w t w∈W exp w · w t ,<label>(2)</label></formula><p>where w t+j and w t are word embeddings of w t+j and w t , and W is the vocabulary size. Since the computation of full softmax is time consuming, the techniques of hierarchical softmax and nega- tive sampling (Mikolov et al., 2013b) are proposed for approximation. Take negative sampling for example. The log probability Pr(w t+j |w t ) can be approximate by</p><formula xml:id="formula_2">log σ w t+j · w t + w∈Nt log σ w · w t ,<label>(3)</label></formula><p>where σ(x) = 1/(1 + exp(−x)), and N t is the set of negative samples as compared to the cor- responding context word w t+j . The task can be regarded as to distinguish the context word w t+j from negative samples.</p><p>For Skip-Gram with negative sampling, we can perform stochastic gradient descent for learning. The update rule for the positive/negative context words u ∈ {w t+j } ∪ N t is</p><formula xml:id="formula_3">u i+1 = u i + γ I wt (u) − σ(u · w t ) w i t , (4)</formula><p>where I wt (u) = 1 when w is the positive contex- t word of w t and I wt (u) = 0 when w is nega- tive, i is the iteration number, and γ is the learning rate. Correspondingly, the update rule for the in- put word w t is</p><formula xml:id="formula_4">w i+1 t = w i t + γ u∈{wt}∪Nt I wt (u) − σ(u · w t ) u i t .</formula><p>(5) We note that, the learning rate γ in Skip-Gram is shared by all word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">OIWE</head><p>In order to learn interpretable word embeddings, we have to make the word embeddings learned in Skip-Gram keep non-negative. In order to achieve this goal, we have to constrain the update rules in Equation <ref type="formula">(4)</ref> and <ref type="formula">(5)</ref> as follows:</p><formula xml:id="formula_5">x i+1 k = P x i k + γf (x k ) ,<label>(6)</label></formula><p>where x may be u or w t , k is the corresponding di- mension in word embedding x, f (x k ) indicates the gradient corresponding to x k , and P [·] is de- fined as</p><formula xml:id="formula_6">P [x] = x if x &gt; 0, 0 if x ≤ 0.<label>(7)</label></formula><p>Motivated by the projected gradient descent meth- ods for NMF <ref type="bibr" target="#b10">(Lin, 2007)</ref>, in this paper we pro- pose two methods for Skip-Gram to realize the constraint in Equation (6). Naive Projected Gradient (NPG). In NPG, we consider the most straightforward update strategy by simply setting</p><formula xml:id="formula_7">x i+1 k = max 0, x i k + γf (x k ) .<label>(8)</label></formula><p>The method has been used for NMF <ref type="bibr" target="#b10">(Lin, 2007)</ref> although the details are not discussed. The NPG method only constrains the violated dimensions without taking the update consisten- cy among dimensions of a word embedding into account. For example, if many dimensions en- counter x i k + γf (x k ) &lt; 0 at the same time, which are set to 0 with Equation (8) with other dimensions unchanged, the updated word embed- ding may heavily deviate from its semantic mean- ing. Hence, NPG may suffer from instable updat- ing results. To address this issue, we propose to employ the following improved projected gradient method.</p><p>Improved Projected Gradient (IPG). In order to make the non-negative update more consistent among dimensions, we design an improved pro- jected gradient by iteratively finding the most ap- propriate learning rate γ. The basic idea is that, we will find a good learning rate γ to make less dimensions violate the non-negative constraint.</p><p>More specifically, in Equation <ref type="formula" target="#formula_5">(6)</ref>, for a learning rate γ, we define the violation ratio as</p><formula xml:id="formula_8">R(γ) = {k|x i k &gt; 0, x i k + γf (x k ) &lt; 0} K ,<label>(9)</label></formula><p>where K is the dimension size of word embed- dings. The violation ratio indicates how many di- mensions violate the non-negative constraint and require to be set to 0. When the learning rate γ de- creases, the violation ratio will also decrease, and the zero-setting in Equation (8) will bring less de- viation to word embeddings. We set a threshold δ for the violation ratio R(γ) and a lower bound γ L for the learning rate γ. S- tarting from an initial learning rate γ 0 , we will re- peatedly decrease the learning rate by</p><formula xml:id="formula_9">γ m+1 = γ m · β<label>(10)</label></formula><p>with 0 &lt; β &lt; 1 until</p><formula xml:id="formula_10">R(γ m+1 ) &lt; δ or γ m+1 ≤ γ L ,<label>(11)</label></formula><p>and then update with Equation (8) using γ m+1 . In nature, the updating constraint of learning rate in Equation (11) play a similar role to Equation <ref type="bibr">(13)</ref> in <ref type="bibr" target="#b10">(Lin, 2007)</ref>, which aims to prevent the pro- jection operation from heavily deviating the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">More Optimization Details</head><p>In experiments, we explore many optimization methods and find the following two strategies are important: (1) Adaptive Gradient Descen- t. Following the idea from (Sun et al., 2012), we maintain different learning rates γ w for each word w, and the learning rates for those high- frequency words may decrease faster than those low-frequency words. This will speedup the con- vergence of word embedding learning. (2) Unified Word Embedding Space. Different from original Skip-Gram ( <ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>) which learn embeddings of w t and its context words w t+j in two separate spaces, in this paper both w t and it- s context words w t+j share the same embedding space. Hence, a word embedding may get more opportunities for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we investigate the representation performance and interpretability of our OIWE models with other baselines including typical N- N and MF methods. The representation performance is evaluated with the word similarity computation task, and the interpretability is evaluated with the word intru- sion detection task. For the both tasks, we train our OIWE models using the text8 corpus obtained from word2vec website 1 , and the OIWE models achieve the best performance by setting the dimen- sion number K = 300, β = 0.6, δ = 1/60, and γ L = 2.5 × 10 −6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Similarity Computation</head><p>Following the settings in ( <ref type="bibr" target="#b14">Murphy et al., 2012)</ref>, we also select the following three sets for word similarity computation: (1) WS-203, the strict- similarity subset of 203 pairs ( <ref type="bibr" target="#b0">Agirre et al., 2009</ref>) selected from the wordsim-353 ( <ref type="bibr" target="#b6">Finkelstein et al., 2001</ref>), (2) RG-65, 65 concrete word pairs built by <ref type="bibr" target="#b16">(Rubenstein and Goodenough, 1965)</ref> and (3) MEN, 3, 000 word pairs built by <ref type="bibr" target="#b3">(Bruni et al., 2014</ref>). The performance is evaluated with the S- pearman coefficient between human judgements and similarities calculated using word embed- dings.</p><p>We select three baselines including Skip-Gram ( <ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>), recurrent neural networks (RNN) <ref type="bibr" target="#b11">(Mikolov et al., 2011</ref>) and NNSE <ref type="bibr" target="#b14">(Murphy et al., 2012</ref>). For Skip-Gram, we report the result we learned using word2vec on text8 cor- pus. The result of RNN is from <ref type="bibr" target="#b5">(Faruqui and Dyer, 2014</ref>) and the one of NNSE is from ( <ref type="bibr" target="#b14">Murphy et al., 2012)</ref>.</p><p>The evaluation results of word similarity com- putation are shown in <ref type="table">Table 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Intrusion Detection</head><p>We evaluate interpretability of word embeddings with the task of word intrusion detection proposed by <ref type="bibr" target="#b14">(Murphy et al., 2012)</ref>. In this task, for each dimension we create a word set containing top-5 words in this dimension, and intruce a noisy word from the bottom half of this dimension which ranks high in other dimensions. Human editors are asked to check each word set and try to pick out the intrusion words, and the detection preci- sion indicates the interpretability of word embed- ding models. Note that, for this task we do not perform normalization for word vectors.  The evaluation results are shown in <ref type="table" target="#tab_2">Table 2</ref>. We can observe that: (1) Skip-Gram performs poor in word intrusion detection without doubt since it is uninterpretable in nature. <ref type="formula" target="#formula_1">(2)</ref> The OIWE-NPG model achieves better interpretability as compared to Skip-Gram, but performs much worse than the OIWE-IPG model. The OIWE-IPG model achieves competitive interpretability with NNSE. This indicates that reducing violation rations in word embedding learning is crucial for preserving interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>In <ref type="table">Table 3</ref>, we show top-5 words for some dimensions, which clearly demonstrate semantic meanings of these dimensions. One can also refer to http://github.com/skTim/OIWE to find top-5 words for all dimensions.</p><p>No.</p><p>Top Words 1 type, form, way, kind, manner 2 translates, describes, combines, includ- ed, includes 3 gospel, baptism, jesus, faith, judaism 4</p><p>Franz, Johann, Wilhelm, Friedrich, von 25 prominent, famous, important, influen- tial, popular <ref type="table">Table 3</ref>: Top words of some dimensions in word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Influence of Dimension Numbers</head><p>The dimension number is an important configura- tion in word embeddings. In <ref type="figure" target="#fig_0">Fig. 1</ref> we show the performance of OIWE and Skip-Gram on word similarity computation with varying dimension numbers.</p><p>From the figure, we can observe that: (1) The both models achieve their best performance un- der the same dimension number. This indicates that OIWE, to some extent, inherits the represen- tation power of Skip-Gram. (2) The performance of OIWE seems to be more sensitive to dimension numbers. When the dimension number changes from 300 to 200 or 400, the performance drops much quickly than Skip-Gram. The reason may be as follows. OIWE has to concern about both representation ability of word embeddings and in- terpretability of each dimension. An appropri- ate dimension number is critical to make each di- mension interpretable, just like the cluster num- ber is important for clustering. On the contrary, Skip-Gram is much free to learn word embeddings only concerning about representation ability. <ref type="formula" target="#formula_2">(3)</ref> The performance of OIWE with various dimen- sions also varies on different evaluation dataset- s. For example, OIWE-IPG with K = 400 get- s 68.74 on MEN, which is much better than that with K = 300. In future work, we will exten- sively investigate the characteristics of OIWE with respect to dimension numbers and other hyperpa- rameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we present online interpretable word embeddings. The OIWE models perform project- ed gradient descent to apply non-negative con- straints on NN methods such as Skip-Gram. Ex- periment results on word similarity computation and word intrusion detection demonstrate the ef- fectiveness and efficiency of our models in both representation ability and interpretability. We al- so note that, our models can be easily extended to other NN methods.</p><p>In future, we will explore the following re- search issues: (1) We will extensively investigate the characteristics of OIWE with respect to var- ious hyperparameters including dimension num- bers. (2) We will evaluate the performance of our OIWE models in various NLP applications. (3) We will also investigate possible extensions of our OIWE models, including multiple-prototype models for word sense embeddings ( <ref type="bibr" target="#b7">Huang et al., 2012;</ref><ref type="bibr" target="#b4">Chen et al., 2014</ref>), semantic composition- s for phrase embeddings ( <ref type="bibr" target="#b19">Zhao et al., 2015)</ref> and knowledge representation ( <ref type="bibr" target="#b2">Bordes et al., 2013;</ref><ref type="bibr" target="#b9">Lin et al., 2015</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Influence of Dimension Number on Words Similarity</figDesc><graphic url="image-1.png" coords="5,72.00,62.81,218.27,159.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>We can ob-
serve that: (1) The OIWE models consistently 
outperform other baselines. (2) IPG generally 
achieves better representation performance than Model 

WS-203 RG-65 MEN 
Skip-Gram 
67.35 
50.49 52.56 
RNN 
49.28 
50.19 43.44 
NNSE 
51.06 
56.48 
-
OIWE-NPG 
63.71 
56.85 57.60 
OIWE-IPG 
71.74 
57.16 56.68 

Table 1: Spearman coefficient results (%) on word 
similarity computation. 

NPG. This indicates consistent updates are im-
portant for learning of word embeddings. One 
can refer to http://github.com/skTim/ 
OIWE for the evaluation results on more evalua-
tion datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experiment results (%) on word intrusion 
detection. 

</table></figure>

			<note place="foot" n="1"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Marius Pasca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sebastien</forename><surname>Senecal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimodal distributional semantics. JAIR</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Community evaluation and exchange of word vectors at wordvectors.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL System Demonstrations</title>
		<meeting>ACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Sebastian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Projected gradient methods for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Bi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-01" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning effective and interpretable semantic models using non-negative sparse embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1933" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of EMNLP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phrase type sensitive tensor indexing model for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
