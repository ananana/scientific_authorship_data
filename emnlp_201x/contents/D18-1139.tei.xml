<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
							<email>martin@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Steinheber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MaibornWolff GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schreiber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MaibornWolff GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution">LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1109" to="1114"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we propose a new model for aspect-based sentiment analysis. In contrast to previous approaches, we jointly model the detection of aspects and the classification of their polarity in an end-to-end trainable neural network. We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. We were able to show considerable performance gains by using the joint modeling approach in all settings compared to pipeline approaches. The combination of a convolutional neural network and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis <ref type="bibr" target="#b9">(Pang and Lee, 2008</ref>) is the au- tomatic detection of the sentiment expressed in a piece of text. Typically, this is modeled as a clas- sification task with at least two classes (positive, negative), sometimes extended to three (neutral) or more fine-grained categories. Aspect-based senti- ment analysis (ABSA) aims at a finer analysis, i.e. it requires that certain aspects of an entity in ques- tion be distinguished and the sentiment be classi- fied with regard to each of them. An example can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>This introduces several new challenges. First, labeled data, which are needed to train statistical models, are more difficult to obtain. Therefore the amount of available training data is limited. Thus a good model for ABSA has to make the best possible use of the available data. Second, the detection of the subset of aspects that occur in a given piece of text is non-trivial. Errors intro- duced at this stage severely limit the performance on the overall ABSA task. Third, the general sen- timent and the sentiment of each aspect can each German: Alle so "Yeah, Streik beendet" Bahn so "Okay, dafür werden dann natürlich die Tick- ets teurer" Alle so "Können wir wieder Streik haben?"</p><p>Translation: Everybody's like "Yeah, strike's over" Bahn goes "Okay, but therefore we're go- ing to raise the prices" Everybody's like "Can we have the strike back?"</p><p>General sentiment: neutral Aspect sentiment: Ticket purchase:negative General:positive be completely different from each other (cf. <ref type="figure" target="#fig_0">Fig- ure 1)</ref>. This means that a model has to be able to distinguish aspects in the text and make indepen- dent decisions for each of them. We want to address each of these challenges by (1) leveraging unlabeled data by modeling word representations and (2) modeling aspect detection and classification of their polarity jointly in an end-to-end trainable system.</p><p>We evaluate our approach on the GermEval 2017 data, i.e. customer reviews about Deutsche Bahn AG on social media. We particularly address subtask C as the typical setting where two pieces of information have to be detected from raw text:</p><p>1. Which aspects are mentioned?</p><p>2. For each mentioned aspect, what is the polar- ity of its sentiment?</p><p>From the new state-of-the-art results we obtain, we conclude that modeling of word representa- tions and joint modeling of aspects and polarity have not yet received the attention they deserve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Two recent shared tasks address ABSA: <ref type="bibr">SemEval 2016</ref><ref type="bibr">Task 5 (Pontiki et al., 2016</ref> and <ref type="bibr">GermEval 2017</ref><ref type="bibr" target="#b14">(Wojatzki et al., 2017</ref>. The SemEval dataset is extremely small. The English laptop reviews, e.g., only contain 395 training instances for the prediction of 88 aspect categories and their po- larities. Because of this sparsity, top-ranked sys- tems rely on feature engineering and hand-crafted rules. GermEval is a larger dataset ( 20K train- ing instances, 20 aspect categories) and thus suited for our goal of evaluating the quality of fully au- tomatic methods for learning aspect and polarity predictions. Furthermore the top systems at Se- mEval 2016, XRCE <ref type="bibr" target="#b1">(Brun et al., 2016)</ref> and IIT- TUDA ( <ref type="bibr" target="#b6">Kumar et al., 2016)</ref>, not only rely heavily on feature engineering but also separate the tasks of aspect detection and aspect polarity classifica- tion into two different parts of their pipeline.</p><p>The winners of GermEval 2017 rely on neural methods ( <ref type="bibr" target="#b7">Lee et al., 2017)</ref>. They try to link all as- pects to a sequence of tokens and model the task as a sequence labeling problem. This leads to prob- lems because some aspects are not assigned to any token but still have to be detected and classified. Our approach always considers the complete doc- ument and produces the set of all detected aspects at once. Although <ref type="bibr" target="#b7">Lee et al. (2017)</ref> incorporate some aspects of multi-task learning, the predic- tion of aspect category and polarity remains sep- arated in each of their approaches. In our work, we show that a joint learning of these two tasks achieves better performance. The approach by <ref type="bibr" target="#b7">Lee et al. (2017)</ref> also relies more heavily on external sources than ours. While we only collected a cor- pus of 113K unlabeled German tweets, <ref type="bibr" target="#b7">Lee et al. (2017)</ref> include annotated English data as well as a much larger unlabeled German corpus (Wikipedia, cf. Al- <ref type="bibr" target="#b0">Rfou et al. (2013)</ref>) in their setting. <ref type="bibr" target="#b12">Ruder et al. (2016)</ref> propose another neural model for ABSA. Similarly to the approaches mentioned above, they assume that aspects have already been detected by some other system in a pipeline architecture, and they concentrate on po- larity classification on the sentence level by unify- ing information from other sentences on the docu- ment level. We compare ourselves to this baseline and show improvements over pipeline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Algorithm</head><p>Word2vec skip-gram ( <ref type="bibr" target="#b8">Mikolov et al., 2013</ref>) is a widely used algorithm to obtain pretrained vector representations for input words. Notably, <ref type="bibr" target="#b7">Lee et al. (2017)</ref> use it for their experiments on the Germ- Eval data. FastText ( <ref type="bibr" target="#b2">Grave et al., 2017</ref>) works in a similar fashion but has the advantage of incor- porating subword information in the embedding learning process. So it can not only learn sim- ilar embeddings for word forms sharing a com- mon stem but also generate embeddings for un- seen words in the test set by combining the learned character ngram embeddings. This can be cru- cial when dealing with a morphologically rich lan- guage such as German. <ref type="bibr">Glove (Pennington et al., 2014</ref>) -similar to word2vec -does not incor- porate character-level information, but uses global rather than local information to learn its word em- beddings.</p><p>We have trained each of these embedding learn- ing algorithms on a corpus of 113K tweets men- tioning at least one of @DB info and @DB Bahn, two official accounts of Deutsche Bahn AG offer- ing information and replying to questions. We col- lected these tweets specifically to build a docu- ment collection that is closely related to the do- main of GermEval 2017. We also included the GermEval training set for the embedding training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pipeline LSTM (baseline)</head><p>We compare our proposed approach to the model described in <ref type="bibr" target="#b12">(Ruder et al., 2016)</ref>. They first encode each sentence with glove word embeddings and a bidirectional LSTM (Hochreiter and Schmidhu- ber, 1997). Then this output is concatenated with an embedding of the aspect addressed in the cur- rent sentence and finally fed in a document-level BiLSTM. As we are dealing with social media texts, our documents are already very short. So we do not split them into shorter units (sentences). Therefore the second hierarchy level of <ref type="bibr" target="#b12">(Ruder et al., 2016)</ref>, that combines the output of consec- utive sentences in a document, is superfluous and omitted in our experiments. In all other aspects -including hyperparameters -we do as <ref type="bibr" target="#b12">(Ruder et al., 2016)</ref>, i.e. we duplicate a tweet for each aspect detected in it, concatenate an aspect em- bedding of size 15 to the output of the BiLSTM encoder, use dropout of 0.5 after the embedding layer and after LSTM cells, and apply a gradient clipping norm of 5. As <ref type="bibr" target="#b12">Ruder et al. (2016)</ref> rely on the detected aspects to be given at test time, for a realistic comparison, we feed in the aspects as detected by the strong GermEval baseline sys- tem based on support vector machines. The so ob- tained system serves as our first baseline, repre- senting a state-of-the-art pipeline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">End-to-end LSTM</head><p>We modify the pipeline model described in the last section as follows: the aspect detection is in- tegrated into the neural network architecture per- mitting an end-to-end optimization of the whole model during training. This is achieved by for- matting the classifier output as a vector z ∈ { 0, 1, 2, 3 } |A| , where A is the set of all 20 aspects (e.g., General, Ticket purchase, Design, Safety, . . . ). This corresponds to predicting one of the four classes N/A, positive, negative and neutral for each aspect. Specifically, we obtain a hidden rep- resentation of an input document X in the follow- ing manner:</p><formula xml:id="formula_0">v = DO(BiLSTM(DO(embed(X))))<label>(1)</label></formula><p>where embed ∈ { word2vec, glove, fasttext } and DO = dropout (Hinton et al., 2012). The design choices for the BiLSTM in this step remain the same as in the baseline model. Then, we transform the feature vector v ex- tracted from the text X to a score vectorˆyvectorˆ vectorˆy (a) for each aspect a ∈ A and apply softmax normaliza- tion:</p><formula xml:id="formula_1">ˆ y (a) = softmax(W (a) v + b (a) )<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">softmax(x) i = exp(x i ) 3 k=0 exp(x k ) for i = 0, . . . , 3 (3)</formula><p>Thus for each aspect, we predict its presence or absence as well as its polarity in one step:</p><formula xml:id="formula_3">z (a) = arg max i ˆ y (a) i (4)</formula><p>The loss is simply the cross entropy summed over all aspects:</p><formula xml:id="formula_4">L(θ) = a∈A H(y (a) , ˆ y (a) )<label>(5)</label></formula><p>with  </p><formula xml:id="formula_5">H(y, ˆ y) = − i y i · log(ˆ y i )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">End-to-End CNN</head><p>Keeping the formalization as an end-to-end task, we replace the BiLSTM by a convolutional neu- ral network (CNN) as described in <ref type="bibr" target="#b5">(Kim, 2014</ref>). As in their setting CNN-non-static, we use 300- dimensional word embeddings, a max-over-time pooling operation, filter sizes of 3, 4, 5, and dropout with a rate of 0.5 (as before). We use ReLu ( f (x) = max(0, x)) as our activation func- tion, and 300 filters of each size, a number also found in related work on sentiment analysis (dos <ref type="bibr" target="#b13">Santos and Gatti, 2014)</ref>. Following <ref type="bibr" target="#b5">(Kim, 2014</ref>), we do not apply dropout after the embedding layer:</p><formula xml:id="formula_6">v = DO(CNN(embed(X)))<label>(7)</label></formula><p>With Equation 7 replacing Equation 1, the aspect- wise classification for the end-to-end CNN then follows the same definitions as described in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Pipeline CNN</head><p>In order to compare the effects of joint end-to- end and pipeline approaches across neural archi- tectures, we also include an experiment where the CNN model from the previous section replaces the BiLSTM in the pipeline setting described in sec- tion 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct our experiments on the GermEval 2017 data ( <ref type="bibr" target="#b14">Wojatzki et al., 2017)</ref>, i.e. customer feedback about Deutsche Bahn AG on social me- dia, microblogs, news, and Q&amp;A sites. The data were collected over the time of one year and manu- ally annotated, resulting in a main dataset of about 26K documents, divided into a training, develop- ment, and test set using a random 80%/10%/10% split. About 1,800 documents from the last 3 months of the data collection period constitute a diachronic test set that can be used to test the ro- bustness of a system over time. We keep the pro- posed data split and filter out training instances where the same aspect category was assigned two different polarity classes (which affects approxi- mately 4% of the data). The development and test data remain the same. We choose our hyperparameters based on the development data using the following procedure: we train initial models with a hyperparameter setting based on values we found in the liter- ature, stochastic gradient descent with a learn- ing rate of 0.01 (as in dos <ref type="bibr" target="#b13">Santos and Gatti (2014)</ref>) and a mini-batch size of 10 (as in <ref type="bibr" target="#b12">Ruder et al. (2016)</ref>). For the best-performing CNN and LSTM architectures (end-to-end + fasttext), we then refine the learning rate and batch size on the development data using random search in the range { 0.001, 0.003, 0.01, 0.03, 0.1 } for learning rate and { 5, 10, 20 } for batch size. For the CNN setting, this results in a learning rate of 0.03 and a batch size of 5 (which we then use for all CNN architectures in the final experiments). For the LSTM setting, this results in a learning rate of 0.01 and a batch size of 10 (which we then use for all LSTM architectures).</p><p>Training our models takes between 1-3 minutes per epoch on a GeForce GTX 1080 GPU, the end- to-end CNN being the fastest model to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Aspect polarity <ref type="table">Table 1</ref> shows the results of our experiments, as well as the results of our strong baselines. Note that the majority class baseline al- ready provides good results. This is due to highly unbalanced data; the aspect category "Allgemein" ("general"), e.g., constitutes 61.5% of the cases. This imbalance makes the task even more chal- lenging.</p><p>Over all architectures, we observe a comparable or better performance when using fasttext embed- dings instead of word2vec or glove. This backs our hypothesis that subword features are important for processing the morphologically rich German language. Leaving everything else unchanged, we can furthermore see an increase in performance for all settings, when switching from the pipeline to an end-to-end approach. The best performance (marked in bold) is achieved by a combination of CNN and FastText embeddings, which outper- forms the highly adapted winning system of the shared task.</p><p>Aspect category only Even though our archi- tectures are designed for the task of joint predic- tion of aspect category and polarity, we can also evaluate them on the detection of aspect categories only. <ref type="table" target="#tab_2">Table 2</ref> shows the results for this task. First of all, we can see that the SVM-based GermEval baseline model has very decent performance as it is practically on par with the best submission for the synchronic and even outperforms the best sub- mission on the diachronic test set. It is therefore well-suited to serve as input to the pipeline LSTM model we compare with in our main task.</p><p>Comparing our architectures, we see again that fasttext embeddings always lead to equal or better performance. And even though we do not directly optimize our models for this task only, our best model (CNN+fasttext) outperforms all baselines, as well as the GermEval winning system. Impact of domain-specific corpus We compare the domain-specific FastText embeddings to Fast- Text embeddings trained on Wikipedia 1 , which is approximately 100 times the size of our domain- specific corpus. We report the results in Ta- ble 3. The embeddings trained on Wikipedia show slightly lower performance on the dev set but slightly higher or equal performance on the test sets. We conclude that the main positive im- pact of FastText stems from its capability to model subword information and that a large domain- independent corpus or a small domain-specific corpus lead to similar performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a new approach to ABSA. By solving the two classification problems (aspect   categories + aspect polarity) inherent to ABSA in a joint manner, we observe significant perfor- mance gains for both of these tasks on the Germ- Eval 2017 data. Our experiments also showed that word representations leveraging subword informa- tion are crucial for a challenging task like ABSA in a morphologically rich language, such as Ger- man. Furthermore we observed consistently bet- ter performance of CNN architectures in otherwise comparable scenarios, which suggests that CNNs cope better with the irregularities of user-written texts on social media, a research question we leave to future work. By establishing a new state of the art in aspect detection and polarity classification, we provide a new practical baseline for future re- search in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example sentence with contained aspects and their polarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic view of end-to-end CNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Micro-averaged F1-scores for the prediction of aspect categories only (i.e. without taking polarity into 
account at all) as computed by the GermEval evaluation script. The results in the bottom part of the table are taken 
from (Wojatzki et al., 2017). 

dev synchr. test diachr. test 
aspect + sent. .502 
.423 
.465 
aspect only 
.610 
.544 
.571 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of the end-to-end CNN model with 
fasttext embeddings trained on the German Wikipedia. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable input. This work was par-tially supported by the European Research Coun-cil, Advanced Grant # 740516 NonSequeToR, and by a Ph.D. scholarship awarded to the first author by the German Academic Scholarship Foundation (Studienstiftung des deutschen Volkes).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">XRCE at semeval-2016 task 5: Feedbacked ensemble modeling on syntactico-semantic knowledge for aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016</title>
		<meeting>the 10th International Workshop on Semantic Evaluation, SemEval@NAACL-HLT 2016<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="277" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>EACL (2)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Ilya Sutskever, and Ruslan Salakhutdinov</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IIT-TUDA at semeval2016 task 5: Beyond sentiment lexicon: Combining domain dependency and distributional semantics features for aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Kohail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-16" />
			<biblScope unit="page" from="1129" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ukp tu-da at germeval 2017: Deep learning for aspect based sentiment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Ung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GermEval 2017-Shared Task on Aspect-based Sentiment in Social Media Customer Feedback</title>
		<meeting>the GermEval 2017-Shared Task on Aspect-based Sentiment in Social Media Customer Feedback</meeting>
		<imprint>
			<publisher>German Society for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Smadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronique</forename><surname>Orphee De Clercq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kotelnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>Núria Bel, Salud María Jiménez-Zafra, and GülsGüls¸en Eryi˘ git; San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A hierarchical model of reviews for aspectbased sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="999" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wojatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Ruppert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<title level="m">Proceedings of the GermEval 2017-Shared Task on Aspect-based Sentiment in Social Media Customer Feedback</title>
		<meeting>the GermEval 2017-Shared Task on Aspect-based Sentiment in Social Media Customer Feedback<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>GermEval 2017: Shared Task on Aspect-based Sentiment in Social Media Customer Feedback</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
