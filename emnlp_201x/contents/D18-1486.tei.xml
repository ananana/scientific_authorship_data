<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MCapsNet: Capsule Network for Text with Multi-Task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Network and Information Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Advanced Optical Communication System and Network</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MCapsNet: Capsule Network for Text with Multi-Task Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4565" to="4574"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4565</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-task learning has an ability to share the knowledge among related tasks and implicitly increase the training data. However, it has long been frustrated by the interference among tasks. This paper investigates the performance of capsule network for text, and proposes a capsule-based multi-task learning architecture , which is unified, simple and effective. With the advantages of capsules for feature clustering, proposed task routing algorithm can cluster the features for each task in the network, which helps reduce the interference among tasks. Experiments on six text classification datasets demonstrate the effectiveness of our models and their characteristics for feature clustering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task learning (MTL) has achieved a great success in the field of natural language processing, which can share the knowledge among multiple tasks, implicitly increasing the volume of training data. The combination of multi-task learning and deep neural networks (DNNs) generates a further synergy via the regularization effect on the DNNs <ref type="bibr" target="#b3">(Collobert and Weston, 2008)</ref>, which helps allevi- ate the overfitting and learn a more universal pre- sentation.</p><p>Inspired by this, more DNN-based multi-task learning models are proposed to improve the per- formance. As depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, they can be categorized into three groups by structure: tree scheme ( <ref type="bibr" target="#b3">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b16">Liu et al., 2015)</ref>, parallel scheme ( <ref type="bibr" target="#b14">Liu et al., 2016</ref>) and me- diate scheme ( . Tree scheme reuses some shallow layers of network and sepa- rates the higher layers for different tasks, which is the most common architecture for MTL but can only share the low-level knowledge. To share deeper level knowledge among the tasks, more layers are linked in parallel and mediate schemes. But this would severely suffer from the interfer- ence among tasks. Useless features following the helpful ones are fully shared among tasks, which may contaminate the feature spaces of tasks by useless ones. Besides, models under these two schemes usually employ multiple subnets in the structures, which would contain more parameters and are hard to train. Apparently, there is a contradiction between knowledge sharing and interference. Sharing too much between tasks would inevitably bring about the interference that feature space for each task may be contaminated by others. Shared useless features may mislead the prediction of network. This dilemma is caused by the lack of manage- ment for sharing process, in which the network can not discriminate the features and collect the appropriate features for each task.</p><p>Capsule network <ref type="bibr">(Hinton et al., 2011;</ref><ref type="bibr" target="#b18">Mousa et al., 2017;</ref><ref type="bibr" target="#b6">Hinton et al., 2018</ref>) embeds the fea- tures into capsules and connects the neighbor lay- ers via "routing-by-agreement". The dynamic routing algorithm has an ability to decide the routes of capsules, namely, to cluster the features for each category. So, intuitively this property of capsule network can be employed in MTL to dis- criminate the features for tasks.</p><p>In this paper, we explore the performance of capsule network for text <ref type="bibr">(CapsNet-1, CapsNet1)</ref> and show the benefits and potential of cap-sule network for NLP. Then we mainly propose a capsule-based architecture for multi-task learn- ing (McapsNet), which is unified, simple, effec- tive and can cluster the feature for tasks. We designed a Task Routing algorithm to route the feature flows to tasks and vote for the classes, which can reduce the interference. In extensive ex- periences, our approach achieves competitive re- sults in single-task scenario and shows obvious improvement in multi-task scenario, which proves our approach effective and its ability to reduce the interference among multiple tasks. Also, our visu- alization experiments intuitively show the feature clustering mechanism and how it helps make right predictions.</p><p>The contribution of this paper are three-folds:</p><p>• This paper investigates the performance of capsule network on text and designs two ef- fective capsule-based models for text classifi- cation, which give clear improvement to sev- eral benchmarks.</p><p>• We novelly combine the capsule and multi- task learning, which can help reduce the in- terference among tasks.</p><p>• Proposed task routing algorithm can route the capsules to multiple tasks, by which the fea- tures is clustered into groups for tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Convolutional Neural Network and Multi-Task Learning</head><p>Capsule network is based on the convolutional neural network (CNN) and uses a lot of convo- lution operations. The main differences between them are that capsule network uses vectors to rep- resent the features and discards the pooling oper- ation. CNN is good at feature extraction and can capture short and long range relations through the feature graph over text sequence <ref type="bibr" target="#b9">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b11">Kim, 2014)</ref>. In this section, we pro- vide some formulations for CNN and some back- ground knowledge for multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-Task CNN for Text Classification</head><p>Given a text sequence x 1:l = x 1 x 2 · · · x l of length l, the target of CNN is to predict the categoryˆycategoryˆ categoryˆy of x 1:l from a set {y 1 , y 2 , · · · , y C }, or a one-hot form ofˆyofˆ ofˆy, where C is the class number. Using f (·) denote the network, the prediction process can be formalized as f (</p><formula xml:id="formula_0">x 1 , x 2 , · · · , x l ) = ˆ y.</formula><p>For details, convolutional neural network f (·) first uses a lookup table to embed the word se- quence x 1:l into vectors x. Then CNN produces the representation of the input sequence by stack- ing the layers of convolution, pooling and fully- connected in order.</p><formula xml:id="formula_1">F = K * x (1) ˆ F = p(F)<label>(2)</label></formula><formula xml:id="formula_2">ˆ y = w ˆ F + b,<label>(3)</label></formula><p>where K is the kernel of convolution operation * ; p(·) denotes the pooling operation; F andˆFandˆ andˆF repre- sent the feature maps; w and b denote the weight and bias respectively in fully connected layer. The parameters of the network are optimized via all kinds of SGD (stochastic gradient decent) algorithms to minimize the loss between predic- tionˆytionˆ tionˆy and ground truth label˜ylabel˜ label˜y</p><formula xml:id="formula_3">l(ˆ y, ˜ y) = − N ∑ i=1 C ∑ j=1˜y j=1˜ j=1˜y i j log(ˆ y i j ),<label>(4)</label></formula><p>where i, j enumerate the training samples and classes respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-Task Learning</head><p>Multi-task learning model is usually the vari- ant or combination of single-task ones (CNNs, RNNs or DNNs) like the architectures illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Given K text classification tasks {T 1 , T 2 , · · · , T K }, a multi-task learning model f (·) shall have the ability to make prediction for samples x</p><formula xml:id="formula_4">(k) i from each task T k . ˆ y (k) i = f (x (k) i )<label>(5)</label></formula><p>And the overall loss for all the tasks is usually a linear combination of the costs for each.</p><formula xml:id="formula_5">L = − K ∑ k=1 λ k N k ∑ i=1 C k ∑ j=1˜y j=1˜ j=1˜y (k) i,j logˆylogˆ logˆy (k) i,j<label>(6)</label></formula><p>where λ k ,N k and C k denote the weight, number of training samples and class number of task T k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Capsule Networks for Text</head><p>Capsule network (CapsNet) is first proposed by <ref type="bibr" target="#b22">Sabour et al. (2017)</ref> for image classification, which is position sensitive and shows strong per- formance on some classification tasks. As de- picted in <ref type="figure">Figure 2</ref>, we propose several capsule net- works for text, which are suitable for text rep- resentation and multi-task learning. They are comprised of convolutional layer, primary capsule layer and class capsule layer. In the rest of this sec- tion, we will first give the formulation of single- task capsule networks (CapsNet-1 and CapsNet- 2) for text classification, and then transfer it into a multi-task version (McapsNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Primary Capsule Layer</head><p>Given an embedded sample of x ∈ R l×d with length of l and word vectors of d-dimension, cap- sule network first employs a plain convolution layer to extract the local features from N-grams. Each kernel K i with a bias b emits a feature maps F i by convolution.</p><formula xml:id="formula_6">F i = x * K i + b (7)</formula><p>By assembling I feature maps together, we have a I-channel layer</p><formula xml:id="formula_7">F = [F 1 , F 2 , · · · , F I ]<label>(8)</label></formula><p>The generated feature maps are then fed into the primary capsule layer, piecing the instantiated parts together via another convolution. Primary capsules use vectors instead of scales to preserve the instantiated parameters for each feature, which can not only represent the intensity of activation but also record some details of the instantiated parts in input. In this way, capsule can be regarded as a short representation of instantiated parts that are detected by kernel.</p><p>Sliding over the feature map F, each kernel K j would output a series of capsules p j ∈ R d of d- dimension. These capsules comprise a channel P j of primary capsule layer.</p><formula xml:id="formula_8">P j = g(K j * F + b) (9)</formula><p>where g is the nonlinear squash function and b is the capsule bias term. All the J channels can be arranged as</p><formula xml:id="formula_9">P = [P 1 , P 2 , · · · , P J ]<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Connection Between Capsule Layers</head><p>Capsule network generates the capsules in next layer using "routing-by-agreement". This pro- cess takes the place of pooling operation that usu- ally discards the location information, which helps augment the robust of the network and also helps cluster features for prediction. Between two neighbor layers l and l + 1, a "prediction vectors" ˆ u j|i is first computed from the capsule u i in lower layer l, by multiplying a weight matrix W ijû</p><formula xml:id="formula_10">ijˆijû j|i = W ij u i (11)</formula><p>Then, in the higher layer l+1 a capsule s j is gener- ated by the linear combination of all the prediction vectors with weights c ij</p><formula xml:id="formula_11">s j = ∑ i c ij u j|i (12)</formula><p>where c ij are coupling coefficients decided by the iterative dynamic routing process. Coupling coefficients are calculated by a "rout- ing softmax" function on original logits b ij , which are the log prior probability that capsule i should be coupled to capsule j.</p><formula xml:id="formula_12">c ij = exp(b ij ) ∑ j exp(b ij )<label>(13)</label></formula><p>This process of "routing softmax" guarantee the sum of all the coefficients for capsule j is 1.</p><p>The length of capsule represents the probability that the input sample has the object capsule de- scribes, that is the activation of capsule. So the length of capsule is limited in range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> with a non-linear squashing function.</p><formula xml:id="formula_13">v j = ∥s j ∥ 2 1 + ∥s j ∥ s j ∥s j ∥ 2<label>(14)</label></formula><p>By that, the short vectors are pushed to shrink to zero length, and long ones are pushed to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamic Routing</head><p>Suppose capsule layer l has been generated. We have to decide the intensity of the connections be- tween capsule i and j from l-th layer to (l + 1)-th layer, that is the coupling coefficient c ij . The ini- tial digit of coupling coefficient b ij is updated with routing by agreement a ij , which is calculated by a scale product between capsules in two layers.</p><formula xml:id="formula_14">a ij = ˆ u j|i · v j<label>(15)</label></formula><p>Value of agreement a ij is added to the digit to calculated the capsules in the next layer.</p><formula xml:id="formula_15">b ij ← b ij + a ij<label>(16)</label></formula><p>And the whole process for update (Eq. <ref type="formula" target="#formula_2">(13)</ref>→ <ref type="formula" target="#formula_1">(12)</ref> → <ref type="formula" target="#formula_3">(14)</ref>→ <ref type="formula" target="#formula_4">(15)</ref> CapsNet-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CapsNet-2 MCapsNet</head><p>Figure 2: Architectures of capsule networks for text</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Class Capsule Layer and Loss</head><p>Class capsule layer, as the top-level layer, is com- prised of C class capsules, in which each one cor- responds to a category. The length of instantiated parameters in each capsule represents the proba- bility that the input sample belongs to this cate- gory, and the direction of each set of instantiated parameters preserves the characteristics of the fea- tures, which could be regarded as an encoded vec- tor for the input sample.</p><p>Margin Loss To increase the difference between the lengths of classes, CapsNet utilizes a separated margin loss:</p><formula xml:id="formula_16">L j =G j max(0, m + − ∥v j ∥) 2 + λ(1 − G j ) max(0, m − − ∥v j ∥) 2<label>(17)</label></formula><p>where v j is the capsule for class j; m + and m − is the top and bottom margins respectively, which help push the length to shrink beyond two mar- gins; G j = 1 if and only if class j is the ground truth:</p><formula xml:id="formula_17">G j = { 1 ˜ y j = 1 0 ˜ y j = 0<label>(18)</label></formula><p>λ is the weight for the absent classes, which reduces the weight of absent classes, avoiding shrinking the lengths of all the capsules too much at prophase training. In this paper, λ is set to 0.5.</p><p>Orphan Category A drawback of CapsNet is that it tends to account for everything in the input sampling, including some "background" informa- tion such as stop word and punctuations that would interfere the prediction. So an orphan category is added in class capsules in the output layer, which belongs to none of the categories of the task. The orphan category would help collect the less con- tributive capsules that contain too much "back- ground" information, which reduces the interfer- ence for normal categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Substitutional Modules for Multi-Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Routing</head><p>Dynamic routing algorithm is first proposed by <ref type="bibr" target="#b22">Sabour et al. (2017)</ref>, which displaces the pooling operation used in conventional convolution neural network. It maintains the position information for features, which is beneficial to both image and text representation. More importantly, this routing-by- agreement method has an ability to cluster the fea- tures into each class.</p><p>Inspirited by this, we employ this thought to cluster the features for different tasks and propose the Task Routing algorithm, which gives a simple and efficient solution to the problem that existing</p><formula xml:id="formula_18">Algorithm 1 Task Routing Algorithm 1: function ROUTING(ˆ u (k) j|i , a (k) ij , r, l ) 2:</formula><p>for i = 0 → r do <ref type="bibr">3:</ref> for all capsule i in layer l and capsule <ref type="bibr">4:</ref> j in task k:</p><formula xml:id="formula_19">5: c (k) i = sof tmax(b (k) i ) 6:</formula><p>for all capsule j in layer l + 1:</p><formula xml:id="formula_20">7: v (k) j = g( ∑ i c (k) j|î j|î u (k) j|i ) 8:</formula><p>for all capsule i in layer l and capsule <ref type="bibr">9:</ref> j in task k:</p><p>10:</p><formula xml:id="formula_21">b (k) ij = b (k) ij + a (k) ij , a (k) ij = ˆ u (k) j|i · v (k) j 11:</formula><p>end for 12: return v More concretely, we introduce a coupling coef- ficient c <ref type="bibr">(k)</ref> ij between capsule i in l-th layer and cap- sule j in class capsule (l + 1)-th layer for task k, which is the result of a softmax function on b</p><formula xml:id="formula_22">(k) ij . c (k) ij = sof tmax(b (k) ij )<label>(19)</label></formula><p>Then, instantiated parameter v</p><formula xml:id="formula_23">(k) j of capsule j in task k is calculated by v (k) j = ∑ i c (k) ij · ˆ u (k) j|i<label>(20)</label></formula><p>wherê u</p><formula xml:id="formula_24">(k) j|i = W k,j,i u i Coupling coefficient c (k)</formula><p>ij is restricted in range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, which represents the probability that cap- sule i belongs to class capsule j in task k. And it is update by the algorithm is described in Algo- rithm 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Loss</head><p>The loss for each task is the sum of margin losses for all the classes</p><formula xml:id="formula_25">∑ C j=1 L (k)</formula><p>j . By linearly combin- ing the loss for every task, we get multi-task loss</p><formula xml:id="formula_26">L = K ∑ k=1 β (k) C ∑ j=1 L (k) j .<label>(21)</label></formula><p>where β (k) is the weight for each loss and ∑ K k=1 β (k) = 1. In this paper, all the β (k) are set to be 1/K to make a balance among K tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Training</head><p>In order to juggle several tasks in a unified net- work, following <ref type="bibr" target="#b3">(Collobert and Weston, 2008)</ref>, each task is trained alternatively in a stochastic manner. The steps can be described as follows: 1. Pick up a task k randomly; 2. Select an arbi- trary sample s from the task k; 3. Feed the sample s into the McapsNet and update the parameters; 4. Go back to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Architectures of CapsNets for Text</head><p>As illustrated in <ref type="figure">Figure 2</ref>, we propose a capsule- based multi-task learning architecture Mcap- sNet, which is base on the single-task structures CapsNet-1 and CapsNet-2. Architectures for them are detailed as following.</p><p>CapsNet-1 As depicted in <ref type="figure">Figure 2</ref>, CapsNet-1 is a fundamental framework with three layers. The first layer is a plain convolution operation with 256 kernels with window size of 3 and stride of 1. For activation function, we use ReLU to augment non- linearity. This layer helps extract local features from the input sequences, which is the base to con- struct primary capsules.</p><p>Primary capsule layer employs 32 kernels with window size of 3 and stride of 1. The emitted pri- mary capsules are 8-dimensional, which have big- ger respective field, helping reassemble the piece features into wholes.</p><p>Last one is the class capsule layer, which is comprised of 16-dimensional capsules for the classes. They are connected to PrimaryCaps with routing-by-agreement and the coupling coef- ficients are updated by dynamic routing algorithm.</p><p>CapsNet-2 On this basis of CapsNet-1, CapsNet-2 upgrades the convolutional layer and uses multiple kernel sizes, which enriches the features. And concatenating 1 them up allows primary capsule see the features with different kernel sizes in the same time.</p><p>MCapsNet McapsNet is a unified multi-task structure based on CapsNet-2. It replaces the dy- namic routing with task routing (Algorithm 3.5), which enables the network to route the features to <ref type="table" target="#tab_2">Dataset Train Dev Test Classes Type  MR  9500  - 1100  2  review  SST-1  8544 1101 2210  5  sentiment  SST-2  6920 872 1821  2  sentiment  Subj  9000  - 1000  2  subjectivity  TREC  5900  - 500  6  question  AG's  120k  - 7600  4  news   Table 1</ref>: Statistics for six datasets each tasks. And the whole network is optimized in a stochastic way with multi-task training (Section 3.5).</p><p>Implement Details For word embedding, we use the word vectors in Word2Vec ( <ref type="bibr" target="#b17">Mikolov et al., 2013)</ref>, which is 300-dimensional and has 3M vo- cabularies. And all the routing logits b</p><formula xml:id="formula_27">(k)</formula><p>ij is ini- tialized to zero, so that all the capsules in adjacent layers (ˆ u j|i , v j ) are connected with equal possi- bility c ij . The coupling coefficients are updated by routing with 3 iterations, which performs best for our approach. For training, we use Adam opti- mizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with exponentially decaying learning rate. Moreover, we use mini- batch with size of 8 for all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We test our capsule-based models on six datasets in both single-task and multi-task scenarios to demonstrate the effectiveness of our approaches. We also in this section conduct some investiga- tions like ablation study and visualization to give a comprehensive understanding to the characteris- tics of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For both single-task and multi-task scenarios, we conduct extensive experiments on six benchmarks: movie reviews (MR) <ref type="bibr" target="#b0">(Bo and Lee, 2005</ref>), Stanford Sentiment Treebank (SST-1 and SST-2) <ref type="bibr" target="#b23">(Socher et al., 2013)</ref>, subjectivity classification (Subj) ( <ref type="bibr" target="#b19">Pang et al., 2004</ref>), question dataset (TREC) ( <ref type="bibr" target="#b13">Li and Roth, 2002</ref>), AG's news corpus ( <ref type="bibr" target="#b18">Mousa et al., 2017)</ref>. These datasets cover a wide range of text classification tasks, which can fully test a model and the details are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Competitors</head><p>To demonstrate the effectiveness of our capsule network, we compare the single-task architec- tures with several state-of-the-art models, involv- ing LSTM/BiLSTM ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>   <ref type="bibr" target="#b11">(Kim, 2014</ref>). Also, we com- pare the multi-task architecture <ref type="figure">(Figure 2</ref>) with several strong baselines of multi-task learning, in- cluding a general architecture for multi-task learn- ing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Single-Task Learning Results</head><p>We first test our approach on six datasets for text classification under the scheme of single-task. As <ref type="table" target="#tab_2">Table 2</ref> shows, our single-task network enhanced by capsules is already a strong model. CapsNet- 1 that has one kernel size obtains the best accu- racy on 2 out of 6 datasets, and gets competitive results on the others. And CapsNet-2 with multi- ple kernel sizes further improves the performance and get best accuracy on 4 datasets. This proves our capsule networks are effective for text. Partic- ularly, our capsule network outperforms conven- tional CNNs like DCNN, CNN-MC and VD-CNN with a large margin (by average 1.1%, 0.7% and 1.0% respectively), which shows the advantages of capsule network over conventional CNNs for clustering features and leveraging the position in- formation.</p><p>Routing Iteration The coupling coefficients c ij are updated by dynamic routing algorithm, which determines the connections between the capsules. To find the best updating iteration for coupling co- efficients, we test the CapsNet-2 with a series of iterations (1, 3 and 5) on MR dataset. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, network with 3 iterations convergences fast and performs best, which stays in line with the conclusion in ( <ref type="bibr" target="#b22">Sabour et al., 2017</ref>). So we utilize 3 iterations in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on Orphan Category</head><p>Orphan category in class capsule layer helps collect the noise capsules that contain the 'background' infor- mation like stop words, punctuations or any unre- lated words. We conduct the ablation experiment on orphan category, and result <ref type="table" target="#tab_2">(Table 2)</ref> shows that network with orphan category perform better than the without one by 0.4%. This demonstrates the effectiveness of orphan category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-Task Learning Results</head><p>Up to now, we have obtained an optimized single- task architecture. In this section, we equip CapsNet-2 with the task routing and multi-task training procedure, namely the model MCapsNet, so that this capsule based architecture can learn several datasets in a unified network. Exten- sive experiments are conducted in this section to demonstrate the effectiveness of our multi-task learning architecture, as well as its ability for fea- ture clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Performance</head><p>We simultaneously train our model McapsNet on six tasks in <ref type="table">Table 1</ref> and compare it with single- task scenario <ref type="table" target="#tab_4">(Table 3</ref>). We can see that our multi- task architecture clearly improves the performance over the single task models, which demonstrates the benefits of our multi-task architecture.  As <ref type="table" target="#tab_4">Table 3</ref> shows, MCapsNet also outperforms the state-of-the-art multi-task learning models by at least 1.1%. This shows the advantages of our task routing algorithm, which can cluster the fea- tures for each task, instead of freely sharing the features among tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Routing Visualization</head><p>To show the mechanism how capsule benefits the multi-task learning, we visualize the coupling co- efficient c (k) ij ∈ [0, 1] between primary and class capsules. We use kernel with size 1 for primary capsule layer so that every capsule represents only one 3-gram phrase. The strength of these connec- tions indicates the importance of these 3-grams to their corresponding task and class.</p><p>We feed a random sample from the dataset MR into MCapsNet. In the first row of <ref type="table">Table 4</ref>, we show the most important 3-gram phrases for two tasks MR and Subj (two classes for each) with word cloud. The sizes of the grams represent the weights of coupling coefficients. We can see that task routing algorithm helps lead the grams into the most related tasks, which allows each task only consider the helpful features for them. In another word, task routing builds a feature space for each task and avoids they contaminate each other. This demonstrates that MCapsNet has the ability of fea- ture clustering, which can benefit MTL by reduc- ing the interference.</p><p>We also illustrate the coupling coefficients se- quentially for each task. The height of the blue and gray lines represents the polarity of positiv- ity and subjectivity respectively. It is clear that MCapsNet can focus on the appropriate positions for each task, which helps make the final correct predication for every task.   <ref type="table">Table 4</ref>: Visualization of the task routing for a positive sample from MR, "it 's not so much enjoyable to watch as it is enlightening to listen to new sides of a previous reality , and to visit with some of the people who were able to make an impact in the theater world"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Related work can be divided into two threads. The first thread is capsule network, which has been proven effective on many classification tasks. Concept of capsule is first proposed by <ref type="bibr">Hinton et al. (2011)</ref>, which first use vector to de- scribe the pose of object. This work improves the representation ability of the neural networks against the vanilla CNNs and also enhances the robust of network for transformation. Then dy- namic routing algorithm is proposed in <ref type="bibr" target="#b22">(Sabour et al., 2017)</ref>, which is aimed to displace the pool- ing operation, building a part-whole relationship for object recognition. Dynamic routing can main- tain the position information of features for objects that pooling operations generally discard. And the result shows the proposed method improves the state-of-the-art performance for MNIST dataset. Next, <ref type="bibr" target="#b6">Hinton et al. (2018)</ref> employs the matrix to depict the pose and, based on EM algorithm de- signs a new routing procedure between capsule layers. This work shows strong ability for address- ing transformation problem and gains significant improvement on smallNORB dataset.</p><p>All these methods are proposed for computer vi- sion, while in this paper we investigate the benefits of capsules for text.</p><p>The other thread is about multi-task learning. The earliest idea can be traced back to <ref type="bibr" target="#b1">(Caruana, 1997)</ref> and there have been some work completed in this field to augment the performance. Collobert and Weston (2008) develop a multi-task learning model based on CNN. It shares only one lookup table to train a better word embedding. And <ref type="bibr" target="#b16">Liu et al. (2015)</ref> propose a DNN-based model for multi-task learning, which shares some low lay- ers but separate the high-level layers to complete several different tasks.</p><p>Some models are proposed to share deeper lay- ers of networks, which can exchange high-level knowledge among tasks and gain better perfor- mance. (  and ( <ref type="bibr" target="#b14">Liu et al., 2016)</ref> introduce some RNN architectures and design dif- ferent schemes for knowledge sharing. These tri- als promote the performance of models, but they give no consideration to the interference in multi- task learning. <ref type="bibr" target="#b15">Liu et al. (2017)</ref> add the adversar- ial losses in multi-task RNNs, which can allevi- ate the interference among tasks by finding a com- mon feature space for tasks. However, the model has multiple subnets and various losses, which re- quires more computation and training skills.</p><p>Different from these methods, we use the thought of capsule in natural language processing (NLP) field. And proposed a capsule based multi- task learning architecture with task routing algo- rithm. This approach can cluster the features for each task, reducing the interference among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper investigates the performance of capsule network for text representation, and proposes sev- eral effective architectures. By means of the char- acteristics of capsule network, we design a unified, sample yet effective architecture with task routing for multi-task learning, which has the ability to clustering the features, building a private feature space for every task.</p><p>In future work, we would like to investigate the relations of various tasks in multi-task learning by exploiting the potential of capsule network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three schemes of multi-task learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>k) j 13: end function MTL models (Liu et al., 2017; Ruder et al., 2017; Fang et al., 2017) want to address: "What fea- ture should be shared and what should not among tasks?" By that, network can decide the contri- bution of the features for each tasks and set the appropriate coupling coefficients between features and tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(MT-GRNN) (Zhang et al., 2017), recurrent neural network based multi-task learning (MT- RNN) (Liu et al., 2016), convolutional neural net- work with multi-task learning (MT-DNN) (Col- lobert and Weston, 2008), deep neural network with multi-task learning (MT-CNN) (Liu et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of routing iteration</figDesc><graphic url="image-13.png" coords="7,102.56,65.13,179.61,122.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>MR</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>→(16)) is conducted iteratively to</head><label></label><figDesc></figDesc><table>optimize the coupling coefficients and the capsules 
in the next layer. …… 

Conv1 
PrimaryCaps 
ClassCaps 

Length 

Dynamic 
Routing 

Word Vector 

256 
8×32 
16 

||L2|| 

Concat 

3 

4 

5 

100 

{ 

{ 

300 

Conv2 

…… 

Task 1 

Task 2 

Task K 

…… 

ClassCaps 

Task 
Routing 

16 
Task Routing 

{ 

{ 

{ 

…… 

Capsule 

Scale 

…… 

Length 

||L2|| 

kernel 
size 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) , LSTM</head><label>,</label><figDesc></figDesc><table>Dataset 
MR SST-1 SST-2 Subj TREC AG's 
LSTM 
75.9 45.9 
80.6 89.3 
86.8 
86.1 
BiLSTM 79.3 46.2 
83.2 90.5 
89.6 
88.2 
LR-LSTM 81.5 48.2 
87.5 89.9 
-
-
VD-CNN 
-
-
-
-
-
91.3 
DCNN 
-
48.5 
86.8 
-
93.0 
-
CNN-MC 81.1 47.4 
88.1 93.2 
92.2 
-
CapsNet-1 81.5 48.1 
86.4 93.3 
91.8 
91.1 
CapsNet-2 82.4 48.7 
87.8 93.6 
92.9 
92.3 
-Orphan 81.9 48.3 
87.2 93.4 
92.6 
91.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Single-task results. Row "-Orphan category" 
denotes a variant of CapsNet-2 without orphan cate-
gory 

regularized by linguistic knowledge (LR-LSTM) 
(Qian et al., 2016), very deep network (VD-CNN) 
(Conneau et al., 2016), dynamic CNN (DCNN) 
(Kalchbrenner et al., 2014), CNN with multiple 
channels (CNN-MC) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Multi-task results of MCapsNet. In column 
Avg.△, we use BiLSTM as baseline and calculate the 
average improvements over it. 

</table></figure>

			<note place="foot" n="1"> We use padding to ensure the sizes of feature maps are equal.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledge</head><p>We appreciate the valuable comments from anony-mous reviewers. We also thank Xuan Luo for building and maintaining the GPU platforms. And this research was funded by Major State Re-search Development Program under Grant No. 2018YFC0830400.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06-05" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic multitask learning with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchun</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuyao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="1668" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sida</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Coling</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye Yi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="912" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjrn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjrn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjrn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Acl</title>
		<meeting>Acl</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Linguistically regularized lstms for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1611.03949</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sgaard</surname></persName>
		</author>
		<title level="m">Sluice networks: Learning what to share between loosely related tasks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A generalized recurrent neural architecture for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="3385" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
