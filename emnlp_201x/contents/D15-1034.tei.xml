<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composing Relationships with Translations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution">UTC CNRS</orgName>
								<address>
									<addrLine>Sorbonne universités, 770 Broadway New York, 112, avenue de Wagram</addrLine>
									<postCode>7253 60203, 75017</postCode>
									<settlement>Heudiasyc, Compì egne, Paris</settlement>
									<region>NY</region>
									<country>France, USA, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
							<email>abordes@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution">UTC CNRS</orgName>
								<address>
									<addrLine>Sorbonne universités, 770 Broadway New York, 112, avenue de Wagram</addrLine>
									<postCode>7253 60203, 75017</postCode>
									<settlement>Heudiasyc, Compì egne, Paris</settlement>
									<region>NY</region>
									<country>France, USA, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
							<email>usunier@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="institution">UTC CNRS</orgName>
								<address>
									<addrLine>Sorbonne universités, 770 Broadway New York, 112, avenue de Wagram</addrLine>
									<postCode>7253 60203, 75017</postCode>
									<settlement>Heudiasyc, Compì egne, Paris</settlement>
									<region>NY</region>
									<country>France, USA, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Composing Relationships with Translations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Performing link prediction in Knowledge Bases (KBs) with embedding-based models , like with the model TransE (Bordes et al., 2013) which represents relationships as translations in the embedding space, have shown promising results in recent years. Most of these works are focused on modeling single relationships and hence do not take full advantage of the graph structure of KBs. In this paper, we propose an extension of TransE that learns to explicitly model composition of relationships via the addition of their corresponding translation vectors. We show empirically that this allows to improve performance for predicting single relationships as well as compositions of pairs of them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Performing link prediction on multi-relational data is becoming essential in order to complete the huge amount of missing information of the knowl- edge bases. These knowledge can be formalized as directed multi-relation graphs, whose node cor- respond to entities connected with edges encod- ing various kind of relationships. We denote these connections via triples (head, label, tail). Link prediction consists in filling in incomplete triples like (head, label, ?) or (?, label, tail).</p><p>In this context, embedding models ( <ref type="bibr" target="#b7">Wang et al., 2014;</ref><ref type="bibr" target="#b4">Lin et al., 2015;</ref><ref type="bibr" target="#b3">Jenatton et al., 2012;</ref><ref type="bibr" target="#b6">Socher et al., 2013</ref>) that attempt to learn low- dimensional vector or matrix representations of entities and relationships have shown promising performance in recent years. In particular, the ba- sic model TRANSE ( <ref type="bibr" target="#b0">Bordes et al., 2013</ref>) has been proved to be very powerful. This model treats each relationship as a translation vector operating on the embedding representing the entities. Hence, for a triple (head, label, tail), the vector embed- dings of head and tail are learned so that they are connected through a translation parameterized by the vector associated with label. Many extensions have been proposed to improve the representation power of TRANSE while still keeping its simplic- ity, by adding some projections steps before the translation ( <ref type="bibr" target="#b7">Wang et al., 2014;</ref><ref type="bibr" target="#b4">Lin et al., 2015)</ref>.</p><p>In this paper, we propose an extension of TRANSE 1 that focuses on improving its represen- tation of the underlying graph of multi-relational data by trying to learn compositions of relation- ships as sequences of translations in the embed- ding space. The idea is to train the embeddings by learning simple reasonings, such as the rela- tionship people/nationality should give a similar result as the composition people/city of birth and city/country. In our approach, called RTRANSE, the training set is augmented with relevant ex- amples of such compositions by performing con- strained walks in the knowledge graph, and train- ing so that sequences of translations lead to the desired result. The idea of compositionality to model multi-relational data was previously intro- duced in <ref type="bibr" target="#b5">(Neelakantan et al., 2015)</ref>. That work composes relationships by means of recurrent neu- ral networks (RNN) (one per relationship) with non-linearities. However, we show that there is a natural way to compose relationships by simply adding translation vectors and not requiring ad- ditional parameters, which makes it specially ap- pealing because of its scalability.</p><p>We present experimental results that show the superiority of RTRANSE over TRANSE in terms of link prediction. A detailed evaluation, in which test examples are classified as easy or hard de- pending on their similarity with training data, highlights the improvement of RTRANSE on both categories. Our experiments include a new eval- uation protocol, in which the model is directly asked to answer questions related to compositions of relations, such as (head, label 1 , label 2 , ?). RTRANSE also achieves significantly better per- formances than TRANSE on this new dataset.</p><p>We describe RTRANSE in the next section, and present our experiments in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>The model we propose is inspired by TRANSE ( <ref type="bibr" target="#b0">Bordes et al., 2013)</ref>. In TRANSE, entities and relationships of a KB are mapped to low dimen- sional vectors, called embeddings. These embed- dings are learnt so that for each fact (h, , t) in the KB, we have h + ≈ t in the embedding space.</p><p>Using translations for relationships naturally leads to embed the composition of two relation- ships as the sum of their embeddings: on a path (h, , t), (t, , t ), we should have h++ ≈ t in the embedding space. The original TRANSE does not enforce that the embeddings accurately repro- duce such compositions. The recurrent TRANSE we propose here has a modified training stage to include such compositions. This should allow to model simple reasonings in the KB, such as peo- ple/nationality is similar to the composition of people/city of birth and city/country.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent TransE</head><p>We describe in this section our model in its full generality, which allows to deal with composi- tions of an arbitrary number of relationships, even though in this first work we experimented only with compositions of two relationships.</p><p>Triples that are the result of a compositions are denoted by (h, { i } p i=1 , t), where p is the number of relationships that are composed to go from h to t. Such a path means that there exist entities e 1 , ..., e p+1 , with e 1 = h and e p+1 = t such that for all k, (e k , k , e k+1 ) is a fact in the KB. Our model, RTRANSE for recurrent TRANSE, repre- sents each step s k (h, { i } p i=1 , t) along the path in the KB with the recurrence relationship (boldface characters denote embedding vectors i.e. h is the embedding vector of the entity h):</p><formula xml:id="formula_0">s 1 (h, { i } p i=1 , t) = h s k+1 (h, { i } p i=1 , t) = s k (h, { i } p i=1 , t) + k .</formula><p>Then, the energy of a triple is computed as</p><formula xml:id="formula_1">d(h, { i } p i=1 , t) = ||s p (h, { i } p i=1 , t) − t|| 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Path construction and filtering</head><p>The experience of the paper is motivated by learn- ing simple reasonings in the KB through the com- positions of relationships. Therefore, we restricted our analysis to paths of length 2 created as follows. First, for each fact (h, , t), retrieve all paths (h, { 1 , 2 }, t) such that there is e such that both (h, 1 , e) and (e, 2 , t) are in the KB. Then, we filter out paths where (h, 1 , e) = (h, , t) or (e, 2 , t) = (h, , t), as well as the paths with 1 = 2 and h = e = t. We focused on "unambiguous" paths, so that the reasoning might actually make sense. In par- ticular, we considered only paths where 1 is either a 1-to-1 or a 1-to-many relationship, and where 2 is either a 1-to-1 or a many-to-1 relationship. In our experiments, the paths created for training only consider the training subset of facts.</p><p>In the remainder of the paper, such paths of length 2 are called quadruples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training and regularization</head><p>Our training objective is decomposed in two parts: the first one is the ranking criterion on triples of TRANSE, ignoring quadruples. Paths are then taken into account through additional regulariza- tion terms.</p><p>Denoting by S the set of facts in the KB, the first part of the training objective is the following ranking criterion that operates on triples</p><formula xml:id="formula_2">(h,,,t)∈S (h ,,,t )∈S (h,,,t) γ + d(h, , t) − d(h , , t ) + ,</formula><p>where [x] + = max(x, 0) is the positive part of x, γ is a margin hyperparameter and S (h,,,t) is the set of corrupted triples created from (h, , t) by re- placing either h or t with another KB entity. This ranking loss effectively trains so that the embedding of the tail is the nearest neighbor of the translated head, but it does not guarantee that the distance between the tail and the translated head is small. The nearest neighbor criterion is sufficient to make inference over simple triples, but making sure that the distance is small is necessary for the composition rule to be accurate. In order to ac- count for the compositionality of relationships, we add two additional regularization terms: <ref type="table" target="#tab_0">FAMILY  FB15K  ENTITIES  721  14,951  RELATIONSHIPS  7  1,345  TRAINING TRIPLES  8,461 483,142  TRAINING QUAD.  - 30,252  VALIDATION TRIPLES  2,820  50,000  TEST TRIPLES  2,821  59,071  TEST QUAD.</ref> - 1,852  The first criterion only applies to original facts of the KB, while the second term applies to quadruples. N →{ 1 ,, 2 } , which involves both the relationships of the quadruple and the relation- ship from which it was created, is the num- ber of paths involving relationships { 1 , 2 } cre- ated from a fact involving , normalized by the total number of quadruples created from facts involving . This criterion puts more weight on paths that are reliable as an alternative for a relationship, for instance {people/city of birth, city/country} is likely a better alternative to peo- ple/nationality than {people/writer of the film, film/film release region}. Finally, a regularization term µ||e|| 2 2 is added for each entity embedding e.</p><formula xml:id="formula_3">• λ (h,,,t)∈S d(h, , t) 2 • α (h,{ 1 ,, 2 },t)∈S N →{ 1 ,, 2 } d(h, { 1 , 2 }, t) 2 . DATA SET</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>This section presents experiments on the bench- mark FB15K introduced in ( <ref type="bibr" target="#b0">Bordes et al., 2013)</ref> and on FAMILY, a slightly extended version of the artificial database described in <ref type="bibr" target="#b1">(García-Durán et al., 2014</ref>). <ref type="table" target="#tab_0">Table 1</ref> gives their statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Protocol</head><p>Data FB15K is a subset of Freebase, a very large database of generic facts gathering more than 1.2 billion triples and 80 million entities. Inspired by <ref type="bibr" target="#b2">(Hinton, 1986)</ref>, FAMILY is a database that contains triples expressing family relationships (cousin of, has ancestor, married to, parent of, related to, sibling of, uncle of) among the mem- bers of 5 families along 6 generations. This dataset is artificial and each family is organized in a lay- ered tree structure where each layer refers to a gen- eration. Families are connected among them by marriage links between two members, randomly sampled from the same layer of different fami- lies. Interestingly on this dataset, there are ob- vious compositional relationships like uncle of ≈ sibling of + parent of or parent of ≈ married to + parent of, among others.</p><p>Setting Our main comparison is TRANSE so we followed the same experimental setting as in <ref type="bibr" target="#b0">(Bordes et al., 2013</ref>), using ranking metrics for eval- uation. For each test triple we replaced the head by each of the entities in turn, and then computed the score of each of these candidates and sorted them. Since other positive candidates (i.e. entities forming true triples) can be ranked higher than the target one, we filtered out all the positive candi- dates existing in either the training, validation and test set, except the target one, from the ranking and then we kept the rank of the target entity. The same procedure is repeated but removing the tail instead of the head. The filtered mean rank (mean rank in the rest) is the average of these ranks, and the filtered Hits@10 (H@10 in the rest) is the pro- portion of target entities in the top 10 predictions. The embedding dimensions were set to 20 for FAMILY and 100 for FB15K. Training was per- formed by stochastic gradient descent, stopping after for 500 epochs. On FB15K, we used the embeddings of TRANSE to initialize RTRANSE, and we set a learning rate of 0.001 to fine-tune RTRANSE. On FAMILY, both algorithms were ini- tialized randomly and used a learning rate of 0.01. The mean rank was used as a validation criterion, and the values of γ, λ, α and µ were chosen re- spectively among {0.25, 0.5, 1}, {1e −4 , 1e −5 , 0}, {0.1, 0.05, 0.1, 0.01, 0.005} and {1e −4 , 1e −5 , 0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Overall performances Experiments on FAM- ILY show a quantitative improvement of the per- formance of RTRANSE : where TRANSE gets a mean rank of 6.7 and a H@5 of 68.7, RTRANSE get a performance of 6.3 and 72.3 respectively.</p><p>Similarly, on FB15K, <ref type="table" target="#tab_1">Table 2</ref> (last row) shows that training on longer paths (length 2 here) actu- ally consistently improves the performance while predicting heads and tails of triples only: the over- all H@10 improves by almost 5% from 71.5 for 288 example shows that answering that question by us- ing that path is sometimes difficult: the members of the cast of that TV show have different nation- alities, so RTRANSE lists the nationalities of these ones and the correct one is ranked third. TRANSE is again more affected than RTRANSE by the cas- cading error. In the third one, RTRANSE deducts the main region where malay is spoken from the continent of the country with the most number of speakers of that language. In the last two exam- ples, our model infers the location of those uni- versities by forcing an equivalence between their location and the location of their respective cam- pus.</p><p>Prediction performance For a more quantita- tive analysis, we have generated a new test dataset of link prediction on quadruples on FB15K. This test set was created by generating the paths from the usual test set (the triple test set) and remov- ing those quadruples that are used for training. We obtain 1,852 quadruples. The overall experimen- tal protocol is the same as before, trying to predict the head or tail of these quadruple in turn.</p><p>On that evaluation protocol, RTRANSE has a mean rank of 114.0 and a H@10 of 68.2%, while TRANSE obtains a mean rank of 159.9 and a H@10 of 65.2% (using the same models as in the previous subsection). We can see that learning on paths improves performances on both metrics, with a gain of 3% in terms of H@10 and an im- portant gain of about 46 in mean rank, which cor- responds to a relative improvement of about 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed to learn embeddings of compo- sitions of relationships in the translation model for link prediction in KBs. Our experimental results show that this approach is promising.</p><p>We considered in this work a restricted set of small paths of length two. We leave the study of more general paths to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistics of the datasets. 

MODEL 
TRANSE 

RTRANSE 

MR 
H@10 
MR 
H@10 
EASY 
17.7 
76.8 
12.5 
82.2 
HARD 
191.0 
48.9 
205.7 
51.0 
EASY W. COMP. 
16.4 
78.8 
11.6 
83.0 
EASY W/O COMP. 
21.6 
71.3 
16.0 
75.3 
HARD W. COMP. 
208.1 
46.8 
212.2 
49.3 
HARD W/O COMP. 122.9 
57.0 
123.8 
57.5 
OVERALL 
50.7 
71.5 
49.5 
76.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Detailed performances on FB15k of TRANSE and RTRANSE. H@10 are in %. W. COMP. indicates examples for which there exist quadruplets in train matching their relationship.</figDesc><table></table></figure>

			<note place="foot" n="1"> Code available in https://github.com/glorotxa/SME</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was carried out in the framework of the Labex MS2T (ANR-11-IDEX-0004-02), and was funded by the French National Agency for Re-search (EVEREST-12-JS02-005-01).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTRANSE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRANSE h: madtv</head><p>U.S.A. Ireland l 1 : regular TV appearance Ireland U.S.A. l 2 : nationality Japan U.K. h: stargate atlantis Hawaii Scotland l 1 : regular TV appearance Scotland Hawaii l 2 : nationality U.S.A. U.K. h: malay southeast asia taiwan l 1 : language/main country malaysia southeast asia l 2 : continent asia philippines h: indiana state university the hoosier state maryland l 1 : institution/campuses terre haute rhode island l 2 : location/state province region rhode island the constitution state h: university of victoria victoria kelowna l 1 : institution/campuses kurnaby toronto l 2 : location/citytown kelowna ottawa Detailed results In order to better understand the gains of RTRANSE, we performed a detailed evaluation on FB15K, by classifying the test triples along two axes: easy vs hard and with composition vs without composition. A test triple (h, , t) is easy if its head and tail are connected by a triple in the training set, i.e. if either (h, , t) or (t, , h) is seen in train for some relationship . Otherwise, the triple is hard. Orthogonally, the test triple (h, , t) is with composition if there is at least one path { 1 , 2 } for the relationship , regardless of the existence of that specific path be- tween the entities h and t. If no such path exists, (h, , t) is without composition.</p><p>The detailed results are shown in <ref type="table">Table 2</ref>. We can see that comparatively to TRANSE, RTANSE particularly improves performances in terms of H@10 on triples with composition, improving on easy triples by 4.2% (from 78.8% to 83,0%) and hard triples by 2.5% (from 46.8% to 49.3%). The main gains are still on easy triples, and in fact the H@10 on easy triples without composition in- creases by 4%, from 71.3% to 75.3%. The mean rank also considerably improves on easy triples, and stays somehow still on hard ones. All in all, the results show that considering paths during training very significantly improves performances, and the results on triples with composition suggest that RTRANSE is indeed capable of capturing the evidence of links that exist in longer paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on quadruples</head><p>While usual evaluations for link prediction in KBs focus on predicting a missing element of a test triple, we propose here to extend the evaluation to answering more complex questions, such as (h, { 1 , 2 }, ?) or (?, { 1 , 2 }, t).</p><p>Examples <ref type="table">Table 3</ref> presents examples of predic- tions of both TRANSE and our model RTRANSE on such quadruples. The two first examples try to predict the origin of two TV series from the na- tionality of the actors that regularly appear in them (regular tv appearance). In the first one, the amer- ican actor phil lamarr is the only entity connected to the american TV show madtv through the rela- tionship regular tv appearance. RTRANSE is able to correctly infer the country of origin from this information since it forces country of origin ≈ regular tv appearance + nationality. On the other side TRANSE is affected by the cascading error since the ranking loss does not guarantee that the distance between h + l 1 and phil lamarr is small, so when summing l 2 it eventually ends up closer to Ireland rather than USA. In contrast, the second</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcíadurán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective blending of two and threeway interactions for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning distributed representations of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference of the cognitive science society</title>
		<meeting>the eighth annual conference of the cognitive science society<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 25</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI&apos;15</title>
		<meeting>AAAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06662</idno>
		<title level="m">Compositional vector space models for knowledge base completion</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reasoning With Neural Tensor Networks For Knowledge Base Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the TwentyEighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
