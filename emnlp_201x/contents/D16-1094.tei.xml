<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University ♠ Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University ♠ Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University ♠ Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="944" to="949"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work we present a generalisation of the Modified Kneser-Ney interpolative smoothing for richer smoothing via additional discount parameters. We provide mathematical underpinning for the estimator of the new discount parameters, and showcase the utility of our rich MKN language models on several Euro-pean languages. We further explore the in-terdependency among the training data size, language model order, and number of discount parameters. Our empirical results illustrate that larger number of discount parameters , i) allows for better allocation of mass in the smoothing process, particularly on small data regime where statistical sparsity is severe , and ii) leads to significant reduction in perplexity, particularly for out-of-domain test sets which introduce higher ratio of out-of-vocabulary words. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Probabilistic language models (LMs) are the core of many natural language processing tasks, such as machine translation and automatic speech recogni- tion. m-gram models, the corner stone of language modeling, decompose the probability of an utter- ance into conditional probabilities of words given a fixed-length context. Due to sparsity of the events in natural language, smoothing techniques are criti- cal for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is small, e.g. building language models for translation or speech recognition in low-resource languages.</p><p>A widely used and successful smoothing method is interpolated Modified Kneser-Ney (MKN) <ref type="bibr" target="#b0">(Chen and Goodman, 1999)</ref>. This method uses a linear in- terpolation of higher and lower order m-gram prob- abilities by preserving probability mass via absolute discounting. In this paper, we extend MKN by in- troducing additional discount parameters, leading to a richer smoothing scheme. This is particularly im- portant when statistical sparsity is more severe, i.e., in building high-order LMs on small data, or when out-of-domain test sets are used.</p><p>Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more com- pact <ref type="bibr" target="#b7">(Stolcke et al., 2011;</ref><ref type="bibr" target="#b2">Heafield, 2011;</ref><ref type="bibr" target="#b6">Shareghi et al., 2015</ref>) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs <ref type="bibr" target="#b9">(Teh, 2006a;</ref><ref type="bibr" target="#b10">Teh, 2006b</ref>) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an ap- proximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference.</p><p>We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European languages in in-and out- of-domain settings. The results show that our dis- counting mechanism significantly improves the per- plexity compared to MKN and offers a more elegant way of dealing with out-of-vocabulary (OOV) words and domain mismatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Enriched Modified Kneser-Ney</head><p>Interpolative Modified Kneser-Ney (MKN) <ref type="bibr" target="#b0">(Chen and Goodman, 1999</ref>) smoothing is widely accepted as a state-of-the-art technique and is implemented in leading LM toolkits, e.g., SRILM <ref type="bibr" target="#b8">(Stolcke, 2002)</ref> and <ref type="bibr">KenLM (Heafield, 2011)</ref>.</p><p>MKN uses lower order k-gram probabilities to smooth higher order probabilities. P (w|u) is de- fined as,</p><formula xml:id="formula_0">c(uw) − D m (c(uw)) c(u) + γ(u) c(u) × ¯ P (w|π(u))</formula><p>where c(u) is the frequency of the pattern u, γ(.) is a constant ensuring the distribution sums to one, and ¯ P (w|π(u)) is the smoothed probability computed recursively based on a similar formula 2 conditioned on the suffix of the pattern u denoted by π(u). Of particular interest are the discount parameters D m (.) which remove some probability mass from the max- imum likelihood estimate for each event which is redistributed over the smoothing distribution. The discounts are estimated as</p><formula xml:id="formula_1">D m (i) =        0, if i = 0 1 − 2 n 2 [m] n 1 [m] n 1 [m] n 1 [m]+2n 2 [m] , if i = 1 2 − 3 n 3 [m] n 2 [m] n 1 [m] n 1 [m]+2n 2 [m] , if i = 2 3 − 4 n 4 [m] n 3 [m] . n 1 [m] n 1 [m]+2n 2 [m] , if i ≥ 3</formula><p>where n i (m) is the number of unique m-grams 3 of frequency i. This effectively leads to three discount parameters {D m (1), D m (2), D m (3+)} for the distri- butions on a particular context length, m. <ref type="bibr" target="#b5">Ney et al. (1994)</ref> characterized the data sparsity us- ing the following empirical inequalities,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generalised MKN</head><formula xml:id="formula_2">3n 3 [m] &lt; 2n 2 [m] &lt; n 1 [m] for m ≤ 3</formula><p>It can be shown (see Appendix A) that these em- pirical inequalities can be extended to higher fre- 2 Note that in all but the top layer of the hierarchy, con- tinuation counts, which count the number of unique contexts, are used in place of the frequency counts <ref type="bibr" target="#b0">(Chen and Goodman, 1999</ref>).</p><p>3 Continuation counts are used for the lower layers.</p><p>quencies and larger contexts m &gt; 3,</p><formula xml:id="formula_3">(N − m)n N −m [m] &lt; ... &lt; 2n 2 [m] &lt; n 1 [m] &lt; i&gt;0 n i [m] n 0 [m] &lt; σ m</formula><p>where σ m is the possible number of m-grams over a vocabulary of size σ, n 0 [m] is the number of m- grams that never occurred, and i&gt;0 n i <ref type="bibr">[m]</ref> is the number of m-grams observed in the training data.</p><p>We use these inequalities to extend the discount depth of MKN, resulting in new discount parame- ters. The additional discount parameters increase the flexibility of the model in altering a wider range of raw counts, resulting in a more elegant way of as- signing the mass in the smoothing process. In our experiments, we set the number of discounts to 10 for all the levels of the hierarchy, (compare this to these in MKN). <ref type="bibr">4</ref> This results in the following esti- mators for the discounts,</p><formula xml:id="formula_4">D m (i) =    0, if i = 0 i − (i + 1) n i+1 [m] n i [m] n 1 [m] n 1 [m]+2n 2 [m] , if i &lt; 10 10 − 11 n 11 [m] n 10 [m] . n 1 [m] n 1 [m]+2n 2 [m] , if i ≥ 10</formula><p>It can be shown that the above estimators for our dis- count parameters are derived by maximizing a lower bound on the leave-one-out likelihood of the training set, following <ref type="bibr" target="#b5">(Ney et al., 1994;</ref><ref type="bibr" target="#b0">Chen and Goodman, 1999</ref>) (see Appendix B for the proof sketch).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We compare the effect of using different numbers of discount parameters on perplexity using the Finnish (FI), Spanish (ES), German (DE), English (EN) por- tions of the Europarl v7 ( <ref type="bibr" target="#b4">Koehn, 2005)</ref> corpus. For each language we excluded the first 10K sentences and used it as the in-domain test set (denoted as EU), skipped the second 10K sentences, and used the rest as the training set. The data was tokenized, sentence split, and the XML markup discarded. We tested the effect of domain mismatch, under two settings for out-of-domain test sets: i) mild using the Span- ish section of news-test 2013, the German, English sections of news-test 2014, and the Finnish section</p><formula xml:id="formula_5">Perplexity size (M) size (K) MKN (D 1...3 ) MKN (D [1...4] ) MKN (D [1..</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.10] ) Training tokens sents</head><p>Test tokens sents OOV% m = 2 m = 5 m = 10 m = 2 m = 5 m = 10 m = 2 m = 5 m = 10   of news-test 2015 (all denoted as NT) <ref type="bibr">5</ref> , and ii) ex- treme using a 24 hour period of streamed Finnish, and Spanish tweets 6 (denoted as TW), and the Ger- man and English sections of the patent description of medical translation task 7 (denoted as MED). See <ref type="table" target="#tab_1">Table 1</ref> for statistics of the training and test sets.  <ref type="bibr">.10]</ref> ). This effect is consistent across the Europarl corpora, and for all LM orders. We observe a substantial improve- ments even for m = 10-gram models (see <ref type="figure" target="#fig_0">Figure 1</ref>). On the medical test set which has 9 times higher OOV ratio, the perplexity reduction shows a simi- lar trend. However, these reductions vanish when an in-domain test set is used. Note that we use the same treatment of OOV words for computing the perplex- ities which is used in KenLM (Heafield, 2013).</p><note type="other">.3 ES 68.0 2.2 EU 281.5 10 2.4 92.7 51.5 51.1 92.8 51.5 51.1 92.8 51.4 51.0 TW 3141.3 293 78.5 17 804.2 14 062.7 14 027.1 17 121.4 13 487.4 13 454.1 14 915.7 11 832.1 11 807.2 NT 64.5 3 18.7 2190.7 1784.6 1781.8 2158.9 1755.8 1753.2 2065.3 1680.6 1678.3 DE 61.2 2.3 EU 244.0 10 4.6 156.9 91.7 91.2 156.9 91.6 91.2 156.4 91.7 91.2 MED 317.7 10 59.8 5135.7 4232.4 4226.7 5007.5 4123.0 4117.5 4636.0 3831.2 3826.6 NT 69.5 3 5.5 1089.2 875.0 872.2 1071.1 857.2 854.4 1011.5 806.7 804.4 EN 67.5 2.2 EU 274.9 10 1.7 90.1 48.4 48.1 90.1 48.3 48.0 90.5 48.3 48.0 MED 405.9 10 44.1 2319.7 1947.9 1942.5 2261.6 1893.3 1888.2 2071.9 1734.9 1730.8</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perplexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>Out-of-domain and Out-of-vocabulary We se- lected the Finnish language for which the number and ratio of OOVs are close on its out-of-domain and in-domain test sets (NT and EU), while show- ing substantial reduction in perplexity on out-of- domain test set, see FI bars on <ref type="figure" target="#fig_0">Figure 1</ref>.     m grows. This indicates that having more discount parameters is not only useful for test sets with ex- tremely high number of OOV, but also allows for a more elegant way of assigning mass in the smooth- ing process when there is a domain mismatch.</p><p>Interdependency of m, data size, and discounts To explore the correlation between these factors we selected the German and investigated this correla- tion on two different training data sizes: Europarl (61M words), and CommonCrawl 2014 (984M words). <ref type="figure">Figure 3</ref> illustrates the correlation between these factors using the same test set but with small and large training sets. Considering the slopes of the surfaces indicates that the small training data regime (left) which has higher sparsity, and more OOV in the test time benefits substantially from the more ac- curate discounting compared to the large training set (right) in which the gain from discounting is slight. 8 the natural language follow the power law <ref type="bibr" target="#b1">(Clauset et al., 2009</ref>), p C(u) = f ∝ f −1− 1 sm , where s m is the parameter of the distribution, and C(u) is the random variable denoting the frequency of the m- grams pattern u. We now compute the expected number of unique patterns having a specific fre- quency E[n i <ref type="bibr">[m]</ref>]. Corresponding to each m-grams pattern u, let us define a random variable X u which is 1 if the frequency of u is i and zero otherwise. It is not hard to see that n i [m] = u X u , and</p><formula xml:id="formula_6">E n i [m] = E u X u = u E[X u ] = σ m E[X u ] = σ m p C(u) = i × 1 + p C(u) = i × 0 ∝ σ m i −1− 1 sm .</formula><p>We can verify that</p><formula xml:id="formula_7">(i + 1)E n i+1 [m] &lt; iE n i [m] ⇔ (i + 1)σ m (i + 1) −1− 1 sm &lt; iσ m i −1− 1 sm ⇔ i 1 sm &lt; (i + 1) 1 sm .</formula><p>which completes the proof of the inequalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discount bounds proof sketch</head><p>The leave-one-out (leaving those m-grams which occurred only once) log-likelihood function of the interpolative smoothing is lower bounded by back- off model's ( <ref type="bibr" target="#b5">Ney et al., 1994)</ref>, hence the estimated discounts for later can be considered as an approx- imation for the discounts of the former. Consider a backoff model with absolute discounting parameter D, were P (w i |w i−1 i−m+1 ) is defined as:</p><formula xml:id="formula_8">         c(w i i−m+1 )−D c(w i−1 i−m+1 ) if c(w i i−m+1 ) &gt; 0 Dn1+(w i−1 i−m+1 · ) c(w i−1 i−m+1 ) ¯ P (w i |w i−1 i−m+2 ) if c(w i i−m+1 ) = 0</formula><p>where n 1+ (w i−1 i−m+1 · ) is the number of unique right contexts for the w i−1 i−m+1 pattern. Assume that for any choice of 0 &lt; D &lt; 1 we can define ¯ P such that P (w i |w i−1 im+1 ) sums to 1. For readability we use the λ(</p><formula xml:id="formula_9">w i−1 i−m+1 ) = n 1+ (w i−1 i−m+1 · ) c(w i−1 i−m+1 )−1 replacement.</formula><p>Following <ref type="bibr" target="#b0">(Chen and Goodman, 1999</ref>), rewriting the leave-one-out log-likelihood for <ref type="bibr">KN (Ney et al., 1994)</ref> to include more discounts (in this proof up to D 4 ), results in:</p><formula xml:id="formula_10">w i i−m+1 c(w i i−m+1 )&gt;4</formula><p>c(w i i−m+1 ) log c(w i i−m+1 ) − 1 − D 4 c(w i−1 i−m+1 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Percentage of perplexity reduction for pplx D [1...4] and pplx D [1...10] compared with pplx D [1..3] on different training corpora (Europarl CZ, FI, ES, DE, EN, FR) and on news-test sets (NT) for m = 2 (left), and m = 10 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>left), shows the full perplexity results for Finnish for vanilla MKN, and our extensions when tested on in-domain (EU) and out-of-domain (NT) test sets. The discount plot, Figure 2 (middle) illustrates the behaviour of the various discount parameters. We also measured the average hit length for queries by varying m on in-domain and out-of-domain test sets. As illustrated in Figure 2 (right) the in-domain test set allows for longer matches to the training data as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Statistics for the Finnish section of Europarl. The left plot illustrates the perplexity when tested on an outof-domain (NT) and in-domain (EU) test sets varying LM order, m. The middle plot shows the discount parameters D i∈[1...10] for different m-gram orders. The right plot correspond to average hit length on EU and NT test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>)</head><label></label><figDesc>− 1 − D 4 )+ 4 j=2 jn j [m] log(j − 1 − D j−1 ) + n 1 [m] log( 4 j=1 n j [m]D j ) + const To find the optimal D 1 , D 2 , D 3 , D 4 we set the par- tial derivatives to zero. For D 3 , ∂ ∂D 3 = n 1 [m] n 3 [m] 4 j=1 n j [m]D j − 4n 4 [m] 3 − D 3 = 0 ⇒ n 1 [m]n 3 [m](3 − D 3 ) = 4n 4 [m] 4 j=1 n j [m]D j ⇒ 3n 1 [m]n 3 [m] − D 3 n 1 [m]n 3 [m] − 4n 4 [m]n 1 [m]D 1 &gt; 0 ⇒ 3 − 4 n 4 [m] n 3 [m] D 1 &gt; D 3 And after taking c(w i i−m+1 ) = 5 out of the summa- tion, for D 4 :⇒ n 1 [m]n 4 [m](4 − D 4 ) &gt; 5n 5 [m] 4 j=1 n j [m]D j ⇒ 4 − 5 n 5 [m] n 4 [m] D 1 &gt; D 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Perplexity for various m-gram orders m ∈ 2, 3, 10 and training languages from Europarl, using different 
numbers of discount parameters for MKN. MKN (D [1...3] ), MKN (D [1...4] ), MKN (D [1...10] ) represent vanilla MKN, 
MKN with 1 more discounts, and MKN with 7 more discount parameters, respectively. Test sets sources EU, NT, 
TW, MED are Europarl, news-test, Twitter, and medical patent descriptions, respectively. OOV is reported as the ratio 

|{OOV ∈test-set}| 

|{w∈test-set}| . 

0 

1 

2 

4 

8 

14 

CZ 
FI 
ES DE EN FR 
CZ 
FI 
ES DE EN FR 

[Corpus] 

[%Perplexity Reduction] 

ppl D[1...10] 
ppl D[1...4] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>substantial reduction in perplexity on 
all languages for out-of-domain test sets when ex-
panding the number of discount parameters from 3 
in vanilla MKN to 4 and 10. Consider the English 

5 http://www.statmt.org/{wmt13,14,15}/test.tgz 
6 Streamed via Twitter API on 17/05/2016. 
7 http://www.statmt.org/wmt14/medical-task/ 

</table></figure>

			<note place="foot" n="1"> For the implementation see: https://github.com/ eehsan/cstlm</note>

			<note place="foot" n="4"> We have selected the value of 10 arbitrarily; however our approach can be used with larger number of discount parameters, with the caveat that we would need to handle sparse counts in the higher orders.</note>

			<note place="foot" n="8"> Nonetheless, the improvement in perplexity consistently grows with introducing more discount parameters even under 4 Conclusions In this work we proposed a generalisation of Modified Kneser-Ney interpolative language modeling by introducing new discount parameters. We provide the mathematical proof for the discount bounds used in Modified Kneser-Ney and extend it further and illustrate the impact of our extension empirically on different Europarl languages using in-domain and out-of-domain test sets. The empirical results on various training and test sets show that our proposed approach allows for a more elegant way of treating OOVs and mass assignments in interpolative smoothing. In future work, we will integrate our language model into the Moses machine translation pipeline to intrinsically measure its impact on translation qualities, which is of particular use for out-of-domain scenario.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the National ICT Australia (NICTA) and Australian Research Council Future Fellowship (project number FT130101105). This work was done when Ehsan Shareghi was an intern at IBM Research Australia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Inequalities</head><p>We prove that these inequalities hold in expectation by making the reasonable assumption that events in the large training data regime, which suggests that more dis-count parameters, e.g., up to D30, may be required for larger training corpus to reflect the fact that even an event with fre-quency of 30 might be considered rare in a corpus of nearly 1 billion words.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Power-law distributions in empirical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cosma</forename><forename type="middle">Rohilla</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="661" to="703" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation</title>
		<meeting>the Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient Language Modeling Algorithms with Applications to Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Translation summit</title>
		<meeting>the Machine Translation summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependences in stochastic language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compact, efficient and unlimited capacity: Language modeling with compressed suffix trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SRILM at sixteen: Update and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop</title>
		<meeting>IEEE Automatic Speech Recognition and Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Spoken Language Processing</title>
		<meeting>the International Conference of Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Bayesian interpretation of interpolated Kneser-Ney</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>NUS School of Computing</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian language model based on Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
