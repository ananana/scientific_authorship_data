<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical CVAE for Fine-Grained Hate Speech Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Elsherief</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Belding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>Santa Barbara Santa Barbara</addrLine>
									<postCode>93106</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical CVAE for Fine-Grained Hate Speech Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3550" to="3559"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3550</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing work on automated hate speech detection typically focuses on binary classification or on differentiating among a small set of categories. In this paper, we propose a novel method on a fine-grained hate speech classification task, which focuses on differentiating among 40 hate groups of 13 different hate group categories. We first explore the Conditional Variational Autoencoder (CVAE) (Larsen et al., 2016; Sohn et al., 2015) as a discriminative model and then extend it to a hierarchical architecture to utilize the additional hate category information for more accurate prediction. Experimentally, we show that incorporating the hate category information for training can significantly improve the classification performance and our proposed model outperforms commonly-used discrimi-native models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The impact of the vast quantities of user-generated web content can be both positive and negative. While it improves information accessibility, it also can facilitate the propagation of online harass- ment, such as hate speech. Recently, the Pew Research Center 1 reported that "roughly four-in- ten Americans have personally experienced online harassment, and 63% consider it a major prob- lem. Beyond the personal experience, two-thirds of Americans reported having witnessed abusive or harassing behavior towards others online."</p><p>In response to the growth in online hate, there has been a trend of developing automatic hate speech detection models to alleviate online harass- ment ( <ref type="bibr" target="#b19">Warner and Hirschberg, 2012;</ref><ref type="bibr" target="#b20">Waseem and Hovy, 2016)</ref>. However, a common problem with these methods is that they focus on coarse-grained classifications with only a small set of categories. To the best of our knowledge, the existing work on hate speech detection formulates the task as a classification problem with no more than seven classes. Building a model for more fine-grained multiclass classification is more challenging since it requires the model to capture finer distinctions between each class.</p><p>Moreover, fine-grained classification is neces- sary for fine-grained hate speech analysis. <ref type="figure" target="#fig_0">Fig- ure 1</ref> is a portion of the hate group map published by the Southern Poverty Law Center (SPLC) <ref type="bibr">2</ref> , where a hate group is defined as "an organization that based on its official statements or principles, the statements of its leaders, or its activities has beliefs or practices that attack or malign an en- tire class of people, typically for their immutable characteristics." The SPLC divides the 954 hate groups in the United States into 17 categories ac-cording to their ideologies (e.g. Racist Skinhead, Anti-Immigrant, and others). The SPLC monitors hate groups throughout the United States by a va- riety of methodologies to determine the activities of groups and individuals: reviewing hate group publications and reports by citizens, law enforce- ment, field sources and the news media, and con- ducting their own investigations. Therefore, build- ing automatic classification models to differentiate between the social media posts from different hate groups is both challenging and of practical signif- icance.</p><p>In this paper, we propose a fine-grained hate speech classification task that separates tweets posted by 40 hate groups of 13 different hate group categories. Although CVAE is commonly used as a generative model, we find it can achieve compet- itive results and tends to be more robust when the size of the training dataset decreases, compared to the commonly used discriminative neural network models. Based on this insight, we design a Hierar- chical CVAE model (HCVAE) for this task, which leverages the additional hate group category (ide- ology) information for training. Our contributions are three-fold:</p><p>• This is the first paper on fine-grained hate speech classification that attributes hate groups to individual tweets.</p><p>• We propose a novel Hierarchical CVAE model for fine-grained tweet hate speech classification.</p><p>• Our proposed model improves the Micro-F1 score of up to 10% over the baselines.</p><p>In the next section, we outline the related work on hate speech detection, fine-grained text classifica- tion, and Variational Autoencoder. In Section 3, we explore the CVAE as a discriminative model, and our proposed method is described in Section 4. Experimental results are presented and discussed in Section 5. Finally, we conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hate Speech Detection</head><p>An extensive body of work has been dedicated to automatic hate speech detection. Most of the work focuses on binary classification. <ref type="bibr" target="#b19">Warner and Hirschberg (2012)</ref> differentiate between anti- semitic or not. <ref type="bibr" target="#b4">Gao et al. (2017)</ref>, <ref type="bibr" target="#b26">Zhong et al. (2016)</ref>, and <ref type="bibr" target="#b11">Nobata et al. (2016)</ref> differentiate be- tween abusive or not. <ref type="bibr" target="#b20">Waseem and Hovy (2016)</ref>, <ref type="bibr" target="#b1">Burnap and</ref><ref type="bibr" target="#b1">Williams (2016), and</ref><ref type="bibr" target="#b2">Davidson et al. (2017)</ref> focus on three-way classification. <ref type="bibr" target="#b20">Waseem and Hovy (2016)</ref> classify each input tweet as racist hate speech, sexist hate speech, or neither. <ref type="bibr" target="#b1">Burnap and Williams (2016)</ref> build classifiers for hate speech based on race, sexual orientation or disability, while <ref type="bibr" target="#b2">Davidson et al. (2017)</ref> train a model to differentiate among three classes: con- taining hate speech, only offensive language, or neither. <ref type="bibr" target="#b0">Badjatiya et al. (2017)</ref> use the dataset pro- vided by <ref type="bibr" target="#b20">Waseem and Hovy (2016)</ref> to do three- way classification. Our work is most closely re- lated to <ref type="bibr">(Van Hee et al., 2015)</ref>, which focuses on fine-grained cyberbullying classification. How- ever, this study only focuses on seven categories of cyberbullying while our dataset consists of 40 classes. Therefore, our classification task is much more fine-grained and challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fine-grained Text Classification</head><p>Our work is also related to text classification. Con- volutional Neural Networks (CNN) have been suc- cessfully applied to the text classification task. <ref type="bibr" target="#b7">Kim (2014)</ref>  We generate from the category-level representa- tions to the group-level representations. Moreover, the most commonly used datasets by these works (Yelp reviews, Yahoo answers, AGNews, IMDB reviews ( <ref type="bibr" target="#b3">Diao et al., 2014)</ref>) have no more than 10 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Variational Autoencoder</head><p>Variational Autoencoder (VAE) <ref type="bibr" target="#b8">(Kingma and Welling, 2013)</ref> has achieved competitive results in x is the embed- ded input text. y g is the ground truth hate group label. ˆ y g is the output prediction of the hate group. p is the posterior distribution of the latent variable z while p is the prior distribution of z. Note that this structure is used for training. During testing, the posterior network is replaced by the prior network to computê y g and thus y g is not available during testing. Refer to Section 3 for detailed explanation. many complicated generation tasks, such as hand- written digits <ref type="bibr" target="#b8">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b13">Salimans et al., 2015)</ref>, faces <ref type="bibr" target="#b8">(Kingma and Welling, 2013;</ref><ref type="bibr" target="#b12">Rezende et al., 2014)</ref>, and machine transla- tion ( <ref type="bibr" target="#b23">Zhang et al., 2016</ref>). CVAE ( <ref type="bibr" target="#b10">Larsen et al., 2016;</ref><ref type="bibr" target="#b15">Sohn et al., 2015</ref>) is an extension of the original VAE framework that incorporates condi- tions during generation. In addition to image gen- eration, CVAE has also been successfully applied to several NLP tasks, such as dialog generation ( <ref type="bibr" target="#b25">Zhao et al., 2017)</ref>. Although so far CVAE has always been used as a generative model, we ex- plore the performance of the CVAE as a discrim- inative model and further propose a hierarchical CVAE model, which exploits the hate group cate- gory (ideology) information for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CVAE Baseline</head><p>We formulate our classification task as the follow- ing equation:</p><formula xml:id="formula_0">Obj = (x,yg)∈X log p(y g |x)<label>(1)</label></formula><p>where x, y g are the tweet text and hate group label respectively, X is the dataset. Instead of directly parameterizing p(y g |x), it can be further written as the following equation:</p><formula xml:id="formula_1">p(y g |x) = z p(y g |z, x)p(z|x)dz (2)</formula><p>where z is the latent variable. Since the integration over z is intractable, we instead try to maximize the corresponding evidence lower bound (ELBO):</p><formula xml:id="formula_2">ELBO =E[log p(y g |z, x)]− D KL [q(z|x, y g )||p(z|x)]<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">D KL is the KullbackLeibler (KL) diver- gence. p(y g |z, x) is the likelihood distribution, q(z|x, y g</formula><p>) is the posterior distribution, and p(z|x) is the prior distribution. These three distributions are parameterized by p ϕ (y g |z, x), q α (z|x, y g ), and p β (z|x). Therefore, the training loss function is:</p><formula xml:id="formula_4">L =L REC + L KL =E z∼pα(z|x,yg) [− log p ϕ (y g |z, x)]+ D KL [q α (z|x, y g )||p β (z|x)]<label>(4)</label></formula><p>The above loss function consists of two parts. The first part L REC is the reconstruction loss. Opti- mizing L REC can push the predictions made by the posterior network and the likelihood network closer to the ground truth labels. The second part L KL is the KL divergence loss. Optimizing it can push the output distribution of the prior network and that of the posterior network closer to each other, such that during testing, when the ground truth label y g is no longer available, the prior net- work can still output a reasonable probability dis- tribution over z. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the structure of the CVAE model during training (the structure used for testing is different). The likelihood network p ϕ (y g |z, x) is a Multilayer Perceptron (MLP). The structure of both the posterior network q α (z|x, y g ) and the prior network p β (z|x) is a bidirectional Long Short-Term Memory (Bi-LSTM) (Hochre- iter and Schmidhuber, 1997) layer followed by an MLP. The Bi-LSTM is used to encode the tweet text. The only difference between the posterior and the prior network is that for the posterior net- work the input for the MLP is the encoded text concatenated with the group label while for the prior network only the encoded text is fed forward. During testing, the posterior network will be re- placed by the prior network to generate the distri- bution over the latent variable (i.e. p will replace p). Thus during testing, the ground truth labels will not be used to make predictions.</p><p>We assume the latent variable z has a multi- variate Gaussian distribution: p = N (µ, Σ) for the posterior network, and p = N (µ , Σ ) for the prior network. The detailed computation process is as follows:</p><formula xml:id="formula_5">e = f (x) (5) µ, Σ = s(y g ⊕ e) (6) µ , Σ = s (f (x))<label>(7)</label></formula><p>where f is the Bi-LSTM function and e is the out- put of the Bi-LSTM layer at the last time step. s is the function of the MLP in the posterior net- work and s is that of the MLP in the prior net- work. The notation ⊕ means concatenation. The latent variables z and z are randomly sampled from N (µ, Σ) and N (µ , Σ ), respectively. Dur- ing training, the input for the likelihood network is z:</p><formula xml:id="formula_6">ˆ y g = w(z)<label>(8)</label></formula><p>where w is the function of the MLP in the likeli- hood network. During testing, the prior network will substitute for the posterior network. Thus for testing, the input for the likelihood is z instead of z:</p><formula xml:id="formula_7">ˆ y g = w(z )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>One problem with the above CVAE method is that it utilizes the group label for training, but ignores the available hate group category (ideology) infor- mation of the hate speech. As mentioned in Sec- tion 1, hate groups can be divided into different categories in terms of ideologies. Each hate group belongs to a specific hate group category. Con- sidering this hierarchical structure, the hate cate- gory information can potentially help the model to better capture the subtle differences between the hate speech from different hate groups. Therefore, we extend the baseline CVAE model to incorpo- rate the category information. In this case, the ob- jective function is as follows:</p><formula xml:id="formula_8">Obj = (x,yc,yg)∈X log p(y c , y g |x) = (x,yc,yg)∈X log p(y c |x)+log p(y g |x, y c )<label>(10)</label></formula><p>where y c is the hate group category label and</p><formula xml:id="formula_9">p(y c |x) = zc p(y c |z c , x)p(z c |x)dz c (11) p(y g |x, y c ) = zg p(y g |z g ,x,y c )p(z g |x,y c )dz g (12)</formula><p>where z c and z g are latent variables. Therefore, the ELBO of our method is the sum of the ELBOs of log p(y c |x) and log p(y g |x, y c ):</p><formula xml:id="formula_10">ELBO =E[log p(y c |z c , x)]− D KL [q(z c |x, y c )||p(z c |x)]+ E[log p(y g |z g , x, y c )]− D KL [q(z g |x, y c , y g )||p(z g |x, y c )]<label>(13)</label></formula><p>During testing, the prior networks will substi- tute the posterior networks and the ground truth labels y c and y g are not utilized. Hence the prior p(z g |x, y c ) in the above equation cannot be parametrized as a network that directly takes the ground truth label y c and x as inputs. Instead, we parameterize it as shown in the right part of <ref type="figure" target="#fig_2">Fig- ure 3</ref>. We assume that u is trained to be a latent representation of y c , so we use u and x as inputs for this prior network. According to the ELBO above, the correspond- ing loss function L is the combination of the loss function for the category classification (L c ) and that for the group classification (L g ). <ref type="figure" target="#fig_2">Figure 3</ref> shows the structure of our model for training. By assuming the latent variables z c and z g have multivariate Gaussian distributions, the actual outputs of the posterior and prior networks are the mean and variance:</p><formula xml:id="formula_11">L =L c + L g , where</formula><formula xml:id="formula_12">L c =E zc∼qα(zc|x,yc) [− log p ϕ (y c |z c , x)]+ D KL [q α (z c |x, y c )||p β (z c |x)], and L g =E zg∼qη(zg|x,yc,yg) [− log p θ (y g |z g , x, y c )]+ D KL [q η (z g |x, y c , y g )||p γ (z g |x, u)]<label>(14)</label></formula><formula xml:id="formula_13">p c = N (µ c , Σ c ), p g = N (µ g , Σ g</formula><p>) for the two posterior networks, and</p><formula xml:id="formula_14">p c = N (µ c , Σ c ), p g = N (µ g , Σ g</formula><p>) for the two prior networks. Note that in addition to these four distributions, there is another distribution p x = N (µ x , σ x ) as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. This distribu- tion is generated only with the input text x, so it provides the basic distribution (in order to avoid confusion, we call it the basic distribution instead of the prior distribution) for both two posterior net- works and two prior networks. With this basic dis- tribution, the two posterior networks only need to capture the additional signals learned from x and labels. Similarly, the prior networks only need to learn the additional signals learned by the poste- rior networks. The detailed computation process during training is shown as the following equa- tions:</p><formula xml:id="formula_15">e = f (x) (15) µ x , Σ x = s x (e) (16) µ c , Σ c = s c (y c ⊕ e) (17) µ g , Σ g = s g (y c ⊕ y g ⊕ e) (18) µ c , Σ c = s c (f c (x))<label>(19)</label></formula><p>where f is the Bi-LSTM function and e is the out- put of the Bi-LSTM layer at the last time step. s x , s c , s g , and s c are the functions of four different MLPs. f c is the Bi-LSTM function in the prior network p β (z c |x). The latent variables z x , z c , z g , and z c are randomly sampled from the Gaussian distributions N (µ x , Σ x ), N (µ c , Σ c ), N (µ g , Σ g ), and N (µ c , Σ c ) respectively. As mentioned above,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Train &amp; Test Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1: function TRAIN(X) 2: randomly initialize network parameters; 3:</head><p>for epoch = 1, E do 4:</p><p>for (text, category, group) in X do 5:</p><p>get embeddings x and one-hot vectors yc, yg; 6:</p><p>compute e with the Bi-LSTM; 7:</p><p>compute µx, Σx, µc, Σc, µg, Σg; 8:</p><p>sample zx = reparameterize(µx, Σx); 9:</p><p>sample zc = reparameterize(µc, Σc); 10:</p><p>sample zg = reparameterize(µg, Σg); 11: u = zx + zc; 12: v = u + zg; 13: computê yc andˆygandˆ andˆyg according to <ref type="bibr">Eq. 24-25; 14</ref>:</p><formula xml:id="formula_16">LREC = BCE( ˆ yc, yc) + BCE( ˆ yg, yg); 15: compute µ c , Σ c ; 16: sample z c = reparameterize(µ c , Σ c ); 17: u = zx + z c ; 18: compute µ g , Σ g ; 19: LKL = DKL(pc||p c ) + DKL(pg||p g ); 20: L = LKL + LREC ; 21:</formula><p>update network parameters on L;</p><note type="other">22: end for 23: end for 24: end function 25: 26: function TEST(X) 27: for text in X do 28: get embeddings x; 29:</note><p>compute e with the Bi-LSTM; 30:</p><formula xml:id="formula_17">compute µx, Σx, µ c , Σ c ; 31: sample zx = reparameterize(µx, Σx); 32: sample z c = reparameterize(µ c , Σ c ); 33: u = zx + z c ; 34: compute µ g , Σ g ; 35: sample z g = reparameterize(µ g , Σ g ); 36: v = u + z g ; 37:</formula><p>computê yc andˆygandˆ andˆyg according to <ref type="bibr">Eq. 26-28; 38:</ref> end for 39: end function p x is the basic distribution while p c , p g , p c , and p g are trained to capture the additional signals. Therefore, z x is added to z c and z c , then the re- sults u and u are further added to z g and z g , re- spectively, as shown in the following equations:</p><formula xml:id="formula_18">u = z x + z c (20) µ g , Σ g = s g (u ⊕ f g (x))<label>(21)</label></formula><formula xml:id="formula_19">u = z x + z c (22) v = u + z g<label>(23)</label></formula><p>where f g is the Bi-LSTM function and s g is the function of the MLP in the prior network p γ (z g |x, u). + is element-wise addition. During training, u and v are fed into the likelihood net- works: where w c and w g are the functions of the MLPs in two likelihood networks. During testing, the prior networks will substitute the posterior networks, so the latent variable z g is randomly sampled from the Gaussian distributions N (µ g , Σ g ), and the last four equations above (Equation 22 -25) will be replaced by the following ones:</p><formula xml:id="formula_20">ˆ y c = w c (e ⊕ u) (24) ˆ y g = w g (e ⊕ v)<label>(25)</label></formula><formula xml:id="formula_21">ˆ y c = w c (e ⊕ u ) (26) v = u + z g (27) ˆ y g = w g (e ⊕ v )<label>(28)</label></formula><p>Algorithm 1 illustrates the complete training and testing process. BCE refers to the Binary Cross Entropy loss. Note that during training, the ground truth category labels and group labels are fed to the posterior network to generate latent variables. But during testing, the latent variables are generated by the prior network, which only utilizes the texts as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We collect the data from 40 hate group Twit- ter accounts of 13 different hate ideologies, e.g., white nationalist, anti-immigrant, racist skinhead, among others. The detailed themes and core val- ues behind each hate ideology are discussed in the SPLC ideology section. <ref type="bibr">3</ref> For each hate ide- ology, we collect a set of Twitter handles based on hate groups identified by the SPLC center. <ref type="bibr">4</ref> For each hate ideology, we select the top three han- dles in terms of the number of followers. Due to ties, there are four different groups in several categories of our dataset. The dataset consists of all the content (tweets, retweets, and replies) posted with each account from the group's incep- tion date, as early as 07-2009, until 10-2017. Each instance in the dataset is a tuple of (tweet text, hate category label, hate group label). The com- plete dataset consists of approximately 3.5 million tweets. Note that due to the sensitive nature of the data, we anonymously reference the Twitter han- dles for each hate group by using IDs throughout this paper. The distribution of the data is illus- trated in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>In addition to the discriminative CVAE model de- scribed in Section 3, we implement for other base- line methods and an upper bound model as fol- lows. Support Vector Machine (SVM): We implement an SVM model with linear kernels. We use L2 reg- ularization and the coefficient is 1. The input fea- tures are the Term Frequency Inverse Document Frequency (TF-IDF) values of up to 2-grams. Logistic Regression (LR): We implement the Lo- gistic Regression model with L2 regularization. The penalty parameter C is set to 1. Similar to the SVM model, we use TF-IDF values of up to 2-grams as features.</p><p>Character-level Convolutional Neural Network (Char-CNN): We implement the character-level CNN model for text classification as described in ( ). It is 9 layers deep with 6 convolutional layers and 3 fully-connected layers.  <ref type="table">Table 1</ref>: Experimental results. Complete: The performance achieved when 90% of the entire dataset is used for training. Subset: The performance achieved when only 10% of the dataset is used for training. The best results are in bold.</p><formula xml:id="formula_22">Dataset Complete Subset Metric Macro-F1 Micro-F1 Weighted-F1 Macro-F1 Micro-F1 Weighted-F1</formula><p>The configurations of the convolutional layers are kept the same as those in <ref type="figure" target="#fig_0">(Zhang et al., 2015)</ref>. Bi-LSTM: The model consists of a bidirectional LSTM layer followed by a linear layer. The em- bedded tweet text x is fed into the LSTM layer and the output at the last time step is then fed into the linear layer to predictˆypredictˆ predictˆy g .</p><p>Upper Bound: The upper bound model also con- sists of a bidirectional LSTM layer followed by a linear layer. The difference between this model and Bi-LSTM is that it takes the tweet text x along with the ground truth category label y c as input during both training and testing. The LSTM layer is used to encode x. The encoding result is con- catenated with the ground truth category label and then fed into the linear layer to give the prediction of the hate groupˆygroupˆ groupˆy g . Since it utilizes y c for predic- tion during testing, this model sets an upper bound performance for our method.</p><p>For the baseline CVAE, Bi-LSTM, the upper bound model, and the HCVAE, we use randomly initialized word embeddings with size 200. All the neural network models are optimized by the Adam optimizer with learning rate 1e-4. The batch size is 20. The hidden size of all the LSTM layers in these models is 64 and all the MLPs are three- layered with non-linear activation function Tanh. For the CVAE and the HCVAE, the size of the la- tent variables is set to 256.</p><p>All the baseline models and our model are eval- uated on two datasets. We first use the complete dataset for training and testing. 90% of the in- stances are used for training and the rest for test- ing. In order to evaluate the robustness of the base- line models and our model, we also randomly se- lected a subset (10%) of the original dataset for training while the testing dataset is fixed. Since the upper bound model is used to set an upper bound on performance, we do not evaluate it on the smaller training dataset. We use Macro-F1, Micro-F1, and Weighted-F1 to evaluate the classi- fication results. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the dataset is highly imbalanced, which causes problems for training the above models. In order to alleviate this problem, we use a weighted random sampler to sample instances from the training data. How- ever, the testing dataset is not sampled, so the dis- tribution of the testing dataset remains the same as that of the original dataset. This allows us to eval- uate the models' performance on the data with a realistic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>The experimental results are shown in <ref type="table">Table 1</ref>. The testing dataset keeps the imbalanced distribu- tion of the original data, so the Macro-F1 score of each method is significantly lower than the Micro- F1 score and the Weighted-F1 score. Comparing the performance of the Bi-LSTM model and that of the baseline CVAE model shows that although CVAE is traditionally used as a generative model, it can also achieve competitive performance as a discriminative model. All the methods achieve lower F1 scores when using the smaller training dataset. Due to the imbalanced distribution of our dataset, half of the 40 groups has less than 1k tweets in the smaller training dataset, which leads to the sharp decline in the Macro-F1 scores of all the methods. However, the performance of both CVAE-based models degrades less than that of the other two neural network models (the Bi- LSTM model and the Char-CNN model) accord- ing to the Micro-F1 Weighted-F1 scores. These two CVAE-based models tend to be more robust when the size of the training dataset is smaller. The difference between the CVAE-based models and the other two models is that both the Bi- LSTM model and the Char-CNN model directly compress the input text into a fixed-length latent  <ref type="figure">, j)</ref>, where r(i, j) is the fraction of the group i's instances among the instances predicted as the group j. A higher difference value corresponds to a lighter color. A grid darker than the background is mapped from a negative value while a grid lighter than the background is mapped from a positive one.</p><p>variable while the CVAE model explicitly mod- els the posterior and likelihood distributions over the latent variable. The result is that, during test- ing, the inference strategy of the Bi-LSTM model and the Char-CNN model is actually comparing the compressed text to the compressed instances in the training dataset while the strategy of the CVAE-based models is to compare the prior dis- tributions over the latent variable. Compared with the compressed text, prior distributions may cap- ture higher level semantic information and thus enable better generalization.</p><p>By further utilizing the hate category label for training, the HCVAE outperforms the baseline CVAE on all three metrics. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates the difference between the predictions made by the HCVAE and the CVAE. As shown in the fig- ure, most of the lighter girds are in the dashed squares while most of the darker girds are out- side. This indicates that the HCVAE model tends to correct the prediction of the CVAE's misclas- sified instances to be closer to the ground truth. In most cases, the misclassified instances can be directly corrected to the ground truth (the diago- nal). In other cases, they are not corrected to the ground truth but are corrected to the hate groups of the same categories as the ground truth (the dashed squares). This shows that additional ideology in- formation is useful for the model to better capture the subtle differences among tweets posted by dif- ferent hate groups.</p><p>Although our method outperforms the baseline methods, there is still a gap between its perfor- mance and the upper bound performance. We an- alyze this in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis</head><p>As mentioned above, in some cases our model misclassified the tweet as a group under the same category as the ground truth. Take, for instance, the tweet from Group1: The only good #muslim is a #dead #muslim. Our HCVAE model predicts it as from Group2. But both Group1 and Group2 are under the category "neo nazi". One possible reason for this kind of error is that the imbalance of the dataset adversely affects the performance of our method. <ref type="figure" target="#fig_5">Figure 6</ref> shows the F1 scores achieved by the HCVAE on each group. The per- formance of the model tends to be much lower when the number of the group's training instances decreases. Although we use the weighted random sampler to alleviate the problem of the imbalanced dataset, repeating the very limited data instances (less than 3k) of a group during training cannot re- ally help the posterior and prior networks to give a reasonable distribution that can generalize well on unseen inputs. Therefore, when the model comes into the instances in the testing dataset, the perfor- mance can be less satisfactory. This is a common problem for all the methods, which is the cause of the significantly lower Macro-F1 scores.</p><p>Another type of error is caused by the noisy dataset. Take, for instance, the tweet from Group3 under the category "ku klux klan": we must secure the existence of our people and future for the White Children !! Our model classifies it as from Group4 under the category "neo nazi", which makes sense but is an error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we explore the CVAE as a discrimi- native model and find that it can achieve compet- itive results. In addition, the performance of the CVAE-based models tend to be more stable than that of the others when the dataset gets smaller. We further propose an extension of the basic dis- criminative CVAE model to incorporate the hate group category (ideology) information for train- ing. Our proposed model has a hierarchical struc- ture that mirrors the hierarchical structure of the hate groups and the ideologies. We apply the HC- VAE to the task of fine-grained hate speech clas- sification, but this Hierarchical CVAE framework can be easily applied to other tasks where the hier- archical structure of the data can be utilized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A portion of the hate group map published by the Southern Poverty Law Center (SPLC). Each marker represents a hate group. The markers with the same pattern indicate the corresponding hate groups share the same ideology. The white box shows an example of a hate group in Auburn, Alabama under the category of "White Nationalist". Due to the sensitivity of the data, we mask the name of the group.</figDesc><graphic url="image-1.png" coords="1,310.73,222.54,208.63,133.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The structure of the baseline CVAE model during training. Bi-LSTM is a bidirectional LSTM layer. MLP is a Multilayer Perceptron. x is the embedded input text. y g is the ground truth hate group label. ˆ y g is the output prediction of the hate group. p is the posterior distribution of the latent variable z while p is the prior distribution of z. Note that this structure is used for training. During testing, the posterior network is replaced by the prior network to computê y g and thus y g is not available during testing. Refer to Section 3 for detailed explanation.</figDesc><graphic url="image-2.png" coords="3,130.66,62.81,98.22,184.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of the HCVAE during training. Bi-LSTM is a bidirectional LSTM layer. MLP is a Multilayer Perceptron. x is the embedded input text. y c and y g are the ground truth hate category label and hate group label. ˆ y c andˆyandˆ andˆy g are the output predictions of the hate category and hate group. z c , z g , and z c are latent variables. In the left dotted box are two posterior networks. In the right dotted box are two prior networks. Note that this structure is used for training. During testing, the posterior networks will be substituted by the posterior networks (i.e. the left dotted box will be substituted with the right one) to computê y c andˆyandˆ andˆy g. Thus y c and y g are not available during testing. Refer to Section 4 for detailed explanation.</figDesc><graphic url="image-3.png" coords="5,73.91,62.81,211.71,174.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The distribution of the data for 40 hate groups. The X-Axis is each hate group. The Y-Axis is the number of tweets collected for each hate group.</figDesc><graphic url="image-4.png" coords="6,75.17,62.81,444.48,158.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: This figure compares a subset (17 hate groups of 6 categories) of the predictions made by the baseline CVAE and the HCVAE. The X and Y axes are the index of each hate group. The hate groups of the same category are grouped together as shown in the dashed squares. The color of the grid (i, j) is mapped from r hcvae (i, j)−r cvae (i, j), where r(i, j) is the fraction of the group i's instances among the instances predicted as the group j. A higher difference value corresponds to a lighter color. A grid darker than the background is mapped from a negative value while a grid lighter than the background is mapped from a positive one.</figDesc><graphic url="image-5.png" coords="8,77.72,62.81,204.09,201.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The F1 score achieved by our method on each hate group. The Y-axis is the F1 score. The X-axis is the hate group sorted by the number of instances in the dataset from larger to smaller. The dashed line shows the F1 scores and the solid line is the corresponding trendline.</figDesc><graphic url="image-6.png" coords="8,307.28,62.81,226.77,128.76" type="bitmap" /></figure>

			<note place="foot" n="1"> http://www.pewinternet.org/2017/07/11/onlineharassment-2017/</note>

			<note place="foot" n="2"> https://www.splcenter.org</note>

			<note place="foot" n="3"> https://www.splcenter.org/fighting-hate/extremistfiles/ideology 4 https://www.splcenter.org/fighting-hate/extremistfiles/groups</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for hate speech detection in tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinkesh</forename><surname>Badjatiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="759" to="760" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Us and Them: Identifying Cyber Hate on Twitter across Multiple Protected Characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Burnap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automated hate speech detection and the problem of offensive language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Warmsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Macy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04009</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects, ratings and sentiments for movie recommendation (jmars)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognizing explicit and implicit hate speech using a weakly supervised two-path bootstrapping approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Kuppersmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="774" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1558" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Abusive language detection in online user content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chikashi</forename><surname>Nobata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achint</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on world wide web</title>
		<meeting>the 25th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="145" to="153" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural language processing</title>
		<meeting>the 2015 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Guy De Pauw, Walter Daelemans, and Véronique Hoste. 2015. Detection and Fine-grained Classification of Cyberbullying Events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Van Hee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Mennes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Desmet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP&apos;15: International Conference Recent Advances in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="672" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting Hate Speech on the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL&apos;12: Proceedings of the 2nd Workshop on Language in Social Media</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
		<respStmt>
			<orgName>Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hateful symbols or hateful people? predictive features for hate speech detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL student research workshop</title>
		<meeting>the NAACL student research workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="88" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01898</idno>
		<title level="m">Generative and discriminative text classification with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Content-Driven Detection of Cyberbullying on the Instagram Social Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoti</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Cinzia</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Michele</forename><surname>Rajtmajer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;16: Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3952" to="3958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chonglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Lau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08630</idno>
		<title level="m">A c-lstm neural network for text classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
