<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-based LSTM Network for Cross-Lingual Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-based LSTM Network for Cross-Lingual Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="247" to="256"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data. However, the labeled resources are usually imbalanced in different languages. Cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages. In this study, we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages. In each language, we use Long Short Term Memory (LSTM) network to model the documents, which has been proved to be very effective for word sequences. Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network. The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive. The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most of the sentiment analysis research focuses on sentiment classification which aims to determine whether the users attitude is positive, neutral or negative. There are two classes of mainstreaming sentiment classification algorithms: unsupervised methods which usually require a sentiment lexicon <ref type="bibr" target="#b21">(Taboada et al., 2011</ref>) and supervised methods ( <ref type="bibr" target="#b17">Pang et al., 2002</ref>) which require manually labeled data. However, both of these sentiment resources are unbalanced in different languages. The sentiment lexicon or labeled data are rich in several languages such as English and are poor in others. Manually building these resources for all the languages will be expensive and time-consuming. Cross-lingual sentiment classification tackles the problem by try- ing to adapt the resources in one language to other languages. It can also be regarded as a special kind of cross-lingual text classification task.</p><p>Recently, there have been several bilingual rep- resentation learning methods such as ( <ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b6">Gouws et al., 2014</ref>) for cross-lingual sentiment or text classification which achieve promising results. They try to learn a joint embedding space for different languages such that the training data in the source language can be directly applied to the test data in the target language. However, most of the studies only use simple functions, e.g. arithmetic average, to synthesize representations for larger text sequences. Some of them use more complicated compositional models such as the bi-gram non-linearity model in ( <ref type="bibr">Hermann and Blunsom, 2014</ref>) which also fail to capture the long distance dependencies in texts.</p><p>In this study, we propose an attention-based bilingual LSTM network for cross-lingual sentiment classification. LSTMs have been proved to be very effective to model word sequences and are powerful to learn on data with long range temporal dependencies. After translating the training data into the target language using machine translation tools, we use the bidirectional LSTM network to model the documents in both of the source and the target languages. The LSTMs show strong ability to capture the compositional semantics for the bilingual texts in our experiments.</p><p>For the traditional LSTM network, each word in the input document is treated with equal importance, which is reasonable for traditional text classification tasks. In this paper, we propose a hierarchical attention mechanism which enables our model to focus on certain part of the input document. The motivation mainly comes from the following three observations: 1) the machine translation tool that we use to translate the documents will always introduce much noise for sentiment classification. We hope that the attention mechanism can help to filter out these noises. 2) In each individual language, the sentiment of a document is usually decided by a relative small part of it. In a long review document, the user might discuss both the advantages and disadvantages of a product. The sentiment will be confusing if we consider each sentence of the same contribution. For example, in the first review of <ref type="table" target="#tab_0">Table 1</ref>, the first sentence reveals a negative sentiment towards the movie but the second one reveals a positive sentiment. As human readers, we can understand that the review is expressing a positive overall sentiment but it is hard for the sequence modeling algorithms including LSTM to capture. 3) At the sentence level, it is important to focus on the sentiment signals such as the sentiment words. They are usually very decisive to determine the polarity even for a very long sentence, e.g. "easy" and "nice" in the second example of <ref type="table" target="#tab_0">Table  1.</ref> "I felt it could have been a lot better with a little less comedy and a little more drama to get the point across. However, its still a must see for any Jim Carrey fan. " "It is easy to read, it is easy to look things up in and provides a nice section on the treatments." In sum, the main contributions of this study are summarized as follows:</p><p>1) We propose a bilingual LSTM network for cross-lingual sentiment classification <ref type="table">. Compared to  the previous methods which only use weighted or  arithmetic average of word embeddings to represent  the document, LSTMs have obvious advantage to  model the compositional semantics and to capture  the long distance dependencies between words for  bilingual texts.</ref> 2) We propose a hierarchical bilingual attention mechanism for our model. To the best of our knowledge, this is the first attention-based model designed for cross-lingual sentiment analysis.</p><p>3) The proposed framework achieves good results on a benchmark dataset from a cross-language sentiment classification evaluation. It outperforms the best team in the evaluation as well as several strong baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sentiment analysis is the field of studying and analyzing peoples opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012). The most common task of sentiment analysis is polarity classification which arises with the emergence of customer reviews on the Internet. <ref type="bibr" target="#b17">Pang et al. (2002)</ref> used supervised learning methods and achieved promising results with simple unigram and bi-gram features. In subsequent research, more features and learning algorithms were tried for sentiment classification by a large number of researchers. Re- cently, the emerging of deep learning has also shed light on this area. Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network <ref type="bibr" target="#b20">(Socher et al., 2013)</ref>, paragraph vector ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>), multi-channel convolutional neural networks <ref type="bibr" target="#b11">(Kim, 2012)</ref>, dynamic convolutional neural network <ref type="bibr" target="#b2">(Blunsom et al., 2014</ref>) and tree structure LSTM ( <ref type="bibr" target="#b22">Tai et al., 2015)</ref>. Very recently, <ref type="bibr" target="#b25">Yang et al. (2016)</ref> proposed a similar hierarchical attention network based on GRU in the monolingual setting. Note that our work is independent with theirs and their study was released online after we submitted this study.</p><p>Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. Cross-lingual sentiment classification has been extensively studied in the very recent years. <ref type="bibr" target="#b15">Mihalcea et al. (2007)</ref> translated English subjec- tivity words and phrases into the target language to build a lexicon-based classifier. <ref type="bibr" target="#b23">Wan (2009)</ref> translated both the training data (English to Chinese) and the test data (Chinese to English) to train differ- ent models in both the source and target languages.  proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training.</p><p>There have also been several studies addressing the task via multi-lingual text representation learn- ing. <ref type="bibr" target="#b24">Xiao and Guo (2013)</ref> learned different repre- sentations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. <ref type="bibr" target="#b12">Klementiev et al. (2012)</ref> treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from co- occurrence statistics in bilingual parallel corpora. <ref type="bibr" target="#b4">Chandar A P et al. (2014)</ref> and <ref type="bibr" target="#b27">Zhou et al. (2015)</ref> used the autoencoders to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences. <ref type="bibr" target="#b18">Pham et al. (2015)</ref> extended the paragraph model into bilingual setting. Each pair of parallel sentences shares the same paragraph vector.</p><p>Compared to the existing studies, we propose to use the bilingual LSTM network to learn the docu- ment representations of reviews in each individual language. It has obvious advantage to model the compositional semantics and to capture the long distance dependencies between words. Besides, we propose a hierarchical neural attention mechanism to capture the sentiment attention in each document. The attention model helps to filter out the noise which is irrelevant to the overall sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Cross-language sentiment classification aims to use the training data in the source language to build a model which is adaptable for the test data in the target language. In our setting, we have labeled training data in English</p><formula xml:id="formula_0">L EN = {x i , y i } N i=1</formula><p>, where x i is the review text and y i is the sentiment label vector. y i = (1, 0) represents the positive sentiment and y i = (0, 1) represents the negative sentiment. In the target language Chinese, we have the test data</p><formula xml:id="formula_1">T CN = {x i } T i=1 and unlabeled data U CN = {x i } M i=1</formula><p>. The task is to use L EN and U CN to learn a model and classify the sentiment polarity for the review texts in T CN .</p><p>In our method, the labeled, unlabeled and test data are all translated into the other language using an online machine translation tool. In the subsequent part of the paper, we refer to a document and its corresponding translation in the other language as a pair of parallel documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN and LSTM</head><p>Recurrent neural network (RNN) <ref type="bibr" target="#b19">(Rumelhart et al., 1988</ref>) is a special kind of feed-forward neural network which is useful for modeling time-sensitive sequences. At each time t, the model receives input from the current example and also from the hidden layer of the network's previous state. The output is calculated given the hidden state at that time stamp. The recurrent connection makes the output at each time associated with all the previous inputs. The vanilla RNN model has been considered to be difficult to train due to the well-known problem of vanishing and exploding gradients. The LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) addresses the problem by re-parameterizing the RNN model. The core idea of LSTM is introducing the "gates" to control the data flow in the recurrent neural unit. The LSTM structure ensures that the gradient of the long-term dependencies cannot vanish. The detailed architecture that we use in shown in <ref type="figure" target="#fig_0">Figure 1</ref>. describe the general architecture of the model and then describe the attention mechanism used in it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>The general architecture of our approach is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For a pair of parallel documents x cn and x en , each of them is sent into the attention based LSTM network. The English-side and Chinese- side architectures are the same but have different parameters. We only show the Chinese-side network in the figure due to space limit. The whole model is divided into four layers. In the input layer, the documents are represented as a word sequence where each position corresponds to a word vector from pre-trained word embeddings. In the LSTM layer, we get the high-level representation from a bidirectional LSTM network. We use the hidden units from both the forward and backward LSTMs. In the document representation layer, we incorporate the attention model into the network and derive the final document representation. At the output layer, we concatenate the representations of the English and Chinese documents and use the softmax function to predict the sentiment label.</p><p>Input</p><note type="other">Layer: The input layer of the network is the word sequences in a document x which can be either Chinese or English. The document x contains several sentences {s i } |x| i=1 and each sentence is composed of several words s</note><formula xml:id="formula_2">i = {w i,j } |s i | j=1 .</formula><p>We represent each word in the document as a fixed-size vector from pre-trained word embeddings.</p><p>LSTM Layer: In each individual language, we use bi-directional LSTMs to model the input sequences. In the bidirectional architecture, there are two layers of hidden nodes from two separate LSTMs. The two LSTMs capture the dependencies in different directions. The first hidden layers have recurrent connections from the past words while second one's direction of recurrent of connections is flipped, passing activation backwards in the texts. Therefore, in the LSTM layer, we can get the forward hidden state h i,j from the forward LSTM network and the backward hidden state h i,j from the backward LSTM network. We represent the final state at position (i, j), i.e. the j-th word in the i-th sentence of the document, with the concatenation of h i,j and h i,j .</p><formula xml:id="formula_3">h i,j = h i,j h i,j</formula><p>It captures the compositional semantics in both directions of the word sequences.</p><p>Document Representation Layer:As described above, different parts of the document usually have different importance for the overall sentiment. Some sentences or words can be decisive while the others are irrelevant. In this study, we use a hierarchical attention mechanism which assigns a real value score for each word and a real value score for each sentence. The detailed strategy of our attention model will be described in the next subsection.</p><p>Suppose we have the sentence attention score A i for each sentence s i ∈ x, and the word attention score a i,j for each word w i,j ∈ s i , both of the scores are normalized which satisfy the following equations,</p><formula xml:id="formula_4">i A i = 1 and j a i,j = 1</formula><p>The sentence attention measures which sentence is more important for the overall sentiment while the word attention captures sentiment signals such as sentiment words in each sentence. Therefore, the document representation r for document x is calculated as follows,</p><formula xml:id="formula_5">r = i [A i · j (a i,j · h i,j )]</formula><p>Note that many LSTM based models represent the word sequences only using the hidden layer at the final node. In this study, the hidden states at all the positions are considered with different attention weights. We believe that, for document sentiment classification, focusing on some certain parts of the document will be effective to filter out the sentiment- irrelevant noise.</p><p>Output Layer: At the output layer, we need to predict the overall sentiment of the document. For each English document x en and its corresponding translation x cn , suppose the document representa- tions of them are obtained in previous steps as r en and r cn , we simply concatenate them as the feature vector and use the softmax function to predict the final sentiment. ˆ y = sof tmax(r cn r en )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Attention Mechanism</head><p>For document-level sentiment classification task, we have shown that capturing both the sentence and word level attention is important. The general idea is inspired by previous works such as <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> and <ref type="bibr" target="#b9">Hermann et al. (2015)</ref> which have successfully applied the attention model to machine translation and question answering. <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref> incorporated the attention model into the sequence to sequence learning framework. During the decoding phase of the machine translation task, the attention model helps to find which input word should be "aligned" to the current output. In our case, the output of the model is not a sequence but only one sentiment vector. We hope to find the important units in the input sequence which are influential for the output.</p><p>We propose to learn a hierarchical attention model jointly with the bilingual LSTM network. The first level is the sentence attention model which measures which sentences are more important for the overall sentiment of a document. For each sentence s i = {w i,j } |s i | j=1 in the document, we represent the sentence via the final hidden state of the forward LSTM and the backward LSTM, i.e.</p><formula xml:id="formula_6">s i = h i,|s i | h i,1</formula><p>We use a two-layer feed-forward neural network to predict the attention score of s i</p><formula xml:id="formula_7">ˆ A i = f (s i ; θ s ) A i = exp( ˆ A i ) j exp( ˆ A j )</formula><p>where f denotes the two-layer feed-forward neural network and θ s denotes the parameters in it.</p><p>At the word level, we represent each word w i,j using its word embedding and the hidden state of the bidirectional LSTM layer, i.e. h i,j . Similarly, we use a two-layer feed forward neural network to predict the attention score of w i,j ,</p><formula xml:id="formula_8">e i,j = w i,j h i,j h i,j ˆ a i,j = f (e i,j ; θ w ) a i,j = exp(ˆ a i,j )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training of the Proposed Model</head><p>The proposed model is trained in a semi-supervised manner. In the supervised part, we use the cross entropy loss to minimize the sentiment prediction er- ror between the output results and the gold standard labels,</p><formula xml:id="formula_9">L 1 = (xen,xcn) i −y i log(ˆ y i )</formula><p>where x en and x cn are a pair of parallel documents in the training data, y is the gold-standard sentiment vector andˆyandˆ andˆy is the predicted vector from our model. The unsupervised part tries to minimize the document representations between the parallel data. Following previous research, we simply measure the distance of two parallel documents via the Euclidean Distance,</p><formula xml:id="formula_10">L 2 = (xen,xcn) r en − r cn 2</formula><p>where x en and x cn are a pair of parallel documents from both the labeled and unlabeled data.</p><p>The final objective function is a weighted sum of</p><formula xml:id="formula_11">L 1 and L 2 , L = L 1 + α · L 2</formula><p>where α is the hyper-parameter controlling the weight. We use Adadelta <ref type="bibr" target="#b26">(Zeiler, 2012)</ref> to update the parameters during training. It can dynamically adapt over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent.</p><p>In the test phase, the test document in T CN is sent into our model along with the corresponding machine translated text in T EN . The final senti- ment is predicted via a softmax function over the concatenated representation of the bilingual texts as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We use the dataset from the cross-language senti- ment classification evaluation of NLP&amp;CC <ref type="bibr">2013</ref> The dataset contains reviews in three domains including book, DVD and music. In each domain, it has 2000 positive reviews and 2000 negative reviews in English for training and 4000 Chinese reviews for test. It also contains 44113, 17815 and 29678 unlabeled reviews for book, DVD and music respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Detail</head><p>We use Google Translate 2 to translate the labeled data to Chinese and translate the unlabeled data and test data to English. All the texts are tokenized and converted into lower case.</p><p>In the proposed framework, the dimensions of the word vectors and the hidden layers of LSTMs are set as 50. The initial word embeddings are trained on both the unlabeled and labeled reviews using word2vec in each individual language. The word vectors are fine-tuned during the training procedure. The hyper-parameter a is set to 0.2. The dropout rate is set to 0.5 to prevent overfitting. Ten percent of the training data are randomly selected as validation set. The training procedure is stopped when the prediction accuracy does not improve for 10 iterations. We implement the framework based on theano ( <ref type="bibr" target="#b1">Bastien et al., 2012</ref>) and use a GTX 980TI graphic card for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines and Results</head><p>To evaluate the performance of our model, we compared it with the following baseline methods:</p><p>LR and SVM: We use logistic regression and SVM to learn different classifiers based on the translated Chinese training data. We simply use unigram features.</p><p>MT-PV: Paragraph vector ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>) is considered as one of the state-of-the-art monolingual document modeling methods. We translate all the training data into Chinese and use paragraph vector to learn a vector representation for the training and test data. A logistic regression classifier is used to predict the sentiment polarity.</p><p>Bi-PV: <ref type="bibr" target="#b18">Pham et al. (2015)</ref> is one the state-of- the-art bilingual document modeling methods. It extends the paragraph vector into bilingual setting.</p><p>Language Processing (NLP) and Chinese Computing (CC) organized by Chinese Computer Federation (CCF).</p><p>2 http://translate.google.com/ Each pair of parallel sentences in the training data shares the same vector representation. BSWE: <ref type="bibr" target="#b27">Zhou et al. (2015)</ref> proposed the bilin- gual sentiment word embedding algorithm based on denoising autoencoders. It learns the vector representations for 2000 sentiment words. Each document is then represented by the sentiment words and the corresponding negation words in it.</p><p>H</p><note type="other">-Eval: Gui et al. (2013) got the highest performance in the NLP&amp;CC 2013 cross-lingual sentiment classification evaluation. It uses a mixed CLSC model by combining co-training and transfer learning strategies.</note><p>A-Eval: This is the average performance of all the teams in the NLP&amp;CC 2013 cross-lingual sentiment classification evaluation.</p><p>The attention-based models EN-Attention, CN- Attention and BI-Attention: Bi-Attention is the model described in the above sections which con- catenate the document representations of the English side and the Chinese side texts. EN-Attention only translates the Chinese test data into English and uses English-side attention model while CN-Attention only uses the Chinese side attention model.   <ref type="table" target="#tab_2">Table 2</ref> shows the cross-lingual sentiment clas- sification accuracy of all the approaches. The first kind baseline algorithms are based on traditional bag-of-word features. SVM performs better than LR on book and DVD but gets much worse result on music. The second kind baseline algorithms are based on deep learning methods which learn the vector representations for words or documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MT-PV achieves similar results with LR. Bi-PV improves the accuracy by about 0.03 using both the bilingual documents. While MT-PV and Bi- PV directly learn document representations, BSWE learns the embedding for the words in a bilingual sentiment lexicon. It gets higher accuracy than both Bi-PV and MT-PV which shows that the sentiment words are very important for this task.</p><p>Our attention based models achieve the highest prediction accuracy among all the approaches. The results show that CN-Attention always outperforms EN-Attention. The combination of the English-side and Chinese-side model brings improvement to both the book and music domains and yields the highest average prediction accuracy. The attention-based models outperform the algorithms using traditional features as well as the existing deep learning based methods. Compared to the highest performance in the NLP&amp;CC evaluation, we improve the average accuracy by about 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Influence of the Attention Mechanism</head><p>In this study, we propose a hierarchical attention mechanism to capture the sentiment-related infor- mation of each document. In table 3, we show the results of models with different attention mech- anisms. All the models are based on the bilingual bi-directional LSTM network as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. LSTM is the basic bilingual bi-directional LSTM network. LSTM+SA considers only sentence-level attention while LSTM+WA considers only word- level attention. LSTM+HA combines both word- level and sentence-level attentions. From the results, we can observe that LSTM+HA outperforms the other three methods, which proves the effectiveness of the hierarchical attention mechanism. Besides, the word-level attention shows better performance than the sentence-level attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Average  We also conduct a case study using the examples in <ref type="table" target="#tab_0">Table 1</ref>. We show the visualized word attention using a heat map in <ref type="figure" target="#fig_3">Figure 3</ref> by drawing the attention of each word in it. The darker color reveals higher attention scores while the lighter part has little importance. We can observe that our model successfully identifies the important units of the sentence. The sentiment word "easy" gets much higher attention score than the other words. The word "nice" gets the third highest score in the sentence right after the two "easy". Note that our attention mechanism considers both the word embedding vector and the hidden state vectors. Therefore, the same word "easy" gets different scores in different positions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Influence of the Word Embeddings</head><p>For the deep learning based methods, the initial word embeddings used as the inputs for the network usually play an important role. We study four different settings called rand, static, fine-tuned and multi-channel, respectively. In rand setting, the word embeddings are randomly initialized. The static setting keeps initial embedding fixed while the fine-tuned setting learns a refined embedding during the training procedure. Multi-channel is the combination of static and fine-tuned. Two same word vectors are concatenated to represent each word. During the training procedure, half of it is fine-tuned while the rest is fixed. Note that fine- tuned is the embedding setting that we use in our model.   <ref type="table" target="#tab_6">Table 4</ref> shows the performance of our model in these settings. Rand gets the lowest accuracy among them. The fine-tuned word embeddings perform better than static which fits the results in previous study <ref type="bibr" target="#b11">(Kim, 2012)</ref>. Multi-channel gets similar results with fine-tuned on DVD and music but is a bit lower on book. We also find that using pre-trained word embeddings helps the model to converge much faster than random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Influence of Vector Sizes</head><p>In our experiment, we set the size of the hidden layers in both the forward and backward LSTMs the same as the size of the input word vectors. There- fore, the dimension of the document representation is twice of the word vector size. In <ref type="figure" target="#fig_4">Figure 4</ref>, we show the performance of our model with different input vector sizes. We use the vector size in the following set {10, 25, 50, 100, 150, 200}. Note that the dimensions of all the units in the model also change with that.</p><p>We can observe from <ref type="figure" target="#fig_4">Figure 4</ref> that the prediction accuracy for the book domain keeps steady when the vector size changes. For DVD and music, the performance increases at the beginning and becomes stable after the vector size grows larger than 50. It shows that our model is robust to a wide range of vector sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an attention based LSTM network for cross-language sentiment classification. We use the bilingual bi-directional LSTMs to model the word sequences in the source and target lan- guages. Based on the special characteristics of the sentiment classification task, we propose a hierar- chical attention model which is jointly trained with the LSTM network. The sentence level attention enables us to find the key sentences in a document and the word level attention helps to capture the sentiment signals. The proposed model achieves promising results on a benchmark dataset using Chinese as the source language and English as the target language. It outperforms the best results in the NLPC&amp;CC cross-language sentiment classification evaluation as well as several strong baselines. In future work, we will evaluate the performance of our model on more datasets and more language pairs. The sentiment lexicon is also another kind of useful resource for classification. We will explore how to make full usages of these resources in the proposed framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The LSTM architecture. The image is adopted from (Jozefowicz et al., 2015).</figDesc><graphic url="image-1.png" coords="4,120.41,57.83,132.72,103.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the proposed framework. The inputs xcn and xen are parallel documents. Due to space limit, we only illustrate the attention based LSTM network in Chinese language. For the English document xen, the network architecture is the same as the Chinese side but has different model parameters.</figDesc><graphic url="image-2.png" coords="4,75.61,262.82,222.30,264.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention visualization for a review sentence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance with different vector sizes</figDesc><graphic url="image-4.png" coords="8,312.04,418.52,231.84,106.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Examples of the sentiment attention</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. 1 1 The dataset can be found at http://tcci.ccf.org.cn/conference/2013/index.html. NLP&amp;CC is an annual conference specialized in the fields of Natural</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Cross-lingual sentiment prediction accuracy of our 

methods and the comparison approaches. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Comparison of different attention mechanisms</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of our model with four different word 

embedding settings 

</table></figure>

			<note place="foot" n="4"> Framework In this study, we try to model the bilingual texts through the attention based LSTM network. We first</note>

			<note place="foot">j exp(ˆ a i,j ) where θ w denotes the parameters for predicting word attention.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by National Natural Sci-ence Foundation of China <ref type="formula">(61331011)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arX- iv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5590</idno>
		<title level="m">Theano: new features and speed improvements</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to adapt credible knowledge in cross-lingual sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xule</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52rd Annual Meeting of the Association for Computational Linguistic</title>
		<meeting>52rd Annual Meeting of the Association for Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2455</idno>
		<title level="m">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A mixed model for cross lingual opinion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52rd Annual Meeting of the Association for Computational Linguistic</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>52rd Annual Meeting of the Association for Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1759" to="1774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sentiment analysis and opinion mining: Synthesis lectures on human language technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<biblScope unit="volume">16</biblScope>
			<pubPlace>San Rafael</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th</title>
		<meeting>the 45th</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Annual Meeting of the Association of Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning distributed representations for multilingual text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lexicon-based methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised representation learning for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1465" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52rd Annual Meeting of the Association for Computational Linguistic</title>
		<meeting>52rd Annual Meeting of the Association for Computational Linguistic</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="440" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
