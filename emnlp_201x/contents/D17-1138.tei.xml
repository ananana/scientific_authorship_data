<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Selectional Preferences for Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Heinzerling</surname></persName>
							<email>benjamin.heinzerling@h-its.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AIPHES Heidelberg Institute for Theoretical Studies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nafise</forename><forename type="middle">Sadat</forename><surname>Moosavi</surname></persName>
							<email>nafise.moosavi@h-its.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AIPHES Heidelberg Institute for Theoretical Studies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
							<email>michael.strube@h-its.org</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AIPHES Heidelberg Institute for Theoretical Studies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting Selectional Preferences for Coreference Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1332" to="1339"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Selectional preferences have long been claimed to be essential for coreference resolution. However, they are mainly mod-eled only implicitly by current corefer-ence resolvers. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. We show that the incorporation of our model improves coreference resolution performance on the CoNLL dataset, matching the state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile such improvements are.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Selectional preferences have long been claimed to be useful for coreference resolution. In his sem- inal work on "Resolving Pronominal References" <ref type="bibr">Hobbs (1978)</ref> proposed a semantic approach that requires reasoning about the "demands the pred- icate makes on its arguments." For example, se- lectional preferences allow resolving the pronoun it in the text "The Titanic hit an iceberg. It sank quickly." Here, the predicate sink 'prefers' certain subject arguments over others: It is plausible that a ship sinks, but implausible that an iceberg does.</p><p>Work on the automatic acquisition of selectional preferences has shown considerable progress <ref type="bibr">(Dagan and Itai, 1990;</ref><ref type="bibr" target="#b7">Resnik, 1993;</ref><ref type="bibr" target="#b0">Agirre and Martinez, 2001;</ref><ref type="bibr" target="#b4">Pantel et al., 2007;</ref><ref type="bibr">Erk, 2007;</ref><ref type="bibr" target="#b8">Ritter et al., 2010</ref>; Van de Cruys, 2014). However, to- day's coreference resolvers <ref type="bibr">(Martschat and Strube, 2015;</ref><ref type="bibr" target="#b12">Wiseman et al., 2016;</ref><ref type="bibr">Clark and Manning, 2016a</ref>, i.a.) capture selectional preferences only * These authors contributed equally to this work. implicitly at best, e.g., via a given mention's de- pendency governor and other contextual features.</p><p>Since negative results do not often get reported, there is no clear evidence in the literature re- garding the non-utility of particular knowledge sources. Consequently, an absence of the explicit modeling of selectional preferences in the recent literature is an indicator that incorporating this knowledge source has not been very successful for coreference resolution.</p><p>More than ten years ago, <ref type="bibr">Kehler et al. (2004)</ref> declared the "non-utility of predicate-argument structures for pronoun resolution" and observed that minor improvements on a small dataset were due to fortuity rather than selectional preferences having captured meaningful world knowledge re- lations.</p><p>The claim by <ref type="bibr">Kehler et al. (2004)</ref> is based on selectional preferences extracted from a, by cur- rent standards, small number of 2.8m predicate- argument pairs. Furthermore, they employ a sim- ple (linear) maximum entropy classifier, which re- quires manual definition of feature combinations and is unlikely to fully capture the complex inter- action between selectional preferences and other coreference features. Therefore, it is worth revis- iting how a better selectional preference model af- fects the performance of a more complex corefer- ence resolver.</p><p>In this work, we propose a fine-grained, high- coverage model of selectional preferences and study its impact on a state-of-the-art, non-linear coreference resolver. We show that the incorpora- tion of our selectional preference model improves the performance. However, it is debatable whether such small improvements, that cost notable extra time or resources, are advantageous.  <ref type="figure">Figure 1</ref>: Dependency-based embedding model of selectional preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Modeling Selectional Preferences</head><p>The main design choice when modeling selec- tional preferences is the selection of a relation in- ventory, i.e. the concepts and entities that can be relation arguments, and the semantic relationships that hold between them. Prior work has studied many relation invento- ries. Predicate-argument statistics for word-word pairs (eat, food) 1 are easy to obtain but do not generalize to unseen pairs <ref type="bibr">(Dagan and Itai, 1990)</ref>. Class-based approaches generalize via word-class pairs (eat, /nutrient/food) <ref type="bibr" target="#b7">(Resnik, 1993)</ref> or class- class pairs (/ingest, /nutrient/food) <ref type="bibr" target="#b0">(Agirre and Martinez, 2001</ref>), but require disambiguation of words to classes and are limited by the coverage of the lexical resource providing such classes (e.g. WordNet).</p><p>Other possible relation inventories include se- mantic representations such as FrameNet frames and roles, event types and arguments, or abstract meaning representations. While these semantic representations are arguably well-suited to model meaningful world knowledge relationships, auto- matic annotation is limited in speed and accu- racy, making it difficult to obtain a large num- ber of such "more semantic" predicate-argument pairs. In comparison, syntactic parsing is both fast and accurate, making it trivial to obtain a large number of accurate, albeit "less semantic" predicate-argument pairs. The drawback of a syn- tactic model of selectional preferences is suscep- tibility to lexical and syntactic variation. For ex- ample, The Titanic sank and The ship went under differ lexically and syntactically, but would have the same or a very similar representation in a se- mantic framework such as FrameNet.</p><p>Our model of selectional preferences <ref type="figure">(Figure 1)</ref> 1 Examples due to <ref type="bibr" target="#b0">Agirre and Martinez (2001).</ref> overcomes this drawback via distributed represen- tation of predicate-argument pairs, using (syntac- tic) dependencies that were specifically designed for semantic downstream tasks, and by resolving named entities to their fine-grained entity types. Distributed representation. Inspired by Struc- tured Vector Space <ref type="bibr">(Erk and Pad√≥, 2008)</ref>, we embed predicates and arguments into a low- dimensional space in which (representations of) predicate slots are close to (representations of) their plausible arguments, as should be arguments that tend to fill the same slots of similar predi- cates, and predicate slots that have similar argu- ments. For example, captain should be close to pilot, ship to airplane, the subject of steer close to both captain and pilot, and also to, e.g., the subject of drive. Such a space allows judging the plausi- bility of unseen predicate-argument pairs. <ref type="bibr">2</ref> We construct this space via dependency-based word embeddings ( <ref type="bibr">Levy and Goldberg, 2014)</ref>. To see why this choice is better-suited for modeling selectional preferences than alternatives such as word2vec ( <ref type="bibr">Mikolov et al., 2013)</ref> or GloVe <ref type="bibr" target="#b6">(Pennington et al., 2014</ref>), consider the following ex- ample:</p><formula xml:id="formula_0">captain nsubj ‚Üê‚àí‚àí steers dobj ‚àí ‚àí ‚Üí ship ::</formula><p>::</p><formula xml:id="formula_1">pilot nsubj ‚Üê‚àí‚àí steers dobj ‚àí ‚àí ‚Üí airplane</formula><p>Here, captain and ship, have high syntagmatic similarity, i.e., these words are semantically re- lated and tend to occur close to each other. This also holds for pilot and airplane. In contrast, cap- tain and pilot, as well as ship and airplane have high paradigmatic similarity, i.e., they are seman-tically similar and occur in similar contexts. A model of selectional preferences requires paradig- matic similarity: The representations of captain and pilot in such a model should be similar, since they both can plausibly fill the subject slot of the predicate steer. Due to their use of linear con- text windows, word2vec and GloVe capture syn- tagmatic similarity, while dependency-based em- beddings capture paradigmatic similarity (cf. <ref type="bibr">Levy and Goldberg, 2014)</ref>. Enhanced++ dependencies. Due to distributed representation, our model generalizes over syn- tactic variation such as active/passive alternations: For example, steer@dobj 3 is highly similar to steer@nsubjpass (see Appendix for more exam- ples). To further mitigate the effect of employ- ing syntax as a proxy for semantics, we use En- hanced++ dependencies <ref type="bibr" target="#b9">(Schuster and Manning, 2016)</ref>. Enhanced++ dependencies aim to sup- port semantic applications by modifying syntac- tic parse trees to better reflect relations between content words. For example, the plain syntactic parse of the sentence Both of the girls laughed identifies Both as subject of laughed. The En- hanced++ representation introduces a subject re- lation between girls and laughed, which allows learning more meaningful selectional preferences: Our model should learn that girls (and other hu- mans) laugh, while learning that an unspecified both laughs is not helpful.</p><p>Fine-grained entity types. A good model of selectional preferences needs to generalize over named entities. For example, having encountered sentences like The Titanic sank, our model should be able to judge the plausibility of an unseen sen- tence like The RMS Lusitania sank. For popular named entities, we can expect the learned repre- sentations of Titanic and RMS Lusitania to be sim- ilar, allowing our model to generalize, i.e., it can judge the plausibility of The RMS Lusitania sank by virtue of the similarity between Titanic and RMS Lusitania. However, this will not work for rare or emerging named entities, for which no, or only low-quality, distributed representations have been learned. To address this issue, we incorpo- rate fine-grained entity typing (Ling and Weld). For each named entity encountered during train- ing, we generate an additional training instance by replacing the named entity with its entity type, e.g. (Titanic, sank@nsubj) yields (/product/ship, sank@nsubj).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>We train our model by combining term-context pairs from two sources. Noun phrases and their dependency context are extracted from <ref type="bibr">GigaWord (Parker et al., 2011</ref>) and entity types in context from Wikilinks ( <ref type="bibr" target="#b10">Singh et al., 2012</ref>). Term-context pairs are obtained by parsing each corpus with the Stanford CoreNLP dependency parser <ref type="bibr" target="#b6">(Manning et al., 2014</ref>). After filtering, this yields ca. 1.4 billion phrase-context pairs such as (Ti- tanic, sank@nsubj) from GigaWord and ca. 12.9 million entity type-context pairs such as (/prod- uct/ship, sank@nsubj) from Wikilinks. Finally, we train dependency-based embeddings using the generalized word2vec version by <ref type="bibr">Levy and Goldberg (2014)</ref>, obtaining distributed representations of selectional preferences. To identify fine-grained types of named entities at test time, we first per- form entity linking using the system by <ref type="bibr">Heinzerling et al. (2016)</ref>, then query Freebase ( <ref type="bibr">Bollacker et al., 2008</ref>) for entity types and apply the mapping to fine-grained types by Ling and Weld.</p><p>The plausibility of an argument filling a partic- ular predicate slot can now be computed via the cosine similarity of their associated embeddings. For example, in our trained model, the similarity of (Titanic, sank@nsubj) is 0.11 while the similar- ity of (iceberg, sank@nsubj) is -0.005, indicating that an iceberg sinking is less plausible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Do Selectional Preferences Benefit</head><p>Coreference Resolution?</p><p>We now investigate the effect of incorporating se- lectional preferences, implicitly and explicitly, in coreference resolution. <ref type="figure" target="#fig_0">Figure 2</ref> shows the selectional preference sim- ilarity of 10.000 coreferent and 10.000 non- coreferent mention pairs sampled randomly from the CoNLL 2012 training set. As we can see, while coreferent mention pairs are more similar than non-coreferent mention pairs according to the selectional preference similarity, there is not a di- rect relation between the similarity values and the coreferent relation. This indicates that coreference does not have a linear relation to the selectional preference similarities. However, it is worth in- vestigating how these similarity values affect the overall performance when they are combined with   other knowledge sources in a non-linear way.</p><formula xml:id="formula_2">MUC B 3 CEAF e CoNLL LEA R P F 1 R P F 1 R P F 1 Avg. F 1 R P F</formula><p>We select the ranking model of deep-coref (Clark and Manning, 2016b) as our baseline. deep-coref is a neural model that combines the in- put features through several hidden layers. Base- line in <ref type="table" target="#tab_2">Table 1</ref>   <ref type="bibr">Luo, 2005</ref>), the average F 1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016b). deep- coref includes the embeddings of the dependency governor of mentions. Combined with the relative position of a mention to its governor, deep-coref may be able to implicitly capture selectional pref- erences to some extent. ‚àígov in <ref type="table" target="#tab_2">Table 1</ref>   eling of selectional preferences does not provide any additional information to the coreference re- solver.</p><p>For each mention, we consider (1) the whole mention string, (2) the whole mention string with- out articles, (3) mention head, (4) context rep- resentation, i.e. governor@dependency-relation, and (5) entity types if the mention is a named en- tity. We obtain an embedding for each of the above properties if they exist in the selectional prefer- ence model, otherwise we set them to unknown.</p><p>For each (antecedent, anaphor) pair, we con- sider all the acquired embeddings of anaphor and antecedent. We try two different ways of incor- porating this knowledge into deep-coref includ- ing: (1) incorporating the computed embeddings directly as a new set of inputs, i.e. +embedding in <ref type="table" target="#tab_5">Table 2</ref>. We add a new hidden layer on top of the new embeddings and combine its output with out- puts of the hidden layers associated with other sets of inputs; and (2) computing a similarity value be- tween all possible combinations of the antecedent- anaphor acquired embeddings and then binarizing all similarity values, i.e. +binned sim. in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Providing selectional preference embeddings directly to deep-coref adds more complexity to the baseline coreference resolver. Yet, it performs on- par with +binned sim. on the development set and generalizes worse on the test set. +SP in <ref type="table" target="#tab_2">Table 1</ref> is the performance of +binned sim. on the test set. As we can see from the results, adding selectional does <ref type="bibr">[that]</ref> ante really impact the case ... <ref type="bibr">[it]</ref> ana just shows (impact@nsubj,shows@nsubj) <ref type="bibr">[it]</ref> ante will ask a U.S. bankruptcy court to allow <ref type="bibr">[it]</ref> ana (ask@nsubj,allow@dobj) [a strain that has n't even presented <ref type="bibr">[itself]</ref> ana ] ante (presented@nsubj,presented@dobj)  preferences as binary features improves over the baseline.</p><p>Reinforce in <ref type="table" target="#tab_2">Table 1</ref> presents the results of the reward-rescaling model of Clark and Manning (2016a) that are so far the highest reported results on the official test set. The reward rescaling model of <ref type="bibr">Clark and Manning (2016a)</ref> casts the ranking model of <ref type="bibr">Clark and Manning (2016b)</ref> in the rein- forcement learning framework which considerably increases the training time, from two days to six days in our experiments.</p><p>We analyze how our selectional preference model affects the resolution of various types of mentions. We use Martschat and Strube (2014)'s toolkit <ref type="bibr">4</ref> to perform recall and error analyses. The differences in the number of recall and precision errors in +SP compared to baseline on the test set are reported in <ref type="table" target="#tab_7">Table 4</ref>.</p><p>By using our selectional preference features, the number of recall errors decreases for all types of mentions. The recall error reduction is more prominent for pronouns. On the other hand, the number of precision errors increases for all types of mentions. The increase in the precision error is the highest for common nouns. Overall, +SP cre- ates about 260 more links than baseline. <ref type="table" target="#tab_6">Table 3</ref> lists a few examples from the de- velopment set in which +SP creates a link that baseline does not. It also includes the similar- ity that has a high value for the linked mentions and probably is the reason for creating the link. For instance, in the first example, based on our model, similarity(impact@nsubj,shows@nsubj) is known and it is also higher than similar- ity(impact@dobj,shows@nsubj).</p><p>In order to estimate a higher bound on the ex- pected performance boost, we run the baseline and +SP models only on anaphoric mentions. By using anaphoric mentions, the performance improves by one percent, based on both the CoNLL score and LEA. This result indicates that the incorporation of selectional preferences creates many links for non- anaphoric mentions, which in turn decreases pre- cision. Therefore, the overall performance does not improve substantially when system mentions are used. deep-coref incorporates anaphoricity scores at resolution time. One possible way to further improve the results of +SP is to incorpo- rate anaphoricity scores at the input level. In this way, the coreference resolver could learn to use se- lectional preferences mainly for mentions that are more likely to be anaphoric. However, given that the F 1 score of current anaphoricity determiners or singleton detectors is only around 85 percent ( <ref type="bibr">Strube, 2016a, 2017)</ref>, the effect of using system anaphoricity scores might be small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduce a new model of selectional prefer- ences, which combines dependency-based word embeddings and fine-grained entity types. In or- der to be effective, a selectional preference model should (1) have a high coverage so it can be used for large datasets like CoNLL, and (2) be com- bined with other knowledge sources in a non- linear way. Our selectional preference model slightly improves coreference resolution perfor- mance, but considering the extra resources that are required to train the model, it is debatable whether such small improvements are advantageous for solving coreference. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Selectional preference similarities of 10k coreferent and 10k non-coreferent mention pairs. Lines and boxes represent quartiles, diamonds outliers, points subsamples with jitter. Coreferent mention pairs are more similar than non-coreferent mention pairs with a Matthews correlation coefficient of 0.30, indicating weak to moderate correlation.</figDesc><graphic url="image-1.png" coords="4,57.60,179.49,242.27,166.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results on the CoNLL 2012 test set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>reports our baseline results on the CoNLL 2012 test set. The results are reported using MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF e (</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>repre- sents deep-coref performance when governors are not incorporated. As we can see, the exclusion of the governor information does not affect the per- formance. This result shows that the implicit mod-</figDesc><table>MUC 
B 3 CEAFe CoNLL 
LEA 
development 
baseline 
74.10 63.95 
59.73 
65.93 60.16 
+embedding 
74.38 64.42 
60.45 
66.42 60.65 
+binned sim. 74.36 64.54 
60.21 
66.37 60.77 
test 
baseline 
74.72 63.26 
58.82 
65.60 59.59 
+embedding 
74.53 63.41 
59.03 
65.66 59.69 
+binned sim. 74.85 63.64 
59.21 
65.90 59.98 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Incorporating the selectional preference 
model as new embeddings (+embedding) vs. as 
new pairwise features (+binned sim.). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 : Examples of +SP correct links on the development set that do not exist in the baseline output.</head><label>3</label><figDesc></figDesc><table>Error type 
Mention type 
Proper Common Pronoun 

Recall 
-28 
-29 
-53 
Precision 
+18 
+74 
+61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Differences in the number of recall and 
precision errors on the CoNLL'12 test set in com-
parison to the baseline. 

</table></figure>

			<note place="foot" n="2"> Prior work generalizes to unseen predicate-argument pairs via WordNet synsets (Resnik, 1993), a generalization corpus (Erk, 2007), or tensor factorization (Van de Cruys, 2010). Closest to our approach is neural model by Van de Cruys (2014), which, however, has much lower coverage since it is limited to 7k verbs and 30k arguments.</note>

			<note place="foot" n="3"> In this work, a predicate&apos;s argument slots are denoted predicate@slot.</note>

			<note place="foot" n="4"> https://github.com/smartschat/cort</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the German Re-search Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Informa-tion from Heterogeneous Sources (AIPHES) un-der grant No. GRK 1994/1 and the Klaus Tschira Foundation, Heidelberg, Germany.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning class-to-class selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning (ConLL)</title>
		<meeting>the ACL 2001 Workshop on Computational Natural Language Learning (ConLL)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego, Cal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1005" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Which coreference evaluation metric do you trust? A proposal for a link-based entity aware metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Use generalized representations, but do not forget surface features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadat</forename><surname>Nafise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</title>
		<meeting>the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ISP: Learning inferential selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonaventura</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
	<note>Proceedings of the Main Conference</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">English Gigaword Fifth Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Selection and Information: A Class-based Approach to Lexical Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Penn</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer and Information Science, University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
	<note>Philadelphia</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent dirichlet allocation method for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced english universal dependencies: An improved representation for natural language understanding tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>UM-CS-2012-015</idno>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Message Understanding Conference (MUC-6)</title>
		<meeting>the 6th Message Understanding Conference (MUC-6)<address><addrLine>San Mateo, Cal</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning global features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, Cal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="12" to="17" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
