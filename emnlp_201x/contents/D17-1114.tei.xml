<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1092" to="1102"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video. In this work, we propose an extrac-tive multi-modal summarization method that can automatically generate a textual summary given a set of documents, images , audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal content. For audio information, we design an approach to selectively use its transcription. For visual information, we learn the joint representations of text and images using a neural network. Finally, all of the multi-modal aspects are considered to generate the textual summary by maximizing the salience, non-redundancy, readability and coverage through the budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese, which is released to the public 1. The experimental results obtained on this dataset demonstrate that our method out-performs other competitive baseline methods .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimedia data (including text, image, audio and video) have increased dramatically recently, which makes it difficult for users to obtain important in- formation efficiently. Multi-modal summarization (MMS) can provide users with textual summaries that can help acquire the gist of multimedia data in a short time, without reading documents or watch- ing videos from beginning to end.</p><p>1 http://www.nlpr.ia.ac.cn/cip/jjzhang.htm</p><p>The existing applications related to MMS in- clude meeting record summarization ( <ref type="bibr" target="#b6">Erol et al., 2003</ref>; <ref type="bibr" target="#b9">Gross et al., 2000</ref>), sport video sum- marization ( <ref type="bibr" target="#b25">Tjondronegoro et al., 2011;</ref><ref type="bibr" target="#b10">Hasan et al., 2013)</ref>, movie summarization ( <ref type="bibr" target="#b7">Evangelopoulos et al., 2013;</ref><ref type="bibr" target="#b18">Mademlis et al., 2016)</ref>, pictorial storyline summarization ( <ref type="bibr" target="#b30">Wang et al., 2012)</ref>, time- line summarization ( <ref type="bibr" target="#b32">Wang et al., 2016b</ref>) and social multimedia summarization <ref type="bibr" target="#b4">(Del Fabro et al., 2012;</ref><ref type="bibr" target="#b0">Bian et al., 2013;</ref><ref type="bibr" target="#b21">Schinas et al., 2015;</ref><ref type="bibr" target="#b1">Bian et al., 2015;</ref><ref type="bibr" target="#b22">Shah et al., 2015</ref><ref type="bibr" target="#b23">Shah et al., , 2016</ref>. When summariz- ing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions. For the summarization of pictorial storylines, the input is a set of images with text descriptions. None of these applications focus on summarizing multimedia data that contain asyn- chronous information about general topics.</p><p>In this paper, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we propose an approach to a generate textual summary from a set of asynchronous documents, images, audios and videos on the same topic.</p><p>Since multimedia data are heterogeneous and contain more complex information than pure tex- t does, MMS faces a great challenge in address- ing the semantic gap between different modali- ties. The framework of our method is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For the audio information contained in videos, we obtain speech transcriptions through Automatic Speech Recognition (ASR) and design a method to use these transcriptions selectively. For visual information, including the key-frames extracted from videos and the images that appear in documents, we learn the joint representations of texts and images by using a neural network; we then can identify the text that is relevant to the im- age. In this way, audio and visual information can be integrated into a textual summary.</p><p>Traditional document summarization involves t- wo essential aspects: (1) Salience: the summa-Twenty-four MSF doctors, nurses, logisticians and hygiene and sanitation experts are already in the country, while additional staff will strengthen the team in the coming days. With the help of the local community, MSF's emergency teams focus on searching.</p><p>The decease's symptoms include severe fever and muscle pain, weakness, vomiting and diarrhea.</p><p>Afterwards, organs shut down, causing unstoppable bleeding. The spread of the illness is said to be through traveling mourners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal Data</head><p>Pre-processing Ebola haemorrhagic fever is a rare but serious disease that spreads rapidly through direct contact with infected people. Emergency teams focus on searching. ... ry should retain significant content of the input documents. (2) Non-redundancy: the summary should contain as little redundant content as pos- sible. For MMS, we consider two additional as- pects: (3) Readability: because speech transcrip- tions are occasionally ill-formed, we should try to get rid of the errors introduced by ASR. For ex- ample, when a transcription provides similar in- formation to a sentence in documents, we should prefer the sentence to the transcription presented in the summary. (4) Coverage for the visual in- formation: images that appear in documents and videos often capture event highlights that are usu- ally very important. Thus, the summary should cover as much of the important visual information as possible. All of the aspects can be jointly opti- mized by the budgeted maximization of submodu- lar functions ( <ref type="bibr" target="#b11">Khuller et al., 1999)</ref>.</p><p>Our main contributions are as follows:</p><p>• We design an MMS method that can automat- ically generate a textual summary from a set of asynchronous documents, images, audios and videos related to a specific topic.</p><p>• To select the representative sentences, we consider four criteria that are jointly opti- mized by the budgeted maximization of sub- modular functions.</p><p>• We introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our system can take advantage of multi-modal information and outperforms other baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-document Summarization</head><p>Multi-document summarization (MDS) attempts to extract important information for a set of docu- ments related to a topic to generate a short sum- mary. Graph based methods <ref type="bibr" target="#b19">(Mihalcea and Tarau, 2004;</ref><ref type="bibr" target="#b29">Wan and Yang, 2006;</ref><ref type="bibr" target="#b35">Zhang et al., 2016)</ref> are commonly used. <ref type="bibr">LexRank (Erkan and Radev, 2011</ref>) first builds a graph of the docu- ments, in which each node represents a sentence and the edges represent the relationship between sentences. Then, the importance of each sentence is computed through an iterative random walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-modal Summarization</head><p>In recent years, much work has been done to sum- marize meeting recordings, sport videos, movies, pictorial storylines and social multimedia. <ref type="bibr" target="#b6">Erol et al. (2003)</ref> aim to create important seg- ments of a meeting recording based on audio, tex- t and visual activity analysis. <ref type="bibr" target="#b25">Tjondronegoro et al. (2011)</ref> propose a way to summarize a sporting event by analyzing the textual information extract- ed from multiple resources and identifying the im- portant content in a sport video. <ref type="bibr" target="#b7">Evangelopoulos et al. (2013)</ref> use an attention mechanism to detect salient events in a movie. <ref type="bibr" target="#b30">Wang et al. (2012)</ref> and <ref type="bibr" target="#b32">Wang et al. (2016b)</ref> use image-text pairs to gen- erate a pictorial storyline and timeline summariza- tion. <ref type="bibr" target="#b15">Li et al. (2016)</ref> develop an approach for mul- timedia news summarization for searching results on the Internet, in which the hLDA model is intro- duced to discover the topic structure of the news documents. Then, a news article and an image are chosen to represent each topic. For social medi- a summarization, <ref type="bibr" target="#b4">Fabro et al. (2012)</ref> and <ref type="bibr" target="#b21">Schinas et al. (2015)</ref> propose to summarize the real-life events based on multimedia content such as pho- tos from Flickr and videos from YouTube. <ref type="bibr" target="#b0">Bian et al. (2013;</ref> propose a multimodal LDA to de- tect topics by capturing the correlations between textual and visual features of microblogs with em- bedded images. The output of their method is a set of representative images that describe the events. <ref type="bibr" target="#b22">Shah et al. (2015;</ref><ref type="bibr" target="#b18">2016)</ref> introduce EventBuilder which produces text summaries for a social event leveraging Wikipedia and visualizes the event with social media activities.</p><p>Most of the above studies focus on synchronous multi-modal content, i.e., in which images are paired with text descriptions and videos are paired with subtitles. In contrast, we perform summa- rization from asynchronous (i.e., there is no given description for images and no subtitle for videos) multi-modal information about news topics, in- cluding multiple documents, images and videos, to generate a fixed length textual summary. This task is both more general and more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The input is a collection of multi-modal data M = {D 1 , ..., D |D| , V 1 , ..., V |V | } related to a news topic T , where each document D i = {T i , I i } consist- s of text T i and image I i (there may be no image for some documents). V i denotes video. | · | de- notes the cardinality of a set. The objective of our work is to automatically generate textual summary to represent the principle content of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>There are many essential aspects in generating a good textual summary for multi-modal data. The salient content in documents should be retained, and the key facts in videos and images should be covered. Further, the summary should be readable and non-redundant and should follow the fixed length constraint. We propose an extraction-based method in which all these aspects can be jointly optimized by the budgeted maximization of sub- modular functions defined as follows:</p><formula xml:id="formula_0">max S⊆T {F(S) : s∈S l s ≤ L} (1)</formula><p>where T is the set of sentences, S is the summary, l s is length (number of words) of sentence s, L is budget, i.e., length constraint for the summary, and submodular function F(S) is the summary score related to the above-mentioned aspects.</p><p>Text is the main modality of documents, and in some cases, images are embedded in documents. Videos consist of at least two types of modalities: audio and visual. Next, we give overall processing methods for different modalities. Audio, i.e., speech, can be automatically tran- scribed into text by using an ASR system 2 . Then, we can leverage a graph-based method to calcu- late the salience score for all of the speech tran- scriptions and for the original sentences in doc- uments. Note that speech transcriptions are of- ten ill-formed; thus, to improve the readability, we should try to avoid the errors introduced by ASR. In addition, audio features including acoustic con- fidence ( <ref type="bibr" target="#b27">Valenza et al., 1999</ref>), audio power <ref type="bibr" target="#b2">(Christel et al., 1998</ref>) and audio magnitude <ref type="bibr">(Dagtas and Abdel-Mottaleb, 2001</ref>) have proved to be helpful for speech and video summarization which will benefit our method.</p><p>For visual, which is actually a sequence of im- ages (frames), because most of the neighboring frames contain redundant information, we first ex- tract the most meaningful frames, i.e., the key- frames, which can provide the key facts for the whole video. Then, it is necessary to perform se- mantic analysis between text and visual. To this end, we learn the joint representations for textu- al and visual modalities and can then identify the sentence that is relevant to the image. In this way, we can guarantee the coverage of generated sum- mary for the visual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Salience for Text</head><p>We apply a graph-based LexRank algorith- m ( <ref type="bibr" target="#b5">Erkan and Radev, 2011</ref>) to calculate salience score of the text unit, including the sentences in documents and the speech transcriptions from videos. LexRank first constructs a graph based on the text units and their relationship and then con- ducts an iteratively random walk to calculate the salience score of the text unit, sa(t i ), until conver- gence using the following equation:</p><formula xml:id="formula_1">Sa(t i ) = µ j Sa(t j ) · M ji + 1 − µ N (2)</formula><p>where µ is the damping factor that is set to 0.85. N is the total number of the text units. M ji is the relationship between text unit t i and t j , which is computed as follows:</p><formula xml:id="formula_2">M ji = sim(t j , t i )<label>(3)</label></formula><p>The text unit t i is represented by averaging the embeddings of the words (except stop-words) in t i . sim(·) denotes cosine similarity between two texts (negative similarities are replaced with 0).  Figure 2: LexRank with guidance strategies. e 1 is guided because speech transcription v 3 is relat- ed to document sentence v 1 ; e 2 and e 3 are guided because of audio features. Other edges without ar- row are bidirectional.</p><p>For MMS task, we propose two guidance strate- gies to amend the affinity matrix M and calculate salience score of the text as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Readability Guidance Strategies</head><p>The random walk process can be understood as a recommendation: M ji in Equation 2 denotes that t j will recommend t i to the degree of M ji . The affinity matrix M in the LexRank model is sym- metric, which means M ij = M ji . In contrast, for MMS, considering the unsatisfactory quality of speech recognition, symmetric affinity matri- ces are inappropriate. Specifically, to improve the readability, for a speech transcription, if there is a sentence in document that is related to this transcription, we would prefer to assign the tex- t sentence a higher salience score than that as- signed to the transcribed one. To this end, the pro- cess of a random walk should be guided to con- trol the recommendation direction: when a doc- ument sentence is related to a speech transcrip- tion, the symmetric weighted edge between them should be transformed into a unidirectional edge, in which we invalidate the direction from docu- ment sentence to the transcribed one. In this way, speech transcriptions will not be recommended by the corresponding document sentences. Impor- tant speech transcriptions that cannot be covered by documents still have the chance to obtain high salience scores. For the pair of a sentence t i and a speech transcription t j , M ij is computed as fol- lows:</p><formula xml:id="formula_3">M ij = 0, if sim(t i , t j ) &gt; T text sim(t i , t j ), otherwise<label>(4)</label></formula><p>where threshold T text is used to determine whether a sentence is related to others. We obtain the proper semantic similarity threshold by testing on Microsoft Research Paraphrase (MSRParaphrase) dataset <ref type="bibr" target="#b20">(Quirk et al., 2004</ref>). It is a publicly avail- able paraphrase corpus that consists of 5801 pairs of sentences, of which 3900 pairs are semantically equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Audio Guidance Strategies</head><p>Some audio features can guide the summariza- tion system to select more important and read- able speech transcriptions. <ref type="bibr" target="#b27">Valenza et al. (1999)</ref> use acoustic confidence to obtain accurate and readable summaries of broadcast news program- s. <ref type="bibr" target="#b2">Christel et al. (1998)</ref> and <ref type="bibr">Dagtas and AbdelMottaleb (2001)</ref> apply audio power and audio magnitude to find significant audio events. In our work, we first balance these three feature s- cores for each speech transcription by dividing their respective maximum values among the whole amount of audio, and we then average these scores to obtain the final audio score for speech transcrip- tion. For each adjacent speech transcription pair (t k , t k ), if the audio score a(t k ) for t k is small- er than a certain threshold while a(t k ) is greater, which means that t k is more important and read- able than t k , then t k should recommend t k , but t k should not recommend t k . We formulate it as follows:</p><formula xml:id="formula_4">M kk = sim(t k , t k ) M k k = 0 if a(t k ) &lt; T audio and a(t k ) &gt; T audio (5)</formula><p>where the threshold T audio is the average audio s- core for all the transcriptions in the audio. Finally, affinity matrices are normalized so that each row adds up to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Text-Image Matching</head><p>The key-frames contained in videos and the im- ages embedded in documents often captures news highlights in which the important ones should be covered by the textual summary. Before measur- ing the coverage for images, we should train the model to bridge the gap between text and image, i.e., to match the text and image.</p><p>We start by extracting key-frames of videos based on shot boundary detection. A shot is de- fined as an unbroken sequence of frames. The abrupt transition of RGB histogram features often indicates shot boundaries ( <ref type="bibr" target="#b36">Zhuang et al., 1998)</ref>. Specifically, when the transition of the RGB his- togram feature for adjacent frames is greater than a certain ratio 3 of the average transition for the w- hole video, we segment the shot. Then, the frames in the middle of each shot are extracted as key- frames. These key-frames and images in docu- ments make up the image set that the summary should cover.</p><p>Next, it is necessary to perform a semantic anal- ysis between the text and the image. To this end, we learn the joint representations for textual and visual modalities by using a model trained on the Flickr30K dataset ( <ref type="bibr" target="#b34">Young et al., 2014</ref>), which con- tains 31,783 photographs of everyday activities, events and scenes harvested from Flickr. Each photograph is manually labeled with 5 textual de- scriptions. We apply the framework of <ref type="bibr" target="#b31">Wang et al. (2016a)</ref>, which achieves state-of-the-art perfor- mance for text-image matching task on the Flick- r30K dataset. The image is encoded by the VG- G model <ref type="bibr" target="#b24">(Simonyan and Zisserman, 2014</ref>) that has been trained on the ImageNet classification task following the standard procedure ( <ref type="bibr" target="#b31">Wang et al., 2016a</ref>). The 4096-dimensional feature from the pre-softmax layer is used to represent the image. The text is first encoded by the Hybrid Gaussian- Laplacian mixture model (HGLMM) using the method of <ref type="bibr" target="#b12">Klein et al. (2014)</ref>. Then, the HGLM- M vectors are reduced to 6000 dimensions through PCA. Next, the sentence vector v s and image vec- tor v i are mapped to a joint space by a two-branch neural network as follows:</p><formula xml:id="formula_5">x = W 2 · f (W 1 · v s + b s ) y = V 2 · f (V 1 · v i + b i )<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">W 1 ∈ R 2048×6000 , b s ∈ R 2048 , W 2 ∈ R 512×2048 , V 1 ∈ R 2048×4096 , b i ∈ R 2048 , V 2 ∈ R 512×2048 , f is Rectified Linear Unit (ReLU).</formula><p>The max-margin learning framework is applied to optimize the neural network as follows:</p><formula xml:id="formula_7">L = i,k max[0, m + s(x i , y i ) − s(x i , y k )] + λ 1 i,k max[0, m + s(x i , y i ) − s(x k , y i )]<label>(7)</label></formula><p>where for positive text-image pair (x i , y i ), the top K most violated negative pairs (x i , y k ) and (x k , y i ) in each mini-batch are sampled. The ob- jective function L favors higher matching score s(x i , y i ) (cosine similarity) for positive text-image pairs than for negative pairs <ref type="bibr">4</ref> .</p><p>shot detection dataset of TRECVID. http://www- nlpir.nist.gov/projects/trecvid/ 4 In the experiments, K = 50, m = 0.1 and λ1 = 2. <ref type="bibr" target="#b31">Wang et al. (2016a)</ref> also proved that structure-preserving con- straints can make 1% Recall@1 improvement.</p><p>Note that the images in Flickr30K are similar to our task. However, the image descriptions are much simpler than the text in news, so the mod- el trained on Flickr30K cannot be directly used for our task. For example, some of the informa- tion contained in the news, such as the time and location of events, cannot be directly reflected by images. To solve this problem, we simplify each sentence and speech transcription based on seman- tic role labelling ( <ref type="bibr" target="#b8">Gildea and Jurafsky, 2002</ref>), in which each predicate indicates an event and the arguments express the relevant information of this event. ARG0 denotes the agent of the event, and ARG1 denotes the action. The assumption is that the concepts including agent, predicate and ac- tion compose the body of the event, so we ex- tract "ARG0+predicate+ARG1" as the simplified sentence that is used to match the images. It is worth noting that there may be multiple predicate- argument structures for one sentence and we ex- tract all of them.</p><p>After the text-image matching model is trained and the sentences are simplified, for each text- image pair (T i , I j ) in our task, we can identify the matched pairs if the score s(T i , I j ) is greater than a threshold T match . We set the threshold as the average matching score for the positive text-image pair in Flickr30K, although the matching perfor- mance for our task could in principle be improved by adjusting this parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-modal Summarization</head><p>We model the salience of a summary S as the sum of salience scores Sa(t i ) 5 of the sentence t i in the summary, combining a λ-weighted redundan- cy penalty term:</p><formula xml:id="formula_8">F s (S) = t i ∈S Sa(t i )− λ s |S| t i ,t j ∈S sim(t i , t j ) (8)</formula><p>We model the summary S coverage for the im- age set I as the weighted sum of image covered by the summary:</p><formula xml:id="formula_9">F c (S) = p i ∈I Im(p i )b i<label>(9)</label></formula><p>where the weight Im(p i ) for the image p i is the length ratio between the shot p i and the w- hole videos. b i is a binary variable to indicate whether an image p i is covered by the summary, i.e., whether there is at least one sentence in the summary matching the image. Finally, considering all the modalities, the ob- jective function is defined as follows:</p><formula xml:id="formula_10">F m (S) = 1 M s t i ∈S Sa(t i ) + 1 M c p i ∈I Im(p i )b i − λ m |S| i,j∈S sim(t i , t j )<label>(10)</label></formula><p>where M s is the summary score obtained by E- quation 8 and M c is the summary score obtained by Equation 9. The aim of M s and M c is to bal- ance the aspects of salience and coverage for im- ages. λ s , and λ m are determined by testing on development set. Note that to guaranteed mono- tone of F, λ s , and λ m should be lower than the minimum salience score of sentences. To further improve non-redundancy, we make sure that sim- ilarity between any pair of sentences in the sum- mary is lower than T text . Equations 8,9 and 10 are all monotone submod- ular functions under the budget constraint. Thus, we apply the greedy algorithm ( <ref type="bibr" target="#b17">Lin and Bilmes, 2010)</ref> guaranteeing near-optimization to solve the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>There is no benchmark dataset for MMS. We con- struct a dataset as follows. We select 50 news top- ics in the most recent five years, 25 in English and 25 in Chinese. We set 5 topics for each language as a development set. For each topic, we collect 20 documents within the same period using Google News search 6 and 5-10 videos in CCTV.com <ref type="bibr">7</ref> and Youtube <ref type="bibr">8</ref> . More details of the corpus are illustrat- ed in <ref type="table">Table 1</ref>. Some examples of news topics are provided <ref type="table" target="#tab_3">Table 2</ref>.</p><p>We employ 10 graduate students to write ref- erence summaries after reading documents and watching videos on the same topic. We keep 3 ref- erence summaries for each topic. The criteria for summarizing documents lie in: (1) retaining im- portant content of the input documents and videos; (2) avoiding redundant information; (3) having a good readability; (4) following the length limit. We set the length constraint for each English and Chinese summary to 300 words and 500 charac- ters, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparative Methods</head><p>Several models are compared in our experiments, including generating summaries with differen- t modalities and different approaches to leverage images.</p><p>Text only. This model generates summaries on- ly using the text in documents.</p><p>Text + audio. This model generates summaries using the text in documents and the speech tran- scriptions but without guidance strategies.</p><p>Text + audio + guide. This model generates summaries using the text in documents and the speech transcriptions with guidance strategies.</p><p>The following models generate summaries us- ing both documents and videos but take advantage of images in different ways. The salience scores for text are obtained with guidance strategies.</p><p>Image caption. The image is first captioned using the model of <ref type="bibr" target="#b28">Vinyals et al. (2016)</ref> which achieved first place in the 2015 MSCOCO Image Captioning Challenge. This model generates sum- maries using text in documents, speech transcrip- tion and image captions.</p><p>Note that the above-mentioned methods gener- ate summaries by using Equation 8 and the follow-ing methods using Equation 8 ,9 and 10.</p><p>Image caption match. This model uses gener- ated image captions to match the text; i.e., if the similarity between a generated image caption and a sentence exceeds the threshold T text , the image and the sentence match.</p><p>Image alignment. The images are aligned to the text in the following ways: The images in a document are aligned to all the sentences in this document and the key-frames in a shot are aligned to all the speech transcriptions in this shot.</p><p>Image match. The texts are matched with im- ages using the approach introduced in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We perform sentence 9 and word tokenization, and all the Chinese sentences are segmented by S- tanford Chinese Word Segmenter ( <ref type="bibr" target="#b26">Tseng et al., 2005</ref>). We apply Stanford CoreNLP toolkit ( <ref type="bibr" target="#b14">Levy and D. Manning, 2003;</ref><ref type="bibr" target="#b13">Klein and D. Manning, 2003)</ref> to perform lexical parsing and use se- mantic role labelling approach proposed by <ref type="bibr" target="#b33">Yang and Zong (2014)</ref>. We use 300-dimension skip- gram English word embeddings which are pub- licly available <ref type="bibr">10</ref> . Given that text-image match- ing model and image caption generation model are trained in English, to create summaries in Chinese, we first translate the Chinese text into English vi- a Google Translation <ref type="bibr">11</ref> and then conduct text and image matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-modal Summarization Evaluation</head><p>We use the ROUGE-1.5.5 toolkit ( <ref type="bibr" target="#b16">Lin and Hovy, 2003</ref>) to evaluate the output summaries. This evaluation metric measures the summary quality by matching n-grams between generated summa- ry and reference summary. <ref type="table" target="#tab_5">Table 3 and Table 4</ref> show the averaged ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) F-scores regard- ing to the three reference summaries for each topic in English and Chinese.</p><p>For the results of the English MMS, from the first three lines in <ref type="table" target="#tab_5">Table 3</ref> we can see that when summarizing without visual information, the method with guidance strategies performs slight- ly better than do the first two methods. Because Rouge mainly measures word overlaps, manual e- valuation is needed to confirm the impact of guid- ance strategies on improving readability. It is in-   troduced in Section 4.5. The rating ranges from 1 (the poorest) to 5 (the best). When summarizing with textual and visual modalities, performances are not always improved, which indicates that the models of image caption, image caption match and image alignment are not suitable to MMS. The image match model has a significant advan- tage over other comparative methods, which illus- trates that it can make use of multi-modal infor- mation. <ref type="table" target="#tab_6">Table 4</ref> shows the Chinese MMS results, which are similar to the English results that the image match model achieves the best performance. We find that the performance enhancement for the im- age match model is smaller in Chinese than it is in English, which may be due to the errors intro- duced by machine translation.</p><formula xml:id="formula_11">Method R-1 R-2 R-SU4</formula><p>We provides a generated summary in English using the image match model, which is shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Manual Summary Quality Evaluation</head><p>The readability and informativeness for sum- maries are difficult to evaluate formally. We ask five graduate students to measure the quality of summaries generated by different methods. We calculate the average score for all of the topics, and the results are displayed in <ref type="table">Table 5</ref>. Overal- l, our method with guidance strategies achieves higher scores than do the other methods, but it is still obviously poorer than the reference sum-Ramchandra Tewari , a passenger who suffered a head injury , said he was asleep when he was suddenly flung to the floor of his coach . The impact of the derailment was so strong that one of the coaches landed on top of another , crushing the one below , said Brig. Anurag Chibber , who was heading the army 's rescue team . `` We fear there could be many more dead in the lower coach , '' he said , adding that it was unclear how many people were in the coach . Kanpur is a major railway junction , and hundreds of trains pass through the city every day . `` I heard a loud noise , '' passenger Satish Mishra said . Some railway officials told local media they suspected faulty tracks caused the derailment . Fourteen cars in the 23-car train derailed , Modak said . We do n't expect to find any more bodies , '' said Zaki Ahmed , police inspector general in the northern city of Kanpur , about 65km from the site of the crash in Pukhrayan . When they tried to leave through one of the doors , they found the corridor littered with bodies , he said . The doors would n't open but we somehow managed to come out . But it has a poor safety record , with thousands of people dying in accidents every year , including in train derailments and collisions . By some analyst estimates , the railways need 20 trillion rupees ( $ 293.34 billion ) of investment by 2020 , and India is turning to partnerships with private companies and seeking loans from other countries to upgrade its network . <ref type="figure">Figure 3</ref>: An example of generated summary for the news topic "India train derailment". The sentences covering the images are labeled by the corresponding colors. The text can be partly related to the image because we use simplified sentence based on SRL to match the images. We can find some mismatched sentences, such as the sentence "Fourteen cars in the 23-car train derailed , Modak said ." where our text-image matching model may misunderstand the "car " as a "motor vehicle" but not a "coach".</p><p>maries. Specifically, when speech transcription- s are not considered, the informativeness of the summary is the worst. However, adding speech transcriptions without guidance strategies decreas- es readability to a large extent, which indicates that guidance strategies are necessary for MMS. The image match model achieves higher informa- tiveness scores than do the other methods without using images.</p><p>We give two instances of readability guidance that arise between document text (DT) and speech transcriptions (ST) in <ref type="table">Table 6</ref>. The errors intro- duced by ASR include segmentation (instance A) and recognition (instance B) mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Read  <ref type="table">Table 5</ref>: Manual summary quality evaluation. "Read" denotes "Readability" and "Inform" de- notes "informativeness".</p><p>A DT There were 12 bodies at least pulled from the rubble in the square. ST Still being pulled from the rubble. CST Many people are still being pulled from the rubble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DT</head><p>Conflict between police and protesters lit up on Tuesday.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST</head><p>Late night tensions between police and protesters briefly lit up this Baltimore neighborhood Tuesday.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CST</head><p>Late-night tensions between police and protesters briefly lit up in a Baltimore neighborhood Tuesday. <ref type="table">Table 6</ref>: Guidance examples. "CST" denotes man- ually modified correct ST. ASR errors are marked red and revisions are marked blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">How Much is the Image Worth</head><p>Text-image matching is the toughest module for our framework. Although we use a state-of-the-art approach to match the text and images, the per- formance is far from satisfactory. To find a some- what strong upper-bound of the task, we choose five topics for each language to manually label the text-image matching pairs. The MMS results on these topics are shown in <ref type="table" target="#tab_8">Table 7</ref> and <ref type="table" target="#tab_9">Table 8</ref>. The experiments show that with the ground truth text- image matching result, the summary quality can be promoted to a considerable extent, which indi- cates visual information is crucial for MMS. An image and the corresponding texts obtained using different methods are given in <ref type="figure" target="#fig_3">Figure 4</ref> an d <ref type="figure" target="#fig_4">Figure 5</ref>. We can conclude that the image caption     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper addresses an asynchronous MMS task, namely, how to use related text, audio and video information to generate a textual summary. We formulate the MMS task as an optimization prob- lem with a budgeted maximization of submodular functions. To selectively use the transcription of audio, guidance strategies are designed using the graph model to effectively calculate the salience score for each text unit, leading to more readable and informative summaries. We investigate vari- ous approaches to identify the relevance between the image and texts, and find that the image match model performs best. The final experimental re- sults obtained using our MMS corpus in both En- glish and Chinese demonstrate that our system can benefit from multi-modal information.</p><p>Adding audio and video does not seem to im- prove dramatically over text only model, which indicates that better models are needed to capture the interactions between text and other modalities, especially for visual. We also plan to enlarge our MMS dataset, specifically to collect more videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of our MMS model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Document</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Image caption:</head><label></label><figDesc>A group of people standing on top of a lush green field. Image caption match: We could barely stay standing. Image hard alignment: The need for doctors would grow as more survivors were pulled from the rubble. Image match: The search, involving US, Indian and Nepali military choppers and a battalion of 400 Nepali soldiers, has been joined by two MV-22B Osprey. Image manually match: The military helicopter was on an aid mission in Dolakha district near Tibet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example image with corresponding English texts that different methods obtain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Image caption match:Figure 5 :</head><label>5</label><figDesc>Figure 5: An example image with corresponding Chinese texts that different methods obtain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Examples of news topics.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results (F-score) for En-
glish MMS. 

Method 
R-1 
R-2 
R-SU4 

Text only 
0.409 0.113 0.167 
Text + audio 
0.407 0.111 0.166 
Text + audio + guide 0.411 0.115 0.173 
Image caption match 0.381 0.092 0.149 
Image alignment 
0.368 0.096 0.143 
Image match 
0.414 0.125 0.173 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results (F-score) for Chi-
nese MMS. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Experimental results (F-score) for En-
glish MMS on five topics with manually labeled 
text-image pairs. 

Method 
R-1 
R-2 
R-SU4 

Text + audio + guide 
0.417 0.115 0.171 
Image caption match 
0.396 0.095 0.152 
Image alignment 
0.306 0.072 0.111 
Image match 
0.401 0.127 0.179 
Image manually match 0.419 0.162 0.208 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Experimental results (F-score) for Chi-
nese MMS on five topics with manually labeled 
text-image pairs. 

</table></figure>

			<note place="foot" n="2"> We use IBM Watson Speech to Text service: www.ibm.com/watson/developercloud/speech-to-text.html</note>

			<note place="foot" n="3"> The ratio is determined by testing on the</note>

			<note place="foot" n="5"> Normalized by the maximum value among all the sentences.</note>

			<note place="foot" n="6"> http://news.google.com/ 7 http://www.cctv.com/ 8 https://www.youtube.com/</note>

			<note place="foot" n="9"> We exclude sentences containing less than 5 words. 10 https://code.google.com/archive/p/word2vec/ 11 https://translate.google.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work has been supported by the Nat-ural Science Foundation of China under <ref type="bibr">Grant No. 61333018 and No. 61403379.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimedia summarization for trending topics in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1807" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimedia summarization for social events in microblog stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evolving video skims into useful multimedia abstractions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on Human factors in computing systems</title>
		<meeting>the SIGCHI conference on Human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extraction of tv highlights using multimedia features</title>
	</analytic>
	<monogr>
		<title level="m">Multimedia Signal Processing</title>
		<editor>Serhan Dagtas and Mohamed Abdel-Mottaleb</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Summarization of real-life events based on community-contributed content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Del Fabro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Sobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><surname>Böszörmenyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth International Conferences on Advances in Multimedia</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Qiqihar Junior Teachers College</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal summarization of meeting recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berna</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2003. ICME&apos;03. Proceedings. 2003 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Evangelopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Athanasia</forename><surname>Zlatintsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1553" to="1568" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Konstantinos Rapantzikos, Georgios Skoumas, and Yannis Avrithis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic labeling of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards a multimodal meeting record</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1593" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-modal highlight generation for sports videos using an informationtheoretic excitability measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taufiq</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hynek</forename><surname>Bořil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Hl</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The budgeted maximum coverage problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph Seffi</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fisher vectors derived from hybrid gaussianlaplacian mixture models for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7399</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Is it harder to parse chinese, or the chinese treebank?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimedia news summarization in search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram co-occurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal stereoscopic movie summarization conforming to narrative characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mademlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anastasios Tefas, Nikos Nikolaidis, and Ioannis Pitas</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="5828" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, chapter TextRank: Bringing Order into Text</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing, chapter TextRank: Bringing Order into Text</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>chapter Monolingual Machine Translation for Paraphrase Generation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal graph-based event detection and summarization in social media streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Schinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symeon</forename><surname>Papadopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
	<note>Georgios Petkos, Yiannis Kompatsiaris, and Pericles A Mitkas</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eventbuilder: Real-time multimedia event summarization by visualizing social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajiv Ratn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anwar Dilawar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangshan</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="185" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Leveraging multimodal information for event summarization and concept-level sentiment analysis. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajiv Ratn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhua</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anwar Dilawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-modal summarization of key events and top players in sports tournament videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Sasongko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cher</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A conditional random field word segmenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Summarisation of spoken audio through information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianne</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESCA Tutorial and Research Workshop (ETRW) on Accessing Information in Spoken Audio</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved affinity graph based multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generating pictorial storylines via minimum-weight connected dominating set approximation in multiview graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsunori</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5005" to="5013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A low-rank approximation approach to learning joint embeddings of news stories and images for timeline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><forename type="middle">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dragomir Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multipredicate semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Abstractive cross-language summarization via translation model enhanced predicate argument structure fusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1842" to="1853" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive key frame extraction using unsupervised clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing, 1998. ICIP 98. Proceedings. 1998 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="866" to="870" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
