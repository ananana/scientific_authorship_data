<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Multilingual Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
							<email>xlchen@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Cornell Unversity Ithaca</orgName>
								<address>
									<postCode>14853</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
							<email>cardie@cs.cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Cornell Unversity Ithaca</orgName>
								<address>
									<postCode>14853</postCode>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Multilingual Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="261" to="270"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>261</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision , which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however , merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs 1 that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition , our model even beats supervised approaches trained with cross-lingual resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous distributional word representa- tions ( <ref type="bibr" target="#b26">Turian et al., 2010</ref>) have become a common technique across a wide variety of NLP tasks. Recent research, moreover, proposes cross-lingual word representations ( <ref type="bibr" target="#b16">Klementiev et al., 2012;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013a</ref>) that create a shared em- bedding space for words across two (Bilingual Word Embeddings, BWE) or more languages (Multilingual Word Embeddings, MWE). Words from different languages with similar meanings will be close to one another in this cross-lingual embedding space. These embeddings have been found beneficial for a number of cross-lingual and even monolingual NLP tasks <ref type="bibr" target="#b13">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b0">Ammar et al., 2016</ref>).</p><p>The most common form of cross-lingual word representations is the BWE, which connects the lexical semantics of two languages. Traditionally for training BWEs, cross-lingual supervision is re- quired, either in the form of parallel corpora <ref type="bibr" target="#b16">(Klementiev et al., 2012;</ref><ref type="bibr" target="#b30">Zou et al., 2013)</ref>, or in the form of bilingual lexica ( <ref type="bibr" target="#b20">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b28">Xing et al., 2015)</ref>. This makes learning BWEs for low-resource language pairs much more dif- ficult. Fortunately, there are attempts to reduce the dependence on bilingual supervision by requir- ing a very small parallel lexicon such as identi- cal character strings ( <ref type="bibr" target="#b23">Smith et al., 2017)</ref>, or nu- merals ( <ref type="bibr" target="#b2">Artetxe et al., 2017</ref>). Furthermore, re- cent work proposes approaches to obtain unsuper- vised BWEs without relying on any bilingual re- sources ( <ref type="bibr" target="#b29">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018b)</ref>.</p><p>In contrast to BWEs that only focus on a pair of languages, MWEs instead strive to leverage the interdependencies among multiple languages to learn a multilingual embedding space. MWEs are desirable when dealing with multiple languages simultaneously and have also been shown to im- prove the performance on some bilingual tasks thanks to its ability to acquire knowledge from other languages ( <ref type="bibr" target="#b0">Ammar et al., 2016;</ref><ref type="bibr" target="#b12">Duong et al., 2017)</ref>. Similar to training BWEs, cross-lingual su- pervision is typically needed for training MWEs, and the prior art for obtaining fully unsupervised MWEs simply maps all the languages indepen- dently to the embedding space of a chosen tar- get language 2 (usually English) ( <ref type="bibr" target="#b18">Lample et al., 2018b</ref>). There are downsides, however, when us- ing a single fixed target language with no interac- tion between any of the two source languages. For instance, French and Italian are very similar, and the fact that each of them is individually converted to a less similar language, English for example, in order to produce a shared embedding space will inevitably degrade the quality of the MWEs.</p><p>For certain multilingual tasks such as translat- ing between any pair of N given languages, an- other option for obtaining UMWEs exists. One can directly train UBWEs for each of such lan- guage pairs (referred to as BWE-Direct). This is seldom used in practice, since it requires training O(N 2 ) BWE models as opposed to only O(N ) in BWE-Pivot, and is too expensive for most use cases. Moreover, this method still does not fully exploit the language interdependence. For exam- ple, when learning embeddings between French and Italian, BWE-Direct only utilizes information from the pair itself, but other Romance languages such as Spanish may also provide valuable infor- mation that could improve performance.</p><p>In this work, we propose a novel unsupervised algorithm to train MWEs using only monolingual corpora (or equivalently, monolingual word em- beddings). Our method exploits the interdepen- dencies between any two languages and maps all monolingual embeddings into a shared multilin- gual embedding space via a two-stage algorithm consisting of (i) Multilingual Adversarial Training (MAT) and (ii) Multilingual Pseudo-Supervised Refinement (MPSR). As shown by experimental results on multilingual word translation and cross- lingual word similarity, our model is as efficient as BWE-Pivot yet outperforms both BWE-Pivot and BWE-Direct despite the latter being much more expensive. In addition, our model achieves a higher overall performance than state-of-the-art supervised methods in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a plethora of literature on learning cross- lingual word representations, focusing either on a pair of languages, or multiple languages at the same time ( <ref type="bibr" target="#b16">Klementiev et al., 2012;</ref><ref type="bibr" target="#b30">Zou et al., 2013;</ref><ref type="bibr" target="#b20">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b15">Gouws et al., 2015;</ref><ref type="bibr" target="#b10">Coulmance et al., 2015;</ref><ref type="bibr" target="#b0">Ammar et al., 2016;</ref><ref type="bibr">Duong et al., 2017, inter alia)</ref>. One shortcom- ing of these methods is the dependence on cross- lingual supervision such as parallel corpora or bilingual lexica. Abundant research efforts have been made to alleviate such dependence <ref type="bibr" target="#b27">(Vuli´cVuli´c and Moens, 2015;</ref><ref type="bibr" target="#b2">Artetxe et al., 2017;</ref><ref type="bibr" target="#b23">Smith et al., 2017)</ref>, but consider only the case of a single pair of languages (BWEs). Furthermore, fully unsu- pervised methods exist for learning <ref type="bibr">BWEs (Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018b;</ref><ref type="bibr" target="#b3">Artetxe et al., 2018a</ref>). For unsupervised MWEs, however, pre- vious methods merely rely on a number of inde- pendent BWEs to separately map each language into the embedding space of a chosen target lan- guage ( <ref type="bibr" target="#b23">Smith et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018b)</ref>.</p><p>Adversarial Neural Networks have been suc- cessfully applied to various cross-lingual NLP tasks where annotated data is not available, such as cross-lingual text classification <ref type="bibr" target="#b8">(Chen et al., 2016)</ref>, unsupervised BWE induction ( <ref type="bibr" target="#b29">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018b</ref>) and unsupervised machine translation ( <ref type="bibr" target="#b17">Lample et al., 2018a;</ref><ref type="bibr" target="#b4">Artetxe et al., 2018b</ref>). These works, however, only consider the case of two languages, and our MAT method ( §3.1) is a generalization to multiple languages. <ref type="bibr" target="#b20">Mikolov et al. (2013a)</ref> first propose to learn cross-lingual word representations by learning a linear mapping between the monolingual embed- ding spaces of a pair of languages. It has then been observed that enforcing the linear mapping to be orthogonal could significantly improve per- formance ( <ref type="bibr" target="#b28">Xing et al., 2015;</ref><ref type="bibr" target="#b1">Artetxe et al., 2016;</ref><ref type="bibr" target="#b23">Smith et al., 2017)</ref>. These methods solve a linear equation called the orthogonal Procrustes prob- lem for the optimal orthogonal linear mapping be- tween two languages, given a set of word pairs as supervision. <ref type="bibr" target="#b2">Artetxe et al. (2017)</ref> find that when using weak supervision (e.g. digits in both lan- guages), applying this Procrustes process itera- tively achieves higher performance. <ref type="bibr" target="#b18">Lample et al. (2018b)</ref> adopt the iterative Procrustes method with pseudo-supervision in a fully unsupervised setting and also obtain good results. In the MWE task, however, the multilingual mappings no longer have a closed-form solution, and we hence pro- pose the MPSR algorithm ( §3.2) for learning mul- tilingual embeddings using gradient-based opti- mization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this work, our goal is to learn a single multi- lingual embedding space for N languages, with- out relying on any cross-lingual supervision. We assume that we have access to monolingual em- beddings for each of the N languages, which can be obtained using unlabeled monolingual cor- pora ( <ref type="bibr" target="#b21">Mikolov et al., 2013b;</ref>. We now present our unsupervised MWE (UMWE) model that jointly maps the monolin- gual embeddings of all N languages into a single space by explicitly leveraging the interdependen- cies between arbitrary language pairs, but is com- putationally as efficient as learning O(N ) BWEs (instead of O(N 2 )).</p><p>Denote the set of languages as L with |L | = N . Suppose for each language l ∈ L with vocab- ulary V l , we have a set of d-dimensional mono- lingual word embeddings E l of size |V l | × d. Let S l denote the monolingual embedding space for l, namely the distribution of the monolingual em- beddings of l. If a set of embeddings E are in an embedding space S, we write E S (e.g. ∀l : E l S l ). Our models learns a set of encoders M l , one for each language l, and the correspond- ing decoders M −1 l . The encoders map all E l to a single target space T :</p><formula xml:id="formula_0">M l (E l ) T . On the other hand, a decoder M −1 l maps an embedding in T back to S l .</formula><p>Previous research ( <ref type="bibr" target="#b20">Mikolov et al., 2013a)</ref> shows that there is a strong linear correlation between the vector spaces of two languages, and that learn- ing a complex non-linear neural mapping does not yield better results. <ref type="bibr" target="#b28">Xing et al. (2015)</ref> further show that enforcing the linear mappings to be orthogo- nal matrices achieves higher performance. There- fore, we let our encoders M l be orthogonal linear matrices, and the corresponding decoders can be obtained by simply taking the transpose:</p><formula xml:id="formula_1">M −1 l = M l .</formula><p>Thus, applying the encoder or decoder to an embedding vector is accomplished by multiplying the vector with the encoder/decoder matrix.</p><p>Another benefit of using linear encoders and de- coders (also referred to as mappings) is that we can learn N − 1 mappings instead of N by choosing the target space T to be the embedding space of a specific language (denoted as the target language) without losing any expressiveness of the model. Given a MWE with an arbitrary T , we can con- struct an equivalent one with only N −1 mappings by multiplying the encoders of each language M l to the decoder of the chosen target language M t :</p><formula xml:id="formula_2">M t = M t M t = I M l E l = (M t M l )E l S t</formula><p>where I is the identity matrix. The new MWE is isomorphic to the original one.</p><p>We now present the two major components of our approach, Multilingual Adversarial Training ( §3.1) and Multilingual Pseudo-Supervised Re- finement ( §3.2).  <ref type="figure">Figure 1</ref>: Multilingual Adversarial Training (Algo- rithm 1). lang i and lang j are two randomly selected languages at each training step. J Dj and J Mi are the objectives of D j and M i , respectively (Eqn. 1 and 2).</p><formula xml:id="formula_3">D j M i M ⊤ j J D j J M i lang i lang i lang j lang j lang j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multilingual Adversarial Training</head><p>In this section, we introduce an adversarial train- ing approach for learning multilingual embed- dings without cross-lingual supervision. Adver- sarial Training is a powerful technique for min- imizing the divergence between complex distri- butions that are otherwise difficult to directly model ( <ref type="bibr" target="#b14">Goodfellow et al., 2014</ref>). In the cross- lingual setting, it has been successfully ap- plied to unsupervised cross-lingual text classifica- tion ( <ref type="bibr" target="#b8">Chen et al., 2016</ref>) and unsupervised bilin- gual word embedding learning ( <ref type="bibr" target="#b29">Zhang et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018b</ref>). However, these methods only consider one pair of languages at a time, and do not fully exploit the cross-lingual relations in the multilingual setting. <ref type="figure">Figure 1</ref> shows our Multilingual Adversarial Training (MAT) model and the training procedure is described in Algorithm 1. Note that as ex- plained in §3, the encoders and decoders adopted in practice are orthogonal linear mappings while the shared embedding space is chosen to be the same space as a selected target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Multilingual Adversarial Training</head><p>Require: Vocabulary Vi for each language lang i ∈ L . Hy- perparameter k ∈ N. 1: repeat 2:</p><p>D iterations 3:</p><p>for diter = 1 to k do 4:</p><formula xml:id="formula_4">loss d = 0 5: for all lang j ∈ L do 6: Select at random lang i ∈ L 7:</formula><p>Sample a batch of words xi ∼ Vi 8:</p><p>Sample a batch of words xj ∼ Vj 9:</p><formula xml:id="formula_5">ˆ xt = Mi(xi) encode to T 10: ˆ xj = M j (ˆ xt) decode to Sj 11: yj = Dj(xj) real vectors 12: ˆ yj = Dj(ˆ xj) converted vectors 13: loss d += L d (1, yj) + L d (0, ˆ yj) 14:</formula><p>Update all D parameters to minimize loss d 15:</p><p>M iteration 16:</p><formula xml:id="formula_6">loss = 0 17: for all lang i ∈ L do 18:</formula><p>Select at random lang j ∈ L 19:</p><p>Sample a batch of words xi ∼ Vi 20:</p><formula xml:id="formula_7">ˆ xt = Mi(xi) encode to T 21: ˆ xj = M j (ˆ xt) decode to Sj 22: ˆ yj = Dj(ˆ xj) 23: loss += L d (1, ˆ yj) 24:</formula><p>Update all M parameters to minimize loss 25:</p><formula xml:id="formula_8">orthogonalize(M) see §3.3 26: until convergence</formula><p>In order to learn a multilingual embedding space without supervision, we employ a series of language discriminators D l , one for each lan- guage l ∈ L . Each D l is a binary classifier with a sigmoid layer on top, and is trained to identify how likely a given vector is from S l , the embed- ding space of language l. On the other hand, to train the mappings, we convert a vector from a ran- dom language lang i to another random language lang j (via the target space T first). The objective of the mappings is to confuse D j , the language dis- criminator for lang j , so the mappings are updated in a way that D j cannot differentiate the converted vectors from the real vectors in S j . This multilin- gual objective enables us to explicitly exploit the relations between all language pairs during train- ing, leading to improved performance.</p><p>Formally, for any language lang j , the objective that D j is minimizing is:</p><formula xml:id="formula_9">J D j = E i∼L E x i ∼S i x j ∼S j L d (1, D j (x j )) + L d 0, D j (M j M i x i )<label>(1)</label></formula><p>where</p><formula xml:id="formula_10">L d (y, ˆ y)</formula><p>is the loss function of D, which is chosen as the cross entropy loss in practice. y is the language label with y = 1 indicates a real embedding from that language.</p><p>Furthermore, the objective of M i for lang i is:</p><formula xml:id="formula_11">J M i = E j∼L E x i ∼S i x j ∼S j L d 1, D j (M j M i x i )<label>(2)</label></formula><p>where M i strives to make D j believe that a con- verted vector to lang j is instead real. This adver- sarial relation between M and D stimulates M to learn a shared multilingual embedding space by making the converted vectors look as authentic as possible so that D cannot predict whether a vector is a genuine embedding from a certain language or converted from another language via M. In addition, we allow lang i and lang j to be the same language in (1) and <ref type="bibr">(2)</ref>. In this case, we are encoding a language to T and back to itself, essentially forming an adversarial autoen- coder ( <ref type="bibr" target="#b19">Makhzani et al., 2015)</ref>, which is reported to improve the model performance ( <ref type="bibr" target="#b29">Zhang et al., 2017)</ref>. Finally, on Line 5 and 17 in Algorithm 1, a for loop is used instead of random sampling. This is to ensure that in each step, every discrimina- tor (or mapping) is getting updated at least once, so that we do not need to increase the number of training iterations when adding more languages. Computationally, when compared to the BWE- Pivot and BWE-Direct baselines, one step of MAT training costs similarly to N BWE training steps, and in practice we train MAT for the same num- ber of iterations as training the baselines. There- fore, MAT training scales linearly with the num- ber of languages similar to BWE-Pivot (instead of quadratically as in BWE-Direct).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multilingual Pseudo-Supervised Refinement</head><p>Using MAT, we are able to obtain UMWEs with reasonable quality, but they do not yet achieve state-of-the-art performance. Previous research on learning unsupervised BWEs ( <ref type="bibr" target="#b18">Lample et al., 2018b</ref>) observes that the embeddings obtained from adversarial training do a good job aligning the frequent words between two languages, but performance degrades when considering the full vocabulary. They hence propose to use an iter- ative refinement method (Artetxe et al., 2017) to repeatedly refine the embeddings obtained from the adversarial training. The idea is that we can anchor on the more accurately predicted relations between frequent words to improve the mappings learned by adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Multilingual Pseudo-Supervised Re- finement</head><p>Require: A set of (pseudo-)supervised lexica of word pairs between each pair of languages Lex(lang i , lang j ).</p><note type="other">1: repeat 2: loss = 0 3: for all lang i ∈ L do 4: Select at random lang j ∈ L 5: Sample (xi, xj) ∼ Lex(lang i , lang j ) 6: ti = Mi(xi) encode xi 7: tj = Mj(xj) encode xj 8: loss += Lr(ti, tj) refinement loss 9: Update all M parameters to minimize loss 10: orthogonalize(M) see §3.3 11: until convergence</note><p>When learning MWEs, however, it is desirable to go beyond aligning each language with the tar- get space individually, and instead utilize the re- lations between all languages as we did in MAT. Therefore, we in this section propose a general- ization of the existing refinement methods to in- corporate a multilingual objective.</p><p>In particular, MAT can produce an approxi- mately aligned embedding space. As mentioned earlier, however, the training signals from D for rare words are noisier and may lead to worse performance. Thus, the idea of Multilingual Pseudo-Supervised Refinement (MPSR) is to in- duce a dictionary of highly confident word pairs for every language pair, used as pseudo supervi- sion to improve the embeddings learned by MAT. For a specific language pair (lang i , lang j ), the pseudo-supervised lexicon Lex(lang i , lang j ) is constructed from mutual nearest neighbors be- tween M i E i and M j E j , among the most frequent 15k words of both languages.</p><p>With the constructed lexica, the MPSR objective is:</p><formula xml:id="formula_12">J r = E (i,j)∼L 2 E (x i ,x j )∼Lex(i,j) L r (M i x i , M j x j ) (3) where L r (x, ˆ</formula><p>x) is the loss function for MPSR, for which we use the mean square loss. The MPSR training is depicted in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Lingual Similarity Scaling (CSLS)</head><p>When constructing the pseudo-supervised lexica, a distance metric between embeddings is needed to compute nearest neighbors. Standard distance metrics such as the Euclidean distance or cosine similarity, however, can lead to the hubness problem in high-dimensional spaces when used to calculate nearest neighbors <ref type="bibr" target="#b22">(Radovanovi´cRadovanovi´c et al., 2010;</ref><ref type="bibr" target="#b11">Dinu and Baroni, 2015)</ref>. Namely, some words are very likely to be the nearest neighbors of many others (hubs), while others are not the nearest neighbor of any word. This problem is addressed in the literature by designing alternative distance metrics, such as the inverted softmax ( <ref type="bibr" target="#b23">Smith et al., 2017</ref>) or the CSLS ( <ref type="bibr" target="#b18">Lample et al., 2018b)</ref>. In this work, we adopt the CSLS similarity as a drop-in replacement for cosine similarity whenever a distance metric is needed. The CSLS similarity (whose negation is a distance metric) is calculated as follows:</p><p>CSLS(x, y) = 2 cos(x, y)</p><formula xml:id="formula_13">− 1 n y ∈N Y (x)</formula><p>cos(x, y )</p><formula xml:id="formula_14">− 1 n x ∈N X (y) cos(x , y)<label>(4)</label></formula><p>where N Y (x) is the set of n nearest neighbors of x in the vector space that y comes from: Y = {y 1 , ..., y |Y | }, and vice versa for N X (y). In prac- tice, we use n = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Orthogonalization</head><p>As mentioned in §3, orthogonal linear mappings are the preferred choice when learning transforma- tions between the embedding spaces of different languages ( <ref type="bibr" target="#b28">Xing et al., 2015;</ref><ref type="bibr" target="#b23">Smith et al., 2017</ref>). Therefore, we perform an orthogonalization up- date ( <ref type="bibr" target="#b9">Cisse et al., 2017)</ref> after each training step to ensure that our mappings M are (approximately) orthogonal:</p><formula xml:id="formula_15">∀l : M l = (1 + β)M l − βM l M l M l</formula><p>where β is set to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unsupervised Multilingual Validation</head><p>In order to do model selection in the unsupervised setting, where no validation set can be used, a sur- rogate validation criterion is required that does not depend on bilingual data. Previous work shows promising results using such surrogate criteria for model validation in the bilingual case ( <ref type="bibr" target="#b18">Lample et al., 2018b</ref>), and we in this work adopt a vari- ant adapted to our multilingual setting:</p><formula xml:id="formula_16">V (M, E) = E (i,j)∼P ij mean csls(M j M i E i , E j ) = i =j p ij · mean csls(M j M i E i , E j )</formula><p>where p ij forms a probability simplex. In this work, we let all p ij = 1 N (N −1) so that V (M, E) reduces to the macro average over all language pairs. Using different p ij values can place varying weights on different language pairs, which might be desirable in certain scenarios.</p><p>The mean csls function is an unsupervised bilingual validation criterion proposed by <ref type="bibr" target="#b18">Lample et al. (2018b)</ref>, which is the mean CSLS similari- ties between the most frequent 10k words and their translations (nearest neighbors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present experimental results to demonstrate the effectiveness of our unsupervised MWE method on two benchmark tasks, the mul- tilingual word translation task, and the SemEval- 2017 cross-lingual word similarity task. We com- pare our MAT+MPSR method with state-of-the- art unsupervised and supervised approaches, and show that ours outperforms previous methods, su- pervised or not, on both tasks.</p><p>Pre-trained 300d fastText (monolingual) em- beddings ( ) trained on the Wikipedia corpus are used for all systems that re- quire monolingual word embeddings for learning cross-lingual embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multilingual Word Translation</head><p>In this section, we consider the task of word trans- lation between arbitrary pairs of a set of N lan- guages. To this end, we use the recently released multilingual word translation dataset on six lan- guages: English, French, German, Italian, Por- tuguese and Spanish ( <ref type="bibr" target="#b18">Lample et al., 2018b</ref>). For any pair of the six languages, a ground-truth bilin- gual dictionary is provided with a train-test split of 5000 and 1500 unique source words, respec- tively. The 5k training pairs are used in training supervised baseline methods, while all unsuper- vised methods do not rely on any cross-lingual re- sources. All systems are tested on the 1500 test word pairs for each pair of languages.</p><p>For comparison, we adopted a state-of-the-art unsupervised BWE method ( <ref type="bibr" target="#b18">Lample et al., 2018b)</ref> and generalize it for the multilingual setting us- ing the two aforementioned approaches, namely BWE-Pivot and BWE-Direct, to produce unsuper- vised baseline MWE systems. English is cho- sen as the pivot language in BWE-Pivot. We fur- ther incorporate the supervised BWE-Direct (Sup- BWE-Direct) method as a baseline, where each BWE is trained on the 5k gold-standard word pairs via the orthogonal Procrustes process ( <ref type="bibr" target="#b2">Artetxe et al., 2017;</ref><ref type="bibr" target="#b18">Lample et al., 2018b</ref>). <ref type="table" target="#tab_2">Table 1</ref> presents the evaluation results, wherein the numbers represent precision@1, namely how many times one of the correct translations of a source word is retrieved as the top candidate. All systems retrieve word translations using the CSLS similarity in the learned embedding space. Ta- ble 1a shows the detailed results for all 30 lan- guage pairs, while <ref type="table" target="#tab_2">Table 1b</ref> summarizes the re- sults in a number of ways. We first observe the training cost of all systems summarized in Ta- ble 1b. #BWEs indicates the training cost of a cer- tain method measured by how many BWE mod- els it is equivalent to train. BWE-Pivot needs to train 2(N −1) BWEs since a separate BWE is trained for each direction in a language pair for increased performance. BWE-Direct on the other hand, trains an individual BWE for all (again, di- rected) pairs, resulting a total of N (N −1) BWEs. The supervised Sup-BWE-Direct method trains the same number of BWEs as BWE-Direct but is much faster in practice, for it does not require the unsupervised adversarial training stage. Finally, while our MAT+MPSR method does not train in- dependent BWEs, as argued in §3.1, the training cost is roughly equivalent to training N −1 BWEs, which is corroborated by the real training time shown in <ref type="table" target="#tab_2">Table 1b</ref>.</p><p>We can see in <ref type="table" target="#tab_2">Table 1a</ref> that our MAT+MPSR method achieves the highest performance on all but 3 language pairs, compared against both the unsupervised and supervised approaches. When looking at the overall performance across all lan- guage pairs, BWE-Direct achieves a +0.6% per- formance gain over BWE-Pivot at the cost of be- ing much slower to train. When supervision is available, Sup-BWE-Direct further improves an- other 0.4% over BWE-Direct. Our MAT+MPSR method, however, attains an impressive 1.3% im- provement against Sup-BWE-Direct, despite the lack of cross-lingual supervision.</p><p>To provide a more in-depth examination of the results, we first consider the Romance language pairs, such as fr-es, fr-it, fr-pt, es-it, it-pt and their reverse directions. BWE-Pivot performs notably worse than BWE-Direct on these pairs, which vali- dates our hypothesis that going through a less sim- ilar language (English) when translating between en-de en-fr en-es en-it en-pt de-fr de-es de-it de-pt fr-es fr-it fr-pt es-it es-pt it-pt Supervised methods with cross-lingual supervision Sup-BWE-Direct 73.5 81.1 81.4 77.3 79.9 73.3 67.7 69.5 59.1 82.6 83.2 78.1 83.5 87.3 81.0 de-en fr-en es-en it-en pt-en fr-de es-de it-de pt-de es-fr it-fr pt-fr it-es pt-es pt-it   similar languages will result in reduced accuracy. Our MAT+MPSR method, however, overcomes this disadvantage of BWE-Pivot and achieves the best performance on all these pairs through an explicit multilingual learning mechanism without increas- ing the computational cost.</p><p>Furthermore, our method also beats the BWE- Direct approach, which supports our second hy- pothesis that utilizing knowledge from languages beyond the pair itself could improve performance. For instance, there are a few pairs where BWE- Pivot outperforms BWE-Direct, such as de-it, it- de and pt-de, even though it goes through a third language (English) in BWE-Pivot. This might suggest that for some less similar language pairs, leveraging a third language as a bridge could in some cases work better than only relying on the language pair itself. German is involved in all these language pairs where BWE-Pivot outper- forms than BWE-Direct, which is potentially due to the similarity between German and the pivot language English. We speculate that if choosing a different pivot language, there might be other pairs that could benefit. This observation serves as a possible explanation of the superior perfor- mance of our multilingual method over BWE- Direct, since our method utilizes knowledge from all languages during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Lingual Word Similarity</head><p>In this section, we evaluate the quality of our MWEs on the cross-lingual word similarity (CLWS) task, which assesses how well the sim- ilarity in the cross-lingual embedding space cor- responds to a human-annotated semantic similar- ity score. The high-quality CLWS dataset from <ref type="bibr">SemEval-2017</ref><ref type="bibr" target="#b6">(Camacho-Collados et al., 2017</ref>) is en-de en-es de-es en-it de-it es-it en-fa de-fa es-fa it-fa Average  used for evaluation. The dataset contains word pairs from any two of the five languages: English, German, Spanish, Italian, and Farsi (Persian), an- notated with semantic similarity scores. In addition to the BWE-Pivot and BWE- Direct baseline methods, we also include the two best-performing systems on <ref type="bibr">SemEval-2017, Luminoso (Speer and</ref><ref type="bibr" target="#b25">Lowry-Duda, 2017)</ref> and NASARI <ref type="bibr" target="#b7">(Camacho-Collados et al., 2016</ref>) for comparison. Note that these two methods are su- pervised, and have access to the Europarl 3 (for all languages but Farsi) and the OpenSubtitles2016 4 parallel corpora. <ref type="table" target="#tab_4">Table 2</ref> shows the results, where the perfor- mance of each model is measured by the Spear- man correlation. When compared to the BWE- Pivot and the BWE-Direct baselines, MAT+MPSR continues to perform the best on all language pairs. The qualitative findings stay the same as in the word translation task, except the margin is less sig- nificant. This might be because the CLWS task is much more lenient compared to the word transla- tion task, where in the latter one needs to correctly identify the translation of a word out of hundreds of thousands of words in the vocabulary. In CLWS though, one can still achieve relatively high corre- lation in spite of minor inaccuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised methods with cross-lingual supervision</head><p>On the other hand, an encouraging result is that when compared to the state-of-the-art super- vised results, our MAT+MPSR method outperforms NASARI by a very large margin, and achieves top-notch overall performance similar to the com- petition winner, Luminoso, without using any bi- texts. A closer examination reveals that our unsu- pervised method lags a few points behind Lumi-noso on the European languages wherein the su- pervised methods have access to the large-scale high-quality Europarl parallel corpora. It is the low-resource language, Farsi, that makes our un- supervised method stand out. All of the unsuper- vised methods outperform the supervised systems from SemEval-2017 on language pairs involving Farsi, which is not covered by the Europarl bitexts. This suggests the advantage of learning unsuper- vised embeddings for lower-resourced languages, where the supervision might be noisy or absent. Furthermore, within the unsupervised methods, MAT+MPSR again performs the best, and attains a higher margin over the baseline approaches on the low-resource language pairs, vindicating our claim of better multilingual performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a fully unsupervised model for learning multilingual word embeddings (MWEs). Although methods exist for learning high-quality unsupervised BWEs ( <ref type="bibr" target="#b18">Lample et al., 2018b</ref>), little work has been done in the unsuper- vised multilingual setting. Previous work relies solely on a number of unsupervised BWE models to generate MWEs (e.g. BWE-Pivot and BWE- Direct), which does not fully leverage the interde- pendencies among all the languages. Therefore, we propose the MAT+MPSR method that explicitly exploits the relations between all language pairs without increasing the computational cost. In our experiments on multilingual word translation and cross-lingual word similarity <ref type="bibr">(SemEval-2017)</ref>, we show that MAT+MPSR outperforms existing unsu- pervised and even supervised models, achieving new state-of-the-art performance.</p><p>For future work, we plan to investigate how our method can be extended to work with other BWE frameworks, in order to overcome the instability issue of <ref type="bibr" target="#b18">Lample et al. (2018b)</ref>. As pointed out by recent work <ref type="bibr" target="#b24">(Søgaard et al., 2018;</ref><ref type="bibr" target="#b3">Artetxe et al., 2018a)</ref>, the method by <ref type="bibr" target="#b18">Lample et al. (2018b)</ref> per- forms much worse on certain languages such as Finnish, etc. More reliable multilingual embed- dings might be obtained on these languages if we adapt our multilingual training framework to work with the more robust methods proposed recently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Embeddings from langi langi Encoder langj Decoder langj Discriminator Shared Embedding Space Embeddings from langj Forward and backward passes when training M Forward and backward passes when training D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>#BWEs time en-xx de-xx fr-xx es-xx it-xx pt-xx xx-en xx-de xx-fr xx-es xx-it xx-pt Overall Supervised methods with cross-lingual supervision Sup-BWE-Direct N (N −1) 4h 78.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Multilingual Word Translation Results for English, German, French, Spanish, Italian and Portuguese. The 
reported numbers are precision@1 in percentage. All systems use the nearest neighbor under the CSLS distance 
for predicting the translation of a certain word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for the SemEval-2017 Cross-Lingual Word Similarity task. Spearman's ρ is reported. Lumi-
noso (Speer and Lowry-Duda, 2017) and NASARI (Camacho-Collados et al., 2016) are the two top-performing 
systems for SemEval-2017 that reported results on all language pairs. 

</table></figure>

			<note place="foot" n="1"> Code: https://github.com/ccsasuke/umwe</note>

			<note place="foot" n="2"> Henceforth, we refer to this method as BWE-Pivot as the target language serves as a pivot to connect other languages.</note>

			<note place="foot" n="3"> http://opus.nlpl.eu/Europarl.php 4 http://opus.nlpl.eu/ OpenSubtitles2016.php</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Massively multilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1602.01925</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="789" to="798" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval2017 task 2: Multilingual and cross-lingual semantic word similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page" from="36" to="64" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>arXiv e-prints 1606.01614v5</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transgram, fast cross-lingual word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Workshop Track</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual training of crosslingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jgou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Radovanovi´cradovanovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjana</forename><surname>Ivanovi´civanovi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2487" to="2531" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the limitations of unsupervised bilingual dictionary induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conceptnet at semeval-2017 task 2: Extending word embeddings with multilingual relational knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Lowry-Duda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-03" />
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="719" to="725" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
