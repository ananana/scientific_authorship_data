<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Patel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sands</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution" key="instit1">Plasticity Inc. San Francisco</orgName>
								<orgName type="institution" key="instit2">Plasticity Inc</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit2">LIMSI</orgName>
								<orgName type="institution" key="instit3">CNRS Université Paris-Saclay</orgName>
								<address>
									<postCode>91403</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Magnitude: A Fast, Efficient Universal Vector Embedding Utility Package</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations)</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations) <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="120" to="126"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>120</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Vector space embedding models like word2vec, GloVe, fastText, and ELMo are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing embeddings. Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings. Magnitude performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnitude is an open source Python package de- veloped by Ajay Patel and Alexander Sands ( <ref type="bibr" target="#b6">Patel and Sands, 2018)</ref>. It provides a full set of features and a new vector storage file format that make it possible to use vector embeddings in a fast, ef- ficient, and simple manner. It is intended to be a simpler and faster alternative to current utilities for word vectors like Gensim <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref>.</p><p>Magnitude's file format (".magnitude") is an ef- ficient universal vector embedding format. The Magnitude library implements on-demand lazy loading for faster file loading, caching for bet- ter performance of repeated queries, and fast pro- cessing of bulk key queries. <ref type="table">Table 1</ref> gives speed benchmark comparisons between Magnitude and Gensim for various operations on the Google News pre-trained word2vec model ( <ref type="bibr" target="#b5">Mikolov et al., 2013</ref> file, a 97x speed-up. Gensim uses 5GB of RAM versus 18KB for Magnitude. Magnitude implements functions for looking up vector representations for misspelled or out-of- vocabulary words, quantization of vector models, exact and approximate similarity search, concate- nating multiple vector models together, and ma- nipulating models that are larger than a computer's main memory. Magnitude's ease of use and simple interface combined with its speed, efficiency, and novel features make it an excellent tool for cases ranging from applications used in production envi- ronments to academic research to students in nat- ural language processing courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Magnitude offers solutions to a number of prob- lems with current utilities.</p><p>Speed: Existing utilities are prohibitively slow for iterative development. Many projects use Gen- sim to load the Google News word2vec model di- rectly from a ".bin" or ".txt" file multiple times. It can take between a minute to a minute and a half to load the file.</p><p>Memory: A production web server will run multiple processes for serving requests. Running Gensim, in the same configuration, will consume &gt;4GB of RAM usage per process.</p><p>Code duplication: Many developers duplicate effort by writing commonly used routines that are not provided in current utilities. Namely, routines for concatenating embeddings, bulk key lookup, out-of-vocabulary search, and building indexes for approximate k-nearest neighbors.</p><p>The Magnitude library uses several well- engineered libraries to achieve its performance im- provements. It uses SQLite 1 as its underlying data store, and takes advantage of database in- dexes for fast key lookups and memory mapping. It uses NumPy 2 to achieve significant performance speedups over native Python code using compu- tations that follow the Single Instruction, Multi- ple Data (SIMD) paradigm. It uses spatial in- dexes to perform fast exact similarity search and Annoy 3 to perform approximate k-nearest neigh- bors in the vector space. To perform feature hashing, it uses xxHash 4 , an extremely fast non- cryptographic hash algorithm, working at speeds close to RAM limits. Magnitude's file format uses LZ4 compression 5 for compact storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design Principles</head><p>Several design principles guided the development of the Magnitude library:</p><p>• The API should be intuitive and beginner friendly. It should have sensible defaults in- stead of requiring configuration choices by the user. The option to configure every set- ting should still be provided to power users.</p><p>• The out of the box configuration should be fast and memory efficient for iterative devel- opment. It should be suitable for deployment in a production environment. Using the same configuration in development and production reduces bugs and makes deployment easier.</p><p>• The library should use lazy loading whenever possible to remain fast, responsive, and mem- ory efficient during development.</p><p>• The library should aggressively index, cache, and use memory maps to be fast, responsive, and memory efficient for production.</p><p>• The library should be able to process data that is too large to fit into a computer's main memory.</p><p>• The library should be thread-safe and employ memory mapping to reduce duplicated mem- ory resources when multiprocessing.</p><p>• The interface should act as a generic key- vector store and remain agnostic to underly- ing models (like word2vec, GloVe, fastText, and ELMo) and remain useable for other do- mains that use vector embeddings like com- puter vision ( <ref type="bibr" target="#b0">Babenko and Lempitsky, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Getting Started with Magnitude</head><p>The system consists of a Python 2.7 and Python 3.x compatible package (accessible through the PyPI index 6 or GitHub <ref type="bibr">7</ref> ) with utilities for using the ".magnitude" format and converting to it from other popular embedding formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Installation</head><p>Installation for Python 2.7 can be performed using the pip command:</p><formula xml:id="formula_0">pip install pymagnitude</formula><p>Installation for Python 3.x can be performed using the pip3 command:</p><p>pip3 install pymagnitude</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Basic Usage</head><p>Here is how to construct the Magnitude object, query for vectors, and compare them:</p><p>from pymagnitude import * vectors = Magnitude (" <ref type="bibr">w2v</ref>. magnitude ") k = vectors . query ("king") q = vectors . query (" queen ") vectors . similarity (k,q) # 0.6510958</p><p>Magnitude queries return almost instantly and are memory efficient. It uses lazy loading di- rectly from disk, instead of having to load the en- tire model into memory. Additionally, Magnitude supports nearest neighbors operations, finding all words that are closer to a key than another key, and analogy solving (optionally with <ref type="bibr" target="#b4">Levy and Goldberg (2014)</ref>'s 3CosMul function): In addition to querying single words, Magnitude also makes it easy to query for multiple words in a single sentence and multiple sentences: </p><formula xml:id="formula_1">vectors . most similar (k, topn =5) #[(' king ', 1.0) , (' kings ', 0.71) , # (' queen ', 0.65) , (' monarch ', 0.64) , # (' crown prince ', 0.62)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Advanced Features</head><p>OOVs: Magnitude implements a novel method for handling out-of-vocabulary (OOV) words. OOVs frequently occur in real world data since pre-trained models are often missing slang, col- loquialisms, new product names, or misspellings. For example, while uber exists in Google News word2vec, uberx and uberxl do not. These prod- ucts were not available when Google News cor- pus was built. Strategies for representing these words include generating random unit-length vec- tors for each unknown word or mapping all un- known words to a token like "UNK" and repre- senting them with the same vector. These solu- tions are not ideal as the embeddings will not cap- ture semantic information about the actual word. Using Magnitude, these OOV words can be simply queried and will be positioned in the vector space close to other OOV words based on their string similarity: A consequence of generating OOV vectors is that misspellings and typos are also sensibly handled:</p><formula xml:id="formula_2">"</formula><p>" missispi " in vectors # False " discrimnatory " in vectors # False " hiiiiiiiiii " in vectors # False vectors . similarity ( " missispi ", " mississippi " ) # Returns : 0.359 vectors . similarity ( " discrimnatory ", " discriminatory " ) # Returns : 0.830 vectors . similarity ( " hiiiiiiiiii ", "hi" ) # Returns : 0.706</p><p>The OOV handling is detailed in Section 5.</p><p>Concatenation of Multiple Models: Magni- tude makes it easy to concatenate multiple types of vector embeddings to create combined models. <ref type="bibr">w2v = Magnitude ("w2v .300d</ref>. magnitude ") gv = Magnitude (" glove .50d. magnitude ") vectors = Magnitude (w2v , gv) # concat vectors . query ("cat") # Returns : 350d NumPy array # 'cat ' from w2v and 'cat ' from gv vectors . query (("cat", "cats")) # Returns : 350d NumPy array # 'cat ' from w2v and 'cats ' from gv Adding Features for Part-of-Speech Tags and Syntax Dependencies to Vectors: Magnitude can directly turn a set of keys (like a POS tag set) into vectors. Given an approximate upper bound on the number of keys and a namespace, it uses the hashing trick ( <ref type="bibr" target="#b10">Weinberger et al., 2009</ref>) to cre- ate an appropriate length dimension for the keys. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>Speed Exact k-NN 0.9155s Approx. k-NN (k=10, effort = 1.0) 0.1873s Approx. k-NN (k=10, effort = 0.1) 0.0199s This can be used with Magnitude's concatena- tion feature to combine the vectors for words with the vectors for POS tags or dependency tags. Homonyms show why this may be useful: vectors = Magnitude (vecs , pos vecs , dep vecs ) vectors . query ([ (" Buffalo ", "JJ", "amod"), (" buffalo ", "NNS", " nsubj "), (" Buffalo ", "JJ", "amod"), (" buffalo ", "NNS", " nsubj "), (" buffalo ", "VBP", " rcmod "), (" buffalo ", "VB", "ROOT"), (" Buffalo ", "JJ", "amod"), (" buffalo ", "NNS", "dobj") ]) # array of 8 x (300 + 4 + 4)</p><p>Approximate k-NN We support approximate similarity search with the most similar approx function. This finds the approximate nearest neighbors more quickly than the exact nearest neighbors search performed by the most similar function. The method accepts an effort argu- ment which accepts the range [0.0, 1.0]. A lower effort will reduce accuracy, but increase speed. A higher effort does the reverse. This trade-off works by searching more-or less-indexed trees. Our approximate k-NN is powered by Annoy, an open source library released by Spotify. <ref type="table" target="#tab_1">Table 2</ref> compares the speed of various configurations for similarity search. Constructing vectors from character n-grams: We generate a vector for an OOV word w based on the character n-gram sequences in the word. First, we pad the word with a character at the beginning of the word and at the end of the word. Next, we generate the set of all character-ngrams in w (de- noted with the fuction CGRAM w ) between length 3 and 6, following , al- though these parameters are tunable arguments in the Magnitude converter. We use the set of char- acter n-grams C to construct a vector OOV d (w) with d dimensions to represent the word w. Each unique character n-gram c from the word con- tributes to the vector through a pseudorandom vector generator function PRVG. Finally, the vec- tor is normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Details of OOV Handling</head><formula xml:id="formula_3">C = CGRAM w (3, 6) oov d (w) = c ∈ C PRVG H(c) (−1.0, 1.0, d) OOV d (w) = oov d (w) |oov d (w)|</formula><p>PRVG's random number generator is seeded by the value "seed", which generates uniformly ran- dom vectors of dimension size d, with values in the range of -1 to 1. The hashing function H pro- duces a 32 bit hash of its input using xxHash. H : {0, 1} * → {0, 1} 32 . Since the PRVG's seed is only conditioned upon the word w, the output is deterministic across different machines. This character n-gram-based method will gener- ate highly similar vectors for a pair of OOVs with similar spellings, like uberx and uberxl. How- ever, they will not be embedded close to similar in-vocabulary words like uber.</p><p>Interpolation with in-vocabulary words To handle matching OOVs to in-vocabulary words, we first define a function MATCH k (a, b, w). MATCH k (a, b, w) returns the normalized mean of the vectors of the top k most string-similar in- vocabulary words using the full-text SQLite in- dex. In practice, we use the top 3 most string- similar words. These are then used to interpo- late the values for the vector representing the OOV word. 30% of the weight for each value comes from the pseudorandom vector generator based on the OOV's n-grams, and the remaining 70% comes from the values of the 3 most string similar in-vocabulary words:</p><formula xml:id="formula_4">oov d (w) = [0.3 * OOV d (w) + 0.7 * MATCH 3 (3, 6, w)]</formula><p>Morphology-aware matching For English, we have implemented a nuanced string similarity met- ric that is prefix-and suffix-aware. While uberifi- cation has a high string similarity to verification and has a lower string similarity to uber, good OOV vectors should weight stems more heavily than suffixes. Details of our morphology-aware matching are omitted for space.</p><p>Other matching nuances We employ other techniques when computing the string similarity metric, such as shrinking repeated character se- quences of three or more to two (hiiiiiiii → hii), ranking strings of a similar length higher, and ranking strings that share the same first or last character higher for shorter words. The input format will automatically be determined by the extension and the contents of the input file. When the vectors are converted, they will also be unit-length normalized. This conversion process only needs to be completed once per model. Af- ter converting, the Magnitude file format is static and it will not be modified or written to in order to make concurrent read access safe. By default, the converter builds a Medium ".magnitude" file. Passing the -s flag will turn off encoding of subword information, and result in a Light flavored file. Passing the -a flag will turn on building the Annoy approximate similarity index, and result in a Heavy flavored file. Refer to the documentation 8 for more information about con- version configuration options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">File Format</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantization</head><p>The converter utility accepts a -p &lt;PRECISION&gt; flag to specify the decimal precision to retain. Since underlying values are stored as integers instead of floats, this is essentially quan- tization 9 for smaller model footprints. Lower dec- imal precision will create smaller files, because SQLite can store integers with either 1, 2, 3, 4, 6, or 8 bytes. <ref type="bibr">10</ref> Regardless of the precision selected, the library will create numpy.float32 vectors. The datatype can be changed by pass- ing dtype=numpy.float16 to the Magnitude con- structor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Magnitude is a new open source Python library and file format for vector embeddings. It makes it easy to integrate embeddings into applications and provides a single interface and configuration that is suitable for both development and produc- tion workloads. The library and file format also enable novel features like OOV handling that al- low models to be more robust to noisy data. The simple interface, ease of use, and speed of the li- brary, compared to other utilities like Gensim, will enable use by beginners to NLP and individuals in educational environments, such as university NLP and AI courses.</p><p>Pre-trained word embeddings have been widely adopted in NLP. Researchers in computer vision have started using pre-trained vector embedding models like Deep1B ( <ref type="bibr" target="#b0">Babenko and Lempitsky, 2016</ref>) for images. The Magnitude library intends to stay agnostic to various domains, instead pro- viding a generic key-vector store and interface that is useful for all domains and for research that crosses the boundaries between NLP and vi- sion ( <ref type="bibr" target="#b2">Hewitt et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Software and Data</head><p>We release the Magnitude package under the permissive MIT open source license. The full source code and pre-converted ".magnitude" mod- els are on GitHub. The full documentation for all classes, methods, and configurations of the li- brary can be found at https://github.com/ plasticityai/magnitude, along with ex- ample usage and tutorials.</p><p>We have pre-converted several popular em- bedding models (Google News word2vec, Stanford GloVe, Facebook fastText, AI2 ELMo) to ".magnitude" in all its variants (Light, Medium, and Heavy).</p><p>You can download them from https://github. com/plasticityai/magnitude#pre- converted-magnitude-formats-of- popular-embeddings-models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Benchmark Comparisons</head><p>All benchmarks 11 were performed on the Google News pre-trained word vectors, "GoogleNews- vectors-negative300.bin" ( <ref type="bibr" target="#b5">Mikolov et al., 2013</ref>) for Gensim and on the "GoogleNews-vectors- negative300.magnitude" 12 for Magnitude, with a MacBook Pro (Retina, 15-inch, Mid 2014) 2.2GHz quad-core Intel Core i7 @ 16GB RAM on a SSD over an average of trials where feasible. We are ex- plicitly not using Gensim's memory-mapped native format as it requires extra configuration from the developer and is not provided out of the box from Gensim's data repository <ref type="bibr">13</ref>  c Gensim has an option to not duplicate unit-normalized vectors in memory, but still requires up to 8GB of memory alloca- tion while processing, before dropping down to half the memory. Moreover, this option is not on by default.</p><p>d Magnitude uses mmap to read from the disk, so the OS will still allocate pages of memory, when memory is available, in its file cache, but it can be shared between processes and is not managed within each process for extremely large files which is a performance win. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc># Returns : a vector for the word vectors . query (["play", " music "]) # Returns : an array with two vectors vectors . query ([ ["play", " music "], ["turn", "on", "the", " lights "], ]) # Returns : 2D array with vectors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>uber" in vectors # True " uberx " in vectors # False " uberxl " in vectors # False vectors . query (" uberx ") # Returns : [ 0.0507 , −0.0708 , ...] vectors . query (" uberxl ") # Returns : [ 0.0473 , −0.08237 , ...] vectors . similarity (" uberx ", " uberxl ") # Returns : 0.955</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>pos vecs =</head><label></label><figDesc>FeaturizerMagnitude ( 100 , namespace = "POS") pos vecs .dim # 4 # number of dims automatically # determined by Magnitude from 100 pos vecs . query ("NN") dep vecs = FeaturizerMagnitude ( 100 , namespace = "Dep") dep vecs .dim # 4 dep vecs . query (" nsubj ")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Facebook's fastText (Bojanowski et al., 2016) provides similar OOV functionality to Magni- tude's. Magnitude allows for OOV lookups for any embedding model, including older models like word2vec and GloVe (Mikolov et al., 2013; Pen- nington et al., 2014), which did not provide OOV support. Magnitude's OOV method can be used with existing embeddings because it does not re- quire any changes to be made at training time like fastText's method does. For ELMo vectors, Mag- nitude will use ELMo's OOV method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 1: Structure of the ".magnitude" file format and its Light, Medium, and Heavy variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). Loading the binary files containing the word vectors takes Gensim 70 seconds, versus 0.72 seconds to load the corresponding Magnitude</figDesc><table>Metric 
Cold Warm 
Initial load time 
97x 
-
Single key query 
1x 
110x 
Multiple key query (n=25) 
68x 
3x 
k-NN search query (k=10) 
1x 5,935x 

Table 1: Speed comparison of Magnitude versus Gen-
sim for common operations. The 'cold' column repre-
sents the first time the operation is called. The 'warm' 
column indicates a subsequent call with the same keys. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Approximate nearest neighbors significantly speeds up similarity searches compared to exact search.</head><label>2</label><figDesc></figDesc><table>Reducing the amount of allowed effort further speeds 
the approximate k-NN search. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Metric 

Gensim 
( ˇ 
RehůřekReh˚Rehůřek and 
Sojka, 2010) 

Magnitude 
Light 

Magnitude 
Medium 

Magnitude 
Heavy 

Initial load time 
70.26s 
0.7210s 
-a 
-a 
Cold single key query 
0.0001s 
0.0001s 
-a 
-a 
Warm single key query 
(same key as cold query) 
0.0044s 
0.00004s 
-a 
-a 
Cold multiple key query 
(n=25) 
3.0050s 
0.0442s 
-a 
-a 
Warm multiple key query 
(n=25) (same keys as cold query) 
0.0001s 
0.00004s 
-a 
-a 
First most similar search query 
(n=10) (worst case) 
18.493s 
247.05s 
-a 
-a 
First most similar search query 
(n=10) (average case) (w/ disk persistent cache) 
18.917s 
1.8217s 
-a 
-a 
Subsequent most similar search 
(n=10) (different key than first query) 
0.2546s 
0.2434s 
-a 
-a 
Warm subsequent most similar search 
(n=10) (same key as first query) 
0.2374s 
0.00004s 
0.00004s 
0.00004s 
First most similar approx search query 
(n=10, effort=1.0) (worst case) 
N/A b 
N/A 
N/A 
29.610s 
First most similar approx search query 
(n=10, effort=1.0) (average case) (w/ disk persistent 
cache) 

N/A 
N/A 
N/A 
0.9155s 

Subsequent most similar approx search 
(n=10, effort=1.0) (different key than first query) 
N/A 
N/A 
N/A 
0.1873s 
Subsequent most similar approx search 
(n=10, effort=0.1) (different key than first query) 
N/A 
N/A 
N/A 
0.0199s 
Warm subsequent most similar approx search 
(n=10, effort=1.0) (same key as first query) 
N/A 
N/A 
N/A 
0.00004s 
File size 
3.64GB 
4.21GB 
5.29GB 
10.74GB 
Process memory (RAM) utilization 
4.875GB 
18KB 
-a 
-a 
Process memory (RAM) utilization after 100 key 
queries 

4.875GB 
168KB 
-a 
-a 

Process memory (RAM) utilization after 100 key 
queries + similarity search 

8.228GB c 
342KB d 
-a 
-a 

a Denotes the same value as the previous column. 
b Gensim does support approximate similarity search, but not out of the box as the index must be built manually with 
gensim.similarities.index first which is a slow operation. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Benchmark comparisons between Gensim, Magnitude Light, Magnitude Medium, and Magnitude Heavy.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://www.sqlite.org/ 2 http://www.numpy.org/ 3 https://github.com/spotify/annoy 4 https://xxhash.org/ 5 http://www.lz4.org/</note>

			<note place="foot" n="6"> https://pypi.org/project/pymagnitude/ 7 https://github.com/plasticityai/ magnitude</note>

			<note place="foot" n="8"> https://github.com/plasticityai/ magnitude#file-format-and-converter 9 https://www.tensorflow.org/ performance/quantization 10 https://www.sqlite.org/datatype3.html</note>

			<note place="foot" n="11"> https://github.com/plasticityai/magnitude/blob/master/tests/benchmark.py 12 http://magnitude.plasticity.ai/word2vec+approx/GoogleNews-vectors-negative300. magnitude 13 https://github.com/RaRe-Technologies/gensim-data</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Erik Bernhardsson for the useful feedback on integrating Annoy index-ing into Magnitude and thank the numerous con-tributors who have opened issues, reported bugs, or suggested technical enhancements for Magni-tude on GitHub.</p><p>This material is funded in part by DARPA under grant number HR0011-15-C-0115 (the LORELEI program) and by NSF SBIR Award #IIP-1820240. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this pub-lication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA, the NSF, and the U.S. Government. This work has also been supported by the French National Research Agency under project ANR-16-CE33-0013.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient Indexing of Billion-Scale Datasets of Deep Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2055" to="2063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Translations via Images with a Massively Multilingual Image Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reno</forename><surname>Kriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Derry Tanti Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2566" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>abs/1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sands</surname></persName>
		</author>
		<idno type="doi">10.5281/zenodo.1255637</idno>
		<idno>plasticityai/magni- tude: Release 0.1.22</idno>
		<ptr target="https://doi.org/10.5281/zenodo.1255637" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<title level="m">Deep contextualized word representations. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature Hashing for Large Scale Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
