<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deal or No Deal? End-to-End Learning for Negotiation Dialogues</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deal or No Deal? End-to-End Learning for Negotiation Dialogues</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2443" to="2453"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other&apos;s reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation , which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Intelligent agents often need to cooperate with oth- ers who have different goals, and typically use natural language to agree on decisions. Negotia- tion is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realised. Such dialogues contain both cooperative and adversarial elements, and re- quire agents to understand, plan, and generate ut- terances to achieve their goals ( <ref type="bibr" target="#b33">Traum et al., 2008;</ref><ref type="bibr" target="#b0">Asher et al., 2012</ref>).</p><p>We collect the first large dataset of natural lan- guage negotiations between two people, and show that end-to-end neural models can be trained to negotiate by maximizing the likelihood of human actions. This approach is scalable and domain- independent, but does not model the strategic skills required for negotiating well. We fur- ther show that models can be improved by train- ing and decoding to maximize reward instead of likelihood-by training with self-play reinforce- ment learning, and using rollouts to estimate the expected reward of utterances during decoding.</p><p>To study semi-cooperative dialogue, we gather a dataset of 5808 dialogues between humans on a negotiation task. Users were shown a set of items with a value for each, and asked to agree how to divide the items with another user who has a dif- ferent, unseen, value function ( <ref type="figure">Figure 1</ref>).</p><p>We first train recurrent neural networks to imi- tate human actions. We find that models trained to maximise the likelihood of human utterances can generate fluent language, but make comparatively poor negotiators, which are overly willing to com- promise. We therefore explore two methods for improving the model's strategic reasoning skills- both of which attempt to optimise for the agent's goals, rather than simply imitating humans:</p><p>Firstly, instead of training to optimise likeli- hood, we show that our agents can be consider- ably improved using self play, in which pre-trained models practice negotiating with each other in or- der to optimise performance. To avoid the models diverging from human language, we interleave re- inforcement learning updates with supervised up- dates. For the first time, we show that end-to- end dialogue agents trained using reinforcement learning outperform their supervised counterparts in negotiations with humans.</p><p>Secondly, we introduce a new form of planning for dialogue called dialogue rollouts, in which an <ref type="figure">Figure 1</ref>: A dialogue in our Mechanical Turk interface, which we used to collect a negotiation dataset. agent simulates complete dialogues during decod- ing to estimate the reward of utterances. We show that decoding to maximise the reward function (rather than likelihood) significantly improves per- formance against both humans and machines.</p><p>Analysing the performance of our agents, we find evidence of sophisticated negotiation strate- gies. For example, we find instances of the model feigning interest in a valueless issue, so that it can later 'compromise' by conceding it. Deceit is a complex skill that requires hypothesising the other agent's beliefs, and is learnt relatively late in child development ( <ref type="bibr" target="#b32">Talwar and Lee, 2002</ref>). Our agents have learnt to deceive without any explicit human design, simply by trying to achieve their goals.</p><p>The rest of the paper proceeds as follows: §2 de- scribes the collection of a large dataset of human- human negotiation dialogues. §3 describes a base- line supervised model, which we then show can be improved by goal-based training ( §4) and de- coding ( §5). §6 measures the performance of our models and humans on this task, and §7 gives a detailed analysis and suggests future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>To enable end-to-end training of negotiation agents, we first develop a novel negotiation task and curate a dataset of human-human dialogues for this task. This task and dataset follow our proposed general framework for studying semi- cooperative dialogue. Initially, each agent is shown an input specifying a space of possible ac- tions and a reward function which will score the outcome of the negotiation. Agents then sequen- tially take turns of either sending natural language messages, or selecting that a final decision has been reached. When one agent selects that an agreement has been made, both agents indepen- dently output what they think the agreed decision was. If conflicting decisions are made, both agents are given zero reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task</head><p>Our task is an instance of multi issue bargaining <ref type="bibr" target="#b11">(Fershtman, 1990)</ref>, and is based on <ref type="bibr" target="#b8">DeVault et al. (2015)</ref>. Two agents are both shown the same col- lection of items, and instructed to divide them so that each item assigned to one agent. Each agent is given a different randomly gen- erated value function, which gives a non-negative value for each item. The value functions are con- strained so that: (1) the total value for a user of all items is 10; (2) each item has non-zero value to at least one user; and (3) some items have non- zero value to both users. These constraints enforce that it is not possible for both agents to receive a maximum score, and that no item is worthless to both agents, so the negotiation will be competitive. After 10 turns, we allow agents the option to com- plete the negotiation with no agreement, which is worth 0 points to both users. We use 3 item types (books, hats, balls), and between 5 and 7 total items in the pool. <ref type="figure">Figure 1</ref> shows our interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Collection</head><p>We collected a set of human-human dialogues us- ing Amazon Mechanical Turk. Workers were paid $0.15 per dialogue, with a $0.05 bonus for max- imal scores. We only used workers based in the United States with a 95% approval rating and at least 5000 previous HITs. Our data collection in- terface was adapted from that of <ref type="bibr" target="#b6">Das et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crowd Sourced Dialogue</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent 1 Input</head><formula xml:id="formula_0">3xbook value=1 2xhat value=3 1xball value=1 Agent 2 Input 3xbook value=2 2xhat value=1 1xball value=2</formula><p>Dialogue Agent 1: I want the books and the hats, you get the ball Agent 2: Give me a book too and we have a deal Agent 1: Ok, deal Agent 2: &lt;choose&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent 1 Output 2xbook 2xhat</head><p>Agent 2 Output 1xbook 1xball</p><p>Perspective: Agent 1</p><p>Perspective: Agent 2</p><formula xml:id="formula_1">Input 3xbook value=1 2xhat value=3 1xball value=1 Output 2xbook 2xhat</formula><p>Dialogue write: I want the books and the hats, you get the ball read: Give me a book too and we have a deal write: Ok, deal read: &lt;choose&gt;</p><formula xml:id="formula_2">Input 3xbook value=2 2xhat value=1 1xball value=2</formula><p>Dialogue read: I want the books and the hats, you get the ball write: Give me a book too and we have a deal read: Ok, deal write: &lt;choose&gt; Output 1xbook 1xball</p><p>Figure 2: Converting a crowd-sourced dialogue (left) into two training examples (right), from the per- spective of each user. The perspectives differ on their input goals, output choice, and in special tokens marking whether a statement was read or written. We train conditional language models to predict the dialogue given the input, and additional models to predict the output given the dialogue.</p><p>We collected a total of 5808 dialogues, based on 2236 unique scenarios (where a scenario is the available items and values for the two users). We held out a test set of 252 scenarios (526 dialogues). Holding out test scenarios means that models must generalise to new situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Likelihood Model</head><p>We propose a simple but effective baseline model for the conversational agent, in which a sequence- to-sequence model is trained to produce the com- plete dialogue, conditioned on an agent's input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Representation</head><p>Each dialogue is converted into two training ex- amples, showing the complete conversation from the perspective of each agent. The examples differ on their input goals, output choice, and whether utterances were read or written.</p><p>Training examples contain an input goal g, specifying the available items and their values, a dialogue x, and an output decision o specifying which items each agent will receive. Specifically, we represent g as a list of six integers correspond- ing to the count and value of each of the three item types. Dialogue x is a list of tokens x 0..T contain- ing the turns of each agent interleaved with sym- bols marking whether a turn was written by the agent or their partner, terminating in a special to- ken indicating one agent has marked that an agree- ment has been made. Output o is six integers de- scribing how many of each of the three item types are assigned to each agent. See <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supervised Learning</head><p>We train a sequence-to-sequence network to gen- erate an agent's perspective of the dialogue condi- tioned on the agent's input goals ( <ref type="figure">Figure 3a)</ref>.</p><p>The model uses 4 recurrent neural networks, implemented as GRUs ( ): GRU w , GRU g , GRU− → o , and GRU← − o .</p><p>The agent's input goals g are encoded using GRU g . We refer to the final hidden state as h g . The model then predicts each token x t from left to right, conditioned on the previous tokens and h g . At each time step t, GRU w takes as input the pre- vious hidden state h t−1 , previous token x t−1 (em- bedded with a matrix E), and input encoding h g . Conditioning on the input at each time step helps the model learn dependencies between language and goals.</p><formula xml:id="formula_3">h t = GRU w (h t−1 , [Ex t−1 , h g ])<label>(1)</label></formula><p>The token at each time step is predicted with a softmax, which uses weight tying with the embed-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Encoder</head><p>Output Decoder write: Take one hat read: I need two write: deal . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Supervised Training</head><p>Input Encoder Output Decoder write: Take one hat read: I need two write: deal . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Decoding, and Reinforcement Learning</head><p>Figure 3: Our model: tokens are predicted conditioned on previous words and the input, then the output is predicted using attention over the complete dialogue. In supervised training <ref type="formula">(3a)</ref>, we train the model to predict the tokens of both agents. During decoding and reinforcement learning (3b) some tokens are sampled from the model, but some are generated by the other agent and are only encoded by the model.</p><formula xml:id="formula_4">ding matrix E (Mao et al., 2015):</formula><formula xml:id="formula_5">p θ (x t |x 0..t−1 , g) ∝ exp(E T h t )<label>(2)</label></formula><p>Note that the model predicts both agent's words, enabling its use as a forward model in Section 5.</p><p>At the end of the dialogue, the agent outputs a set of tokens o representing the decision. We gen- erate each output conditionally independently, us- ing a separate classifier for each. The classifiers share bidirectional GRUs and an attention mech- anism ( ) over the dialogue, and additionally condition on the input goals.</p><formula xml:id="formula_6">h − → o t = GRU− → o (h − → o t−1 , [Ex t , h t ]) (3) h ← − o t = GRU← − o (h ← − o t+1 , [Ex t , h t ])<label>(4)</label></formula><formula xml:id="formula_7">h o t = [h ← − o t , h − → o t ]<label>(5)</label></formula><formula xml:id="formula_8">h a t = W a [tanh(W h h o t )]<label>(6)</label></formula><formula xml:id="formula_9">α t = exp(w · h a t ) t exp(w · h a t ) (7) h s = tanh(W s [h g , t α t h t ])<label>(8)</label></formula><p>The output tokens are predicted using softmax:</p><formula xml:id="formula_10">p θ (o i |x 0..t , g) ∝ exp(W o i h s )<label>(9)</label></formula><p>The model is trained to minimize the negative log likelihood of the token sequence x 0..T con- ditioned on the input goals g, and of the outputs o conditioned on x and g. The two terms are weighted with a hyperparameter α.</p><formula xml:id="formula_11">L(θ) = − x,g t log p θ (x t |x 0..t−1 , g) Token prediction loss − α x,g,o j log p θ (o j |x 0..T , g) Output choice prediction loss (10)</formula><p>Unlike the Neural Conversational Model ( <ref type="bibr" target="#b34">Vinyals and Le, 2015)</ref>, our approach shares all parameters for reading and generating tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>During decoding, the model must generate an output token x t conditioned on dialogue history x 0..t−1 and input goals g, by sampling from p θ :</p><formula xml:id="formula_12">x t ∼ p θ (x t |x 0..t−1 , g)<label>(11)</label></formula><p>If the model generates a special end-of-turn to- ken, it then encodes a series of tokens output by the other agent, until its next turn <ref type="figure">(Figure 3b)</ref>.</p><p>The dialogue ends when either agent outputs a special end-of-dialogue token. The model then outputs a set of choices o. We choose each item independently, but enforce consistency by check- ing the solution is in a feasible set O:</p><formula xml:id="formula_13">o * = argmax o∈O i p θ (o i |x 0..T , g)<label>(12)</label></formula><p>In our task, a solution is feasible if each item is as- signed to exactly one agent. The space of solutions is small enough to be tractably enumerated.  The model first generates a small set of candidate responses. For each candidate, it then simulates a number of possible complete future conversations by sampling, and estimates the expected future reward by averaging the scores. The system outputs the candidate with the highest expected reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Goal-based Training</head><p>our fixed supervised model that was trained to im- itate humans. The second model is fixed as we found that updating the parameters of both agents led to divergence from human language. In effect, agent A learns to improve by simulating conversa- tions with the help of a surrogate forward model. Agent A reads its goals g and then generates tokens x 0..n by sampling from p θ . When x gener- ates an end-of-turn marker, it then reads in tokens x n+1..m generated by agent B. These turns alter- nate until one agent emits a token ending the di- alogue. Both agents then output a decision o and collect a reward from the environment (which will be 0 if they output different decisions). We denote the subset of tokens generated by A as X A (e.g. tokens with incoming arrows in <ref type="figure">Figure 3b)</ref>.</p><p>After a complete dialogue has been generated, we update agent A's parameters based on the out- come of the negotiation. Let r A be the score agent A achieved in the completed dialogue, T be the length of the dialogue, γ be a discount factor that rewards actions at the end of the dialogue more strongly, and µ be a running average of completed dialogue rewards so far 2 . We define the future re- ward R for an action x t ∈ X A as follows:</p><formula xml:id="formula_14">R(x t ) = xt∈X A γ T −t (r A (o) − µ)<label>(13)</label></formula><p>We then optimise the expected reward of each action x t ∈ X A :</p><formula xml:id="formula_15">L RL θ = E xt∼p θ (xt|x 0..t−1 ,g) [R(x t )]<label>(14)</label></formula><p>2 As all rewards are non-negative, we instead re-scale them by subtracting the mean reward found during self play. Shift- ing in this way can reduce the variance of our estimator.</p><p>The gradient of L RL θ is calculated as in REIN- FORCE <ref type="bibr" target="#b37">(Williams, 1992)</ref>:</p><formula xml:id="formula_16">θ L RL θ = xt∈X A E xt [R(x t ) θ log(p θ (x t |x 0..t−1 , g))]<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Goal-based Decoding</head><p>Likelihood-based decoding ( §3.3) may not be op- timal. For instance, an agent may be choosing be- tween accepting an offer, or making a counter of- fer. The former will often have a higher likelihood under our model, as there are fewer ways to agree than to make another offer, but the latter may lead to a better outcome. Goal-based decoding also al- lows more complex dialogue strategies. For exam- ple, a deceptive utterance is likely to have a low model score (as users were generally honest in the supervised data), but may achieve high reward. We instead explore decoding by maximising ex- pected reward. We achieve this by using p θ as a forward model for the complete dialogue, and then deterministically computing the reward. Rewards for an utterance are averaged over samples to cal- culate expected future reward <ref type="figure" target="#fig_0">(Figure 4)</ref>.</p><p>We use a two stage process: First, we gener- ate c candidate utterances U = u 0..c , represent- ing possible complete turns that the agent could make, which are generated by sampling from p θ until the end-of-turn token is reached. Let x 0..n−1 be current dialogue history. We then calculate the expected reward R(u) of candidate utterance u = x n,n+k by repeatedly sampling x n+k+1,T from p θ , then choosing the best output o using Equation 12, and finally deterministically comput- ing the reward r(o). The reward is scaled by the probability of the output given the dialogue, be-Algorithm 1 Dialogue Rollouts algorithm.</p><p>1: procedure ROLLOUT(x 0..i , g)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>u * ← ∅</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for c ∈ {1..C} do C candidate moves 4:</p><formula xml:id="formula_17">j ← i 5:</formula><p>do Rollout to end of turn 6:</p><p>j ← j + 1</p><p>7:</p><formula xml:id="formula_18">x j ∼ p θ (x j |x 0..j−1 , g) 8:</formula><p>while x k / ∈ {read:, choose:}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>u ← x i+1 ..x j u is candidate move <ref type="bibr">10:</ref> for s ∈ {1..S} do S samples per move <ref type="bibr">11:</ref> k ← j Start rollout from end of u</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>while x k = choose: do Rollout to end of dialogue 13:</p><formula xml:id="formula_19">k ← k + 1</formula><p>14:</p><formula xml:id="formula_20">x k ∼ p θ (x k |x 0..k−1 , g)</formula><p>Calculate rollout output and reward 15:</p><formula xml:id="formula_21">o ← argmax o ∈O p(o |x 0..k , g) 16: R(u) ← R(u) + r(o)p(o |x 0..k , g) 17:</formula><p>if R(u) &gt; R(u * ) then 18:</p><formula xml:id="formula_22">u * ← u 19:</formula><p>return u * Return best move cause if the agents select different outputs then they both receive 0 reward.</p><formula xml:id="formula_23">R(x n..n+k ) = E x (n+k+1..T ;o) ∼p θ [r(o)p θ (o|x 0..T )]<label>(16)</label></formula><p>We then return the utterance maximizing R.</p><formula xml:id="formula_24">u * = argmax u∈U R(u)<label>(17)</label></formula><p>We use 5 rollouts for each of 10 candidate turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training Details</head><p>We implement our models using PyTorch. All hyper-parameters were chosen on a development dataset. The input tokens are embedded into a 64-dimensional space, while the dialogue tokens are embedded with 256-dimensional embeddings (with no pre-training). The input GRU g has a hidden layer of size 64 and the dialogue GRU w is of size 128. The output GRU− → o and GRU← − o both have a hidden state of size 256, the size of h s is 256 as well. During supervised training, we optimise using stochastic gradient descent with a minibatch size of 16, an initial learning rate of 1.0, Nesterov momentum with µ=0.1 <ref type="bibr" target="#b25">(Nesterov, 1983)</ref>, and clipping gradients whose L 2 norm ex- ceeds 0.5. We train the model for 30 epochs and pick the snapshot of the model with the best val- idation perplexity. We then annealed the learn- ing rate by a factor of 5 each epoch. We weight the terms in the loss function (Equation 10) using α=0.5. We do not train against output decisions where humans selected different agreements. To- kens occurring fewer than 20 times are replaced with an 'unknown' token. During reinforcement learning, we use a learn- ing rate of 0.1, clip gradients above 1.0, and use a discount factor of γ=0.95. After every 4 rein- forcement learning updates, we make a supervised update with mini-batch size 16 and learning rate 0.5, and we clip gradients at 1.0. We used 4086 simulated conversations.</p><p>When sampling words from p θ , we reduce the variance by doubling the values of logits (i.e. us- ing temperature of 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison Systems</head><p>We compare the performance of the following: LIKELIHOOD uses supervised training and decod- ing ( §3), RL is fine-tuned with goal-based self- play ( §4), ROLLOUTS uses supervised training combined with goal-based decoding using rollouts ( §5), and RL+ROLLOUTS uses rollouts with a base model trained with reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Intrinsic Evaluation</head><p>For development, we use measured the perplexity of user generated utterances, conditioned on the input and previous dialogue.</p><p>Results are shown in <ref type="table" target="#tab_4">Table 3</ref>, and show that the simple LIKELIHOOD model produces the most human-like responses, and the alternative training and decoding strategies cause a divergence from human language. Note however, that this diver- gence may not necessarily correspond to lower quality language-it may also indicate different strategic decisions about what to say. Results in §6.4 show all models could converse with humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">End-to-End Evaluation</head><p>We measure end-to-end performance in dialogues both with the likelihood-based agent and with hu- mans on Mechanical Turk, on held out scenarios.</p><p>Humans were told that they were interacting with other humans, as they had been during the collection of our dataset (and few appeared to re- alize they were in conversation with machines).    We measure the following statistics: Score: The average score for each agent (which could be a human or model), out of 10. Agreement: The percentage of dialogues where both agents agreed on the same decision. Pareto Optimality: The percentage of Pareto optimal solutions for agreed deals (a solution is Pareto optimal if neither agent's score can be im- proved without lowering the other's score). Lower scores indicate inefficient negotiations.</p><p>Results are shown in <ref type="table" target="#tab_2">Table 1</ref>. Firstly, we see that the RL and ROLLOUTS models achieve significantly better results when negotiat- ing with the LIKELIHOOD model, particularly the RL+ROLLOUTS model. The percentage of Pareto optimal solutions also increases, showing a bet- ter exploration of the solution space. Compared to human-human negotiations <ref type="table" target="#tab_3">(Table 2)</ref>, the best models achieve a higher agreement rate, better scores, and similar Pareto efficiency. This result confirms that attempting to maximise reward can outperform simply imitating humans.</p><p>Similar trends hold in dialogues with humans, with goal-based reasoning outperforming imita- tion learning. The ROLLOUTS model achieves comparable scores to its human partners, and the RL+ROLLOUTS model actually achieves higher scores. However, we also find significantly more cases of the goal-based models failing to agree a deal with humans-largely a consequence of their more aggressive negotiation tactics (see §7). <ref type="table" target="#tab_2">Table 1</ref> shows large gains from goal-based meth- ods. In this section, we explore the strengths and weaknesses of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Goal-based models negotiate harder. The RL+ROLLOUTS model has much longer dialogues with humans than LIKELIHOOD (7.2 turns vs. 5.3 on average), indicating that the model is accepting deals less quickly, and negotiating harder. A negative consequence of more aggressive negotiation strategies is that humans were more likely to walk away with no deal, resulting in lower agreement rates. Even though failing to agree was worth 0 points, people often preferred this course over capitulating to an uncompromis- ing opponent-a factor not well captured by the simulated partner in reinforcement learning train- ing or rollouts (as reflected by the larger gains from goal-based models in dialogues with the LIKELIHOOD model). In particular, the goal-based models are prone to simply rephrasing the same demand each turn, which is a more effective strat- egy against the LIKELIHOOD model than humans.</p><p>One approach to mitigating this issue would be to learn the impact of uncooperative strategies across multiple rounds of negotiation ( <ref type="bibr" target="#b18">Lerer and Peysakhovich, 2017)</ref>. In single dialogues, failing to agree is never better than reaching some agree- ment. However, across multiple rounds, learning to walk away without a deal may incentivize part- ners to cooperate more in future. <ref type="figure">Figure 5</ref> shows an example of our goal-based <ref type="table">You can have the balls if i can have the hat and book  Human</ref> Nope, give me the books and you can have everything else RL+ROLLOUTS Ok. I will take the book and one hat Human I get all the balls? RL+ROLLOUTS Yes. I can do that. Human</p><formula xml:id="formula_25">Input Human 1xbook value=3 1xhat value=1 3xball value=2 RL+ROLLOUTS 1xbook value=6 1xhat value=4 3xball value=0 RL+ROLLOUTS</formula><p>Ok, deal Output Reward RL+ROLLOUTS 1xbook 1xhat 10/10 Human 3xball 6/10</p><p>Figure 5: Example of a model stubbornly holding out for maximum points in a dialogue. Figure 6: Example of model compromising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>model stubbornly negotiating until it achieves a good outcome.</p><p>Models learn to be deceptive. Deception can be an effective negotiation tactic. We found numer- ous cases of our models initially feigning interest in a valueless item, only to later 'compromise' by conceding it. <ref type="figure">Figure 7</ref> shows an example. This strategy has previously been coded into negotia- tion agents by <ref type="bibr" target="#b13">Gratch et al. (2016)</ref>, whereas our agents have learned it from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models produce meaningful novel sentences.</head><p>One interesting question is whether our models are capable of generating novel sentences in the new circumstances they find themselves in, or if they simply repeat messages from the training data ver- batim. We find that 76% of messages produced by the LIKELIHOOD model in self-play were found in the training data. We manually examined the novel utterances produced by our model, and found that the overwhelming majority were fluent English sentences in isolation-showing that the model has learnt a good language model for the domain (in addition to results that show it uses language effectively to achieve its goals). These results sug- gest that although neural models are prone to the safer option of repeating sentences from training data, they are capable of generalising when nec-</p><formula xml:id="formula_26">Input RL+ROLLOUTS 4xbook value=0 2xhat value=5 1xball value=0</formula><p>Human 4xbook value=1 2xhat value=2 1xball value=2 RL+ROLLOUTS You can have the ball if I can have everything else. Human</p><p>No can't do that the ball is not going to give me anything RL+ROLLOUTS I need the hats and you can have the rest Human Deal RL+ROLLOUTS Great! Thank you! Output Reward RL+ROLLOUTS 2xhat</p><p>10/10 Human 4xbook 1xball 6/10</p><p>Figure 7: Dialogue in which the model's initial in- terest in the valueless books allows it to compro- mise while achieving a maximum score.</p><p>essary. Future work should choose domains that force a higher degree of diversity in utterances.</p><p>Maintaining multi-sentence coherence is chal- lenging. One common linguistic error we see RL+ROLLOUTS make is to start a message by in- dicating agreement (e.g. I agree or Deal), but then going on to propose a counter offer-a behaviour that human partners found frustrating. One ex- planation is that the model has learnt that in the supervised data, messages beginning with I agree are often at the end of the dialogue, and partners rarely reply with further negotiation-so the mod- els using rollouts and reinforcement learning be- lieve this tactic will help their offer to be accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Most work on goal orientated dialogue systems has assumed that state representations are anno- tated in the training data <ref type="bibr" target="#b36">(Williams and Young, 2007;</ref><ref type="bibr" target="#b15">Henderson et al., 2014;</ref><ref type="bibr">Wen et al., 2016)</ref>. The use of state annotations allows a cleaner sep- aration of the reasoning and natural language as- pects of dialogues, but our end-to-end approach makes data collection cheaper and allows tasks where it is unclear how to annotate state.  explore end-to-end goal orien- tated dialogue with a supervised model-we show improvements over supervised learning with goal- based training and decoding. Recently, <ref type="bibr" target="#b14">He et al. (2017)</ref> use task-specific rules to combine the task input and dialogue history into a more structured state representation than ours. Reinforcement learning (RL) has been applied in many dialogue settings. RL has been widely used to improve dialogue managers, which man- age transitions between dialogue states <ref type="bibr" target="#b31">(Singh et al., 2002;</ref><ref type="bibr" target="#b26">Pietquin et al., 2011;</ref><ref type="bibr" target="#b27">Rieser and Lemon, 2011;</ref><ref type="bibr" target="#b12">Gašic et al., 2013;</ref><ref type="bibr" target="#b10">Fatemi et al., 2016)</ref>. In contrast, our end-to-end approach has no explicit dialogue manager that can be updated in isolation, and we found it necessary to inter- leave RL and supervised learning to avoid RL re- ducing the quality of language generation. <ref type="bibr" target="#b20">Li et al. (2016)</ref> improve metrics such as diversity for non- goal-orientated dialogue using RL, which would make an interesting extension to our work. <ref type="bibr" target="#b7">Das et al. (2017)</ref> use reinforcement learning to improve cooperative bot-bot dialogues. RL has also been used to allow agents to invent new languages ( <ref type="bibr" target="#b7">Das et al., 2017;</ref><ref type="bibr" target="#b23">Mordatch and Abbeel, 2017)</ref>. To our knowledge, our model is the first to use RL to im- prove the performance of an end-to-end goal ori- entated dialogue system in dialogues with humans.</p><p>Work on learning end-to-end dialogues has con- centrated on 'chat' settings, without explicit goals ( <ref type="bibr" target="#b28">Ritter et al., 2011;</ref><ref type="bibr" target="#b34">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b19">Li et al., 2015)</ref>. These dialogues contain a much greater di- versity of vocabulary than our domain, but do not have the challenging adversarial elements. Such models are notoriously hard to evaluate ( <ref type="bibr" target="#b21">Liu et al., 2016)</ref>, because the huge diversity of reasonable responses, whereas our task has a clear objec- tive. Our end-to-end approach would also be much more straightforward to integrate into a general- purpose dialogue agent than one that relied on an- notated dialogue states ( <ref type="bibr" target="#b9">Dodge et al., 2016</ref>).</p><p>There is a substantial literature on multi-agent bargaining in game-theory, e.g. Nash <ref type="bibr" target="#b24">Jr (1950)</ref>. There has also been computational work on mod- elling negotiations ( <ref type="bibr" target="#b1">Baarslag et al., 2013</ref>)-our work differs in that agents communicate in unre- stricted natural language, rather than pre-specified symbolic actions, and our focus on improving per- formance relative to humans rather than other au- tomated systems. Our task is based on that of <ref type="bibr" target="#b8">DeVault et al. (2015)</ref>, who study natural language negotiations for pedagogical purposes-their ver- sion includes speech rather than textual dialogue, and embodied agents, which would make inter- esting extensions to our work. The only au- tomated natural language negotiations systems we are aware of have first mapped language to domain-specific logical forms, and then focused on choosing the next dialogue act ( <ref type="bibr" target="#b29">Rosenfeld et al., 2014;</ref><ref type="bibr" target="#b5">Cuayáhuitl et al., 2015;</ref><ref type="bibr" target="#b16">Keizer et al., 2017)</ref>. Our end-to-end approach is the first to learn com- prehension, reasoning and generation skills in a domain-independent data driven way.</p><p>Our use of a combination of supervised and re- inforcement learning for training, and stochastic rollouts for decoding, builds on strategies used in game playing agents such as <ref type="bibr">AlphaGo (Silver et al., 2016)</ref>. Our work is a step towards real- world applications for these techniques. Our use of rollouts could be extended by choosing the other agent's responses based on sampling, us- ing Monte Carlo Tree Search (MCTS) ( <ref type="bibr" target="#b17">Kocsis and Szepesvári, 2006</ref>). However, our setting has a higher branching factor than in domains where MCTS has been successfully applied, such as Go ( <ref type="bibr" target="#b30">Silver et al., 2016)</ref>-future work should explore scaling tree search to dialogue modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have introduced end-to-end learning of natu- ral language negotiations as a task for AI, argu- ing that it challenges both linguistic and reason- ing skills while having robust evaluation metrics. We gathered a large dataset of human-human ne- gotiations, which contain a variety of interesting tactics. We have shown that it is possible to train dialogue agents end-to-end, but that their ability can be much improved by training and decoding to maximise their goals, rather than likelihood. There remains much potential for future work, particularly in exploring other reasoning strate- gies, and in improving the diversity of utterances without diverging from human language. We will also explore other negotiation tasks, to investi- gate whether models can learn to share negotiation strategies across domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Decoding through rollouts: The model first generates a small set of candidate responses. For each candidate, it then simulates a number of possible complete future conversations by sampling, and estimates the expected future reward by averaging the scores. The system outputs the candidate with the highest expected reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>End task evaluation on heldout scenarios, against the LIKELIHOOD model and humans from 
Mechanical Turk. The maximum score is 10. Score (all) gives 0 points when agents failed to agree. 

Metric 
Dataset 
Number of Dialogues 
5808 
Average Turns per Dialogue 
6.6 
Average Words per Turn 
7.6 
% Agreed 
80.1 
Average Score (/10) 
6.0 
% Pareto Optimal 
76.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics on our dataset of crowd-
sourced dialogues between humans. 

Model 
Valid PPL Test PPL Test Avg. Rank 

LIKELIHOOD 

5.62 
5.47 
521.8 

RL 

6.03 
5.86 
517.6 

ROLLOUTS 

-
-
844.1 

RL+ROLLOUTS 

-
-
859.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Intrinsic evaluation showing the average 
perplexity of tokens and rank of complete turns 
(out of 2083 unique human messages from the test 
set). Lower is more human-like for both. 

</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/ end-to-end-negotiator</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Luke Zettlemoyer and the anonymous EMNLP reviewers for their insightful comments, and the Mechanical Turk workers who helped us collect data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modelling Strategic Conversation: The STAC project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Guhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stergos</forename><surname>Afantenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Vieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemDial</title>
		<meeting>SemDial</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating Practical Negotiating Agents: Results and Analysis of the 2011 International Competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Baarslag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhide</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Enrico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Gerding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Hindriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catholijn</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raz</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="page" from="73" to="103" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<title level="m">Learning End-to-End Goal-oriented Dialog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<title level="m">On the Properties of Neural Machine Translation: Encoder-decoder Approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08099</idno>
		<title level="m">Strategic Dialogue Management via Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08669</idno>
		<title level="m">Visual Dialog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06585</idno>
		<title level="m">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward Natural Turn-taking in a Virtual Human Negotiation Agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Devault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Mell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Turn-taking and Coordination in HumanMachine Interaction</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1511.06931</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03152</idno>
		<title level="m">Policy Networks with Two-stage Training for Dialogue Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Importance of the Agenda in Bargaining. Games and Economic Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaim</forename><surname>Fershtman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">POMDPbased Dialogue Manager Adaptation to Extended Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gašic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL</title>
		<meeting>SIGDIAL<address><addrLine>Blaise Thomson, Pirros Tsiakoulis, and Steve Young</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Misrepresentation Game: How to win at negotiation while seeming like a nice guy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahra</forename><surname>Nazari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems</title>
		<meeting>the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
	<note>International Foundation for Autonomous Agents and Multiagent Systems</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Second Dialog State Tracking Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">263</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Guhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heriberto</forename><surname>Cuayáhuitl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Efstathiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Peter</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dobre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Lascarides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo Planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levente</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Maintaining Cooperation in Complex Social Dilemmas using Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01068</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A Diversity-promoting Objective Function for Neural Conversation Models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01541</idno>
		<title level="m">Deep Reinforcement Learning for Dialogue Generation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Like a Child: Fast Novel Visual Concept Learning From Sentence Descriptions of Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04908</idno>
		<title level="m">Emergence of Grounded Compositional Language in Multi-Agent Populations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Bargaining Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John F Nash</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sampleefficient Batch Reinforcement Learning for Dialogue Management Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Olivier Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthilkumar</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé Frezza-Buet</forename><surname>Chandramohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reinforcement Learning for Adaptive Dialogue Systems: A Datadriven Methodology for Dialogue Management and Natural Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data-driven Response Generation in Social Media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">International Foundation for Autonomous Agents and Multiagent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inon</forename><surname>Zuckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erel</forename><surname>Segal-Halevi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osnat</forename><surname>Drein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarit</forename><surname>Kraus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS &apos;14</title>
		<meeting>the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS &apos;14<address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
	<note>NegoChat: A Chat-based Negotiation Agent</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mastering the Game of Go with Deep Neural Networks and Tree Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="133" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Development of lying to conceal a transgression: Children&apos;s control of expressive behaviour during verbal deception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Behavioral Development</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-party, Multiissue, Multi-strategy Negotiation for Multi-modal Virtual Agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacy</forename><forename type="middle">C</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Hartholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Intelligent Virtual Agents, IVA &apos;08</title>
		<meeting>the 8th International Conference on Intelligent Virtual Agents, IVA &apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>SpringerVerlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A Neural Conversational Model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<title level="m">Stefan Ultes, and Steve Young. 2016. A Networkbased End-to-End Trainable Task-oriented Dialogue System</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Partially Observable Markov Decision Processes for Spoken Dialog Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="393" to="422" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simple Statistical Gradientfollowing Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
