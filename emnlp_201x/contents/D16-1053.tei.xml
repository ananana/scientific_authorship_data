<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long Short-Term Memory-Networks for Machine Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
							<email>{jianpeng.cheng,li.dong}@ed.ac.uk, mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Long Short-Term Memory-Networks for Machine Reading</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="551" to="561"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>How can a sequence-level network induce relations which are presumed latent during text processing? How can a recurrent network attentively memorize longer sequences in a way that humans do? In this paper we design a machine reader that automatically learns to understand text. The term machine read- ing is related to a wide range of tasks from answer- ing reading comprehension questions <ref type="bibr" target="#b8">(Clark et al., 2013)</ref>, to fact and relation extraction ), ontology learning <ref type="bibr" target="#b34">(Poon and Domingos, 2010)</ref>, and textual entailment <ref type="bibr" target="#b9">(Dagan et al., 2005</ref>). Rather than focusing on a specific task, we develop a general-purpose reading simula- tor, drawing inspiration from human language pro- cessing and the fact language comprehension is in- cremental with readers continuously extracting the meaning of utterances on a word-by-word basis.</p><p>In order to understand texts, our machine reader should provide facilities for extracting and repre- senting meaning from natural language text, storing meanings internally, and working with stored mean- ings to derive further consequences. Ideally, such a system should be robust, open-domain, and de- grade gracefully in the presence of semantic rep- resentations which may be incomplete, inaccurate, or incomprehensible. It would also be desirable to simulate the behavior of English speakers who pro- cess text sequentially, from left to right, fixating nearly every word while they read <ref type="bibr" target="#b35">(Rayner, 1998)</ref> and creating partial representations for sentence pre- fixes <ref type="bibr" target="#b25">(Konieczny, 2000;</ref><ref type="bibr" target="#b43">Tanenhaus et al., 1995)</ref>.</p><p>Language modeling tools such as recurrent neural networks (RNN) bode well with human reading be- havior ( <ref type="bibr" target="#b15">Frank and Bod, 2011)</ref>. RNNs treat each sen- tence as a sequence of words and recursively com- pose each word with its previous memory, until the meaning of the whole sentence has been derived. In practice, however, sequence-level networks are met with at least three challenges. The first one concerns model training problems associated with vanishing and exploding gradients <ref type="bibr" target="#b20">(Hochreiter, 1991;</ref><ref type="bibr" target="#b2">Bengio et al., 1994)</ref>, which can be partially ameliorated with gated activation functions, such as the Long Short- Term Memory (LSTM) (Hochreiter and Schmidhu- ber, 1997), and gradient clipping ( <ref type="bibr">Pascanu et al., 2013</ref>). The second issue relates to memory com- pression problems. As the input sequence gets com- pressed and blended into a single dense vector, suf- <ref type="figure">Figure 1</ref>: Illustration of our model while reading the sentence The FBI is chasing a criminal on the run. Color red represents the current word being fixated, blue represents memories. Shading indicates the de- gree of memory activation.</p><p>ficiently large memory capacity is required to store past information. As a result, the network general- izes poorly to long sequences while wasting memory on shorter ones. Finally, it should be acknowledged that sequence-level networks lack a mechanism for handling the structure of the input. This imposes an inductive bias which is at odds with the fact that language has inherent structure. In this paper, we develop a text processing system which addresses these limitations while maintaining the incremental, generative property of a recurrent language model. Recent attempts to render neural networks more structure aware have seen the incorporation of exter- nal memories in the context of recurrent neural net- works ( <ref type="bibr" target="#b41">Sukhbaatar et al., 2015;</ref>. The idea is to use multiple memory slots outside the recurrence to piece-wise store representations of the input; read and write operations for each slot can be modeled as an at- tention mechanism with a recurrent controller. We also leverage memory and attention to empower a recurrent network with stronger memorization capa- bility and more importantly the ability to discover relations among tokens. This is realized by insert- ing a memory network module in the update of a re- current network together with attention for memory addressing. The attention acts as a weak inductive module discovering relations between input tokens, and is trained without direct supervision. As a point of departure from previous work, the memory net- work we employ is internal to the recurrence, thus strengthening the interaction of the two and lead- ing to a representation learner which is able to rea- son over shallow structures. The resulting model, which we term Long Short-Term Memory-Network (LSTMN), is a reading simulator that can be used for sequence processing tasks. <ref type="figure">Figure 1</ref> illustrates the reading behavior of the LSTMN. The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed. As a result, the model induces undirected relations among tokens as an intermediate step of learning representations. We validate the perfor- mance of the LSTMN in language modeling, sen- timent analysis, and natural language inference. In all cases, we train LSTMN models end-to-end with task-specific supervision signals, achieving perfor- mance comparable or better to state-of-the-art mod- els and superior to vanilla LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our machine reader is a recurrent neural network ex- hibiting two important properties: it is incremental, simulating human behavior, and performs shallow structure reasoning over input streams.</p><p>Recurrent neural network (RNNs) have been suc- cessfully applied to various sequence modeling and sequence-to-sequence transduction tasks. The latter have assumed several guises in the literature such as machine translation ( ), sen- tence compression <ref type="bibr" target="#b37">(Rush et al., 2015)</ref>, and reading comprehension ( . A key con- tributing factor to their success has been the abil- ity to handle well-known problems with exploding or vanishing gradients ( <ref type="bibr" target="#b2">Bengio et al., 1994)</ref>, leading to models with gated activation functions <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber, 1997;</ref>, and more advanced architectures that enhance the in- formation flow within the network <ref type="bibr" target="#b26">(Koutník et al., 2014;</ref><ref type="bibr" target="#b7">Chung et al., 2015;</ref><ref type="bibr" target="#b48">Yao et al., 2015)</ref>.</p><p>A remaining practical bottleneck for RNNs is memory compression ( ): since the inputs are recursively combined into a single memory representation which is typically too small in terms of parameters, it becomes difficult to accu- rately memorize sequences ( <ref type="bibr" target="#b49">Zaremba and Sutskever, 2014</ref>). In the encoder-decoder architecture, this problem can be sidestepped with an attention mech- anism which learns soft alignments between the de- coding states and the encoded memories ( ). In our model, memory and attention are added within a sequence encoder allowing the network to uncover lexical relations between tokens.</p><p>The idea of introducing a structural bias to neu- ral models is by no means new. For example, it is reflected in the work of <ref type="bibr" target="#b39">Socher et al. (2013a)</ref> who apply recursive neural networks for learning natural language representations. In the context of recur- rent neural networks, efforts to build modular, struc- tured neural models date back to <ref type="bibr" target="#b10">Das et al. (1992)</ref> who connect a recurrent neural network with an ex- ternal memory stack for learning context free gram- mars. Recently,  propose Mem- ory Networks to explicitly segregate memory stor- age from the computation of neural networks in gen- eral. Their model is trained end-to-end with a mem- ory addressing mechanism closely related to soft at- tention ( <ref type="bibr" target="#b41">Sukhbaatar et al., 2015)</ref> and has been ap- plied to machine translation ( <ref type="bibr" target="#b30">Meng et al., 2015)</ref>.  define a set of differen- tiable data structures (stacks, queues, and dequeues) as memories controlled by a recurrent neural net- work. <ref type="bibr" target="#b44">Tran et al. (2016)</ref> combine the LSTM with an external memory block component which interacts with its hidden state. <ref type="bibr" target="#b27">Kumar et al. (2016)</ref> employ a structured neural network with episodic memory modules for natural language and also visual ques- tion answering ( <ref type="bibr" target="#b47">Xiong et al., 2016)</ref>.</p><p>Similar to the above work, we leverage memory and attention in a recurrent neural network for induc- ing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate rela- tions we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural mod- els ( <ref type="bibr" target="#b5">Bowman et al., 2016)</ref> where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction ( <ref type="bibr" target="#b24">Klein and Manning, 2004</ref>) where the learned head-modifier re- lations are directed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Machine Reader</head><p>In this section we present our machine reader which is designed to process structured input while retain- ing the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem- ory (LSTM) unit with an extended memory tape that explicitly simulates the human memory span. The model performs implicit relation analysis between tokens with an attention-based memory addressing mechanism at every time step. In the following, we first review the standard Long Short-Term Memory and then describe our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long Short-Term Memory</head><p>A Long Short-Term Memory (LSTM) recurrent neu- ral network processes a variable-length sequence x = (x 1 , x 2 , · · · , x n ) by incrementally adding new content into a single memory slot, with gates con- trolling the extent to which new content should be memorized, old content should be erased, and cur- rent content should be exposed. At time step t, the memory c t and the hidden state h t are updated with the following equations:</p><formula xml:id="formula_0">2 6 6 4 i t f t o t ˆ c t 3 7 7 5 = 2 6 6 4 s s s tanh 3 7 7 5 W · [h t1 , x t ]</formula><p>(1)</p><formula xml:id="formula_1">c t = f t c t1 + i t ˆ c t (2) h t = o t tanh(c t )<label>(3)</label></formula><p>where i, f , and o are gate activations. Compared to the standard RNN, the LSTM uses additive mem- ory updates and it separates the memory c from the hidden state h, which interacts with the environment when making predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Long Short-Term Memory-Network</head><p>The first question that arises with LSTMs is the ex- tent to which they are able to memorize sequences under recursive compression. LSTMs can produce a list of state representations during composition, however, the next state is always computed from the current state. That is to say, given the current state h t , the next state h t+1 is conditionally independent of states h 1 · · · h t1 and tokens x 1 · · · x t . While the recur- sive state update is performed in a Markov manner, it is assumed that LSTMs maintain unbounded mem- ory (i.e., the current state alone summarizes well the tokens it has seen so far). This assumption may fail in practice, for example when the sequence is long or when the memory size is not large enough. An- other undesired property of LSTMs concerns model- ing structured input. An LSTM aggregates informa- tion on a token-by-token basis in sequential order, but there is no explicit mechanism for reasoning over structure and modeling relations between tokens.</p><p>Our model aims to address both limitations. Our solution is to modify the standard LSTM structure by replacing the memory cell with a memory net- work ( ). The resulting Long Short-Term Memory-Network (LSTMN) stores the contextual representation of each input token with a unique memory slot and the size of the memory grows with time until an upper bound of the memory span is reached. This design enables the LSTM to reason about relations between tokens with a neural attention layer and then perform non-Markov state updates. Although it is feasible to apply both write and read operations to the memories with attention, we concentrate on the latter. We conceptualize the read operation as attentively linking the current to- ken to previous memories and selecting useful con- tent when processing it. Although not the focus of this work, the significance of the write operation can be analogously justified as a way of incremen- tally updating previous memories, e.g., to correct wrong interpretations when processing garden path sentences <ref type="bibr" target="#b14">(Ferreira and Henderson, 1991)</ref>.</p><p>The architecture of the LSTMN is shown in <ref type="figure" target="#fig_0">Fig- ure 2</ref> and the formal definition is provided as fol- lows. The model maintains two sets of vectors stored in a hidden state tape used to interact with the environment (e.g., computing attention), and a mem- ory tape used to represent what is actually stored in memory. 1 Therefore, each token is associated with a hidden vector and a memory vector. Let x t de- note the current input; C t1 = (c 1 , · · · , c t1 ) denotes the current memory tape, and H t1 = (h 1 , · · · , h t1 ) the previous hidden tape. At time step t, the model computes the relation between x t and x 1 · · · x t1 through h 1 · · · h t1 with an attention layer:</p><formula xml:id="formula_2">a t i = v T tanh(W h h i +W x x t +W˜h˜h+W˜ +W˜h +W˜h˜ +W˜h˜h t1 )<label>(4)</label></formula><formula xml:id="formula_3">s t i = softmax(a t i )<label>(5)</label></formula><p>This yields a probability distribution over the hidden state vectors of previous tokens. We can then com- pute an adaptive summary vector for the previous hidden tape and memory tape denoted by˜cby˜ by˜c t and˜hand˜ and˜h t , respectively:</p><formula xml:id="formula_4"> ˜ h t ˜ c t = t1 Â i=1 s t i ·  h i c i<label>(6)</label></formula><p>and use them for computing the values of c t and h t in the recurrent update as: </p><formula xml:id="formula_5">c t = f t ˜ c t + i t ˆ c t<label>(7)</label></formula><formula xml:id="formula_6">h t = o t tanh(c t )<label>(8)</label></formula><p>where v, W h , W x and W ˜ h are the new weight terms of the network.</p><p>A key idea behind the LSTMN is to use attention for inducing relations between tokens. These rela- tions are soft and differentiable, and components of a larger representation learning network. Although it is appealing to provide direct supervision for the attention layer, e.g., with evidence collected from a dependency treebank, we treat it as a submod- ule being optimized within the larger network in a downstream task. It is also possible to have a more structured relational reasoning module by stacking multiple memory and hidden layers in an alternat- ing fashion, resembling a stacked LSTM (Graves, <ref type="bibr">et al., 2015)</ref>. This can be achieved by feeding the output h k t of the lower layer k as input to the upper layer (k + 1). The attention at the (k + 1)th layer is computed as:</p><note type="other">2013) or a multi-hop memory network (Sukhbaatar</note><formula xml:id="formula_8">a t i,k+1 = v T tanh(W h h k+1 i +W l h k t +W˜h˜h+W˜ +W˜h +W˜h˜ +W˜h˜h k+1 t1 )<label>(10)</label></formula><p>Skip-connections ( <ref type="bibr" target="#b16">Graves, 2013)</ref> can be applied to feed x t to upper layers as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling Two Sequences with LSTMN</head><p>Natural language processing tasks such as machine translation and textual entailment are concerned with modeling two sequences rather than a single one. A standard tool for modeling two sequences with recurrent networks is the encoder-decoder ar- chitecture where the second sequence (also known as the target) is being processed conditioned on the first one (also known as the source). In this section we explain how to combine the LSTMN which ap- plies attention for intra-relation reasoning, with the encoder-decoder network whose attention module learns the inter-alignment between two sequences. Figures 3a and 3b illustrate two types of combina- tion. We describe the models more formally below.</p><p>Shallow Attention Fusion Shallow fusion simply treats the LSTMN as a separate module that can be readily used in an encoder-decoder architecture, in lieu of a standard RNN or LSTM. As shown in <ref type="figure" target="#fig_3">Figure 3a</ref>, both encoder and decoder are modeled as LSTMNs with intra-attention. Meanwhile, inter- attention is triggered when the decoder reads a tar- get token, similar to the inter-attention introduced in .</p><p>Deep Attention Fusion Deep fusion combines inter-and intra-attention (initiated by the decoder) when computing state updates. We use different no- tation to represent the two sets of attention. Follow- ing Section 3.2, C and H denote the target memory tape and hidden tape, which store representations of the target symbols that have been processed so far. The computation of intra-attention follows Equa- tions (4)-(9). Additionally, we use A = [a 1 , · · · , a m ] and Y = [g 1 , · · · , g m ] to represent the source mem- ory tape and hidden tape, with m being the length of the source sequence conditioned upon. We compute inter-attention between the input at time step t and tokens in the entire source sequence as follows:</p><formula xml:id="formula_9">b t j = u T tanh(W g g j +W x x t +W˜g˜g+W˜ +W˜g+W˜g˜ +W˜g˜g t1 )<label>(11)</label></formula><formula xml:id="formula_10">p t j = softmax(b t j )<label>(12)</label></formula><p>After that we compute the adaptive representation of the source memory tapeãtape˜tapeã t and hidden tape˜gtape˜ tape˜g t as:</p><formula xml:id="formula_11"> ˜ g t ˜ a t = m Â j=1 p t j ·  g j a j<label>(13)</label></formula><p>We can then transfer the adaptive source represen- tatioñ a t to the target memory with another gating operation r t , analogous to the gates in Equation <ref type="formula" target="#formula_5">(7)</ref>.</p><formula xml:id="formula_12">r t = s(W r · [˜ g t , x t ])<label>(14)</label></formula><p>The new target memory includes inter-alignment r t ˜ a t , intra-relation f t ˜ c t , and the new input in- formation i t ˆ c t :</p><formula xml:id="formula_13">c t = r t ˜ a t + f t ˜ c t + i t ˆ c t<label>(15)</label></formula><formula xml:id="formula_14">h t = o t tanh(c t )<label>(16)</label></formula><p>As shown in the equations above and <ref type="figure" target="#fig_3">Figure 3b</ref>, the major change of deep fusion lies in the recurrent storage of the inter-alignment vector in the target memory network, as a way to help the target net- work review source information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we present our experiments for eval- uating the performance of the LSTMN machine reader. We start with language modeling as it is a natural testbed for our model. We then as- sess the model's ability to extract meaning repre- sentations for generic sentence classification tasks such as sentiment analysis. Finally, we examine whether the LSTMN can recognize the semantic relationship between two sentences by applying it to a natural language inference task. Our code is available at https://github.com/cheng6076/ SNLI-attention.    <ref type="table">Table 1</ref>: Language model perplexity on the Penn Treebank. The size of memory is 300 for all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Modeling</head><p>Our language modeling experiments were con- ducted on the English Penn Treebank dataset. Fol- lowing common practice ( <ref type="bibr" target="#b31">Mikolov et al., 2010)</ref>, we trained on sections 0-20 (1M words), used sec- tions 21-22 for validation (80K words), and sec- tions 23-24 (90K words for testing). The dataset contains approximately 1 million tokens and a vo- cabulary size of 10K. The average sentence length is 21. We use perplexity as our evaluation metric: PPL = exp(NLL/T ), where NLL denotes the nega- tive log likelihood of the entire test set and T the corresponding number of tokens. We used stochas- tic gradient descent for optimization with an ini- tial learning rate of 0.65, which decays by a factor of 0.85 per epoch if no significant improvement has been observed on the validation set. We renormal- ize the gradient if its norm is greater than 5. The mini-batch size was set to 40. The dimensions of the word embeddings were set to 150 for all models.</p><p>In this suite of experiments we compared the LSTMN against a variety of baselines. The first one is a Kneser-Ney 5-gram language model (KN5) which generally serves as a non-neural baseline for the language modeling task. We also present per- plexity results for the standard RNN and LSTM models. We also implemented more sophisti- cated LSTM architectures, such as a stacked LSTM (sLSTM), a gated-feedback LSTM (gLSTM; <ref type="bibr" target="#b7">Chung et al. (2015)</ref>) and a depth-gated LSTM (dLSTM; <ref type="bibr" target="#b48">Yao et al. (2015)</ref>). The gated-feedback LSTM has feedback gates connecting the hidden states across multiple time steps as an adaptive control of the in- formation flow. The depth-gated LSTM uses a depth gate to connect memory cells of vertically adjacent layers. In general, both gLSTM and dLSTM are able to capture long-term dependencies to some de- gree, but they do not explicitly keep past memories. We set the number of layers to 3 in this experiment, mainly to agree with the language modeling exper- iments of <ref type="bibr" target="#b7">Chung et al. (2015)</ref>. Also note that that there are no single-layer variants for gLSTM and dLSTM; they have to be implemented as multi-layer systems. The hidden unit size of the LSTMN and all comparison models (except KN5) was set to 300.</p><p>The results of the language modeling task are shown in <ref type="table">Table 1</ref>. Perplexity results for KN5 and RNN are taken from <ref type="bibr" target="#b32">Mikolov et al. (2015)</ref>. As can be seen, the single-layer LSTMN outperforms these he sits down at the piano and plays our view is that we may see a profit decline products &lt; unk &gt; have to be first to be winners everyone in the world is watching us very closely <ref type="figure">Figure 4</ref>: Examples of intra-attention (language modeling). Bold lines indicate higher attention scores. Arrows denote which word is being focused when attention is computed, but not the direction of the relation.</p><p>two baselines and the LSTM by a significant mar- gin. Amongst all deep architectures, the three-layer LSTMN also performs best. We can study the mem- ory activation mechanism of the machine reader by visualizing the attention scores. <ref type="figure">Figure 4</ref> shows four sentences sampled from the Penn Treebank val- idation set. Although we explicitly encourage the reader to attend to any memory slot, much attention focuses on recent memories. This agrees with the linguistic intuition that long-term dependencies are relatively rare. As illustrated in <ref type="figure">Figure 4</ref> the model captures some valid lexical relations (e.g., the de- pendency between sits and at, sits and plays, every- one and is, is and watching). Note that arcs here are undirected and are different from the directed arcs denoting head-modifier relations in dependency graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sentiment Analysis</head><p>Our second task concerns the prediction of senti- ment labels of sentences. We used the Stanford Sen- timent <ref type="bibr">Treebank (Socher et al., 2013a</ref>), which con- tains fine-grained sentiment labels (very positive, positive, neutral, negative, very negative) for 11,855 sentences. Following previous work on this dataset,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Fine-grained Binary RAE <ref type="bibr" target="#b38">(Socher et al., 2011)</ref> 43.2 82.4 RNTN <ref type="bibr" target="#b40">(Socher et al., 2013b)</ref> 45.7 85.4 DRNN <ref type="bibr" target="#b21">(Irsoy and Cardie, 2014)</ref> 49.8 86.6 DCNN <ref type="bibr" target="#b3">(Blunsom et al., 2014)</ref> 48.5 86.8 CNN-MC <ref type="bibr" target="#b22">(Kim, 2014)</ref> 48.0 88.1 T-CNN ( <ref type="bibr" target="#b29">Lei et al., 2015)</ref> 51.2 88.6 PV ( <ref type="bibr" target="#b28">Le and Mikolov, 2014)</ref> 48.7 87.8 CT-LSTM <ref type="bibr" target="#b42">(Tai et al., 2015)</ref> 51.0 88.0 LSTM <ref type="bibr" target="#b42">(Tai et al., 2015)</ref> 46.4 84.9 2-layer LSTM <ref type="bibr" target="#b42">(Tai et al., 2015)</ref> 46 we used 8,544 sentences for training, 1,101 for val- idation, and 2,210 for testing. The average sentence length is 19.1. In addition, we also performed a bi- nary classification task (positive, negative) after re- moving the neutral label. This resulted in 6,920 sen- tences for training, 872 for validation and 1,821 for testing. <ref type="table" target="#tab_1">Table 2</ref> reports results on both fine-grained and binary classification tasks.</p><note type="other">.0 86.3 LSTMN 47.6 86.3 2-layer LSTMN 47.9 87.0</note><p>We experimented with 1-and 2-layer LSTMNs. For the latter model, we predict the sentiment la- bel of the sentence based on the averaged hidden vector passed to a 2-layer neural network classifier with ReLU as the activation function. The mem- ory size for both LSTMN models was set to 168 to be compatible with previous LSTM models ( <ref type="bibr" target="#b42">Tai et al., 2015</ref>) applied to the same task. We used pre- trained 300-D Glove 840B vectors ( <ref type="bibr" target="#b33">Pennington et al., 2014</ref>) to initialize the word embeddings. The gradient for words with Glove embeddings, was scaled by 0.35 in the first epoch after which all word embeddings were updated normally.</p><p>We used Adam ( <ref type="bibr" target="#b23">Kingma and Ba, 2015</ref>) for op- timization with the two momentum parameters set to 0.9 and 0.999 respectively. The initial learning rate was set to 2E-3. The regularization constant was 1E-4 and the mini-batch size was 5. A dropout rate of 0.5 was applied to the neural network classifier.</p><p>We compared our model with a wide range of top- performing systems. Most of these models (includ- ing ours) are LSTM variants (third block in <ref type="table" target="#tab_1">Table 2</ref>), recursive neural networks (first block), or convolu-tional neural networks (CNNs; second block). Re- cursive models assume the input sentences are rep- resented as parse trees and can take advantage of annotations at the phrase level. LSTM-type models and CNNs are trained on sequential input, with the exception of CT-LSTM ( <ref type="bibr" target="#b42">Tai et al., 2015</ref>) which op- erates over tree-structured network topologies such as constituent trees. For comparison, we also report the performance of the paragraph vector model (PV; <ref type="bibr" target="#b28">Le and Mikolov (2014)</ref>; see <ref type="table" target="#tab_1">Table 2</ref>, second block) which neither operates on trees nor sequences but learns distributed document representations param- eterized directly.</p><p>The results in <ref type="table" target="#tab_1">Table 2</ref> show that both 1-and 2-layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art. The number of layers for our models was set to be comparable to previously published results. On the fine-grained and binary classification tasks our 2-layer LSTMN performs close to the best system T-CNN ( <ref type="bibr" target="#b29">Lei et al., 2015</ref>). <ref type="figure">Figure 5</ref> shows examples of intra-attention for sentiment words. Interestingly, the network learns to associate sentiment important words such as though and fantastic or not and good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Natural Language Inference</head><p>The ability to reason about the semantic relation- ship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradic- tory, or neutral. For this task we used the Stan- ford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>, which contains premise- hypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we end up with 549,367 pairs for training, 9,842 for development and 9,824 for testing. The vocabulary size is 36,809 and the average sentence length is 22. We performed lower-casing and tok- enization for the entire dataset.</p><p>Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logi- cal relationship <ref type="bibr">(Rocktäschel et al., 2016;</ref><ref type="bibr" target="#b45">Wang and Jiang, 2016</ref>). Furthermore, <ref type="bibr">Rocktäschel et al. (2016)</ref> show that a non-standard encoder-decoder architec- ture which processes the hypothesis conditioned on it 's tough to watch but it 's a fantastic movie although i did n't hate this one , it 's not very good either <ref type="figure">Figure 5</ref>: Examples of intra-attention (sentiment analysis). Bold lines (red) indicate attention be- tween sentiment important words.</p><p>the premise results significantly boosts performance. We use a similar approach to tackle this task with LSTMNs. Specifically, we use two LSTMNs to read the premise and hypothesis, and then match them by comparing their hidden state tapes. We perform average pooling for the hidden state tape of each LSTMN, and concatenate the two averages to form the input to a 2-layer neural network classifier with ReLU as the activation function.</p><p>We used pre-trained 300-D Glove 840B vectors ( <ref type="bibr" target="#b33">Pennington et al., 2014</ref>) to initialize the word em- beddings. Out-of-vocabulary (OOV) words were initialized randomly with Gaussian samples (µ=0, s=1). We only updated OOV vectors in the first epoch, after which all word embeddings were up- dated normally. The dropout rate was selected from [0.1, 0.2, 0.3, 0.4]. We used Adam ( <ref type="bibr" target="#b23">Kingma and Ba, 2015)</ref> for optimization with the two momentum pa- rameters set to 0.9 and 0.999 respectively, and the initial learning rate set to 1E-3. The mini-batch size was set to 16 or 32. For a fair comparison against previous work, we report results with different hid- den/memory dimensions (i.e., 100, 300, and 450).</p><p>We compared variants of our model against dif- ferent types of LSTMs (see the second block in Ta- ble 3). Specifically, these include a model which encodes the premise and hypothesis independently with two <ref type="bibr">LSTMs (Bowman et al., 2015</ref>), a shared LSTM ( <ref type="bibr">Rocktäschel et al., 2016</ref>), a word-by-word attention model <ref type="bibr">(Rocktäschel et al., 2016)</ref>, and a matching LSTM (mLSTM; <ref type="bibr" target="#b45">Wang and Jiang (2016)</ref>). This model sequentially processes the hypothesis, and at each position tries to match the current word with an attention-weighted representation of the premise (rather than basing its predictions on whole sentence embeddings). We also compared our mod-Models h |q| M Test BOW concatenation - - 59.8 LSTM <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> 100 221k 77.6 LSTM-att <ref type="bibr">(Rocktäschel et al., 2016)</ref> 100 252k 83.5 mLSTM <ref type="bibr" target="#b45">(Wang and Jiang, 2016)</ref> 300  <ref type="table">Table 3</ref>: Parameter counts |q| M , size of hidden unit h, and model accuracy (%) on the natural lan- guage inference task. els with a bag-of-words baseline which averages the pre-trained embeddings for the words in each sen- tence and concatenates them to create features for a logistic regression classifier (first block in <ref type="table">Table 3</ref>). LSTMNs achieve better performance compared to LSTMs (with and without attention; 2nd block in <ref type="table">Table 3</ref>). We also observe that fusion is gen- erally beneficial, and that deep fusion slightly im- proves over shallow fusion. One explanation is that with deep fusion the inter-attention vectors are re- currently memorized by the decoder with a gating operation, which also improves the information flow of the network. With standard training, our deep fu- sion yields the state-of-the-art performance in this task. Although encouraging, this result should be in- terpreted with caution since our model has substan- tially more parameters compared to related systems. We could compare different models using the same number of total parameters. However, this would in- evitably introduce other biases, e.g., the number of hyper-parameters would become different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper we proposed a machine reading simula- tor to address the limitations of recurrent neural net- works when processing inherently structured input. Our model is based on a Long Short-Term Mem- ory architecture embedded with a memory network, explicitly storing contextual representations of in- put tokens without recursively compressing them. More importantly, an intra-attention mechanism is employed for memory addressing, as a way to in- duce undirected relations among tokens. The at- tention layer is not optimized with a direct super- vision signal but with the entire network in down- stream tasks. Experimental results across three tasks show that our model yields performance comparable or superior to state of the art.</p><p>Although our experiments focused on LSTMs, the idea of building more structure aware neural models is general and can be applied to other types of net- works. When direct supervision is provided, simi- lar architectures can be adapted to tasks such as de- pendency parsing and relation extraction. In the fu- ture, we hope to develop more linguistically plausi- ble neural architectures able to reason over nested structures and neural models that learn to discover compositionality with weak or indirect supervision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Long Short-Term Memory-Network. Color indicates degree of memory activation.</figDesc><graphic url="image-1.png" coords="4,64.72,57.83,234.00,178.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LSTMNs for sequence-to-sequence modeling. The encoder uses intra-attention, while the decoder incorporates both intra-and inter-attention. The two figures present two ways to combine the intra-and inter-attention in the decoder.</figDesc><graphic url="image-3.png" coords="6,301.51,87.84,234.01,135.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model accuracy (%) on the Sentiment Tree-
bank (test set). The memory size of LSTMN models 
is set to 168 to be compatible with previously pub-
lished LSTM variants (Tai et al., 2015). 

</table></figure>

			<note place="foot" n="1"> For comparison, LSTMs maintain a hidden vector and a memory vector; memory networks (Weston et al., 2015) have a set of key vectors and a set of value vectors.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank members of the ILCC at the School of Informatics and the anonymous reviewers for help-ful comments. The support of the European Re-search Council under award number 681760 "Trans-lating Multiple Modalities into Text" is gratefully acknowledged.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 NAACL: HLT</title>
		<meeting>the 2016 NAACL: HLT<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ICLR</title>
		<meeting>the 2014 ICLR<address><addrLine>Banff, Alberta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd ACL</title>
		<meeting>the 52nd ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast unified model for parsing and sentence understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1466" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 EMNLP</title>
		<meeting>the 2014 EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ICML</title>
		<meeting>the 32nd ICML<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A study of the knowledge base requirements for passing an elementary science test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Automated KB Construction</title>
		<meeting>the 3rd Workshop on Automated KB Construction<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Sreerupa Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 14th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="791" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open information extraction: The second generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd IJCAI</title>
		<meeting>the 22nd IJCAI<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 EMNLP</title>
		<meeting>the 2011 EMNLP<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recovery from misanalyses of garden-path sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="725" to="745" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Insensitivity of the human sentence-processing system to hierarchical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pyschological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="829" to="834" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1819" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universität München</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 EMNLP</title>
		<meeting>the 2014 EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ICLR</title>
		<meeting>the 2015 ICLR<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACL</title>
		<meeting>the 42nd ACL<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Locality and parsing complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Konieczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psycholinguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="627" to="645" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A clockwork RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ICML</title>
		<meeting>the 31st ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ICML</title>
		<meeting>the 33rd ICML<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ICML</title>
		<meeting>the 31st ICML<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1565" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep memory-based architecture for sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR-Workshop</title>
		<meeting>ICLR-Workshop<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th Interspeech</title>
		<meeting>11th Interspeech<address><addrLine>Makuhari, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`nock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio</title>
		<meeting><address><addrLine>San Diego, California; Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
	<note>Proceedings of the 30th ICML</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 EMNLP</title>
		<meeting>the 2014 EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised ontology induction from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Eye movements in reading and information processing: 20 years of research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="422" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Karl Moritz Hermann, Tomáš Kočisk`Kočisk`y, and Phil Blunsom. 2016. Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ICLR</title>
		<meeting>the 2016 ICLR<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 EMNLP</title>
		<meeting>the 2013 EMNLP<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 EMNLP</title>
		<meeting>the 2013 EMNLP<address><addrLine>Seattle, Washingtton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Integration of visual and linguistic information in spoken language comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Tanenhaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Spivey-Knowlton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Eberhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julue</forename><forename type="middle">C</forename><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1632" to="1634" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recurrent memory network for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th NAACL</title>
		<meeting>the 15th NAACL<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 NAACL: HLT</title>
		<meeting>the 2016 NAACL: HLT<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ICLR</title>
		<meeting>the 2015 ICLR<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ICML</title>
		<meeting>the 33rd ICML<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03790</idno>
		<title level="m">Depth-gated recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
