<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1973" to="1978"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variable-order CRFs achieve faster inference by including features for only some of the tag n-grams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2-6x speedup over a baseline, with no significant drop in accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conditional Random Fields (CRFs) ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>) are a convenient formalism for sequence label- ing tasks common in NLP. A CRF defines a feature- rich conditional distribution over tag sequences (out- put) given an observed word sequence (input).</p><p>The key advantage of the CRF framework is the flexibility to consider arbitrary features of the input, as well as enough features over the output structure to encourage it to be well-formed and consistent. How- ever, inference in CRFs is fast only if the features over the output structure are limited. For example, an order-k CRF (or "k-CRF" for short, with k &gt; 1 being "higher-order") allows expressive features over a window of k+1 adjacent tags (as well as the input), and then inference takes time O(n·|Y | k+1 ), where Y is the set of tags and n is the length of the input.</p><p>How large does k need to be? Typically k = 2 works well, with big gains from 0 → 1 and modest * Equal contribution Figure 1: Speed-accuracy tradeoff curves on test data for the 5 languages. Large dark circles represent the k- CRFs of ascending orders along x-axis (marked on for Slovenian). Smaller triangles each represent a VoCRF discovered by sweeping the speed parameters γ. We find faster models at similar accuracy to the best k-CRFs ( §5).</p><p>gains from 1 → 2 ( <ref type="figure">Fig. 1</ref>). Small k may be sufficient when there is enough training data to allow the model to attend to many fine-grained features of the input ( <ref type="bibr" target="#b18">Toutanova et al., 2003;</ref><ref type="bibr" target="#b9">Liang et al., 2008</ref>). For ex- ample, when predicting POS tags in morphologically- rich languages, certain words are easily tagged based on their spelling without considering the context (k = 0). In fact, such languages tend to have a more free word order, making tag context less useful. We investigate a hybrid approach that gives the accuracy of higher-order models while reducing run- time. We build on variable-order CRFs ( <ref type="bibr" target="#b21">Ye et al., 2009</ref>) (VoCRF), which support features on tag sub- sequences of mixed orders. Since only modest gains are obtained from moving to higher-order models, we posit that only a small fraction of the higher-order features are necessary. We introduce a hyperparam- eter γ that discourages the model from using many higher-order features (= faster inference) and a hy- perparameter λ that encourages generalization. Thus, sweeping a range of values for γ and λ gives rise to a number of operating points along the speed-accuracy curve (triangle points in <ref type="figure">Fig. 1</ref>).</p><p>We present three contributions: (1) A simplified exposition of VoCRFs, including an algorithm for computing gradients that is asymptotically more ef- ficient than prior art ( <ref type="bibr" target="#b3">Cuong et al., 2014)</ref>. <ref type="formula">(2)</ref> We develop a structure learning algorithm for discover- ing the essential set of higher-order dependencies so that inference is fast and accurate. (3) We investigate the effectiveness of our approach on POS tagging in five diverse languages. We find that the amount of required context for accurate prediction is highly language-dependent. In all languages, however, our approach meets the accuracy of fixed-order models at a fraction of the runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variable-Order CRFs</head><p>An order-k CRF (k-CRF, for short) is a conditional probability distribution of the form</p><formula xml:id="formula_0">p θ (y | x) = 1 Z θ (x) exp n+1 t=1 θ f (x, t, y t−k . . . y t )</formula><p>where n is the length of the input x, θ ∈ R d is the model parameter, and f is an arbitrary user-defined function that computes a vector in R d of features of the tag substring s = y t−k . . . y t when it appears at position t of input x. We define y i to be a distin- guished boundary tag # when i / ∈ <ref type="bibr">[1, n]</ref>. A variable-order CRF or VoCRF is a refinement of the k-CRF, in which f may not always depend on all k + 1 of the tags that it has access to. The features of a particular tag substring s may sometimes be determined by a shorter suffix of s.</p><p>To be precise, a VoCRF specifies a finite set W ⊂ Y * that is sufficient for feature computation (where Y * denotes the set of all tag sequences). <ref type="bibr">1</ref> The VoCRF's featurization function f (x, t, s) is then de- fined as f (x, t, w(s)) where f can be any function and w(s) ∈ Y * is the longest suffix of s that appears in W (or ε if none exists). The full power of a k- CRF can be obtained by specifying W = Y k+1 , but smaller W will in general allow speedups.</p><p>To support our algorithms, we define W to be the closure of W under prefixes and last-character substitution. Formally, W is the smallest nonempty superset of W such that if hy ∈ W for some h ∈ Y * Algorithm 1 FORWARD: Compute log Z θ (x). α(·, ·) = 0; α(0, #) = 1 initialization for t = 1 to n + 1 :</p><formula xml:id="formula_1">if t = n + 1 then Y t = {#} else y t = Y \{#} for h ∈ H, y t ∈ Y t : h = NEXT(h, y t ) z = exp θ f (x, t, w(hy t )) α(t, h ) += α(t−1, h) · z Z = h∈H α(n + 1, h) sum over final states return log Z Algorithm 2 GRADIENT: Compute θ log Z θ (x). β(·, ·) = 0; ∆ = 0 β(n + 1, h) = 1 for all h ∈ H initialization for t = n + 1 downto 1 : for h ∈ H, y t ∈ Y t : h = NEXT(h, y t ) z = exp θ f (x, t, w(hy t )) ∆ += f (x, t, w(hy t ))·α(t−1, h)·z ·β(t, h ) β(t−1, h) += z · β(t, h ) return ∆/Z</formula><p>and y ∈ Y , then h ∈ W and also hy ∈ W for all y ∈ Y . This implies that we can factor W as H × Y , where H ⊂ Y * is called the set of histories.</p><p>We now define NEXT(h, y) to return the longest suffix of hy that is in H (which may be hy itself, or even ε). We may regard NEXT as the transition func- tion of a deterministic finite-state automaton (DFA) with state set H and alphabet Y . If this DFA is used to read any tag sequence y ∈ Y * , then the arc that reads y t comes from a state h such that hy t is the longest suffix of s = y t−k . . . y t that appears in W-and thus w(hy t ) = w(s) ∈ W and provides sufficient information to compute f (x, t, s). <ref type="bibr">2</ref> For a given x of length n and given parameters θ, the log-normalizer log Z θ (x)-which will be needed to compute the log-probability in eq. (1) below-can be found in time O(|W| n) by dynamic program- ming. Concise pseudocode is in Alg. 1. In effect, this runs the forward algorithm on the lattice of taggings given by length-n paths through the DFA.</p><p>For finding the parameters θ that minimize eq. (1) below, we want the gradient θ log Z θ (x). By applying algorithmic differentiation to Alg. 1, we obtain Alg. 2, which uses back-propagation to compute the gradient (asymptotically) as fast as Alg. 1 and |H| times faster than <ref type="bibr" target="#b3">Cuong et al. (2014)</ref>'s algorithm-a significant speedup since |H| is often quite large (up to 300 in our experiments). Algs. 1-2 together effectively run the forward-backward algorithm on the lattice of taggings. <ref type="bibr">3</ref> It is straightforward to modify Alg. 1 to obtain a Viterbi decoder that finds the most-likely tag se- quence under p θ (· | x). It is also straightforward to modify Alg. 2 to compute the marginal probabilities of tag substrings occurring at particular positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Structured Sparsity and Active Sets</head><p>We begin with a k-CRF model whose feature vector f (x, t, y t−k . . . y t ) is partitioned into non-stationary local features f (1) (x, t, y t ) and stationary higher- order features f (2) (y t−k . . . y t ). Specifically, f <ref type="bibr">(2)</ref> includes an indicator feature for each tag string w ∈</p><formula xml:id="formula_2">Y * with 1 ≤ |w| ≤ k + 1, where f (2) w (y t−k . . . y t )</formula><p>is 1 if w is a suffix of y t−k . . . y t and is 0 otherwise. <ref type="bibr">4</ref> To obtain the advantages of a VoCRF, we merely have to choose a sparse weight vector θ. The set W can then be defined to be the set of strings in Y * whose features have nonzero weight. Prior work ( <ref type="bibr" target="#b3">Cuong et al., 2014</ref>) has left the construction of W to domain experts or "one size fits all" strategies (e.g., k-CRF). Our goal is to choose θ-and thus W-so that inference is accurate and fast.</p><p>Our approach is to modify the usual L 2 - regularized log-likelihood training criterion with a carefully defined runtime penalty scaled by a param- eter γ to balance competing objectives: likelihood on the data {(</p><formula xml:id="formula_3">x (i) , y (i) )} m i=1 vs. efficiency (small W). − m i=1 log p θ (y (i) | x (i) ) loss + λ||θ|| 2 2 generalization + γR(θ) runtime (1)</formula><p>Recall that the runtime of inference on a given sentence is proportional to the size of W, the closure 3 Eisner (2016) explains the connection between algorithmic differentiation and the forward-backward algorithm. <ref type="bibr">4</ref> Extensions to richer sets of higher-order features are possible, such as conjunctions with properties of the words at position t. of W under prefixes and last-character replacement.</p><formula xml:id="formula_4">ε N V NN NV VN VV G V G"</formula><p>(Any tag strings in W\W can get nonzero weight without increasing runtime.) Thus, R(θ) would ide- ally measure |W|, or proportionately, |H|. Experi- mentally, we find that |W| has &gt; 99% Pearson cor- relation with wallclock time, making it an excellent proxy for wallclock time while being more replicable. We relax this regularizer to a convex function- a tree-structured group lasso objective ( <ref type="bibr" target="#b22">Yuan and Lin, 2006;</ref><ref type="bibr" target="#b12">Nelakanti et al., 2013</ref>). For each string h ∈ Y * , we have a group G h consisting of the in- dicator features (in f <ref type="bibr">(2)</ref> ) for all strings w ∈ W that have h as a proper prefix. <ref type="figure" target="#fig_1">Fig. 2</ref> gives a visual depic- tion. We now define R(θ) = h∈Y * ||θ G h || 2 . This penalty encourages each group of weights to remain all at zero (thereby conserving runtime, in our setting, because it means that h does not need to be added to H). Once a single weight in a group becomes nonzero, the "initial inertia" induced by the group lasso penalty is overcome, and other features in the group can be more cheaply adjusted away from zero.</p><p>Although eq. (1) is now convex, directly optimiz- ing it would be expensive for large k, since θ then contains very many parameters. We thus use a heuris- tic optimization algorithm, the active set method <ref type="bibr" target="#b14">(Schmidt, 2010)</ref>, which starts with a low-dimensional θ and incrementally adds features to the model. This also frees us from needing to specify a limit k. Rather, W grows until further extensions are unhelpful, and then implicitly k = max w∈W |w| − 1.</p><p>The method defines f (2) to include indicator fea- tures for all tag sequences w in an active set W active . Thus, θ (2) is always a vector of |W active | real numbers. Initially, we take W active = Y and θ = 0. At each active set iteration, we fully optimize eq. (1) to obtain a sparse θ and a set W = {w ∈ W active | θ (2) w = 0} of features that are known to be "useful." <ref type="bibr">5</ref> We then update W active to {wy | w ∈ W, y ∈ Y }, so that it includes single-tag extensions of these useful fea- tures; this expands θ to consider additional features that plausibly might prove useful. Finally, we com- plete the iteration by updating W active to its closure W active , simply because this further expansion of the feature set will not slow down our algorithms. When eq. <ref type="formula">(1)</ref> is re-optimized at the next iteration, some of these newly added features in W active may acquire nonzero weights and thus enter W, allowing further extensions. We can halt once W no longer changes.</p><p>As a final step, we follow common practice by running "debiasing" <ref type="figure" target="#fig_1">(Martins et al., 2011a)</ref>, where we fix our f (2) feature set to be given by the final W, and retrain θ without the group lasso penalty term.</p><p>In practice, we optimized eq. (1) using the online proximal gradient algorithm SPOM <ref type="bibr" target="#b11">(Martins et al., 2011b</ref>) and Adagrad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>) with η = 0.01 and 15 inner epochs. We limited to 3 active set iterations, and as a result, our final W contained at most tag trigrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our paper can be seen as transferring methods of <ref type="bibr" target="#b2">Cotterell and Eisner (2015)</ref> to the CRF setting. They too used tree-structured group lasso and active set to select variable-order n-gram features W for globally-normalized sequence models (in their case, to rapidly and accurately approximate beliefs during message-passing inference). Similarly, <ref type="bibr" target="#b12">Nelakanti et al. (2013)</ref> used tree-structured group lasso to regu- larize a variable-order language model (though their focus was training speed). Here we apply these tech- niques to conditional models for tagging.</p><p>Our work directly builds on the variable-order CRF of <ref type="bibr" target="#b3">Cuong et al. (2014)</ref>, with a speedup in Alg. 2, but our approach also learns the VoCRF structure. Our method is also related to the generative variable-order tagger of <ref type="bibr" target="#b15">Schütze and Singer (1994)</ref>.</p><p>Our static feature selection chooses a single model that permits fast exact marginal inference, similar to learning a low-treewidth graphical model ( <ref type="bibr" target="#b1">Bach and Jordan, 2001;</ref><ref type="bibr" target="#b6">Elidan and Gould, 2008</ref>). This con- trasts with recent papers that learn to do approximate 1-best inference using a sequence of models, whether by dynamic feature selection within a greedy infer- ence algorithm ( <ref type="bibr" target="#b17">Strubell et al., 2015)</ref>, or by gradually increasing the feature set of a 1-best global inference algorithm and pruning its hypothesis space after each increase <ref type="bibr" target="#b20">(Weiss and Taskar, 2010;</ref><ref type="bibr" target="#b7">He et al., 2013)</ref>. <ref type="bibr" target="#b14">Schmidt (2010)</ref> explores the use of group lasso penalties and the active set method for learning the structure of a graphical model, but does not consider learning repeated structures (in our setting, W defines a structure that is reused at each position). <ref type="bibr" target="#b16">Steinhardt and Liang (2015)</ref> jointly modeled the amount of context to use in a variable-order model that dynamically determines how much context to use in a beam search decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments 6</head><p>Data: We conduct experiments on multilingual POS tagging. The task is to label each word in a sen- tence with one of |Y | = 17 labels. We train on five typologically-diverse languages from the Universal Dependencies (UD) corpora (Petrov et al., 2012): Basque, Bulgarian, Hindi, Norwegian and Slovenian. For each language, we start with the original train / dev / test split in the UD dataset, then move random sentences from train into dev until the dev set has 3000 sentences. This ensures more stable hyperpa- rameter tuning. We use these new splits below.</p><p>Eval: We train models with (λ, γ) ∈ {10 −4 · m, 10 −3 ·m, 10 −2 ·m}×{0, 0.1·m, 0.2·m, . . . , m}, where m is the number of training sentences. To tag a dev or test sentence, we choose its most probable tag sequence. For each of several model sizes, Ta- ble 1 selects the model of that size that achieved the highest per-token tagging accuracy on the dev set, and reports that model's accuracy on the test set.</p><p>Features: Recall from §3 that our features include non-stationary zeroth-order features f (1) as well as the stationary features based on W. For f (1) (x, t, y t ) we consider the following language-agnostic proper- ties of (x, t):</p><p>• The identities of the tokens x t−3 , ..., x t+3 , and the token bigrams (x t+1 , x t ), (x t , x t−1 ),</p><formula xml:id="formula_5">k-CRF (|W| = 17 k+1 )</formula><p>VoCRF at different model sizes |W| (which is proportional to runtime) 0 (17) 1 <ref type="formula">(289)</ref>   <ref type="table">Table 1</ref>: Part-of-speech tagging with Universal Tags: This table shows test results on 5 languages at different target runtimes. Each row's best results are in boldface, where ties in accuracy are broken in favor of faster models. Superscript k indicates that the accuracy is significantly different from the k-CRF (paired permutation test, p &lt; 0.05) and this superscript is in blue/red if the accuracy is higher/lower than the k-CRF. In all cases, we find a VoCRF (underlined) that is about as accurate as the 2-CRF (i.e., not significantly less accurate) and far faster, since the 2-CRF has |W| = 4913. <ref type="figure">Fig. 1</ref> plots the Pareto frontiers.</p><p>(x t−1 , x t+1 ). We use special boundary symbols for tokens at positions beyond the start or end of the sentence.</p><p>• Prefixes and suffixes of x t , up to 4 characters long, that occur ≥ 5 times in the training data.</p><p>• Indicators for whether x t is all caps, is lowercase, or has a digit.</p><p>• Word shape of x t , which maps the token string into the following character classes (uppercase, lowercase, number) with punctuation unmod- ified (e.g., VoCRF-like ⇒ AaAAA-aaaa, $5,432.10 ⇒ $8,888.88). For efficiency, we hash these properties into 2 22 bins. The f (1) features are obtained by conjoining these bins with y t ( <ref type="bibr" target="#b19">Weinberger et al., 2009</ref>): e.g., there is a feature that returns 0 unless y t = NOUN, in which case it counts the number of bin 1234567's properties that (x, t) has. (The f (2) features are not hashed.)</p><p>Results: Our results are presented in <ref type="figure">Fig. 1</ref> and <ref type="table">Table 1</ref>. We highlight two key points: (i) Across all languages we learned a tagger about as accurate as a 2-CRF, but much faster. (ii) The size of the set W required is highly language-dependent. For many languages, learning a full k-CRF is wasteful; our method resolves this problem.</p><p>In each language, the fastest "good" VoCRF is rather faster than the fastest "good" k-CRF (where "good" means statistically indistinguishable from the 2-CRF). These two systems are underlined; the un- derlined VoCRF systems are smaller than the under- lined k-CRF systems (for the 5 languages respec- tively) by factors of 1.9, 6.4, 3.4, 1.9, and 2.9. In every language, we learn a VoCRF with |W| ≤ 850 that is not significantly worse than a 2-CRF with |W| = 17 3 = 4913.</p><p>We also notice an interesting language-dependent effect, whereby certain languages require a small number of tag strings in order to perform well. For example, Hindi has a competitive model that ignores the previous tag y t−1 unless it is in {NOUN, VERB, ADP, PROPN}: thus the stationary fea- tures are 17 unigrams plus 4 × 17 bigrams, for a total of |W| = 85. At the other extreme, the Slavic lan- guages Slovenian and Bulgarian seem to require more expressive models over the tag space, remembering as many as 98 useful left-context histories (unigrams and bigrams) for the current tag. An interesting direc- tion for future research would be to determine which morpho-syntactic properties of a language tend to increase the complexity of tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a structured sparsity approach for struc- ture learning in VoCRFs, which achieves the accu- racy of higher-order CRFs at a fraction of the runtime. Additionally, we derive an asymptotically faster al- gorithm for the gradients necessary to train a VoCRF than prior work. Our method provides an effective speed-accuracy tradeoff for POS tagging across five languages-confirming that significant speed-ups are possible with little-to-no loss in accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A visual depiction of the tree-structured group lasso penalty. Each node represents a tag string feature. The group indexed by a node's tag string is defined as the set of features that are proper descendants of the node. For example, the lavender box indicates the largest group G ε and the aubergine box indicates a smaller group G V. To avoid clutter, not all groups are marked.</figDesc></figure>

			<note place="foot" n="1"> The constructions given in this section assume that W does not contain ε nor any sequence having ## as a proper prefix.</note>

			<note place="foot" n="2"> Our DFA construction is essentially that of Cotterell and Eisner (2015, Appendix B.5). However, Appendix B of that paper also gives a construction that obtains an even smaller DFA by using failure arcs (Allauzen et al., 2003), which remove the requirement that W be closed under last-character substitution. This would yield a further speedup to our Alg. 1 (replacing it with the efficient backward algorithm in footnote 16 of that paper) and similarly to our Alg. 2 (by differentiating the new Alg. 1).</note>

			<note place="foot" n="5"> Each gradient computation in this inner optimization takes time O(|Wactive| n), which is especially fast at early iterations.</note>

			<note place="foot" n="6"> Code and data are available at the following URLs: http://github.com/timvieira/vocrf http://universaldependencies.org</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized algorithms for constructing statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Thin junction trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Penalized expectation propagation for graphical models over strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="932" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conditional random field with high-order dependencies for sequence labeling and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Viet Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="981" to="1009" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inside-outside and forward-backward algorithms are just backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 16 Workshop on Structured Prediction for NLP</title>
		<meeting>the EMNLP 16 Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning bounded treewidth Bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1455" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure compilation: trading structure for features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="592" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structured sparsity in structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1500" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online learning of structured predictors with multiple kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="507" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured penalties for log-linear language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Nelakanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<title level="m">Graphical Model Structure Learning with 1-Regularization</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of British Columbias</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging using a variable memory Markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="181" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reified context models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1043" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning dynamic feature selection for fast sequential prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured prediction cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="916" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional random fields with high-order features for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2196" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
