<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Brain-Like Computing and Machine Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Brain-Like Computing and Machine Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Brain-Like Computing and Machine Intelligence</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Multilingual Translation Laboratory</orgName>
								<orgName type="institution" key="instit1">MASTAR Project</orgName>
								<orgName type="institution" key="instit2">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Multilingual Translation Laboratory</orgName>
								<orgName type="institution" key="instit1">MASTAR Project</orgName>
								<orgName type="institution" key="instit2">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5 Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="189" to="195"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary. In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>'Language Model (LM) Growing' refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus.</p><p>There are various methods for adding n-grams selected by different criteria from a monolingual corpus ( <ref type="bibr" target="#b22">Ristad and Thomas, 1995;</ref><ref type="bibr" target="#b19">Niesler and Woodland, 1996;</ref><ref type="bibr" target="#b29">Siu and Ostendorf, 2000;</ref><ref type="bibr" target="#b28">Siivola et al., 2007)</ref>. However, all of these approach- es need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs <ref type="bibr" target="#b6">(Clarkson and Robinson, 1997;</ref><ref type="bibr" target="#b9">Iyer et al., 1997;</ref><ref type="bibr" target="#b3">Bellegarda, 2004;</ref><ref type="bibr" target="#b12">Koehn and Schroeder, *</ref> Part of this work was done as Rui <ref type="bibr">Wang visited in NICT. 2007</ref>). In addition, it is very difficult or even im- possible to collect an extra large corpus for some special domains such as the TED corpus ( <ref type="bibr" target="#b5">Cettolo et al., 2012</ref>) or for some rare languages. There- fore, to improve the performance of LMs, without assistance of extra corpus, is one of important re- search topics in SMT.</p><p>Recently, Continues Space Language Model (CSLM), especially Neural Network based Lan- guage Model (NNLM) ( <ref type="bibr" target="#b4">Bengio et al., 2003;</ref><ref type="bibr" target="#b26">Schwenk, 2007;</ref><ref type="bibr" target="#b17">Mikolov et al., 2010;</ref><ref type="bibr" target="#b15">Le et al., 2011)</ref>, is being actively used in SMT ( <ref type="bibr" target="#b23">Schwenk et al., 2006;</ref><ref type="bibr" target="#b30">Son et al., 2010;</ref><ref type="bibr" target="#b27">Schwenk, 2010;</ref><ref type="bibr" target="#b24">Schwenk et al., 2012;</ref><ref type="bibr" target="#b31">Son et al., 2012;</ref><ref type="bibr" target="#b18">Niehues and Waibel, 2012)</ref>. One of the main advantages of CSLM is that it can more accurately predic- t the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLM- s have not been widely used in the current SMT systems, due to their too high computational cost.</p><p>Vaswani and colleagues <ref type="bibr">(2013)</ref> propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempt- ing to implement neural network based LM or translation model for SMT <ref type="bibr" target="#b7">(Devlin et al., 2014;</ref><ref type="bibr" target="#b16">Liu et al., 2014;</ref><ref type="bibr" target="#b2">Auli et al., 2013)</ref>. However, the decoding speed using n-gram LM is still state-of- the-art one. Some approaches calculate the prob- abilities of the n-grams n-grams before decoding, and store them in the n-gram format ( <ref type="bibr" target="#b35">Wang et al., 2013a;</ref><ref type="bibr" target="#b0">Arsoy et al., 2013;</ref><ref type="bibr" target="#b1">Arsoy et al., 2014</ref>). The 'converted CSLM' can be directly used in SMT. Though more n-grams which are not in the train-ing corpus can be generated by using some of these 'converting' methods, these methods only consider the monolingual information, and do not take the bilingual information into account.</p><p>We observe that the translation output of a phrase-based SMT system is concatenation of phrases from the phrase table, whose probabilities can be calculated by CSLM. Based on this obser- vation, a novel neural network based bilingual LM growing method is proposed using the 'connecting phrases'. The remainder of this paper is organized as follows: In Section 2, we will review the exist- ing CSLM converting methods. The new neural network based bilingual LM growing method will be proposed in Section 3. In Section 4, the exper- iments will be conducted and the results will be analyzed. We will conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Existing CSLM Converting Methods</head><p>Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks <ref type="bibr" target="#b39">(Zhang and Zhao, 2013;</ref><ref type="bibr" target="#b11">Jia and Zhao, 2014;</ref><ref type="bibr" target="#b40">Zhang et al., 2012;</ref><ref type="bibr" target="#b38">Xu and Zhao, 2012;</ref><ref type="bibr" target="#b36">Wang et al., 2013b;</ref><ref type="bibr" target="#b10">Jia and Zhao, 2013;</ref><ref type="bibr" target="#b37">Wang et al., 2014)</ref>. Recently, CSLMs become popular because they can obtain more accurate probability estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Continues Space Language Model</head><p>A CSLM implemented in a multi-layer neural net- work contains four layers: the input layer projects (first layer) all words in the context h i onto the projection layer (second layer); the hidden layer (third layer) and the output layer (fourth layer) achieve the non-liner probability estimation and calculate the LM probability P (w i |h i ) for the giv- en context <ref type="bibr" target="#b26">(Schwenk, 2007)</ref>.</p><p>CSLM is able to calculate the probabilities of all words in the vocabulary of the corpus given the context. However, due to too high computational complexity, CSLM is mainly used to calculate the probabilities of a subset of the whole vocabulary <ref type="bibr" target="#b26">(Schwenk, 2007)</ref>. This subset is called a short- list, which consists of the most frequent words in the vocabulary. CSLM also calculates the sum of the probabilities of all words not included in the short-list by assigning a neuron with the help of BNLM. The probabilities of other words not in the short-list are obtained from an BNLM <ref type="bibr" target="#b26">(Schwenk, 2007;</ref><ref type="bibr" target="#b27">Schwenk, 2010;</ref><ref type="bibr" target="#b35">Wang et al., 2013a)</ref>.</p><p>Let w i and h i be the current word and history, respectively. CSLM with a BNLM calculates the probability P (w i |h i ) of w i given h i , as follows:</p><formula xml:id="formula_0">P (wi|hi) =    Pc(w i |h i ) ∑ w∈V 0 Pc(w|h i ) Ps(hi) if wi ∈ V0 P b (wi|hi)</formula><p>otherwise <ref type="formula">(1)</ref> where V 0 is the short-list, P c (·) is the probabil- ity calculated by CSLM, ∑ w∈V 0 P c (w|h i ) is the summary of probabilities of the neuron for all the words in the short-list, P b (·) is the probability cal- culated by the BNLM, and</p><formula xml:id="formula_1">Ps(hi) = ∑ v∈V 0 P b (v|hi).<label>(2)</label></formula><p>We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Existing Converting Methods</head><p>As baseline systems, our approach proposed in ( <ref type="bibr" target="#b35">Wang et al., 2013a</ref>) only re-writes the probabil- ities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the o- riginal one. The main difference between our pro- posed method in this paper and our previous ap- proach is that n-grams outside the corpus are gen- erated firstly and the probabilities using CSLM are calculated by using the same method as our previ- ous approach. That is, the proposed new method is the same as our previous one when no grown n-grams are generated.</p><p>The method developed by Arsoy and colleagues <ref type="bibr" target="#b0">(Arsoy et al., 2013;</ref><ref type="bibr" target="#b1">Arsoy et al., 2014</ref>) adds al- l the words in the short-list after the tail word of the i-grams to construct the (i+1)-grams. For ex- ample, if the i-gram is "I want", then the (i+1)- grams will be "I want *", where "*" stands for any word in the short list. Then the probabilities of the (i+1)-grams are calculated using (i+1)-CSLM. So a very large intermediate (i+1)-grams will have to be grown 1 , and then be pruned into smaller suitable size using an entropy-based LM pruning method modified from <ref type="bibr" target="#b33">(Stolcke, 1998</ref>). The (i+2)- grams are grown using (i+1)-grams, recursively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bilingual LM Growing</head><p>The translation output of a phrase-based SMT sys- tem can be regarded as a concatenation of phrases in the phrase table (except unknown words). This leads to the following procedure:</p><p>Step 1. All the n-grams included in the phrase table should be maintained at first.</p><p>Step 2. The connecting phrases are defined in the following way.</p><p>The w b a is a target language phrase starting from the a-th word ending with the b-th word, and βw b a γ is a phrase including w b a as a part of it, where β and γ represent any word sequence or none. An i-gram</p><formula xml:id="formula_2">phrase w k 1 w i k+1 (1 ≤ k ≤ i − 1) is a connecting phrase 2 , if :</formula><p>(1) w k 1 is the right (rear) part of one phrase βw k 1 in the phrase table, or (2) w i k+1 is the left (front) part of one phrase w i k+1 γ in the phrase table.</p><p>After the probabilities are calculated using C- SLM (Eqs.1 and 2), we combine the n-grams in the phrase table from Step 1 and the connecting phrases from Step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ranking the Connecting Phrases</head><p>Since the size of connecting phrases is too huge (usually more than one Terabyte), it is necessary to decide the usefulness of connecting phrases for SMT. The more useful connecting phrases can be selected, by ranking the appearing probabilities of the connecting phrases in SMT decoding.</p><p>Each line of a phrase table can be simplified (without considering other unrelated scores in the phrase table) as</p><formula xml:id="formula_3">f ||| e ||| P (e|f ),<label>(3)</label></formula><p>where the P (e|f ) means the translation probabili- ty from f (source phrase) to e(target phrase), which can be calculated using bilingual parallel training data. In decoding, the probability of a tar- get phrase e appearing in SMT should be</p><formula xml:id="formula_4">Pt(e) = ∑ f Ps(f ) × P (e|f ),<label>(4)</label></formula><p>where the P s (f ) means the appearing probability of a source phrase, which can be calculated using source language part in the bilingual training data. Using P t (e) 3 , we can select the connecting phrases e with high appearing probabilities as the n-grams to be added to the original n- grams. These n-grams are called 'grown n- grams'. Namely, we build all the connecting phrases at first, and then we use the appearing probabilities of the connecting phrases to decide which connecting phrases should be selected. For an i-gram connecting phrase w k 1 w i k+1 , where w k 1 is part of βw k 1 and w i k+1 is part of w i k+1 γ (the βw k 1 and w i k+1 γ are from the phrase table), the prob- ability of the connecting phrases can be roughly estimated as</p><formula xml:id="formula_5">Pcon(w k 1 w i k+1 ) = i−1 ∑ k=1 ( ∑ β Pt(βw k 1 )× ∑ γ Pt(w i k+1 γ)).<label>(5)</label></formula><p>A threshold for P con (w k 1 w i k+1 ) is set, and only the connecting phrases whose appearing probabil- ities are higher than the threshold will be selected as the grown n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Calculating the Probabilities of Grown</head><p>N -grams Using CSLM To our bilingual LM growing method, a 5-gram LM and n-gram (n=2,3,4,5) CSLMs are built by using the target language of the parallel corpus, and the phrase table is learned from the parallel corpus.</p><p>The probabilities of unigram in the original n- gram LM will be maintained as they are. The n-grams from the bilingual phrase table will be grown by using the 'connecting phrases' method. As the whole connecting phrases are too huge, we use the ranking method to select the more useful connecting phrases. The distribution of different n-grams (n=2,3,4,5) of the grown LMs are set as the same as the original LM.</p><p>The probabilities of the grown n-grams (n=2,3,4,5) are calculated using the 2,3,4,5- CSLM, respectively. If the tail (target) words of the grown n-grams are not in the short-list of C- SLM, the P b (·) in Eq. 1 will be applied to calcu- late their probabilities.</p><p>We combine the n-grams <ref type="figure" target="#fig_0">(n=1,2,3,4,5)</ref> togeth- er and re-normalize the probabilities and backof- f weights of the grown LM. Finally the original BNLM and the grown LM are interpolated. The entire process is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting up</head><p>The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied ( , togeth- er with GIZA++ ( <ref type="bibr" target="#b20">Och and Ney, 2003</ref>) for align- ment and MERT <ref type="bibr" target="#b21">(Och, 2003)</ref> for tuning on the de- velopment data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data.</p><p>We used the patent data for the Chinese to En- glish patent translation subtask from the NTCIR-9 patent translation task <ref type="bibr" target="#b8">(Goto et al., 2011</ref>). The par- allel training, development, and test data sets con- sist of 1 million (M), 2,000, and 2,000 sentences, respectively.</p><p>Using SRILM <ref type="bibr" target="#b33">(Stolcke, 2002;</ref><ref type="bibr" target="#b32">Stolcke et al., 2011</ref>), we trained a 5-gram LM with the interpo- lated Kneser-Ney smoothing method using the 1M English training sentences containing 42M words without cutoff. The 2,3,4,5-CSLMs were trained on the same 1M training sentences using CSLM toolkit <ref type="bibr" target="#b26">(Schwenk, 2007;</ref><ref type="bibr" target="#b27">Schwenk, 2010)</ref>. The set- tings for CSLMs were: input layer of the same dimension as vocabulary size (456K), projection layer of dimension 256 for each word, hidden lay- er of dimension 384 and output layer (short-list) of dimension 8192, which were recommended in the CSLM toolkit and ( <ref type="bibr" target="#b35">Wang et al., 2013a</ref>) 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The experiment results were divided into four groups: the original BNLMs (BN), the CSLM Re-ranking (RE), our previous converting (WA), the Arsoy's growing, and our growing methods. For our bilingual LM growing method, 5 bilingual grown LMs (BI-1 to 5) were conducted in increas- ing sizes. For the method of Arsoy, 5 grown LMs (AR-1 to 5) with similar size of BI-1 to 5 were also conducted, respectively.</p><p>For the CSLM re-ranking, we used CSLM to re-rank the 100-best lists of SMT. Our previous converted LM, Arsoy's grown LMs and bilingual grown LMs were interpolated with the original BNLMs, using default setting of SRILM 5 . To re- duce the randomness of MERT, we used two meth- ods for tuning the weights of different SMT fea- tures, and two BLEU scores are corresponding to these two methods. The BLEU-s indicated that the same weights of the BNLM (BN) features were used for all the SMT systems. The BLEU-i indi- cated that the MERT was run independently by three times and the average BLEU scores were taken.</p><p>We also performed the paired bootstrap re- sampling test <ref type="bibr" target="#b14">(Koehn, 2004)</ref>  <ref type="bibr">6</ref> . Two thousands samples were sampled for each significance test. The marks at the right of the BLEU score indicated whether the LMs were significantly better/worse than the Arsoy's grown LMs with the same IDs for SMT ("++/−−": significantly better/worse at α = 0.01, "+/−": α = 0.05, no mark: not signif- icantly better/worse at α = 0.05).</p><p>From the results shown in <ref type="table" target="#tab_0">Table 1</ref>, we can get the following observations:</p><p>(1) Nearly all the bilingual grown LMs outper- formed both BNLM and our previous converted LM on PPL and BLEU. As the size of grown LM- s is increased, the PPL always decreased and the BLEU scores trended to increase. These indicated that our proposed method can give better probabil- ity estimation for LM and better performance for SMT.</p><p>(2) In comparison with the grown LMs in Ar- 84K words as vocabulary, and 20K words as short-list. In this paper, we used the same setting as our previous work, which covers 92.89% of the frequency of words in the training cor- pus, for all the baselines and our method for fair comparison. <ref type="bibr">5</ref> In our previous work, we used the development data to tune the weights of interpolation. In this paper, we used the default 0.5 as the interpolation weights for fair comparison. <ref type="bibr">6</ref> We used the code available at http://www.ark.cs. cmu.edu/MT soy's method, our grown LMs obtained better P- PL and significantly better BLEU with the sim- ilar size. Furthermore, the improvement of PPL and BLEU of the existing methods became satu- rated much more quickly than ours did, as the LMs grew.</p><p>(3) The last column was the Average Length of the n-grams Hit (ALH) in SMT decoding for dif- ferent LMs using the following function</p><formula xml:id="formula_6">ALH = 5 ∑ i=1 Pi−gram × i,<label>(6)</label></formula><p>where the P i−gram means the ratio of the i-grams hit in SMT decoding. There were also positive correlations between ALH, PPL and BLEUs. The ALH of bilingual grown LM was longer than that of the Arsoy's grown LM of the similar size. In another word, less back-off was used for our pro- posed grown LMs in SMT decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on TED Corpus</head><p>The TED corpus is in special domain as discussed in the introduction, where large extra monolingual corpora are hard to find. In this subsection, we conducted the SMT experiments on TED corpora using our proposed LM growing method, to eval- uate whether our method was adaptable to some special domains. We mainly followed the baselines of the IWSLT 2014 evaluation campaign <ref type="bibr">7</ref> , only with a few mod- ifications such as the LM toolkits and n-gram or- der for constructing LMs. The Chinese (CN) to English (EN) language pair was chosen, using de- v2010 as development data and test2010 as evalu- ation data. The same LM growing method was ap- 7 https://wit3.fbk.eu/ plied on TED corpora as on NTCIR corpora. The results were shown in <ref type="table" target="#tab_1">Table 2</ref>.  <ref type="table" target="#tab_1">Table 2</ref> indicated that our proposed LM grow- ing method improved both PPL and BLEU in com- parison with both BNLM and our previous CSLM converting method, so it was suitable for domain adaptation, which is one of focuses of the current SMT research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a neural network based bilingual LM growing method by using the bilingual parallel corpus only for SMT. The results show that our proposed method can improve both LM and SMT performance, and outperforms the existing LM growing methods significantly with- out extra corpus. The connecting phrase-based method can also be applied to LM adaptation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NN based bilingual LM growing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Performance of the Grown LMs</head><label>1</label><figDesc></figDesc><table>LMs 
n-grams 
PPL BLEU-s BLEU-i ALH 
BN 
73.9M 108.8 32.19 
32.19 
3.03 
RE 
N/A 
97.5 32.34 
32.42 
N/A 
WA 
73.9M 104.4 32.60 
32.62 
3.03 
AR-1 
217.6M 103.3 32.55 
32.75 
3.14 
AR-2 
323.8M 103.1 32.61 
32.64 
3.18 
AR-3 
458.5M 103.0 32.39 
32.71 
3.20 
AR-4 
565.6M 102.8 32.67 
32.51 
3.21 
AR-5 
712.2M 102.5 32.49 
32.60 
3.22 
BI-1 
223.5M 101.9 32.81+ 
33.02+ 
3.20 
BI-2 
343.6M 101.0 32.92+ 
33.11++ 3.24 
BI-3 
464.5M 100.6 33.08++ 33.25++ 3.26 
BI-4 
571.0M 100.3 33.15++ 33.12++ 3.28 
BI-5 
705.5M 100.1 33.11++ 33.24++ 3.31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : CN-EN TED Experiments</head><label>2</label><figDesc></figDesc><table>LMs n-grams PPL BLEU-s 
BN 
7.8M 87.1 12.41 
WA 
7.8M 85.3 12.73 
BI-1 
23.1M 79.2 12.92 
BI-2 
49.7M 78.3 13.16 
BI-3 
73.4M 77.6 13.24 

</table></figure>

			<note place="foot" n="1"> In practice, the probabilities of all the target/tail words in the short list for the history i-grams can be calculated by the neurons in the output layer at the same time, which will save some time. According to our experiments, the time cost for Arsoy&apos;s growing method is around 4 times more than our proposed method, if the LMs which are 10 times larger than the original one are grown with other settings all the same.</note>

			<note place="foot" n="2"> We are aware that connecting phrases can be applied to not only two phrases, but also three or more. However the appearing probabilities (which will be discussed in Eq. 5 of next subsection) of connecting phrases are approximately estimated. To estimate and compare probabilities of longer phrases in different lengths will lead to serious bias, and the experiments also showed using more than two connecting phrases did not perform well (not shown for limited space), so only two connecting phrases are applied in this paper.</note>

			<note place="foot" n="3"> This Pt(e) hence provides more bilingual information, in comparison with using monolingual target LMs only.</note>

			<note place="foot" n="4"> Arsoy used around 55 M words as the corpus, including</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We appreciate the helpful discussion with Dr. Isao Goto and Zhongye Jia, and three anony-mous reviewers for valuable comments and sug-gestions on our paper. Rui Wang, Hai Zhao and Bao-Liang Lu were partially supported by the National Natural Science Foundation of <ref type="bibr">China (No. 60903119, No. 61170114, and No. 61272248)</ref> <ref type="bibr">201304490199 and 201304490171)</ref>, and the art and science interdis-cipline funds of Shanghai Jiao Tong University (A study on mobilization mechanism and alerting threshold setting for online community, and media image and psychology evaluation: a computation-al intelligence approach). The corresponding au-thor of this paper, according to the meaning given to this role by Shanghai Jiao Tong University, is Hai Zhao.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sethy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP-2013</title>
		<meeting>ICASSP-2013<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Converting neural network language models into back-off language models for efficient decoding in automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sethy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processings of EMNLP-2013</title>
		<meeting>essings of EMNLP-2013<address><addrLine>Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical language model adaptation: review and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jerome R Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptation Methods for Speech Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wit 3 : Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EAMT-2012</title>
		<meeting>EAMT-2012<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language model adaptation using mixtures and an exponentially decaying cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP-1997</title>
		<meeting>ICASSP-1997<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Munich</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="799" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL2014</title>
		<meeting>ACL2014<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the NTCIR-9 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-9 Workshop Meeting</title>
		<meeting>NTCIR-9 Workshop Meeting<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page" from="559" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using out-of-domain data to improve indomain language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukmini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="221" to="223" />
			<date type="published" when="1997" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kyss 1.0: a framework for automatic evaluation of chinese input method engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1195" to="1201" />
		</imprint>
	</monogr>
	<note>Proceedings of IJCNLP-2013</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A joint graph model for pinyin-to-chinese conversion with typo correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2014</title>
		<meeting>ACL-2014<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experiments in domain adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2007 Workshop on Statistical Machine Translation</title>
		<meeting>ACL-2007 Workshop on Statistical Machine Translation<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2007</title>
		<meeting>ACL-2007<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2004</title>
		<meeting>EMNLP-2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured output layer neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP-2011</title>
		<meeting>ICASSP-2011<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="5524" to="5527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2014</title>
		<meeting>ACL-2014<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH-2010</title>
		<meeting>INTERSPEECH-2010</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous space language models using restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWSLT-2012</title>
		<meeting>IWSLT-2012<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A variablelength category-based n-gram language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Niesler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><forename type="middle">Woodland</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP-1996</title>
		<meeting>ICASSP-1996</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="164" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-2003</title>
		<meeting>ACL-2003<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">New techniques for context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sven Ristad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">G</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Massachusetts. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-1995</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continuous space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dchelotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING ACL-2006</title>
		<meeting>COLING ACL-2006<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large, pruned or continuous space language models on a gpu for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012</title>
		<meeting>the NAACL-HLT 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, WLM &apos;12</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continuous-space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On growing and pruning kneser-ney smoothed n-gram models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vesa</forename><surname>Siivola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teemu</forename><surname>Hirsimki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Virpioja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1617" to="1624" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Variable ngrams and extensions for conversational speech language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manhung</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training continuous space language models: some practical issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hai Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">October. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP-2010</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Continuous space translation models with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hai Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT2012</title>
		<meeting>NAACL HLT2012<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SRILM at sixteen: Update and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH 2011</title>
		<meeting>INTERSPEECH 2011<address><addrLine>Waikoloa, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Entropy-based pruning of backoff language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of DARPA Broadcast News Transcription and Understanding Workshop</title>
		<meeting>DARPA Broadcast News Transcription and Understanding Workshop<address><addrLine>Lansdowne, VA, USA. Andreas Stolcke; Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
	<note>Proceedings of INTERSPEECH-2002</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2013</title>
		<meeting>EMNLP-2013<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Converting continuous-space language models into n-gram language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-2013</title>
		<meeting>EMNLP-2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="845" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Labeled alignment for recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Federation of Natural Language Processing</title>
		<meeting><address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
	<note>Proceedings of IJCNLP-2013</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parallelized extreme learning machine ensemble based on minmax modular network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoliang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="31" to="41" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using deep linguistic features for finding deceptive opinion spam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING-2012</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving function word alignment with frequency and syntactic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-2013</title>
		<meeting>IJCAI-2013</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2211" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A machine learning approach to convert CCGbank to Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING2012</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An empirical study on word segmentation for chinese machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoliang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<editor>Alexander Gelbukh</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">7817</biblScope>
			<biblScope unit="page" from="248" to="263" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
