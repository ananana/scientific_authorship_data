<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bi-directional Attention with Agreement for Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bi-directional Attention with Agreement for Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2204" to="2214"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, several neural network models have been developed for efficiently accessing long-term mem- ory and discovering dependencies in sequential data. The memory network framework has been studied in the context of question answering and language modeling ( <ref type="bibr" target="#b36">Sukhbaatar et al., 2015)</ref>, whereas the neural attention model under the encoder-decoder framework has been applied to machine translation ( <ref type="bibr" target="#b4">Bahdanau et al., 2015</ref>) and constituency parsing ( <ref type="bibr" target="#b40">Vinyals et al., 2015b</ref>). Both frameworks learn the latent alignment between the source and target sequences, and the mechanism of attention over the encoder can be viewed as a soft operation on the memory. Although already used in the encoder for capturing global context informa- tion ( <ref type="bibr" target="#b4">Bahdanau et al., 2015)</ref>, the bi-directional recur- rent neural network (RNN) has yet to be employed in the decoder. Bi-directional decoding is expected to be advantageous over the previously developed uni-directional counterpart, because the former ex- ploits richer contextual information. Intuitively, we can use two separate uni-directional RNNs where each one constructs its respective attended encoder context vectors for computing RNN hidden states. However, the drawback of this approach is that the decoder would often produce different alignments resulting in discrepancies for the forward and back- ward directions. In this paper, we design a training objective function to enforce attention agreement between both directions, inspired by the alignment- by-agreement idea from <ref type="bibr" target="#b22">Liang et al. (2006)</ref>. Specif- ically, we develop a dependency parser (BiAtt-DP) using a bi-directional attention model based on the memory network. Given that the golden alignment is observed for dependency parsing in the training stage, we further derive a simple and interpretable approximation for the agreement objective, which makes a natural connection between the latent and observed alignment cases.</p><p>The proposed BiAtt-DP parses a sentence in a linear order via sequentially querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all pos- sible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MST- Parser ( <ref type="bibr" target="#b28">McDonald et al., 2005</ref>). The consideration of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural de- pendency parsers <ref type="bibr" target="#b10">(Chen and Manning, 2014;</ref>, which use a transition- based algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history <ref type="bibr" target="#b27">(McDonald and Pereira, 2006</ref>), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n 2 ) for a sentence with n words. This is achieved by feed- ing the RNN in the query component with a soft headword embedding, which is computed as the probability-weighted sum of all headword embed- dings in the memory component.</p><p>To the best of our knowledge, this is the first at- tempt to apply memory network models to graph- based dependency parsing. Moreover, it is the first extension of neural attention models from uni- direction to multi-direction by enforcing agreement on alignments. Experiments on English, Chinese, and 12 languages from the CoNLL 2006 shared task show the BiAtt-DP can achieve competitive parsing accuracy with several state-of-the-art parsers. Fur- thermore, our model achieves the highest unlabeled attachment score (UAS) on Chinese, Czech, Dutch, German, Spanish and Turkish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A MemNet-based Dependency Parser</head><p>The proposed parser first encodes each word in a sentence by continuous embeddings using a bi- directional RNN, and then performs two types of operations, i.e. 1) headword predictions based on bi- directional parsing history and 2) the relation pre- diction conditioned on the current modifier and its predicted headword both in the embedding space. In the following, we first present how the token em- beddings are constructed. Then, the key components of the proposed parser, i.e. the memory component and the query component, are discussed in detail. Lastly, we describe the parsing algorithm using a bi- directional attention model with agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Token Embeddings</head><p>In the proposed BiAtt-DP, the memory and query components share the same token embeddings. We use the notion of additive token embedding as in <ref type="bibr" target="#b8">(Botha and Blunsom, 2014</ref>) to utilize the available information about the token, e.g., its word form, lemma, part-of-speech (POS) tag, and morpholog- ical features. Specifically, the token embedding is computed as</p><formula xml:id="formula_0">E form e form i + E pos e pos i + E lemma e lemma i + · · · ,</formula><p>where e i 's are one-hot encoding vectors for the i- th word, and E's are parameters to be learned that store the continuous embeddings for corresponding feature. Note those one-hot encoding vectors have different dimensions, depending on individual vo- cabulary sizes, and all E's have the same first di- mension but different second dimension. The addi- tive token embeddings allow us to easily integrate a variety of information. Moreover, we only need to make a single decision on the dimensionality of the token embedding, rather than a combination of deci- sions on word embeddings and POS tag embeddings as in concatenated token embeddings used by <ref type="bibr" target="#b10">Chen and Manning (2014)</ref>,  and . It reduces the number of model param- eters to be tuned, especially when lots of different features are used. In our experiments, the word form and fine-grained POS tag are always used, whereas other features are used depending on their availabil- ity in the dataset. All singleton words, lemmas, and POS tags are replaced by special tokens.</p><p>The additive token embeddings are transformed into another space before they are used by the mem- ory and query components, i.e.</p><formula xml:id="formula_1">x i = LReL P E form e form i + · · · ,</formula><p>where P is the projection matrix and is shared by the memory and query components as well. The ac- tivation function of this projection layer is the leaky rectified linear (LReL) function ( <ref type="bibr" target="#b26">Mass et al., 2013</ref>) with 0.1 as the slope of the negative part. In the re- maining part of the paper, we refer to x i ∈ R p as the token embedding for word at position i. Note the subscript i is substituted by j and t for the memory and query components, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Components</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the proposed BiAtt-DP has three components, i.e. a memory component, a left- to-right query component, and a right-to-left query component. Given a sentence of length n, the parser first uses a bi-directional RNN to construct n + 1 headword embeddings, m 0 , m 1 , . . . , m n ∈ R e , with m 0 reserved for the ROOT symbol. Each query component is an uni-directional attention model. In a query component, a sequence of n modifier em- beddings q 1 , . . . , q n ∈ R d are constructed recur- sively by conditioning on all headword embeddings. To address the vanishing gradient issue in RNNs, we use the gated recurrent unit (GRU) proposed by <ref type="bibr" target="#b11">Cho et al. (2014)</ref>, where an update gate and a reset gate are employed to control the information flow. We re- place the hyperbolic tangent function in GRU with the LReL function, which is faster to compute and achieves better parsing accuracy in our preliminary studies. In the following, we refer to headword and modifier embeddings as memory and query vectors, respectively.</p><p>Memory Component: The proposed BiAtt-DP uses a bi-directional RNN to obtain the memory vec- tors. At time step j, the current hidden state vec- tor h l j ∈ R e/2 (or h r j ∈ R e/2 ) is computed as a non-linear transformation based on the current in- put vector x j and the previous hidden state vec- tor</p><formula xml:id="formula_2">h l j−1 (or h r j+1 ), i.e. h l j = GRU(h l j−1 , x j ) (or h r j = GRU(h r j+1 , x j )).</formula><p>Ideally, the recursive nature of the RNN allows it to capture all context infor- mation from one-side, and a bi-directional RNN can thus capture context information from both sides. We concatenate the hidden layers of the left-to-right RNN and the right-to-left RNN for the word at posi- tion j as the memory vector m j = h l j ; h r j . These memory vectors are expected to encode the words and their context information in the headword space.</p><p>Query Component: For each query component, we use a single-directional RNN with GRU to obtain the query vectors q j 's, which are the hidden state vectors of the RNN. Each q t is used to query the memory component, returning association scores s t,j 's between the word at position t and the head- word at position j for j ∈ {0, · · · , n}, i.e.</p><formula xml:id="formula_3">s t,j = v T φ (Cm j + Dq t ) ,<label>(1)</label></formula><p>where φ(·) is the element-wise hyperbolic tangent function, and C ∈ R h×e , D ∈ R h×d and v ∈ R h are model parameters. Then, we can obtain proba- bilities (aka attention weights), a t,0 , · · · , a t,n , over all headwords in the sentence by normalizing s t,j 's, using a softmax function</p><formula xml:id="formula_4">a t = softmax(s t ).<label>(2)</label></formula><p>The soft headword embedding is then defined as˜m as˜ as˜m t = n j=1 a t,j m j . At each time step t, the RNN takes the soft headword embedding˜membedding˜ embedding˜m l t−1 or˜m or˜ or˜m r t+1 as the input, in addition to the token embed- ding x t . Formally, for the forward case, the q t can be computed as q t = GRU (q t−1 , [ ˜ m t ; x t ]). Al- though the RNN is able to capture long-span con- text information to some extent, the local context may very easily dominate the hidden state. There- fore, this additional soft headword embedding al- lows the model to access long-span context infor- mation in a different channel. On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history infor- mation, which can potentially improve the parsing accuracy ( <ref type="bibr" target="#b43">Yamada and Matsumoto, 2003;</ref><ref type="bibr" target="#b27">McDonald and Pereira, 2006</ref>). However, for a graph-based dependency parser, utilizing parsing history features is computationally expensive. For example, an k-th order MSTParser ( <ref type="bibr" target="#b27">McDonald and Pereira, 2006</ref>) has O(n k+1 ) complexity for a sentence of n words. In contrast, the BiAtt-DP implicitly captures high-order parsing history while keeping the complexity in the order of O(n 2 ), i.e. for each direction. we compute n(n+1) pair-wise probabilities a t,j for t = 1, · · · , n and j = 0, · · · , n.</p><p>In this paper, we choose to use soft headword em- beddings rather than making hard decisions on head- words. In the latter case, beam search may poten- tially improve the parsing accuracy at the cost of higher computational complexity, i.e. O(Bn 2 ) with a beam width of B. When using soft headword em- beddings, there is no need to perform beam search. Moreover, it is straightforward to incorporate pars- ing history from both directions by using two query components at the cost of O(2n 2 ), which cannot be easily achieved when using beam search. The pars- ing decision can be made directly based on atten- tion weights from the two query components or fur- ther rescored by the maximum spanning tree (MST) search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parsing by Attention with Agreement</head><p>For the bi-directional attention model, the underly- ing probability distributions a l t and a r t may not agree with each other. In order to encourage the agree- ment, we use the mathematically convenient metric, i.e. the squared Hellinger distance H 2 a l t ||a r where indicates element-wise multiplication. The resulting loss function is equivalent to the cross- entropy loss, which is widely adopted for training neural networks.</p><p>As we can see, the loss function (3) tries to min- imize the distance between the golden alignment and the intersection of the two directional attention alignments at every time step. Therefore, during inference, the headword prediction for the word at time step t can be obtained as argmax j log a l t,j + log a r t,j , seeking for agreement between both query compo- nents. This parsing procedure is also similar to the exhaustive left-to-right modifier-first search al- gorithm described in <ref type="bibr" target="#b13">(Covington, 2001</ref>), but it is en- hanced by an additional right-to-left search with the agreement enforcement. Alternatively, we can treat (log a l t,j + log a r t,j ) as a score of the corresponding arc and then search for the MST to form a depen- dency parse tree, as proposed in <ref type="bibr" target="#b28">(McDonald et al., 2005</ref>). The MST search is achieved via the Chu- Liu-Edmonds algorithm ( <ref type="bibr" target="#b12">Chu and Liu, 1965;</ref><ref type="bibr" target="#b16">Edmonds, 1967</ref>), which can be implemented in O(n 2 ) for dense graphs according to <ref type="bibr" target="#b37">Tarjan (1977)</ref>. In prac- tice, the MST search slows down the parsing speed by 6-10%. However, it forces the parser to produce a valid tree, and we observe a slight improvement on parsing accuracy in most cases.</p><p>After obtaining each modifier and its soft header embeddings, we use a single-layer perceptron to pre- dict the head-modifier relation, i.e.</p><formula xml:id="formula_5">y t = softmax U ˜ m l t ; ˜ m r t + W q l t ; q r t ,<label>(4)</label></formula><p>where y t,1 , · · · , y t,m are the probabilities of m pos- sible relations, and U ∈ R m×2e and W ∈ R m×2d are model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Learning</head><p>For the t-th word (modifier) w t in a sentence of length n, let H l t and H r t denote random variables representing the predicted headword from forward (left-to-right) and backward (right-to-left) parsing directions, respectively. Also let R t denote the ran- dom variable representing the dependency relation for w t . The joint probability of headword and rela- tion predictions can be written as</p><formula xml:id="formula_6">P (R 1:n , H l 1:n , H r 1:n |w 1:n ) = n t=1 P (R t |w 1:n )P (H l t |w 1:n )P (H r t |w 1:n ) = n t=1 y l t,Rt · a l t,H l t · a r t,H r t<label>(5)</label></formula><p>where at each time step we assume head-modifier relations and headwords from both directions are independent with each other when conditioned on the global knowledge of the whole sentence. Note that the long-span context and high-order parsing history information are injected when we model P (H l t |w 1:n ), P (H r t |w 1:n ) and P (R t |w 1:n ), as dis- cussed in Section 2.2.</p><p>As discussed in Section 2.3, the model can be trained by encouraging attention agreement between two query components. From (5), we observe that it is equivalent to maximizing the log-likelihood of the golden dependency tree (or minimizing the cross- entropy) for each training sentence, i.e. n t=1 log y t,relationt + log a l t,headt + log a r t,headt , where a t,j and y t,r are defined in (2) and (4), re- spectively, and relation t and head t are golden relation and headword labels, respectively. The gra- dients are computed via the back-propagation algo- rithm ( <ref type="bibr" target="#b34">Rumelhart et al., 1986)</ref>. Errors of y t come from the arc labels, whereas there are two source of errors for a t , one from the headword labels and the other back-propagated from errors of y t . We use stochastic gradient descent with the Adam al- gorithm proposed in ( <ref type="bibr" target="#b18">Kingma and Ba, 2015</ref>). The learning rate is halved at each iteration once the log- likelihood of the dev set decreases. The whole train- ing procedure terminates when the log-likelihood decreases for the second time. All learning param- eters except bias terms are initialized randomly ac- cording to the Gaussian distribution N (0, 10 −2 ). In our experiments, we tune the initial learning rate with a step size of 0.0002, and choose the best one based on the log-likelihood on the dev set at the first epoch. Empirically, the selected initial learning rates fall in the range of [0.0004, 0.0010] for hidden layer size <ref type="bibr">[128,</ref><ref type="bibr">320]</ref>, and tend to be larger when using a smaller hidden layer size, i.e. [0.0016, 0.0034] for hidden layer size around 80. The training data are randomly shuffled at every epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the parsing accuracy of the proposed BiAtt-DP on 14 languages. We re- port both UAS and labeled attachment score (LAS), obtained by the CoNLL-X eval.pl script 2 which ig- nores punctuation symbols. The headword pre- dictions are made through the MST search, which slightly improves both UAS and LAS (less than 0.3% absolutely). Overall, the proposed BiAtt-DP achieves competitive parsing accuracy on all lan- guages as state-of-the-art parsers, and obtains better UAS in 6 languages. We also show the impact of using POS tags and pre-trained word embeddings. Moreover, different variants of the full model are compared in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We work on the English Treebank-3 (PTB) dataset <ref type="bibr" target="#b23">(Marcus et al., 1999</ref> whereas for Chinese, we use gold segmentation and POS tags. When constructing the token embeddings for English and Chinese, both the word form and the POS tag are used. We also initialize E form by pre- trained word embeddings <ref type="bibr">3</ref> .</p><p>For the 12 other languages, we randomly hold out 5% of the training data as the dev set. In addition to the word form and find-grained POS tags, we use extra features such as lemmas, coarse-grained POS tags, and morphemes when they are available in the dataset. No pre-trained word embeddings are used for these 12 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Configurations</head><p>The hidden layer size is kept the same across all RNNs in the proposed BiAtt-DP. We also require the dimension of the token embeddings to be the same as the hidden layer size. Note that we concatenate the hidden layers of two RNNs for constructing m j , and thus we have e = 2d. The weight matrices C and D respectively project vectors m j and q t to the same dimension h, which is equivalent to d. For English and Chinese, since the dimension of pre- trained word embeddings are 300, we use 300 × h as the dimension of embedding parameters E's. For the 12 other languages, we use square matrices for the embedding parameters E's. For all languages, We tune the hidden layer size and choose one ac- cording to UAS on the dev set. The selected hidden layer sizes for these languages are: 368 (English), 114 (Chinese), 128 (Arabic), 160 (Bulgarian), 224 (Czech), 176 (Danish), 220 (Dutch), 200 (German), 128 (Japanese), 168 (Portuguese), 128 (Slovene), 144 (Spanish), 176 (Swedish), and 128 (Turkish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We first compare our parser with state-of-the-art neural transition-based dependency parsers on PTB and CTB. For English, we also compare with state- of-the-art graph-based dependency parsers. The re- sults are shown in <ref type="table" target="#tab_2">Table 1 and Table 2</ref>, respectively. It can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB. Compared with <ref type="bibr">3</ref> For English, we use the dependency-based word embed- dings at https://goo.gl/tWke3I ( <ref type="bibr" target="#b21">Levy and Goldberg, 2014</ref>). For Chinese, we pre-train 192-dimension skip-gram em- beddings ( <ref type="bibr" target="#b29">Mikolov et al., 2013</ref>) on Chinese Gigawords ( <ref type="bibr" target="#b17">Graff et al., 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Method UAS LAS  the transition-based parsers, it achieves better accu- racy than <ref type="bibr" target="#b10">Chen and Manning (2014)</ref>, which uses a feed-forward neural network, and , which uses three stack LSTM networks. Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to . On CTB, it achieves best UAS and similar LAS. This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length, which biases the joint objective to be more sensitive to UAS. The parsing speed is around 50-60 sents/sec measured on a desktop with Intel Core i7 CPU @ 3.33GHz using single thread. Next, in <ref type="table">Table 3</ref> we show the parsing accuracy of the proposed BiAtt-DP on 12 languages in the CoNLL 2006 shared task, including comparison with state-of-the-art parsers. Specifically, we show UAS of the 3rd-order RBGParser as reported in ( ) since it also uses low-dimensional continuous embeddings. However, there are sev- eral major differences between the RBGParser and the BiAtt-DP. First, in ( ), the low- dimensional continuous embeddings are derived    Third, the RBGParser employs a third-order parsing algorithm based on ( ), although it also implements a first-order parsing algorithm, which achieves lower UAS in general. In <ref type="table">Table 3</ref>, we show that the proposed BiAtt-DP outperforms the RBGParser in most languages except Japanese, Slovene, and Swedish.</p><p>It can be observed from <ref type="table">Table 3</ref> that the BiAtt- DP has highly competitive parsing accuracy as state- of-the-art parsers. Moreover, it achieves best UAS for 5 out of 12 languages. For the remaining seven languages, the UAS gaps between the BiAtt-DP and state-of-the-art parsers are within 1.0%, except Swedish. An arguably fair comparison for the BiAtt- DP is the MSTParser ( <ref type="bibr" target="#b27">McDonald and Pereira, 2006</ref>), since the BiAtt-DP replaces the scoring function for arcs but uses exactly the same search algorithm. Due to the space limit, we refer readers to ( ) for results of the MSTParsers (also shown in Appendix B). The BiAtt-DP consistently outper- forms both parser by up to 5% absolute UAS score.</p><p>Finally, following <ref type="bibr" target="#b33">(Pitler and McDonald, 2015)</ref>, we also analyze the performance of the BiAtt-DP on both crossed and uncrossed arcs. Since the BiAtt- DP uses a graph-based non-projective parsing algo- rithm, it is interesting to evaluate the performance on crossed arcs, which result in the non-projectivity of the dependency tree. The last three columns of <ref type="table">Table 3</ref> show the recall of crossed arcs, that of un- crossed arcs, and the percentage of crossed arcs in the test set. <ref type="bibr" target="#b33">Pitler and McDonald (2015)</ref> reported numbers on the same data for Dutch, German, Por- tuguese, and Slovene as in this paper. For these four languages, the BiAtt-DP achieves better UAS than that reported in <ref type="bibr" target="#b33">(Pitler and McDonald, 2015)</ref>. More importantly, we observe that the improvement on re- call of crossed arcs (around 10-18% absolutely) is much more significant than that of uncrossed arcs (around 1-3% absolutely), which indicates the ef- fectiveness of the BiAtt-DP in parsing languages with non-projective trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablative Study</head><p>Here we try to study the impact of using pre-trained word embeddings, POS tags, as well as the bi- directional query components on our model. First of all, we start from our best model (Model 1 in  The results are summarized in <ref type="table" target="#tab_5">Table 4</ref>. Compar- ing Models 1-3, it can be observed that without us- ing pre-trained word embeddings, both UAS and LAS drop by 0.6%, and without using POS tags in token embeddings, the numbers further drop by 1.6% in UAS and around 2.6% in LAS. In terms of query components, using single query compo- nent (Models 4-5) degrades UAS by 0.7-0.9% and LAS by around 1.0%, compared with Model 2. For Model 6, the soft headword embedding is only used for arc label predictions but not fed into the next hid- den state, which is around 0.3% worse than Model 2. This supports the hypothesis about the usefulness of the parsing history information. We also implement a variant of Model 6 which produces one a t instead two by using both q l t and q r t in (1). It gets 92.44% UAS and 89.26% LAS, indicating that naively ap- plying a bi-directional RNN may not be enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural Dependency Parsing: Recently de- veloped neural dependency parsers are mostly transition-based models, which read words sequen- tially from a buffer into a stack and incrementally build a parse tree by predicting a sequence of transitions ( <ref type="bibr" target="#b43">Yamada and Matsumoto, 2003;</ref><ref type="bibr" target="#b30">Nivre, 2003;</ref><ref type="bibr" target="#b31">Nivre, 2004)</ref>. A feed-forward neural network is used in <ref type="bibr" target="#b10">(Chen and Manning, 2014</ref>), where they represent the current state with 18 selected elements such as the top words on the stack and buffer. Each element is encoded by concatenated embeddings of words, POS tags, and arc labels. Their dependency parser achieves improvement on both accuracy and parsing speed.  improve the parser using semi-supervised structured learning and unlabeled data. The model is extended to integrate parsing and tagging in ). On the other hand,  develop the stack LSTM architecture, which uses three LSTMs to respectively model the sequences of buffer states, stack states, and actions. Unlike the transition-based formulation, the proposed BiAtt-DP directly predicts the headword and the dependency relation at each time step. Specifically, there is no explicit representation of actions or headwords in our model. The model learns to retrieve the most relevant information from the input memory to make decisions on headwords and head-modifier relations.</p><p>Graph-based Dependency Parsing: In addition to the transition-based parsers, another line of re- search in dependency parsing uses graph-based models. Graph-based parser usually build a de- pendency tree from a directed graph and learns to scoring the possible arcs. Due to this nature, non- projective parsing can be done straightforwardly by most graph-based dependency parsers. The MST- Parser ( <ref type="bibr" target="#b28">McDonald et al., 2005</ref>) and the TurboParser ( <ref type="bibr" target="#b24">Martins et al., 2010</ref>) are two examples of graph- based parsers. The MSTParser formulates the pars- ing as searching for the MST, whereas the Tur- boParser performs approximate variational infer- ence over a factor graph. The RBGParser pro- posed in ( ) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MST- Parser/TurboParser. It also employs a sampler-based algorithm for parsing ( ).</p><p>Neural Attention Model: The proposed BiAtt- DP is closely related to the memory network ( <ref type="bibr" target="#b36">Sukhbaatar et al., 2015</ref>) for question answering, as well as the neural attention models for machine translation ( <ref type="bibr" target="#b4">Bahdanau et al., 2015</ref>) and constituency parsing ( <ref type="bibr" target="#b40">Vinyals et al., 2015b</ref>). The way we query the memory component and obtain the soft head- word embeddings is essentially the attention mech- anism. However, different from the above studies where the alignment information is latent, in de- pendency parsing, the arc between the modifier and headword is known during training. Thus, we can utilize these labels for attention weights. The similar idea is employed by the pointer network in ( <ref type="bibr" target="#b39">Vinyals et al., 2015a)</ref>, which is used to solve three different combinatorial optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we develop a bi-directional attention model by encouraging agreement between the la- tent attention alignments. Through a simple and in- terpretable approximation, we make the connection between latent and observed alignments for train- ing the model. We apply the bi-directional attention model incorporating the agreement objective during training to the proposed memory-network-based de- pendency parser. The resulting parser is able to im- plicitly capture the high-order parsing history with- out suffering from issue of high computational com- plexity for graph-based dependency parsing.</p><p>We have carried out empirical studies over 14 languages. The parsing accuracy of the proposed model is highly competitive with state-of-the-art de- pendency parsers. For English, the proposed BiAtt- DP outperforms all graph-based parsers. It also achieves state-of-the-art performance in 6 languages in terms of UAS, demonstrating the effectiveness of the proposed mechanism of bi-directional attention with agreement and its use in dependency parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of the BiAtt-DP. The figure only illustrates the parsing process at the time step for has. Blue and yellow circles are memory and query vectors, respectively. Red and purple circles represent headword probabilities predicted from corresponding query components. Green circles represent soft headword embeddings. Black arrowed lines are connections carrying weight matrices. ⊗ and ⊕ indicate element-wise multiplication and addition, respectively. For simplicity, we ignore the token embedding xt connected to the RNN hidden layers mj, q l t and q r t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Language</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 :</head><label>3</label><figDesc>UAS on 12 languages in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). We also report corresponding LAS in squared brackets. The results of the 3rd-order RBGParser are reported in (Lei et al., 2014). Best published results on the same dataset in terms of UAS among (Pitler and McDonald, 2015), (Zhang and McDonald, 2014), (Zhang et al., 2013), (Zhang and McDonald, 2012), (Rush and Petrov, 2012), (Martins et al., 2013), (Martins et al., 2010), and (Koo et al., 2010). To study the effectiveness of the parser in dealing with non-projectivity, we follow (Pitler and McDonald, 2015), to compute the recall of crossed and uncrossed arcs in the gold tree, as well as the percentage of crossed arcs. from low-rank tensors. Second, the RBGParser uses combined scoring of arcs by including tradi- tional features from the MSTParser (McDonald and Pereira, 2006) / TurboParser (Martins et al., 2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Parsing accuracy on CTB dev and test sets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 )</head><label>4</label><figDesc>on English, which uses 300 as the token embedding dimension and 368 as the hidden layer size. We keep those model parameter dimensions unchanged and analyze different factors by compar- ing the parsing accuracy on PTB dev set.</figDesc><table>No. INIT POS L2R R2L UAS 

LAS 
1 




93.99 91.32 
2 



93.36 90.42 
3 


91.87 87.85 
4 


92.64 89.66 
5 


92.47 89.47 
6 

 † 
 † 93.03 90.06 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parsing accuracy on PTB dev set for different variants 

of the full model. INIT refers to using pre-trained word em-

bddings to initialize E form . POS refers to using POS tags in 

token embeddings. L2R and R2L respectively indicate whether 

to use the left-to-right and right-to-left query components.  † 

means the query component drops soft headword embeddings 

when constructing RNN hidden states. 

</table></figure>

			<note place="foot" n="1"> Our software and models are available at https:// github.com/hao-cheng/biattdp.</note>

			<note place="foot">t , for quantifying the distance between these two distributions. For dependency parsing, when the golden alignment is known during training, we can derive an upper bound on the latent agreement objective as H 2 (a l t , a r t ) ≤ 2 D(g t ||a l t ) + D(g t ||a r t ), where D(·||·) is the KL-divergence. The complete derivation is provided in the Appendix A. During optimization, we can safely drop the constant scaler and the square root operation in the upper bound, leading to the following loss function D(g t ||a l t ) + D(g t ||a r t ) = 2D(g t ||a l t a r t ), (3)</note>

			<note place="foot" n="2"> http://ilk.uvt.nl/conll/software.html</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Upper Bound on H 2 (p, q)</head><p>Here, we use the following definition of squared Hellinger distance for countable space</p><p>where p, q ∈ ∆ k are two k-simplexes. Introducing g ∈ ∆ k , the squared Hellinger distance can be upper bounded as</p><p>where (6), <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref> follow the inequalities be- tween the 1 -norm and the 2 -norm, the triangle inequality defined for a metric, and the Cauchy- Schwarz's inequality, respectively. Using the rela- tionship between the KL-divergence and the squared Hellinger distance, (8) can be further bounded by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B UAS Scores of MSTParsers</head><p>Language 1st-order 2nd-order Arabic</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">We use the numbers reported in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAS scores of 1st-order and 2-nd order MSTParsers on 12 languages in the CoNLL 2006 shared task (Buchholz and Marsi</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Numbers in brackets indicate the absolute improvement of the proposed BiAtt-DP over the MSTParsers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing and tagging with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1354" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Very high accurarcy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computational Linguistics (COLING)</title>
		<meeting>Int. Conf. Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computational Natural Language Learning (CoNLL)</title>
		<meeting>Conf. Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahadanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethhi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the shortest arborescene of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fundamental algorithm for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Covington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. ACM Southeast Conf</title>
		<meeting>Annu. ACM Southeast Conf</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Language Resources and Evaluation (LREC)</title>
		<meeting>Int. Conf. Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
		<title level="m">Optimum branchings. Journal of Research of the National Bureau of Standards</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">718</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Chinese Gigaword Second Edition LDC2005T14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Web Download</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual decomposition for parsing with non-projective head automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Tasker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technology Conf. and Conf. North American Chapter Assoc. for Computational Linguistics (HLT-NAACL)</title>
		<meeting>Human Language Technology Conf. and Conf. North American Chapter Assoc. for Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Treebank-3 LDC99T42. Web Download</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Turbo parsers: Dependency parsing by approximate variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andrè</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Turing on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andrè</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Mass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Chapter Assoc. for Computational Linguistics (EACL)</title>
		<meeting>European Chapter Assoc. for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pererira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technology Conf. and Conf. Empirical Methods Natural Language Process. (HLT/EMNLP)</title>
		<meeting>Human Language Technology Conf. and Conf. Empirical Methods Natural Language ess. (HLT/EMNLP)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop at Int. Conf. Learning Representations</title>
		<meeting>Workshop at Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Parsing Technologies (IWPT)</title>
		<meeting>Int. Conf. Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing: Bringing engineering and cognition together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop at ACL</title>
		<meeting>Workshop at ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsan-Kuang</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Chinese Treebank 5.0 LDC2005T01. Web Download</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A linear-time translation system for crossing interval trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter Assoc. for Computational Linguistics (NAACL)</title>
		<meeting>Conf. North American Chapter Assoc. for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="662" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations by backpropogating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>Conf. North American Chapter Assoc. for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
		<title level="m">Finding optimum branchings. Networks</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Language Technology Conf. and Conf. North American Chapter Assoc. for Computational Linguistics (HLT-NAACL)</title>
		<meeting>Human Language Technology Conf. and Conf. North American Chapter Assoc. for Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS)</title>
		<meeting>Annu. Conf. Neural Inform. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Parsing Technologies (IWPT)</title>
		<meeting>Int. Conf. Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A tale of two parsers: investigating and combining graph-based and transition-based depdency parsing using beam-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enforcing structural diversity in cube-pruned dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Online learning for inexact hypergraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)</title>
		<meeting>Conf. Empirical Methods Natural Language ess. (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="908" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Steps to excellence: Simple inference with the refined scoring of dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Golberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL)</title>
		<meeting>Annu. Meeting Assoc. for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
