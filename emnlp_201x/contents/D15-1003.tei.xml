<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building a shared world: Mapping distributional to model-theoretic semantic spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
							<email>aurelie.herbelot@cantab.net</email>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Cambridge Computer Laboratory Cambridge</orgName>
								<orgName type="institution">Universität Stuttgart Institut für Maschinelle Sprachverarbeitung Stuttgart</orgName>
								<address>
									<country>Germany, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
							<email>eva.vecchi@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">University of Cambridge Computer Laboratory Cambridge</orgName>
								<orgName type="institution">Universität Stuttgart Institut für Maschinelle Sprachverarbeitung Stuttgart</orgName>
								<address>
									<country>Germany, UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Building a shared world: Mapping distributional to model-theoretic semantic spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we introduce an approach to automatically map a standard distributional semantic space onto a set-theoretic model. We predict that there is a functional relationship between distributional information and vecto-rial concept representations in which dimensions are predicates and weights are gener-alised quantifiers. In order to test our prediction , we learn a model of such relationship over a publicly available dataset of feature norms annotated with natural language quan-tifiers. Our initial experimental results show that, at least for domain-specific data, we can indeed map between formalisms, and generate high-quality vector representations which encapsulate set overlap information. We further investigate the generation of natural language quantifiers from such vectors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the complementarity of distributional and formal semantics has become increasingly evi- dent. While distributional semantics <ref type="bibr" target="#b40">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b12">Clark, 2012;</ref><ref type="bibr" target="#b20">Erk, 2012</ref>) has proved very suc- cessful in modelling lexical effects such as graded sim- ilarity and polysemy, it clearly has difficulties account- ing for logical phenomena which are well covered by model-theoretic semantics <ref type="bibr" target="#b27">(Grefenstette, 2013)</ref>.</p><p>A number of proposals have emerged from these considerations, suggesting that an overarching seman- tics integrating both distributional and formal aspects would be desirable <ref type="bibr" target="#b14">(Coecke et al., 2011;</ref><ref type="bibr" target="#b9">Bernardi et al., 2013;</ref><ref type="bibr" target="#b27">Grefenstette, 2013;</ref><ref type="bibr" target="#b6">Baroni et al., 2014a;</ref><ref type="bibr" target="#b8">Beltagy et al., 2013;</ref><ref type="bibr" target="#b33">Lewis and Steedman, 2013</ref>). We will use the term 'Formal Distributional Se- mantics' (FDS) to refer to such proposals. This paper follows this line of work, focusing on one central ques- tion: the formalisation of the systematic dependencies between lexical and set-theoretic levels.</p><p>Let us consider the following examples.</p><p>1. Kim writes books.</p><p>2. Kim likes books.</p><p>The preferred reading of 1 has a logical form where the object is treated as an existential, while the object in 2 has a generic reading:</p><formula xml:id="formula_0">• ∃x * [book (x * ) ∧ write (Kim, x * )]</formula><p>• GEN x[book (x) → like ( <ref type="bibr">Kim, x)]</ref> with x * indicating a plurality and GEN the generic quantifier.</p><p>It is generally accepted that the appropriate choice of quantifier for an ambiguous bare plural object de- pends, amongst other things, on the lexical semantics of the verb (e.g. <ref type="bibr" target="#b26">Glasbey (2006)</ref>). This type of inter- action implies the existence of systematic influences of the lexicon over logic, which could in principle be for- malised.</p><p>A model of the lexicon/logic interface would be de- sirable to explain how speakers resolve standard cases of ambiguity like the bare plural in 1 and 2, but more generally, it could be the basis for answering a more fundamental question: how do speakers construct a model of a sentence for which they have no prior per- ceptual data?</p><p>People can make complex inferences about state- ments without having access to their real-world ref- erence. As an example, consider the sentence The kouprey is a mammal. English speakers have no problem ascertaining that if x is a kouprey, x is a mammal (which set-theoretic semantics would express as ∀x[kouprey (x) → mammal (x)]), regardless of whether they have ever encountered a kouprey. The in- ference is supported by the lexical semantics of mam- mal, which applies a property (being a mammal) to all instances of a class. Much more complex inferences are routinely performed by speakers, down to estimat- ing the cardinality of the entities involved in a partic- ular situation. Compare e.g. The cats are on the sofa (2 / a few cats?), I picked pears today (a few / a few dozen?) and The protesters were blocking the entire avenue (hundreds/thousands of protesters?).</p><p>Understanding how this process works would not only give us an insight into a complex cognitive pro- cess, but also make a crucial contribution to NLP tasks relying on inference (e.g. the Recognising Textual En- tailment challenge, RTE: <ref type="bibr" target="#b16">Dagan et al. (2009)</ref>). In- deed, while systems have successfully been developed to model entailment between quantifiers, ranging from natural logic approaches <ref type="bibr" target="#b34">(MacCartney and Manning, 2008)</ref> to distributional semantics solutions ( <ref type="bibr" target="#b5">Baroni et al., 2012)</ref>, they rely on an explicit representation of quantification. That is, they can model the entailment All koupreys are mammals |= This kouprey is a mam- mal, but not Koupreys are mammals |= This kouprey is a mammal.</p><p>In this work, we assume the existence of a mapping between language (distributional models) and world (set-theoretic models), or to be more precise, between language and a shared set of beliefs about the world, as negotiated by a group of speakers. To operationalise this mapping, we propose that set-theoretic models, like distributions, can be expressed in terms of vec- tors -giving us a common representation across for- malisms. Using a publicly available dataset of feature norms annotated with quantifiers 1 <ref type="bibr" target="#b29">(Herbelot and Vecchi, 2015)</ref>, we show that human-like intuitions about the quantification of simple subject/predicate pairs can be induced from standard distributional data. This paper is structured as follows. §2 reviews re- lated work, focusing in turn on approaches to formal distributional semantics, computational work on quan- tification, and mapping between semantic spaces. In §3, we describe our dataset. §4 and §5 describe our experiments, reporting correlation against human an- notations. We discuss our results in §6 and end with an attempt at generating natural language quantifiers from our mapped vectors ( §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formal Distributional Semantics</head><p>The relation between distributional and formal seman- tics has been the object of a number of studies in re- cent years. Proposals for a FDS, i.e. a combination of both formalisms, roughly fall into two groups: a) the fully distributional approaches, which redefine the concepts of formal semantics in distributional terms <ref type="bibr" target="#b14">(Coecke et al., 2011;</ref><ref type="bibr" target="#b9">Bernardi et al., 2013;</ref><ref type="bibr" target="#b27">Grefenstette, 2013;</ref><ref type="bibr" target="#b31">Hermann et al., 2013;</ref><ref type="bibr" target="#b6">Baroni et al., 2014a;</ref><ref type="bibr" target="#b13">Clarke, 2012)</ref>; b) the hybrid approaches, which try to keep the set-theoretic apparatus for function words and integrate distributions as content words representations <ref type="bibr" target="#b21">(Erk, 2013;</ref><ref type="bibr" target="#b8">Beltagy et al., 2013;</ref><ref type="bibr" target="#b33">Lewis and Steedman, 2013)</ref>. This paper follows the hy- brid frameworks in that we fully preserve the principles of set theory and do not attempt to give a distributional interpretation to phenomena traditionally catered for by formal semantics such as quantification or negation.</p><p>Our account is also similar to that proposed by <ref type="bibr" target="#b22">Erk (2015)</ref>. Erk suggests that distributional data influences semantic 'knowledge' 2 : specifically, while a speaker may not know the extension of the word alligator, they maintain an information state which models properties of alligators (for instance, that they are animals). This information state is described in terms of probabilistic logic, which accounts for an agent's uncertainty about what the world is like. The probability of a sentence is the summed probability of the possible worlds that make it true. Similarly, we assume a systematic relation between distributional information and world knowl- edge, expressed set-theoretically. The knowledge rep- resentation we derive is not a model proper: it cannot be said to be a description of a world -either the real one or a speaker's set of beliefs (c.f. §4 for more de- tails). But it is a good approximation of the shared in- tuitions people have about the world, in the way that distributional representations are an averaged represen- tation of how a group of speakers use their language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generalised quantifiers</head><p>Computational semantics has traditionally focused on very specific aspects of quantification. There is a large literature on the computational formalisation of quan- tifiers as automata, starting with Van Benthem (1986). In parallel to this work, much research has been done on drawing inferences from explicitly quantified state- ments -i.e. statements quantified with determiners such as some/most/all, which give information about the set overlap of a subject-predicate pair <ref type="bibr" target="#b15">(Cooper et al., 1996;</ref><ref type="bibr" target="#b0">Alshawi and Crouch, 1992;</ref><ref type="bibr" target="#b34">MacCartney and Manning, 2008)</ref>. Recent work in this area has even shown that entailment between explicit quantifiers can be modelled distributionally ( <ref type="bibr" target="#b5">Baroni et al., 2012)</ref>. A complementary object of focus, actively pursued in the 1990s, has been inference between generic statements <ref type="bibr" target="#b2">(Bacchus, 1989;</ref><ref type="bibr" target="#b42">Vogel, 1995)</ref>.</p><p>Beside those efforts, computational approaches have been developed to convert arbitrary text into logical forms. The techniques range from completely super- vised ( <ref type="bibr" target="#b3">Baldwin et al., 2004;</ref><ref type="bibr" target="#b10">Bos, 2008)</ref> to lightly su- pervised ( <ref type="bibr" target="#b43">Zettlemoyer and Collins, 2005)</ref>. Such work has shown that it was possible to automatically give complex formal semantics analyses to large amounts of data. But the formalisation of quantifiers in those systems either remains very much underspecified (e.g. bare plurals are not resolved into either existentials or generics) or relies on some grounded information, for example in the form of a database.</p><p>To the best of our knowledge, no existing system is able to universally predict the generalised quantifica- tion of noun phrases, including those introduced by the (in)definite singulars a/the and definite plurals the. 'model-theoretic vectors' can be built out of distribu- tional vectors supplemented with manually annotated training data. The proposed implementation, however, fails to validate the theory.</p><p>Our work follows the intuition that distributions can be translated into set-theoretic equivalents. But it im- plements the mapping as a systematic linear transfor- mation. Our approach is similar to <ref type="bibr" target="#b28">Gupta et al. (2015)</ref>, who predict numerical attributes for unseen concepts (countries and cities) from distributional vectors, get- ting comparably accurate estimates for features such as the GDP or CO 2 emissions of a country. We comple- ment such research by providing a more formal inter- pretation of the mapping between language and world knowledge. In particular, we offer a) a vectorial repre- sentation of set-theoretic models; b) a mechanism for predicting the application of generalised quantifiers to the sets in a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mapping between Semantic Spaces</head><p>The mapping between different semantic modalities or semantic spaces has been explored in various aspects. In cognitive science, research by <ref type="bibr" target="#b39">Riordan and Jones (2011)</ref> and <ref type="bibr" target="#b1">Andrews et al. (2009)</ref> show that models that map between and integrate perceptual and linguistic in- formation perform better at fitting human semantic in- tuition. In NLP, <ref type="bibr" target="#b38">Mikolov et al. (2013b)</ref> show that a linear mapping between vector spaces of different lan- guages can be learned to infer missing dictionary en- tries by relying on a small amount of bilingual infor- mation. <ref type="bibr" target="#b23">Frome et al. (2013)</ref> learn a linear regression to transform vector-based image representations onto vectors representing the same concepts in a linguistic semantic space, and <ref type="bibr" target="#b32">Lazaridou et al. (2014)</ref> explore mapping techniques to learn a cross-modal mapping between text and images with promising performance. We follow the basic intuition introduced by these pre- vious studies: a simple linear function can map be- tween semantic spaces, in this case between a linguistic (distributional) semantic space and a model-theoretic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotated datasets 3.1 The quantified McRae norms</head><p>The McRae norms <ref type="bibr" target="#b35">(McRae et al., 2005</ref>) are a set of feature norms elicited from 725 human participants for 541 concepts covering living and non-living entities (e.g. alligator, chair, accordion). The annotators were given concepts and asked to provide features for them, covering physical, functional and other properties. The result is a set of 7257 concept-feature pairs such as air- plane used-for-passengers or bear is-brown.</p><p>In our work, we use the annotation layer pro- duced by <ref type="bibr" target="#b29">Herbelot and Vecchi (2015)</ref> for the McRae norms (henceforth QMR): for each concept-feature pair (C, f ), the annotation provides a natural language quantifier expressing the ratio of instances of C having the feature f , as elicited by three coders. The quan- tifiers in use are NO, FEW, SOME, MOST, ALL. Ta- ble 1 provides example annotations for concept-feature pairs (reproduced from the original paper). An addi- tional label, KIND, was introduced for usages of the concept as a kind, where quantification does not ap- ply (e.g. beaver symbol-of-Canada). A subset of the annotation layer is available for training computational models, corresponding to all instances with a majority label (i.e. those where two or three coders agreed on a label). The reported average weighted Cohen kappa on this data is κ = 0.59.</p><p>In the following, we use a derived gold standard in- cluding all 5 quantified classes in QMR (removing the KIND items), with the annotation set to majority opin- ion (6156 instances). The natural language quantifiers are converted to a numerical format (see §4 for details). Using the numerical data, we can calculate the mean Spearman rank correlation between the three annota- tors, which comes to 0.63.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Additional animal data</head><p>QMR gives us an average of 11 features per con- cept. This results in fairly sparse vectors in the model- theoretic semantic space (see §4). In order to remedy data sparsity, we consider the use of additional data in the form of the animal dataset from Herbelot (2013) (henceforth AD). AD 3 is a set of 72 animal concepts with quantification annotations along 54 features. The main differences between QMR and AD are as follows:</p><p>• Nature of features: the features in AD are not hu- man elicited norms, but linguistic predicates ob- tained from a corpus analysis.</p><p>• Comprehensiveness of annotation: the 72 con- cepts were annotated along all 54 features. This ensures the availability of a large number of nega- tively quantified pairs (e.g. cat is-fish).</p><p>We manually align the AD concepts and features to the QMR format, changing e.g. bat to bat (animal). The QMR and AD sets have an overlap of 39 concepts and 33 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semantic spaces</head><p>We construct two distinct semantic spaces (distribu- tional and model-theoretic), as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The distributional semantic space</head><p>We consider two distributional semantic space archi- tectures which have each shown to have considerable success in a number of semantic tasks. First, we build a co-occurrence based space (DS cooc ), in which a word is represented by co-occurrence counts with content words (nouns, verbs, adjectives and adverbs). As a source corpus, we use a concatenation of the ukWaC, a 2009 dump of the English Wikipedia and the BNC 4 , which consists of about 2.8 billion tokens. We select the top 10K content words for the contexts, using a bag- of-words approach and counting co-occurrences within a sentence. We then apply positive Pointwise Mutual Information to the raw counts, and reduce the dimen- sions to 300 through Singular Value Decomposition. <ref type="bibr">5</ref> Next we consider the context-predicting vectors (DS M ikolov ) available as part of the word2vec 6 project ( <ref type="bibr" target="#b37">Mikolov et al., 2013a</ref>). We use the publicly avail- able vectors which were trained on a Google News dataset of circa 100 billion tokens. <ref type="bibr" target="#b7">Baroni et al. (2014b)</ref> showed that vectors constructed under this architecture outperform the classic count-based approaches across many semantic tasks, and we therefore explore this op- tion as a valid distributional representation of a word's semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The model-theoretic space</head><p>Our 'model-theoretic space' differs in a couple of important respects from traditional formal semantics models. So it may be helpful to first come back to the standard definition of a model, which relies on two components: an ontology and a denotation function <ref type="bibr" target="#b11">(Cann, 1993)</ref>. The ontology describes a world (which can be a simple situation or 'state of affairs'), with ev- erything that is contained in that world. Ontologies can be represented in various ways, but in this paper, we assume they are formalised in terms of sets of entities. The denotation function associates words with their ex- tensions in the model, i.e. the sets they refer to. Thanks to the availability of the ontology, it is possible to define a truth function for sentences, which computes whether a particular statement corresponds to the model or not.</p><p>In our account, we do not have an a priori model of the world: we wish to infer it from our observation of language data. We believe this to be an advantage over traditional formal semantics, which requires full onto- logical data to be available in order to account for refer- ence and truth conditions, but never spells out how this data comes into being. This however implies that our produced ontology will necessarily be partial: we can only model what can be inferred from language use. This has consequences for the denotation function.</p><p>Let's imagine a world with three cats and two horses. In model theory, the word horse has an extension in that world which is the set of horses, with a cardinality of two. This can be trivially derived because the world is fully described in the ontology. In our approach, how- ever, it is unlikely we might be able to learn the cardi- nality of any set in any world. And in fact, it is clear that 'in real life', speakers do miss this information for many sets (how many horses are there in the world?) Note that we do not in principle reject the possibility to learn cardinalities from distributional data (for an example of this, see <ref type="bibr" target="#b28">Gupta et al. (2015)</ref>). We simply remark that this will not always possible, or even desir- able from a cognitive point of view. By extension, this means that a model built from distributional data does not support denotation in the standard way, and thus precludes the definition of a truth function: we cannot verify the truth of the sentence There are 25,957 white horses in the world. Our 'model-theoretic' space may then be described as an underspecified set-theoretic representation of some shared beliefs about the world.</p><p>Our 'ontology' can be defined as follows. To each word w k in vocabulary V = w 1...m corresponds a set w k with underspecified cardinality. A num- ber of predicates p 1...n are similarly defined as sets with an unknown number of elements. Our claim is that this very underspecified model can be fur- ther specified by learning a function F from dis- tributions to generalised quantifiers. Specifically,</p><formula xml:id="formula_1">F ( w k ) = {Q 1 (w k , p 1 ), Q 2 (w k , p 2 )...Q n (w k , p n )}, where</formula><p>w k is the distribution of w k and Q 1 ...Q n ∈ {no, f ew, some, most, all} . That is, F takes a dis- tribution w k and returns a quantifier for each predicate in the model, corresponding to the set overlap between w k and p 1...n . Note that we focus here on 5 quanti- fiers only, but as mentioned above, we do not preclude the possibility of learning others (including cardinals in appropriate cases).</p><p>F ( w k ) lives in a model-theoretic space which broadly follows the representation suggested by <ref type="bibr" target="#b30">Herbelot (2013)</ref>. We assume a space with n dimensions d 1 ...d n which correspond to predicates p 1...n (e.g. is fluffy, used for transportation). In that space, F ( w k ) is weighted along the dimension d m in proportion to the set overlap w k ∩p m . <ref type="bibr">7</ref> The following shows a toy vector with only four dimensions for the concept horse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a mammal</head><p>1 has f our legs 0.95 is brown 0.35 is scaly 0</p><p>This vector tells us that the set of horses includes the set of mammals (the number of horses that are also mammals divided by the number of horses comes to 1, i.e. all horses are mammals), and that the set of horses and the set of things that are scaly are disjoint (no horse is scaly). We also learn that a great majority of horses have four legs and that some are brown.</p><p>In the following, we experiment with 3 model- theoretic spaces built from the McRae and AD datasets described in §3. As both datasets are annotated with natural language quantifiers rather than cardinality ra- tios, we convert the annotation into a numerical for- mat, where ALL → 1, MOST → 0.95, SOME → 0.35, FEW → 0.05, and NO → 0. These values correspond to the weights giving the best inter-annotator agree- ment in <ref type="bibr" target="#b29">Herbelot and Vecchi (2015)</ref>, when calculating weighted Cohen's kappa on QMR.</p><p>In each model-theoretic space, a concept is repre- sented as a vector in which the dimensions are features (has buttons, is green), and the values of the vectors along each dimension are quantifiers (in numerical for- mat). When a feature does not occur with a concept in QMR, the concept's vector receives a weight of 0 on the corresponding dimension. <ref type="bibr">8</ref> We define 3 spaces as follows. The McRae-based model-theoretic space (MT QM R ) contains 541 concepts, as described in §3.1. The second space is constructed specifically for the ad- ditional animal data from §3.2 (MT AD ). Finally, we merge the two into a single space of 555 unique con- cepts (MT QM R+AD ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>To map from one semantic representation to another, we learn a function f : DS → MT that transforms a distributional semantic vector for a concept to its model-theoretic equivalent.</p><p>Following previous research showing that similari- ties amongst word representations can be maintained within linear transformations ( <ref type="bibr" target="#b38">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b23">Frome et al., 2013)</ref>, we learn the mapping as a linear relationship between the distributional representation of a word and its model-theoretic representation. We estimate the coefficients of the function using (multi- variate) partial least squares regression (PLSR) as im- plemented in the R pls package <ref type="bibr" target="#b36">(Mevik and Wehrens, 2007)</ref>.</p><p>We learn a function from the distributional space to each of the model-theoretic spaces (c.f. §4). The dis- tribution of training and test items is outlined in Ta- ble 2, expressed as a number of concept vectors. We also include the number of quantified instances in the test set (i.e. the number of actual concept-feature pairs that were explicitly annotated in QM R/AD and that <ref type="bibr">8</ref> No transformations or dimensionality reductions were performed on the MT spaces.  we can thus evaluate -this is a portion of each concept vector in the spaces including QM R data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We first consider a preliminary quantitative analysis to better understand the behavior of the transformations, while a more qualitative analysis is provided in §6. The results in <ref type="table" target="#tab_4">Table 3</ref> show the degree to which predicted values for each dimension in a model-theoretic space correlate with the gold annotations, operationalised as the Spearman ρ (rank-order correlation). Wherever ap- propriate, we also report the mean Spearman correla- tion between the three human annotators for the par- ticular test set under consideration, showing how much they agreed on their judgements. 9 These figures pro- vide an upper bound performance for the system, i.e. we will consider having reached human performance if the correlation between system and gold standard is in the same range as the agreement between humans. For each mapping tested, <ref type="table" target="#tab_4">Table 3</ref> provides details about the training data used to learn the mapping function and the test data for the respective results. Also for each mapping, results are reported when learned from either the co-occurrence distributional space (DS cooc ) or the context-predicting distributional space (DS M ikolov ). The top section of the table reports results for the QMR and AD dataset taken separately, as well as their concatenation. Performance on the domain-specific AD is very promising, at 0.641 correlation, calculated over 648 test instances. The results when trained on just the QMR features (MT QM R ) are much lower (0.35 over 1570 test instances), which we put down to the wider variety of concepts in that dataset; we however observe a substantial increase in performance when we train and test over the two datasets (MT QM R+AD : 0.569 over 1595 instances).</p><p>We investigate whether merging the datasets gen- erally benefits QMR concepts or just the animals (see middle section in <ref type="table" target="#tab_4">Table 3</ref>). The result on the MT animals test set, which includes animals from the AD and QMR datasets, shows that this category fares indeed very well, at ρ = 0.663. But while augment- ing the training data with category-specific datapoints benefits that category, it does not improve the results    <ref type="table" target="#tab_2">Table 2</ref>. For further analysis we report the results when tested only on animal test items (animals), or on all test items but animals (no-animals). MT animals contains test items from both AD and the animal section of the McRae norms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model-Theoretic</head><p>See text for more details.</p><p>for concepts of other classes (c.f. compare MT animals with MT no-animals ). Finally, we quantify the specific improvement to the QMR animal concepts by comparing the correlation obtained on MT QM R animals (a test set consisting only of QMR animal features) after training on a) the QMR data alone and b) the merged dataset (third section of <ref type="table" target="#tab_4">Table 3</ref>). Performance increases from 0.419 to 0.666 on that specific set. This is in line with the inter-annotator agreement (0.663).</p><p>To summarise, we find that the best correlations with the gold annotations are seen when we in- clude the animal-only dataset in training (MT AD and MT QM R+AD ) and test on just animal concepts (MT AD , MT animals and MT QM R animals ). As one might expect, category-specific training data yields high performance when tested on the same category. Although this expectation seems intuitive, it is worth noting that our system produces promisingly high cor- relations, reaching human-performance on a subset of our data. The assumption we can draw from these results is that, given a reasonable amount of training data for a category, we can proficiently generate model- theoretic representations for concept-feature pairs from a distributional space. The empirical question remains whether this can be generalized for all categories in the QMR dataset.</p><p>It is important to keep in mind that the MT spaces are not full matrices, meaning that we have 'miss- ing values' for various dimensions when a concept is converted into a vector. For example, the feature has a tail is not among the annotated features for bear in QMR and has a weight of 0, even though most bears have a tail. This is a consequence of the original McRae dataset, rather than the design of our approach. But it follows that in this quantitative analysis, we are not able to confirm the accuracy of the predicted values on dimensions for which we do not have gold anno- tations. This may also affect the performance of the system by including 'false' 0 weights in the training % of gold in...  data. Although this does not affect our reported cor- relation results -we test the correlations on those val- ues for which we have gold annotations only -it does open the door to a natural next step in the evaluation. In order to judge the performance of the system on the missing gold dimensions, we need a manual analysis to assess the quality of the whole vectors, which goes hand-in-hand with obtaining additional annotations for the missing dimensions. It seems, therefore, that an ac- tive learning strategy would allow us to not only eval- uate the model-theoretic vectors more fully, but also improve the system by capturing new data. <ref type="bibr">10</ref> In this analysis, we focused primarily on the com- parison between transformations using various truth- theoretic datasets for training and generation. We leave it to further work to extensively compare the effect of varying the type of the distributional space. Our re- sults show, however, that the M ikolov model performs slightly worse than the co-occurrence space (cooc), dis- proving the idea that predictive models always outper- form count-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>To further assess the quality of the produced space, we perform a nearest-neighbour analysis of our results to evaluate the coherence of the estimated vectors: for  <ref type="table">a tool  a tool  is sharp  is sharp  has a handle  has a handle  used for cutting  used for cutting  has a metal blade  made of metal  a weapon</ref> an axe has a head is small used for chopping - has a blade - is dangerous - is heavy - used by lumberjacks - used for killing - <ref type="table">Table 5</ref>: McRae feature norms for axe and hatchet each concept in our test set, we return its nearest neigh- bours from the gold dataset, as given by the cosine sim- ilarity measure, hoping to find that the estimated vector is close to its ideal representation (see F˘ ag˘ ar˘ as¸anas¸an et al.</p><p>(2015) for a similar evaluation on McRae norms). Re- sults are shown in <ref type="table" target="#tab_5">Table 4</ref>. We find that the gold vector is among the top 5 nearest neighbours to the predicted equivalent in nearly 20% of concepts, with the percent- age of gold items in the top neighbours improving as we increase the size of the neighbourhood. We per- form a more in-depth analysis of the neighbourhoods for each concept to gain a better understanding of their behaviour and quality. We discover that, in many cases, the mapped vector is close to a similar concept in the gold standard, but not to itself. So for instance, Two reasons can be identified for these compara- tively low 11 similarities. First, the McRae norms do not make for a consistent semantic space because a feature that -from an extensional point of view -seems rele- vant to two concepts may only have been produced by the annotators for one of them. As an example of this, see <ref type="table">Table 5</ref>, which shows the feature norms for axe and hatchet after processing ( §3). Although the concepts share 4 features, they also differ quite strongly, an axe being seen as a weapon with a blade, while the hatchet is itself referred to as an axe. Extensionally, of course, there is no reason to think that a hatchet does not have a blade or might not be dangerous, but those features do not appear in the norms for the concept. This re- sults in the two vectors being clearly separated in the set-theoretic space. This means that the distribution of axe may well be mapped to a region close to hatchet, but thereby ends up separated from the gold axe vector.</p><p>The second, related issue is that the animal con- cepts in the McRae norms are annotated along fewer dimensions than in AD. For example, alligator -which only appears in the McRae set -has 13 features, while crocodile (in both sets) has 70. Given that features which are not mentioned for a concept receive a weight of 0, this also results in very different vectors.</p><p>In <ref type="table" target="#tab_7">Table 6</ref>, we provide the top weighted features for a small set of concepts. As expected, the animal repre- sentations (bear, housefly) have higher quality than the other two (plum, cottage). But overall, the ranking of dimensions is sensible. We see also that these represen- tations have 'learnt' features for which we do not have values in our gold data -thereby correcting some of the 0 values in the training vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Generating natural language quantifiers</head><p>In a last experiment, we attempt to map the set- theoretic vectors obtained in §5 back to natural lan- guage quantifiers. This last step completes our pipeline, giving us a system that produces quantified statements of the type All dogs are mammals or Some bears are brown from distributional data.</p><p>For each mapped vector F ( w k ) = v k and a set of di- mensions d 1...n corresponding to properties p 1...n , the value of v k along each dimension is indicative of the proportion of instances of w k having the property sig- nalled by the dimension. The smaller the value, the smaller the overlap between the set of instances of w k and the set of things having the property. Deriving natural language quantifiers from these values involves setting four thresholds t all , t most , t some and t f ew so that for instance, if the value of v k along d m is more than t all , it is the case that all instances of w k have property p m , and similarly for the other quantifiers (no has a special status as it is not entailed by any of the other quantifiers under consideration). We set the t- thresholds by a systematic search on a training set (see below).</p><p>To evaluate this step, we propose a function that cal- culates precision while taking into account the two fol- lowing factors: a) some errors are worse than others: the system shouldn't be overly penalised for classifying a property as MOST rather than ALL, but much more for classifying a gold standard ALL as SOME; b) errors that are conducive to false inferences should be strongly pe- nalised, e.g. generating all dogs are black is more seri- ous than some dogs are mammals, because the former might lead to incorrect inferences with respect to indi- vidual dogs while the latter is true, even though it is pragmatically odd.     We set a distance matrix, which we will use for pe- nalising errors. This matrix, shown in <ref type="table" target="#tab_8">Table 7</ref>, is ba- sically equivalent to the matrix used by <ref type="bibr" target="#b29">Herbelot and Vecchi (2015)</ref> to calculate weighted kappa between annotators, with the difference that all errors involv- ing NO cause incorrect inferences and receive special treatment. Cases where the gold quantifier entails the mapped quantifier (all cats |= some cats) have posi- tive distances, while cases where the entailment doesn't hold have negative distances. Using the distance ma- trix, we give a score to each instance in our test data as follows:</p><formula xml:id="formula_2">s = 1 − d if d ≥ 0 d if d &lt; 0 (1)</formula><p>where d is obtained from the distance matrix. This has the effect that when the mapped quantifier equals the gold quantifier, the system scores 1; when the mapped value deviates from the gold standard but produces a true sentence (some dogs are mammals), the system gets a partial score proportional to the distance between its output and the gold data; when the map- ping results in a false sentence (all dogs are black), the  In what follows, we report the average performance of the system as P = sm N where s m is the score assigned to a particular test instance, and N is the number of test instances. We evaluate on the 648 test instances of MT AD , as this is the only dataset con- taining a fair number of negatively quantified concept- predicate pairs. We perform 5-fold cross-evaluation on this data, using 4 folds to set the t thresholds, and test- ing on one fold. We obtain an average P of 0.61. Infer- ence is preserved in 73% of cases (also averaged over the 5 folds). <ref type="table" target="#tab_10">Table 8</ref> shows the confusion matrix for our results. We note that the system classifies NO-quantified in- stances with good accuracy (72% -most confusions being with FEW). Because of the penalty given to instances that violate proper entailment, the system is conservative and prefers FEW to SOME, as well as MOST to ALL. <ref type="table" target="#tab_11">Table 9</ref> shows randomly selected in- stances, together with their mapped quantifier and the label from the gold standard. <ref type="table">Mapped Gold  raven a bird  most  all  pigeon has hair  few  no  elephant has eyes  most  all  crab is blind  few  few  snail a predator  no  no  octopus is stout  no  few  turtle roosts  no  few  moose is yellow  no  no  cobra hunted by people  some some  snail forages  few  no  chicken is nocturnal</ref> few no moose has a heart most all pigeon hunted by people no few cobra bites few most </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we introduced an approach to map from distributional to model-theoretic semantic vectors. Us- ing traditional distributional representations for a con- cept, we showed that we are able to generate vecto- rial representations that encapsulate generalised quan- tifiers.</p><p>We found that with a relatively "cheap" linear func- tion -cheap in that it is easy to learn and requires mod- est training data -we can reproduce the quantifiers in our gold annotation with high correlation, reaching hu- man performance on a domain-specific test set. In fu- ture work, we will however explore the effect of more powerful functions to learn the transformations from distributional to model-theoretic spaces.</p><p>Our qualitative analysis showed that our predicted model-theoretic vectors sensibly model the concepts under consideration, even for features which do not have gold annotations. This is not only a promising result for our approach, but it provides potential as a next step to this work: expanding our training data with non-zero dimensions in an active learning procedure. We also experimented with generating natural language quantifiers from the mapped vectorial representations, producing 'true' quantified sentences with a 73% accu- racy.</p><p>We note that our approach gives a systematic way to disambiguate non-explicitly quantified sentences such as generics, opening up new possibilities for im- proved semantic parsing and recognising entailment. Right now, many parsers give the same broad anal- ysis to Mosquitoes are insects and Mosquitoes carry malaria, involving an underspecified/generic quanti- fier. This prevents inferring, for instance, that Mandie the mosquito is definitely an insect but may or may not carry malaria. In contrast, our system would at- tribute the most plausible quantifiers to those sentences (all/few), allowing us to produce correct inferences.</p><p>The focus of this paper was concept-predicate pairs out of context. That is, we considered quantified sen- tences where the restrictor was the entire set denoted by a lexical item. A natural next step is to inves- tigate the quantification of statements involving con- textualised subsets. For instance, we should obtain a different quantifier for taxis are yellow depending on whether the sentence starts with In London... or In New York... In future work, we will test our system on such context-specific examples, using contextualised vector representations such as the ones proposed by e.g. <ref type="bibr" target="#b19">Erk and Padó (2008)</ref> and <ref type="bibr" target="#b17">Dinu and Lapata (2010)</ref>. We conclude by noting again that the set-theoretic models produced in this work differ from formal se- mantics models in important ways. They do not rep- resent the world per se, but rather some shared beliefs about the world, induced from an annotated dataset of feature norms. This calls for a modified version of the standard denotation function and for the replacement of the truth function with a 'plausibility' function, which would indicate how likely a stereotypical speaker might be to agree with a particular sentence. While this would be a fundamental departure from the core philosophy of model theory, we feel that it may be a worthwhile en- deavour, allowing us to preserve the immense benefits of the set-theoretic apparatus in a cognitively plausible fashion. Following this aim, we hope to expand the pre- liminary framework presented here into a more expres- sive vector-based interpretation of set theory, catering for aspects not covered in this paper (e.g. cardinality, non-intersective modification) and refining our notion of a model, together with its relation to meaning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Distributional</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>− −−−−− → alligator mapped is very close to − −−−−− → crocodile gold , but not to − −−−−− → alligator gold . Similar find- ings are made for church/cathedral, axe/hatchet, dish- washer/fridge, etc. A further investigation show that in the gold standard itself, those pairs are not as close to each other as they should be. Here are some relevant cosine similarities: alligator − crocodile 0.47 church − cathedral 0.45 axe − hatchet 0.50 dishwasher − f ridge 0.21</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Distribution of training/test items for each 
model-theoretic semantic space. We also provide the 
number of dimensions for each space, and the actual 
number of concept-feature instances tested on. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>(Spearman) correlations of mapped dimensions with gold annotations for all test items. The table reports 
results (ρ) when mapped from a distributional space (DS cooc or DS M ikolov ) to each MT space, as well as the 
correlation with human annotations when available. The train/test data for the mappings is specified in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Percentage of gold vectors found in the top neighbours of the mapped concepts, shown for the DS cooc → MT QM R+AD transformation.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Example of 20 most weighted contexts in the predicted model-theoretic vectors for 4 test concepts, shown 
for the DS cooc → MT M cRae+AD transformation. Features marked with an asterisk ( * ) are not among the concept's 
features in the gold data. 

Gold 
no 
few 
some most 
all 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Distance matrix for the evaluation of the natu-
ral language quantifiers generation step. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Confusion matrix for the results of the natural 
language quantifiers generation. 

system is penalised with minus points. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 : Examples of mapped concept-predicate pairs</head><label>9</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Data available at http://www.cl.cam.ac.uk/ ˜ ah433/mcrae-quantified-majority.txt</note>

			<note place="foot" n="2"> We use the term knowledge loosely, to refer to a speaker&apos;s beliefs about the world or a state of affairs.</note>

			<note place="foot" n="3"> Data available at http://www.cl.cam.ac.uk/ ˜ ah433/material/herbelot_iwcs13_data. txt. 24</note>

			<note place="foot" n="4"> http://wacky.sslmit.unibo.it, http: //www.natcorp.ox.ac.uk 5 All semantic spaces, both distributional and modeltheoretic, were built using the DISSECT toolkit (Dinu et al., 2013). 6 https://code.google.com/p/word2vec</note>

			<note place="foot" n="7"> In Herbelot (2013), weights are taken to be probabilities, but we prefer to talk of quantifiers, as the notion models our data more directly.</note>

			<note place="foot" n="9"> These figures are only available for the QMR dataset, as AD only contains one annotation per subject-predicate pair.</note>

			<note place="foot" n="10"> As suggested by a reviewer, one could also treat the missing entries as latent dimensions and define the loss function on only the known entries. We leave it to future work to test this promising option to resolve the issue of data sparsity.</note>

			<note place="foot" n="11"> Compare with e.g. ape-monkey, Sim = 0.97.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Marco Baroni, Stephen Clark, Ann Copes-take and Katrin Erk for their helpful comments on a previous version of this paper, and the three anonymous reviewers for their thorough feedback on this work. Eva Maria Vecchi is supported by ERC Starting Grant DisCoTex (306920).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monotonic semantic interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Crouch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 30th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating experiential and distributional data to learn semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="498" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A modest, but semantically well founded, inheritance reasoner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahiem</forename><surname>Bacchus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 11th International Joint Conference on Artificial Intelligence<address><addrLine>Detroit, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="1104" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Road-testing 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Flickinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ara</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">the English Resource Grammar over the British National Corpus</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC2004)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC2004)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifteenth Conference of the European Chapter of the Association for Computational Linguistics (EACL2012)</title>
		<meeting>the fifteenth Conference of the European Chapter of the Association for Computational Linguistics (EACL2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Frege in space: A program of compositional distributional semantics. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaela</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Montague meets Markov: Deep semantics with probabilistic logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Islam Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (*SEM2013)</title>
		<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A relatedness benchmark to test the role of determiners in compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wide-coverage semantic analysis with Boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Semantics in Text Processing (STEP2008)</title>
		<meeting>the 2008 Conference on Semantics in Text Processing (STEP2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Formal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><surname>Cann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vector space models of lexical meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Contemporary Semantics-second edition</title>
		<editor>Shalom Lappin and Chris Fox</editor>
		<imprint>
			<publisher>Wiley-Blackwell</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A context-theoretic framework for compositionality in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="71" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis: A Festschrift for Joachim Lambek</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="345" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A framework for computational semantics (FraCaS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eijckl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Genabith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Japars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Milward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poesio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The FraCaS Consortium</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Rational, evaluation and approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="459" to="476" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Measuring distributional similarity in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2010)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1162" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DISSECT: DIStributional SEmantics Composition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the System Demonstrations of ACL 2013</title>
		<meeting>the System Demonstrations of ACL 2013<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A structured vector space model for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008)</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008)<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector space models of word meaning and phrase meaning: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="635" to="653" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards a semantics for distributional representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013)</title>
		<meeting>the Tenth International Conference on Computational Semantics (IWCS2013)<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What do you know about an alligator when you know the company it keeps? Unpublished draft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<ptr target="https://utexas.box.com/s/ekznoh08afi1kpkbf0hb" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From distributional semantics to feature norms: Grounding semantic models in human perceptual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Luana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Vecchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics</title>
		<meeting>the 11th International Conference on Computational Semantics<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IWCS 2015</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A formal approach to linking logical form and vector-space lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing Meaning</title>
		<editor>Harry Bunt, Johan Bos, and Stephen Pulman</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bare plurals in object position: which verbs fail to give existential readings, and why?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Glasbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Non-definiteness and Plurality</title>
		<editor>Liliane Tasmowski and Svetlana Vogeleer</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Benjamins</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="133" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards a formal distributional semantics: Simulating logical calculi with tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM2013)</title>
		<meeting>the Second Joint Conference on Lexical and Computational Semantics (*SEM2013)<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributional vectors encode referential attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijeet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedingsof the Conference on Empirical Methods in Natural Language Processing (EMNLP2015)</title>
		<meeting>of the Conference on Empirical Methods in Natural Language Processing (EMNLP2015)<address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From concepts to models: some issues in quantifying feature norms. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What is in a text, what isn&apos;t, and what this has to do with lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Computational Semantics (IWCS2013)</title>
		<meeting>the Tenth International Conference on Computational Semantics (IWCS2013)<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Not not bad&quot; is not &quot;bad&quot;: A distributional account of negation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality (ACL2013)</title>
		<meeting>the 2013 Workshop on Continuous Vector Space Models and their Compositionality (ACL2013)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1403" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combined Distributional and Logical Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling semantic containment and exclusion in natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics (COLING08)</title>
		<meeting>the 22nd International Conference on Computational Linguistics (COLING08)<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic feature production norms for a large set of living and nonliving things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George S Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The pls package: Principal component and partial least squares regression in R</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Björn-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Mevik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wehrens</surname></persName>
		</author>
		<ptr target="http://www.jstatsoft.org/v18/i02/" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Riordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael N Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="345" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Essays in logical semantics. Number 29</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F A K</forename><surname>Van Benthem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
	<note>Reidel</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Inheritance reasoning: Psychological plausibility, proof theory and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. College of Science and Engineering. School of Informatics</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Uncertainty in AI</title>
		<meeting>the 21st Conference on Uncertainty in AI</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
