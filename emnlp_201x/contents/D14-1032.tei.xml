<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can&apos;t See What I Mean</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
							<email>felix.hill@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">Computer Laboratory University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
							<email>anna.korhonen@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">Computer Laboratory University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can&apos;t See What I Mean</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="255" to="265"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in NLP because of the obvious parallels with human language learning. Performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts. However, such concepts are comparatively rare in everyday language. In this work, we present a new means of extending the scope of multi-modal models to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings. Our architecture out-performs previous approaches in combining input from distinct modalities, and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives. We discuss the implications of our results both for optimizing the performance of multi-modal models and for theories of abstract conceptual representation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal models that learn semantic represen- tations from both language and information about the perceptible properties of concepts were orig- inally motivated by parallels with human word learning ( <ref type="bibr" target="#b0">Andrews et al., 2009</ref>) and evidence that many concepts are grounded in perception <ref type="bibr" target="#b1">(Barsalou and Wiemer-Hastings, 2005</ref>). The perceptual information in such models is generally mined di- rectly from images <ref type="bibr" target="#b11">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b3">Bruni et al., 2012</ref>) or from data collected in psychologi- cal studies <ref type="bibr" target="#b29">(Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b28">Roller and Schulte im Walde, 2013)</ref>.</p><p>By exploiting the additional information en- coded in perceptual input, multi-modal models can outperform language-only models on a range of semantic NLP tasks, including modelling sim- ilarity ( <ref type="bibr" target="#b4">Bruni et al., 2014;</ref><ref type="bibr" target="#b19">Kiela et al., 2014</ref>) and free association <ref type="bibr" target="#b29">(Silberer and Lapata, 2012)</ref>, pre- dicting compositionality <ref type="bibr" target="#b28">(Roller and Schulte im Walde, 2013)</ref> and concept categorization <ref type="bibr" target="#b30">(Silberer and Lapata, 2014)</ref>. However, to date, these pre- vious approaches to multi-modal concept learning focus on concrete words such as cat or dog, rather than abstract concepts, such as curiosity or loyalty. However, differences between abstract and con- crete processing and representation <ref type="bibr" target="#b27">(Paivio, 1991;</ref><ref type="bibr" target="#b15">Hill et al., 2013;</ref><ref type="bibr" target="#b19">Kiela et al., 2014)</ref> suggest that conclusions about concrete concept learning may not necessarily hold in the general case. In this pa- per, we therefore focus on multi-modal models for learning both abstract and concrete concepts.</p><p>Although concrete concepts might seem more basic or fundamental, the vast majority of open- class, meaning-bearing words in everyday lan- guage are in fact abstract. 72% of the noun or verb tokens in the British National Corpus ( <ref type="bibr" target="#b20">Leech et al., 1994)</ref> are rated by human judges 1 as more abstract than the noun war, for instance, a con- cept many would already consider to be quite abstract. Moreover, abstract concepts by defi- nition encode higher-level (more general) princi- ples than concrete concepts, which typically re- side naturally in a single semantic category or do- main (Crutch and <ref type="bibr" target="#b8">Warrington, 2005</ref>). It is there- fore likely that abstract representations may prove highly applicable for multi-task, multi-domain or transfer learning models, which aim to acquire 'general-purpose' conceptual knowledge without reference to a specific objective or task <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b22">Mesnil et al., 2012)</ref>.</p><p>In a recent paper,  investigate whether the multi-modal models cited above are effective for learning concepts other than concrete nouns. They observe that representations of cer- tain abstract concepts can indeed be enhanced in multi-modal models by combining perceptual and linguistic input with an information propagation step.  propose ridge regression as an alternative to the nearest-neighbour averaging proposed by <ref type="bibr" target="#b18">Johns and Jones (2012)</ref> for such prop- agation, and show that it is more robust to changes in the type of concept to be learned. However, both methods are somewhat inelegant, in that they learn separate linguistic and 'pseudo-perceptual' repre- sentations, which must be combined via a separate information combination step. Moreover, for the majority of abstract concepts, the best performing multi-modal model employing these techniques remains less effective than conventional text-only representation learning model.</p><p>Motivated by these observations, we introduce an architecture for learning both abstract and con- crete representations that generalizes the skipgram model of <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref> from text-based to multi-modal learning. Aspects of the model de- sign are influenced by considering the process of human language learning. The model moderates the training input to include more perceptual infor- mation about commonly-occurring concrete con- cepts and less information about rarer concepts. Moreover, it integrates the processes of combin- ing perceptual and linguistic input and propagat- ing information from concrete to abstract concepts into a single representation update process based on back-propagation. We train our model on running-text language and two sources of perceptual descriptors for con- crete nouns: the ESPGame dataset of annotated images <ref type="bibr" target="#b34">(Von Ahn and Dabbish, 2004</ref>) and the CSLB set of concept property norms <ref type="bibr" target="#b9">(Devereux et al., 2013</ref>). We find that our model combines in- formation from the different modalities more ef- fectively than previous methods, resulting in an improved ability to model the USF free associa- tion gold standard ( <ref type="bibr" target="#b26">Nelson et al., 2004</ref>) for con- crete nouns. In addition, the architecture propa- gates the extra-linguistic input for concrete nouns to improve representations of abstract concepts more effectively than alternative methods. While this propagation can effectively extend the advan- tage of the multi-modal approach to many more concepts than simple concrete nouns, we observe that the benefit of adding perceptual input appears to decrease as target concepts become more ab- stract. Indeed, for the most abstract concepts of all, language-only models still provide the most effective learning mechanism.</p><p>Finally, we investigate the optimum quantity and type of perceptual input for such models. Be- tween the most concrete concepts, which can be effectively represented directly in the perceptual modality, and the most abstract concepts, which cannot, we identify a set of concepts that cannot be represented effectively directly in the percep- tual modality, but still benefit from perceptual in- put propagated in the model via concrete concepts.</p><p>The motivation in designing our model and ex- periments is both practical and theoretical. Taken together, the empirical observations we present are potentially important for optimizing the learning of representations of concrete and abstract con- cepts in multi-modal models. In addition, they of- fer a degree of insight into the poorly understood issue of how abstract concepts may be encoded in human memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Design</head><p>Before describing how our multi-modal architec- ture encodes and integrates perceptual informa- tion, we first describe the underlying corpus-based representation learning model.</p><p>Language-only Model Our multi-modal archi- tecture builds on the continuous log-linear skip- gram language model proposed by <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref>. This model learns lexical representa- tions in a similar way to neural-probabilistic lan- guage models (NPLM) but without a non-linear hidden layer, a simplification that facilitates the efficient learning of large vocabularies of dense representations, generally referred to as embed- dings ( <ref type="bibr" target="#b33">Turian et al., 2010)</ref>. Embeddings learned by the model achieve state-of-the-art performance on several evaluations including sentence comple- tion and analogy modelling ( <ref type="bibr" target="#b23">Mikolov et al., 2013)</ref>.</p><p>For each word type w in the vocabulary V , the model learns both a 'target-embedding' r w ∈ R d and a 'context-embedding' ˆ r w ∈ R d such that, given a target word, its ability to predict nearby context words is maximized. The probability of seeing context word c given target w is defined as:</p><formula xml:id="formula_0">p(c|w) = e ˆ rc·rw v∈V e ˆ rv·rw w n Target Representation Score: p(c|w) Context Representations Information Source w n+2 p w n+2 w n+1 p w n+1 w n-1 p w n-1 w n-2 p w n-2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text8 Corpus</head><p>Perceptual P ESP P CSBL <ref type="figure">Figure 1</ref>: Our multi-modal model architecture. Light boxes are elements of the original <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref> model. For target words w n in the domain of P (concrete concepts), the model updates its representations based on corpus context words w n±i , then on words p w n±i in perceptual pseudo-sentences. For w n not in the domain of P (abstract concepts), updates are based solely on the w n±i .</p><p>The model learns from a set of target-word, context-word pairs, extracted from a corpus of sentences as follows. In a given sentence S (of length N ), for each position n ≤ N , each word w n is treated in turn as a target word. An inte- ger t(n) is then sampled from a uniform distribu- tion on {1, . . . k}, where k &gt; 0 is a predefined maximum context-window parameter. The pair to- kens {(w n , w n+j ) : −t(n) ≤ j ≤ t(n), w i ∈ S} are then appended to the training data. Thus, tar- get/context training pairs are such that (i) only words within a k-window of the target are selected as context words for that target, and (ii) words closer to the target are more likely to be selected than those further away.</p><p>The training objective is then to maximize the sum of the log probabilities T across of all such examples from S and across all sentences in the corpus, where T is defined as follows:</p><formula xml:id="formula_1">T = 1 N N n=1 −t(n)≤j≤t(n),j =0 log(p(w n+j |w n ))</formula><p>The model free parameters (target-embeddings and context-embeddings of dimension d for each word in the corpus with frequency above a certain threshold f ) are updated according to stochastic gradient descent and backpropation, with learning rate controlled by <ref type="bibr">Adagrad (Duchi et al., 2011</ref>). For efficiency, the output layer is encoded as a hierarchical softmax function based on a binary Huffman tree <ref type="bibr" target="#b24">(Morin and Bengio, 2005</ref>). As with other distributional architectures, the model captures conceptual semantics by exploit- ing the fact that words appearing in similar lin- guistic contexts are likely to have similar mean- ings. Informally, the model adjusts its embeddings to increase the 'probability' of seeing the language in the training corpus. Since this probability in- creases with the p(c|w), and the p(c|w) increase with the dot productˆrproductˆ productˆr c · r w , the updates have the effect of moving each target-embedding incremen- tally 'closer' to the context-embeddings of its col- locates. In the target-embedding space, this results in embeddings of concept words that regularly oc- cur in similar contexts moving closer together.</p><p>Multi-modal Extension We extend the <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref> architecture via a simple means of in- troducing perceptual information that aligns with human language learning. Based on the assump- tion that frequency in domain-general linguistic corpora correlates with the likelihood of 'experi- encing' a concept in the world <ref type="bibr" target="#b5">(Bybee and Hopper, 2001;</ref><ref type="bibr" target="#b6">Chater and Manning, 2006</ref>), perceptual information is introduced to the model whenever designated concrete concepts are encountered in the running-text linguistic input. This has the ef- fect of introducing more perceptual input for com- monly experienced concrete concepts and less in- put for rarer concrete concepts.</p><p>To implement this process, perceptual informa- tion is extracted from external sources and en- coded in an associative array P, which maps (typ- ically concrete) words w to bags of perceptual fea- tures b(w). The construction of this array depends on the perceptual information source; the process for our chosen sources is detailed in Section 2.1.</p><p>Training our model begins as before on running- text. When a sentence S m containing a word w in the domain of P is encountered, the model finishes training on S m and begins learning from a per- ceptual pseudo-sentencê S m (w). ˆ S m (w) is con- structed by alternating the token w with a fea-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>257ˆS</head><p>257ˆ 257ˆS(crocodile) = Crocodile legs crocodile teeth crocodile teeth crocodile scales crocodile green crocodile. ˆ S(screwdriver) = Screwdriver handle screwdriver flat screwdriver long screwdriver handle screwdriver head.</p><p>Figure 2: Example pseudo-sentences generated by our model. ture sampled at random from b(w) untiî S m (w) is the same length as S m (see <ref type="figure">Figure 2</ref>). Because we want the ensuing perceptual learning process to focus on how w relates to its perceptual prop- erties (rather than how those properties relate to each other), we insert multiple instances of w intôintô S m (w). This ensures that the majority of train- ing cases derived fromˆSfromˆ fromˆS m (w) are instances of (w, feature) rather than (feature, feature) pairs. Once training onˆSonˆ onˆS m (w) is complete, the model reverts to the next 'genuine' (linguistic) sentence S m+1 , and the process continues. Thus, when a concrete concept is encountered in the corpus, its embed- ding is first updated based on language (moved in- crementally closer to concepts appearing in sim- ilar linguistic contexts), and then on perception (moved incrementally closer to concepts with the same or similar perceptual features).</p><p>For greater flexibility, we introduce a parameter α reflecting the raw quantity of perceptual infor- mation relative to linguistic input. When α = 2, two pseudo-sentences are generated and inserted for every corpus occurrence of a token from the domain of P. For non-integral α, the number of sentences inserted is α, and a further sentence is added with probability α − α.</p><p>In all experiments reported in the following sec- tions we set the window size parameter k = 5 and the minimum frequency parameter f = 3, which guarantees that the model learns embeddings for all concepts in our evaluation sets. While the model learns both target and context-embeddings for each word in the vocabulary, we conduct our experiments with the target embeddings only. We set the dimension parameter d = 300 as this pro- duces high quality embeddings in the language- only case <ref type="bibr" target="#b23">(Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Information Sources</head><p>We construct the associative array of perceptual information P from two sources typical of those used for multi-modal semantic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ESPGame Dataset</head><p>The ESP-Game dataset (ESP) <ref type="bibr" target="#b34">(Von Ahn and Dabbish, 2004</ref>) consists of 100,000 images, each annotated with a list of lex- ical concepts that appear in that image.</p><p>For any concept w identified in an ESP im- age, we construct a corresponding bag of features b(w). For each ESP image I that contains w, we append the other concept tokens identified in I to b(w). Thus, the more frequently a concept co- occurs with w in images, the more its correspond- ing lexical token occurs in b(w). The array P ESP in this case then consists of the (w, b(w)) pairs. CSLB Property Norms The Centre for Speech, Language and the Brain norms (CSLB) <ref type="bibr" target="#b9">(Devereux et al., 2013</ref>) is a recently-released dataset contain- ing semantic properties for 638 concrete concepts produced by human annotators. The CSLB dataset was compiled in the same way as the <ref type="bibr" target="#b21">McRae et al. (2005)</ref> property norms used widely in multi- modal models <ref type="bibr" target="#b29">(Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b28">Roller and Schulte im Walde, 2013)</ref>; we use CSLB be- cause it contains more concepts. For each concept, the proportion of the 30 annotators that produced a given feature can also be employed as a measure of the strength of that feature.</p><p>When encoding the CSLB data in P, we first map properties to lexical forms (e.g. is green becomes green). By directly identifying percep- tual features and linguistic forms in this way, we treat features observed in the perceptual data as (sub)concepts to be acquired via the same multi-modal input streams and stored in the same domain-general memory as the evaluation con- cepts. This design decision in fact corresponds to a view of cognition that is sometimes disputed <ref type="bibr" target="#b12">(Fodor, 1983)</ref>. In future studies we hope to com- pare the present approach to architectures with domain-specific conceptual memories.</p><p>For each concept w in CSLB, we then con- struct a feature bag b(w) by appending lexical forms to b(w) such that the count of each fea- ture word is equal to the strength of that feature for w. Thus, when features are sampled from b(w) to create pseudo-sentences (as detailed pre- viously) the probability of a feature word occur- ring in a sentence reflects feature strength. The array P CSLB then consists of all (w, b(w)) pairs.  <ref type="table">Table 2</ref>: Example concept pairs (with mean con- creteness rating) and free-association scores from the USF dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linguistic Input</head><p>Wikipedia text, split into sentences and with punc- tuation removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>We evaluate the quality of representations by how well they reflect free association scores, an em- pirical measure of cognitive conceptual proxim- ity.  on the intuition that information propagation may occur differently from noun to noun or from noun to verb (because of their distinct structural rela- tionships in sentences). POS-tags are not assigned as part of the USF data, so we draw the noun/verb distinction based on the majority POS-tag of USF concepts in the lemmatized British National Cor- pus ( <ref type="bibr" target="#b20">Leech et al., 1994)</ref>. The abstract/concrete and noun/verb dichotomies yield four distinct con- cept lists. For consistency, the concrete noun list is filtered so that each concrete noun concept w has a perceptual representation b(w) in both P ESP and P CSLB . For the four resulting concept lists C (concrete/abstract, noun/verb), a correspond- ing set of evaluation pairs {(w 1 , w 2 ) ∈ U SF : w 1 , w 2 ∈ C} is extracted (see <ref type="table" target="#tab_2">Table 3</ref> for details).</p><note type="other">Concept Type List Pairs Examples concrete nouns 541 1418 yacht, cup abstract nouns 100 295 fear, respect all nouns 666 1815 fear, cup concrete verbs 50 66 kiss, launch abstract verbs 50 127 differ, obey all verbs 100 221 kiss, obey</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head><p>Our experiments were designed to answer four questions, outlined in the following subsec- tions: (1) Which model architectures perform best at combining information pertinent to multiple modalities when such information exists explicitly (as common for concrete concepts)? (2) Which model architectures best propagate perceptual in- formation to concepts for which it does not exist explicitly (as is common for abstract concepts)? (3) Is it preferable to include all of the perceptual input that can be obtained from a given source, or to filter this input stream in some way? (4) How much perceptual vs. linguistic input is optimal for learning various concept types?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Combining information sources</head><p>To evaluate our approach as a method of in- formation combination we compared its perfor- mance on the concrete noun evaluation set against three alternative methods. The first alternative is simple concatenation of these perceptual vec- tors with linguistic vectors embeddings learned by the <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>model on the Text8</head><p>Corpus. In the second alternative (proposed for multi-modal models by <ref type="bibr" target="#b29">Silberer and Lapata (2012)</ref>), canonical correlation analysis (CCA) ( <ref type="bibr" target="#b14">Hardoon et al., 2004</ref>) was applied to the vec- tors of both modalities. CCA yields reduced- dimensionality representations that preserve un- derlying inter-modal correlations, which are then concatenated. The final alternative, proposed by <ref type="bibr" target="#b4">Bruni et al. (2014)</ref> involves applying Singular Value Decomposition (SVD) to the matrix of con- catenated multi-modal representations, yielding smoothed representations. <ref type="bibr">3</ref> When implementing the concatenation, CCA and SVD methods, we first encoded the percep- tual input directly into sparse feature vectors, with coordinates for each of the 2726 features in CSLB and for each of the 100,000 images in ESP. This sparse encoding matches the approach taken by <ref type="bibr" target="#b29">Silberer and Lapata (2012)</ref>, for CCA and concate- nation, and by  for the ridge re- gression method of propagation (see below).</p><p>We compare these alternatives to our proposed model with α = 1. In The CSLB and ESP models, all training pseudo-sentences are generated from the arrays P CSLB and P ESP respectively. In the models classed as CSLB&amp;ESP, a random choice between P CSLB and P ESP is made every time perceptual input is included (so that the overall quantity of perceptual information is the same).</p><p>As shown in <ref type="figure">Figure 2</ref> (left side), the embed- dings learned by our model achieve a higher cor- relation with the USF data than simple concatena- tion, CCA and SVD regardless of perceptual input source. With the optimal perceptual source (ESP only), for instance, the correlation is 11% higher that the next best alternative method, CCA.</p><p>One possible factor behind this improvement is that, in our model, the learned representations fully integrate the two modalities, whereas for both CCA and the concatenation method each rep- resentation feature (whether of reduced dimension or not) corresponds to a particular modality. This deeper integration may help our architecture to overcome the challenges inherent in information combination such as inter-modality differences in information content and representation sparsity. It is also important to note that <ref type="bibr" target="#b4">Bruni et al. (2014)</ref> ap- 3 CCA was implemented using the CCA package in R. SVD was implemented using SVDLIBC (http:// tedlab.mit.edu/ ˜ dr/SVDLIBC/), with truncation factor k = 1024 as per ( <ref type="bibr" target="#b4">Bruni et al., 2014</ref>). plied their SVD method with comparatively dense perceptual representations extracted from images, whereas our dataset-based perceptual vectors were sparsely-encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Propagating input to abstract concepts</head><p>To test the process of information propagation in our model, we evaluated the learned embeddings of more abstract concepts. We compared our approach with two recently-proposed alternative methods for inferring perceptual features when ex- plicit perceptual information is unavailable.</p><p>Johns and Jones In the method of Johns and Jones (2012), pseudo-perceptual representations for target concepts without a perceptual repre- sentations (uni-modal concepts) are inferred as a weighted average of the perceptual representations of concepts that do have such a representation (bi- modal concepts).</p><p>In the first step of their two-step method, for each uni-modal concept k, a quasi-perceptual rep- resentation is computed as an average of the perceptual representations of bi-modal concepts, weighted by the proximity between each of these concepts and k</p><formula xml:id="formula_2">k p = c∈ ¯ C S(k l , c l ) λ · c p</formula><p>where ¯ C is the set of bi-modal concepts, c p and k p are the perceptual representations for c and k re- spectively, and c l and k l the linguistic representa- tions. The exponent parameter λ reflects the learn- ing rate.</p><p>In step two, the initial quasi-perceptual repre- sentations are inferred for a second time, but with the weighted average calculated over the percep- tual or initial quasi-perceptual representations of all other words, not just those that were originally bi-modal. As with Johns and Jones (2012), we set the learning rate parameter λ to be 3 in the first step and 13 in the second.</p><p>Ridge Regression An alternative, proposed for the present purpose by , uses ridge regression <ref type="bibr" target="#b25">(Myers, 1990)</ref>. Ridge regression is a variant of least squares regression in which a reg- ularization term is added to the training objective to favor solutions with certain properties.</p><p>For bi-modal concepts of dimension n p , we ap- ply ridge regression to learn n p linear functions f i : R n l → R that map the linguistic represen- tations (of dimension n l ) to a particular percep- tual feature i. These functions are then applied together to map the linguistic representations of uni-modal concepts to full quasi-perceptual repre- sentations.</p><p>Following , we take the Euclid- ian l 2 norm of the inferred parameter vector as the regularization term. This ensures that the regres- sion favors lower coefficients and a smoother so- lution function, which should provide better gen- eralization performance than simple linear regres- sion. The objective for learning the f i is then to minimize</p><formula xml:id="formula_3">aX − Y i 2 2 + a 2 2</formula><p>where a is the vector of regression coefficients, X is a matrix of linguistic representations and Y i a vector of the perceptual feature i for the set of bi- modal concepts.</p><p>Comparisons We applied the Johns and Jones method and ridge regression starting from linguis- tic embeddings acquired by the <ref type="bibr" target="#b23">Mikolov et al. (2013)</ref> model on the Text8 Corpus, and concate- nated the resulting pseudo-perceptual and linguis- tic representations. As with the implementation of our model, the perceptual input for these alter- native models was limited to concrete nouns (i.e. concrete nouns were the only bi-modal concepts in the models). <ref type="figure" target="#fig_0">Figure 3</ref> (right side) shows the propagation per- formance of the three models. While the corre- lations overall may seem somewhat low, this is a consequence of the difficulty of modelling the USF data. In fact, the performance of both the language-only model and our multi-modal exten- sion across the concept types (from 0.18 to 0.36) is equal to or higher than previous models evaluated on the same data <ref type="bibr" target="#b11">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b29">Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b31">Silberer et al., 2013)</ref>.</p><p>For learning representations of concrete verbs, our approach achieves a 69% increase in perfor- mance over the next best alternative. The perfor- mance of the model on abstract verbs is marginally inferior to Johns and Jones' method. Neverthe- less, the clear advantage for concrete verbs makes our model the best choice for learning represen- tations of verbs in general, as shown by perfor- mance on the set all verbs, which also includes mixed abstract-concrete pairs.</p><p>Our model is also marginally inferior to alterna- tive approaches in learning representations of ab- stract nouns. However, in this case, no method improves on the linguistic-only baseline. It is possible that perceptual information is simply so removed from the core semantics of these con- cepts that they are best acquired via the linguis- tic medium alone, regardless of learning mecha- nism. The moderately inferior performance of our method in such cases is likely caused by its greater inherent inter-modal dependence compared with methods that simply concatenate uni-modal rep- resentations. When the perceptual signal is of low quality, this greater inter-modal dependence allows the linguistic signal to be obscured.</p><p>The trade-off, however, is generally higher- quality representations when the perceptual signal is stronger, exemplified by the fact that our pro- posed approach outperforms alternatives on pairs generated from both abstract and concrete nouns (all nouns). Indeed, the low performance of the Johns and Jones method on all nouns is strik- ing given that: (a) It performs best on abstract nouns (ρ = .282), and (b) For concrete nouns it reverts to simple concatenation, which also per- forms comparatively well (ρ = .249). The poor performance of the Jobns and Jones method on all nouns must therefore derive its comparisons of mixed abstract-concrete or concrete-abstract pairs. This suggests that the pseudo-perceptual representations inferred by this method for ab- stract concepts method may not be compatible with the directly-encoded perceptual representa- tions of concrete concepts, rendering the compar- ison computation between items of differing con- creteness inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Direct representation vs. propagation</head><p>Although property norm datasets such as the CSLB data typically consist of perceptual fea- ture information for concrete nouns only, image- based datasets such as ESP do contain informa- tion on more abstract concepts, which was omit- ted from the previous experiments. Indeed, im- age banks such as Google Images contain millions of photographs portraying quite abstract concepts, such as love or war. On the other hand, encod- ings or descriptions of abstract concepts are gen- erally more subjective and less reliable than those of concrete concepts <ref type="bibr" target="#b35">(Wiemer-Hastings and Xu, 2005)</ref>. We therefore investigated whether or not it is preferable to include this additional informa- tion as model input or to restrict perceptual input  to concrete nouns as previously.</p><p>Of our evaluation sets, it was possible to con- struct from ESP (and add to P ESP ) representa- tions for all of the concrete verbs, and for ap- proximately half of the abstract verbs and abstract nouns. <ref type="figure" target="#fig_1">Figure 4</ref> (top), shows the performance of a our model trained on all available perceptual in- put versus the model in which the perceptual input was restricted to concrete nouns.</p><p>The results reflect a clear manifestation of the abstract/concrete distinction. Concrete verbs be- have similarly to concrete nouns, in that they can be effectively represented directly from perceptual information sources. The information encoded in these representations is beneficial to the model and increases performance. In contrast, constructing 'perceptual' representations of abstract verbs and abstract nouns directly from perceptual informa- tion sources is clearly counter-productive (to the extent that performance also degrades on the com- bined sets all nouns and all verbs). It appears in these cases that the perceptual input acts to ob- scure or contradict the otherwise useful signal in- ferred from the corpus.</p><p>As shown in the previous section, the inclusion of any form of perceptual input inhibits the learn- ing of abstract nouns. However, this is not the case for abstract verbs. Our model learns higher qual- ity representations of abstract verbs if perceptual input is restricted to concrete nouns than if no per- ceptual input is included at all and when percep- tual input is included for both concrete nouns and abstract verbs. This supports the idea of a grad- ual scale of concreteness: The most concrete con- cepts can be effectively represented directly in the perceptual modality; somewhat more abstract con- cepts cannot be represented directly in the percep- tual modality, but have representations that are im- proved by propagating perceptual input from con- crete concepts via language; and the most abstract concepts are best acquired via language alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Source and quantity of perceptual input</head><p>For different concept types, we tested the effect of varying the proportion of perceptual to linguistic input (the parameter α). Perceptual input was re- stricted to concrete nouns as in Sections 3.1-3.2.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, performance on concrete nouns improves (albeit to a decreasing degree) as α increases. When learning concrete noun rep- resentations, linguistic input is apparently redun- dant if perceptual input is of sufficient quality and quantity. For the other concept types, in each case there is an optimal value for α in the range .5-2, above which perceptual input obscures the linguis- tic signal and performance degrades. The prox- imity of these optima to 1 suggests that for op- timal learning, when a concrete concept is experi- enced approximately equal weight should be given to available perceptual and linguistic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Motivated by the notable prevalence of abstract concepts in everyday language, and their likely importance to flexible, general-purpose represen- tation learning, we have investigated how abstract and concrete representations can be acquired by multi-modal models. In doing so, we presented a simple and easy-to-implement architecture for ac- quiring semantic representations of both types of  concept from linguistic and perceptual input. While neuro-probabilistic language models have been applied to the problem of multi-modal representation learning previously ( <ref type="bibr" target="#b32">Srivastava and Salakhutdinov, 2012;</ref><ref type="bibr" target="#b36">Wu et al., 2013;</ref><ref type="bibr" target="#b30">Silberer and Lapata, 2014</ref>) our model and experiments develop this work in several important ways. First, we ad- dress the problem of learning abstract concepts. By isolating concepts of different concreteness and part-of-speech in our evaluation sets, and sep- arating the processes of information combination and propagation, we demonstrate that the multi- modal approach is indeed effective for some, but perhaps not all, abstract concepts. In addition, our model introduces a clear parallel with human lan- guage learning. Perceptual input is introduced pre- cisely when concrete concepts are 'experienced' by the model in the corpus text, much like a lan- guage learner experiencing concrete entities via sensory perception.</p><p>Taken together, our findings indicate the utility of distinguishing three concept types when learn- ing representations in the multi-modal setting.</p><p>Type I Concepts that can be effectively repre- sented directly in the perceptual modality. For such concepts, generally concrete nouns or con- crete verbs, our proposed approach provides a sim- ple means of combining perceptual and linguistic input. The resulting multi-modal representations are of higher quality than those learned via other approaches, resulting in a performance improve- ment of over 10% in modelling free association.</p><p>Type II Concepts, including abstract verbs, that cannot be effectively represented directly in the perceptual modality, but whose representations can be improved by joint learning from linguis- tic input and perceptual information about related concepts. Our model can effectively propagate perceptual input (exploiting the relations inferred from the linguistic input) from Type I concepts to enhance the representations of Type II concepts above the language-only baseline. Because of the frequency of abstract concepts, such propagation extends the benefit of the multi-modal approach to a far wider range of language than models based solely in the concrete domain.</p><p>Type III Concepts that are more effectively learned via language-only models than multi- modal models, such as abstract nouns. Neither our proposed approach nor alternative propagation methods achieve an improvement in representa- tion quality for these concepts over the language- only baseline. Of course, it is an empirical ques- tion whether a multi-modal approach could ever enhance the representation learning of these con- cepts, one with potential implications for cognitive theories of grounding (a topic of much debate in psychology <ref type="bibr" target="#b13">(Grafton, 2009;</ref><ref type="bibr" target="#b2">Barsalou, 2010)</ref>).</p><p>Additionally, we investigated the optimum type and quantity of perceptual input for learning con- cepts of different types. We showed that too much perceptual input can result in degraded represen- tations. For concepts of type I and II, the op- timal quantity resulted from setting α = 1; i.e. whenever a concrete concept was encountered, the model learned from an equal number of language- based and perception-based examples. While we make no formal claims here, such observations may ultimately provide insight into human lan- guage learning and semantic memory.</p><p>In future we will address the question of whether Type III concepts can ever be enhanced via multi-modal learning, and investigate multi- modal models that optimally learn concepts of each type. This may involve filtering the percep- tual input stream for concepts according to con- creteness, and possibly more elaborate model ar- chitectures that facilitate distinct representational frameworks for abstract and concrete concepts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline. For brevity we include both perceptual input sources ESP and CSLB when comparing means of propagation; results with individual information sources were similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing α on correlation with USF pairs (Spearman ρ) for each concept type. Horizontal dashed lines indicate language-only model baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Details the subsets of USF data used in 
our evaluations, downloadable from our website. 

</table></figure>

			<note place="foot" n="1"> Contributors to the USF dataset (Nelson et al., 2004).</note>

			<note place="foot" n="2"> From http://mattmahoney.net/dc/textdata.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to the Royal Society and St John's College for supporting this research, and to Yoshua Bengio and Diarmuid´ODiarmuid´ Diarmuid´O Séaghdha for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integrating experiential and distributional data to learn semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">463</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Situating abstract concepts. Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thought</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Barsalou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiemer-Hastings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="129" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Grounded cognition: past, present, and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barsalou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="716" to="724" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Frequency and the Emergence of Linguistic Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Joan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul J</forename><surname>Bybee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hopper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Benjamins Publishing</publisher>
			<biblScope unit="volume">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic models of language processing and acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="335" to="344" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Abstract and concrete concepts have structurally different representational frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><forename type="middle">K</forename><surname>Crutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warrington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="615" to="627" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The centre for speech, language and the brain (cslb) concept property norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><forename type="middle">K</forename><surname>Barry J Devereux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billi</forename><surname>Geertzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual information in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The modularity of mind: An essay on faculty psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jerry A Fodor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Embodied cognition and the simulation of action to understand others</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott T Grafton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1156</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="117" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>David R Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shawetaylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A quantitative empirical analysis of the abstract/concrete distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bentz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-modal models for abstract and concrete concept semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceptual inference through global lexical similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael N Jones</forename><surname>Johns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="120" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving multi-modal representations using image dispersion: Why less is sometimes more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the Association for Computational Linguistics. ACL</title>
		<meeting>the annual meeting of the Association for Computational Linguistics. ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Claws4: the tagging of the British National Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Garside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th conference on Computational linguistics</title>
		<meeting>the 15th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="622" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic feature production norms for a large set of living and nonliving things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Mcrae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George S Cree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Seidenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcnorgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised and transfer learning challenge: a deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lavoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warde-Farley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research-Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="97" to="110" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference of Learning Representations</title>
		<meeting>International Conference of Learning Representations<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the international Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Classical and Modern Regression with Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Myers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Duxbury Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Belmont, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The University of South Florida free association, rhyme, and word fragment norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cathy</forename><forename type="middle">L</forename><surname>Douglas L Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas A</forename><surname>Mcevoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="402" to="407" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dual coding theory: Retrospect and current status</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Paivio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">255</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A multimodal LDA model integrating textual, cognitive and visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1146" to="1157" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grounded models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1423" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Models of semantic representation with visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2231" to="2239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Content differences for abstract and concrete concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Wiemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Hastings</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="719" to="736" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online multimodal deep similarity learning with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Multimedia</title>
		<meeting>the 21st ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
