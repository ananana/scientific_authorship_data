<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aligning Knowledge and Text Embeddings by Entity Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Zhong</surname></persName>
							<email>{zhonghp@mail2,wangzh56@mail2,wanhai@mail}.sysu.edu.cn †</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Yat-Sen University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Aligning Knowledge and Text Embeddings by Entity Descriptions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of jointly embedding a knowledge base and a text corpus. The key issue is the alignment model making sure the vectors of entities, relations and words are in the same space. Wang et al. (2014a) rely on Wikipedia anchors , making the applicable scope quite limited. In this paper we propose a new alignment model based on text descriptions of entities, without dependency on anchors. We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description. Extensive experiments show that, the proposed approach consistently performs comparably or even better than the method of Wang et al. (2014a), which is encouraging as we do not use any anchor information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge base embedding has attracted surging interest recently. The aim is to learn continuous vector representations (embeddings) for entities and relations of a structured knowledge base (KB) such as Freebase. Typically it optimizes a global objective function over all the facts in the KB and hence the embedding vector of an entity / relation is expected to encode global information in the KB. It is capable of reasoning missing facts in a KB and helping facts extraction <ref type="bibr" target="#b0">(Bordes et al., 2011;</ref><ref type="bibr" target="#b1">Bordes et al., 2012;</ref><ref type="bibr" target="#b9">Socher et al., 2013;</ref><ref type="bibr" target="#b3">Chang et al., 2013;</ref><ref type="bibr" target="#b12">Wang et al., 2014b;</ref><ref type="bibr" target="#b4">Lin et al., 2015)</ref>.</p><p>Although seeming encouraging, the approaches in the aforementioned literature suffer from two common issues: (1) Embeddings are exclusive to entities/relations within KBs. Computation between KBs and text cannot be handled, which are prevalent in practice. For example, in fact extraction, a candidate value may be just a phrase in text. (2) KB sparsity. The above approaches are only based on structured facts of KBs, and thus cannot work well on entities with few facts.</p><p>An important milestone, the approach of <ref type="bibr" target="#b11">Wang et al. (2014a)</ref> solves issue (1) by jointly embed- ding entities, relations, and words into the same vector space and hence is able to deal with word- s/phrases beyond entities in KBs. The key com- ponent is the so-called alignment model, which makes sure the embeddings of entities, relations, and words are in the same space. Two alignment models are introduced there: one uses entity names and another uses Wikipedia anchors. How- ever, both of them have drawbacks. As reported in the paper, using entity names severely pollutes the embeddings of words. Thus it is not recommended in practice. Using Wikipedia anchors completely relies on the special data source and hence the approach cannot be applied to other customer data.</p><p>To fully address the two issues, this paper pro- poses a new alignment method, aligning by entity descriptions. We only assume some entities in KBs have text descriptions, which almost always holds in practice. We require the embedding of an entity not only fits the structured constraints in KBs but also equals the vector computed from the text description. Meanwhile, if an entity has few facts, the description will provide information for embedding, thus the issue of KB sparsity is also well handled. We conduct extensive experiments on the tasks of triplet classification, link prediction, relational fact extraction, and analogical reasoning to compare with the previous approach ( <ref type="bibr" target="#b11">Wang et al., 2014a)</ref>. Results show that our approach consistently achieves better or comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>TransE This is a representative knowledge em- bedding model proposed by . For a fact (h, r, t) in KBs, where h is the head en- tity, r is the relation, and t is the tail entity, TransE models the relation r as a translation vector r con- necting the embeddings h and t of the two entities, i.e., h + r is close to t. The model is simple, ef- fective and efficient. Most knowledge embedding models thereafter including this paper are variants of this model ( <ref type="bibr" target="#b12">Wang et al., 2014b;</ref><ref type="bibr" target="#b11">Wang et al., 2014a;</ref><ref type="bibr" target="#b4">Lin et al., 2015</ref>).</p><p>Skip-gram This is an efficient word embedding method proposed by <ref type="bibr" target="#b5">Mikolov et al. (2013a)</ref>, which learns word embeddings from word concurrencies in text windows. Without any supervision, it amazingly recovers the semantic relations between words in a vector space such as 'King' − 'Queen' ≈ 'Man' − 'Women'. However, as it is unsupervised, it cannot tell the exact relation between two words. <ref type="bibr" target="#b11">Wang et al. (2014a)</ref> combines knowledge embed- ding and word embedding in a joint framework so that the entities/relations and words are in the same vector space and hence operators like inner product (similarity) between them are meaning- ful. This brings convenience to tasks requiring computation between knowledge bases and text. Meanwhile, jointly embedding utilizes informa- tion from both structured KBs and unstructured text and hence the knowledge embedding and word embedding can be enhanced by each other. Their model is composed of three components: a knowledge model to embed entities and relations, a text model to embed words, and an alignment model to make sure entities/relations and words are in the same vector space. The knowledge model and text model are variants of TransE and Skip-gram respectively. The key component is the alignment model. They introduced two: alignment by entity names and alignment by Wikipedia anchors. (1) Alignment by Entity Names makes a replicate of KB facts but replaces each entity ID with its name string, i.e., the vector of a name phrase is encouraged to equal to the vector of the entity (identified by ID). It has problems with ambiguous entity names and observed polluting word embeddings thus it is not recommended by the authors. (2) Alignment by Wikipedia Anchors replaces the surface phrase v of a Wikipedia anchor with its corresponding Freebase entity e v and defines the likelihood</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge and Text Jointly Embedding</head><formula xml:id="formula_0">L AA = (w,v)∈C,v∈A log Pr(w|e v ) (1)</formula><p>where C is the collection of observed word and context pairs and A refers to the set of all anchors in Wikipedia. Pr(w|e v ) is the probability of the anchor predicting its context word, which takes a form similar to Skip-gram for word embedding. Alignment by anchors works well in both improv- ing knowledge embedding and word embeddings. However, it completely relies on the special data source of Wikipedia anchors and cannot be applied to other general data settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Alignment by Entity Descriptions</head><p>We first describe the settings and notations. Giv- en a knowledge base, i.e., a set of facts (h, r, t), where h, t ∈ E (the set of entities) and r ∈ R (the set of relations). Some entities have text descrip- tions. The description of entity e is denoted as D e . w i,n is the n th word in the description of e i . N i is the length (in words) of the description of e i . We try to learn embeddings e i , r j and w l for each en- tity e i , relation r j and word w l respectively. The vocabulary of words is V. The union vocabulary of entities and words together is I = E ∪ V. In this paper "word(s)" refers to "word(s)/phrase(s)". We follow the jointly embedding framework of ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>), i.e., learning optimal embed- dings by minimizing the following loss</p><formula xml:id="formula_1">L ({e i }, {r j }, {w l }) = L K + L T + L A , (2)</formula><p>where L K , L T and L A are the component loss functions of the knowledge model, text model and alignment model respectively. Our focus is on a new alignment model L A while the knowledge model L K and text model L T are the same as the counterparts in ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>). However, to make the content self-contained, we still need to briefly explain L K and L T .</p><p>Knowledge Model Describes the plausibility of a triplet (h, r, t) by defining</p><formula xml:id="formula_2">Pr(h|r, t) = exp{z(h, r, t)} ˜ h∈I exp{z( ˜ h, r, t)} ,<label>(3)</label></formula><p>where z(h, r, t) = b − 0.5 · h + r − t 2 2 , b = 7 as suggested by <ref type="bibr" target="#b11">Wang et al. (2014a)</ref>. Pr(r|h, t) and Pr(t|h, r) are defined in the same way. The loss function of knowledge model is then defined as</p><formula xml:id="formula_3">L K = − (h,r,t)</formula><p>log Pr(h|r, t) + log Pr(t|h, r) + log Pr(r|h, t)</p><p>Text Model Defines the probability of a pair of words w and v co-occurring in a text window:</p><formula xml:id="formula_5">Pr(w|v) = exp{z(w, v)} ˜ w∈V exp{z( ˜ w, v)}<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">z(w, v) = b − 0.5 · w − v 2 2 .</formula><p>Then the loss function of text model is</p><formula xml:id="formula_7">L T = − (w,v)</formula><p>log Pr(w|v)</p><p>Alignment Model This part is different from <ref type="bibr" target="#b11">Wang et al. (2014a)</ref>. For each word w in the description of entity e, we define Pr(w|e), the conditional probability of predicting w given e:</p><formula xml:id="formula_9">Pr(w|e) = exp{z(e, w)} ˜ w∈V exp{z(e, ˜ w)} ,<label>(7)</label></formula><p>where z(e, w) = b − 0.5 · e − w 2 2 . Notice that e is the same vector of entity e appearing in the knowledge model of Eq. (3).</p><p>We also define Pr(e|w) in the same way by re- vising the normalization term</p><formula xml:id="formula_10">Pr(e|w) = exp{z(e, w)} ˜ e∈E exp{z(˜ e, w)}<label>(8)</label></formula><p>Then the loss function of alignment model is</p><formula xml:id="formula_11">L A = − e∈E w∈De</formula><p>[log Pr(w|e) + log Pr(e|w)]</p><p>Training We use stochastic gradient descent (S- GD) to minimize the overall loss of Eq. <ref type="formula">(2)</ref>, which sequentially updates the embeddings. Negative sampling is used to calculate the normalization items over large vocabularies. We implement a multi-threading version to deal with large data set- s, where memory is shared and lock-free.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on the following tasks: link prediction ( ), triplet clas- sification ( <ref type="bibr" target="#b9">Socher et al., 2013)</ref>, relational fact ex- traction ( , and analogical rea- soning ( <ref type="bibr" target="#b6">Mikolov et al., 2013b</ref>). The last one e- valuates quality of word embeddings. We try  to study whether the proposed alignment mod- el, without using any anchor information, is able to achieve comparable or better performance than alignment by anchors. As to the methods, "Sep- arately" denotes the method of separately embed- ding knowledge bases and text. "Jointly(anchor)" and "Jointly(name)" denote the jointly embedding methods based on Alignment by Wikipedia An- chors and Alignment by Entity Names in (Wang et al., 2014a) respectively. "Jointly(desp)" is the joint embedding method based on alignment by entity descriptions.</p><p>Data For link prediction, FB15K from ( ) is used as the knowledge base. For triplet classification, a large dataset provided by ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>) is used as the knowledge base. Both sets are subsets of Freebase. For all tasks, Wikipedia articles are used as the text cor- pus. As many Wikipedia articles can be mapped to Freebase entities, we regard a Wikipedia arti- cle as the description for the corresponding entity in Freebase. Following the settings in ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>), we apply the same preprocessing step- s, including sentence segmentation, tokenization, and named entity recognition. We combine the consecutive tokens covered by an anchor or iden- tically tagged as "Location/Person/Organization" and regard them as phrases.</p><p>Link Prediction This task aims to complete a fact (h, r, t) in absence of h or t, simply based on h + r − t. We follow the same protocol in ). We directly copy the results of the baseline (TransE) from  and implement "Jointly(anchor)". The results are in <ref type="table" target="#tab_0">Table 1</ref> 10 predictions containing the true entity. Lower "MEAN" and higher "HITS@10" is better. "Raw" and "Filtered" are two settings on processing can- didates ( ).</p><p>We train "Jointly(anchor)" and "Joint- ly(desp)" with the embedding dimension k among {50, 100, 150}, the learning rate α in {0.01, 0.025}, the number of negative examples per positive example c in {5, 10}, the max skip- range s in {5, 10} and traverse the text corpus with only 1 epoch. The best configurations of "Jointly(anchor)" and "Jointly(desp)" are exactly the same: k = 100, α = 0.025, c = 10, s = 5.</p><p>From the results, we observe that: (1) Both jointly embedding methods are much better than the baseline TransE, which demonstrates that ex- ternal textual resources make entity embeddings become more discriminative. Intuitively, "Joint- ly(anchor)" indicates "how to use an entity in tex- t", while "Jointly(desp)" shows "what is the def- inition/meaning of an entity". Both are helpful to distinguish an entity from others. (2) Under the setting of "Raw", "Jointly(desp)" and "Joint- ly(anchor)" are comparable. In other settings "Jointly(desp)" wins.</p><p>Triplet Classification This is a binary classifi- cation task, predicting whether a candidate triplet (h, r, t) is a correct fact or not. It is used in <ref type="bibr" target="#b9">(Socher et al., 2013;</ref><ref type="bibr" target="#b12">Wang et al., 2014b;</ref><ref type="bibr" target="#b11">Wang et al., 2014a</ref>). We follow the same protocol in ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>).</p><p>We train their models via our own implemen- tation on our dataset. The results are in <ref type="table" target="#tab_1">Table 2</ref>. "e-e" means both sides of a triplet (h, r, t) are en- tities in KB, "e-w" means the tail side is a word out of KB entity vocabulary, similarly for "w-e" and "w-w". The best configurations of the mod- els are: k = 150, α = 0.025, c = 10, s = 5 and traversing the text corpus with 6 epochs. The results reveal that: (1) Jointly embedding is indeed effective. Both jointly embedding methods can well handle the cases of "e-w", "w-e" and "w- w", which means the vector computation between entities/relations and words are really meaning- ful. Meanwhile, even the case of "e-e" is also improved. (2) Our method, "Jointly(desp)", out- performs "Jointly(anchor)" on all types of triplets. We believe that the good performance of "Joint- ly(desp)" is due to the appropriate design of the alignment mechanism. Using entity's description information is a more straightforward and effec- tive way to align entity embeddings and word em- beddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relational Fact Extraction</head><p>This task is to ex- tract facts (h, r, t) from plain text.  show that combing scores from TransE and some text side base extractor achieved much bet- ter precision-recall curve compared to the base extractor. <ref type="bibr" target="#b11">Wang et al. (2014a)</ref> confirm this ob- servation and show that jointly embedding brings further encouraging improvement over TransE. In this experiment, we follow the same settings as ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>) to investigate the perfor- mance of our new alignment model. We use the In order to combine the score of a base extrac- tor and the score from embeddings, we only re- serve the testing triplets whose entitites and rela- tions can be mapped to the embeddings learned from the triplet classification experiment. Since both Mintz and MIML are probabilistic models, we use the same method in ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>) to linearly combine the scores.</p><p>The precision-recall curves are plot in <ref type="figure" target="#fig_0">Fig. (1)</ref>. On both base extractors, the jointly embedding methods outperform separate embedding. More- over, "Jointly(desp)" is slightly better than "Joint- ly(anchor)", which is in accordance with the re- sults from the link prediction experiment and the triplet classification experiment.</p><p>Analogical Reasoning This task evaluates the quality of word embeddings ( <ref type="bibr" target="#b6">Mikolov et al., 2013b</ref>). We use the original dataset released by <ref type="bibr" target="#b6">(Mikolov et al., 2013b</ref>) and follow the same evaluation protocol of ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>). For a true analogical pair like ("France", "Paris") and ("China", "Beijing"), we hide "Beijing" and pre- dict it by selecting the word from the vocabu- lary whose vector has highest similarity with the vector of "China" + "Paris" -"France". We use the word embeddings learned for the triplet classification experiment and conduct the analogi- cal reasoning experiment for "Skip-gram", "Joint- ly(anchor)", "Jointly(name)" and "Jointly(desp)".</p><p>Results are presented in <ref type="table" target="#tab_2">Table 3</ref>. "Acc" is the accuracy of the predicted word. "HITS@10" is the accuracy of the top 10 candidates containing the ground truth. The evaluation analogical pairs are organized into two groups, "Words" and "Phras- es", by whether an analogical pair contains phras- es (i.e., multiple words). From the table we ob- serve that: (1) Both "Jointly(anchor)" and "Joint- ly(desp)" outperform "Skip-gram". (2) "Joint- ly(desp)" achieves the best results, especially for the case of "Phrases". Both "Jointly(anchor)" and "Skip-gram" only consider the context of words, while "Jointly(desp)" not only consider the con- text but also use the whole document to disam- biguate words. Intuitively, the whole document is also a valuable resource to disambiguate word- s. (3) We further verify that "Jointly(name)", i.e., using entity names for alignment, indeed pollutes word embeddings, which is consistent with the re- ports in ( <ref type="bibr" target="#b11">Wang et al., 2014a</ref>).</p><p>The above four experiments are consisten- t in results: without using any anchor informa- tion, alignment by entity description is able to achieve better or comparable performance, com- pared to alignment by Wikipedia anchors pro- posed by <ref type="bibr" target="#b11">Wang et al. (2014a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a new alignment model based on enti- ty descriptions for jointly embedding a knowledge base and a text corpus. Compared to the method of alignment using Wikipedia anchors <ref type="bibr" target="#b11">Wang et al. (2014a)</ref>, our method has no dependency on special data sources of anchors and hence can be applied to any knowledge bases with text descriptions for entities. Extensive experiments on four prevalen- t tasks to evaluate the quality of knowledge and word embeddings produce very consistent results: our alignment model achieves better or compara- ble performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>.Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision-recall curves for relation extraction. (a) Mintz (Mintz et al., 2009) as base extractor (b) MIML (Surdeanu et al., 2012) as base extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Link prediction results.</head><label>1</label><figDesc></figDesc><table>Metric 
MEAN 
HITS@10 
Raw Filtered Raw Filtered 

TransE 
243 
125 
34.9 
47.1 
Jointly(anchor) 
166 
47 
49.9 
72.0 

Jointly(desp) 
167 
39 
51.7 
77.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Triplet classification results.</head><label>2</label><figDesc></figDesc><table>Type 
e -e w -e e -w w -w 
all 

Separately 
94.0 51.7 
51.0 
69.0 
73.6 
Jointly(anchor) 95.2 65.3 
65.1 
76.2 
79.9 

Jointly(desp) 
96.1 66.7 
66.1 
76.4 
80.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Analogical reasoning results</head><label>3</label><figDesc></figDesc><table>Metric 
Words 
Phrases 
Acc. Hits@10 Acc. Hits@10 

Skip-gram 
67.4 
86.7 
22.0 
63.6 
Jointly(anchor) 69.4 
87.7 
26.2 
68.1 
Jointly(name) 
44.5 
69.7 
11.5 
46.0 

Jointly(desp) 
69.3 
88.3 
49.0 
86.5 

same public dataset NYT+FB, released by Riedel 
et al. (2010) and used in (Weston et al., 2013) and 
(Wang et al., 2014a). We use Mintz (Mintz et al., 
2009) and MIML (Surdeanu et al., 2012) as our 
base extractors. 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 25th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7973</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
