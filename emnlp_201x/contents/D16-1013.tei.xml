<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Language Comprehension with the EpiReader</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
							<email>k.suleman @maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Language Comprehension with the EpiReader</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="128" to="137"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model&apos;s response to the questions. EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that EpiReader sets a new state-of-the-art on the CNN and Children&apos;s Book Test benchmarks, outperforming previous neural models by a significant margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When humans reason about the world, we tend to for- mulate a variety of hypotheses and counterfactuals, then test them in turn by physical or thought exper- iments. The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations: if several theories are consistent with the observed data, retain them all until more data is observed. In this pa- per, we argue that the same principle can be applied to machine comprehension of natural language. We propose a deep neural comprehension model, trained end-to-end, that we call EpiReader.</p><p>Comprehension of natural language by machines, at a near-human level, is a prerequisite for an ex- tremely broad class of useful applications of artificial intelligence. Indeed, most human knowledge is col- lected in the natural language of text. Machine com- prehension (MC) has therefore garnered significant attention from the machine learning research commu- nity. Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage, then scoring a system's answers to those questions. Such tests are objectively gradable and may assess a range of abilities, from basic understand- ing to causal reasoning to inference ( <ref type="bibr">Richardson et al., 2013</ref>).</p><p>In the past year, two large-scale MC datasets have been released: the CNN/Daily Mail corpus, consist- ing of news articles from those outlets ( <ref type="bibr" target="#b5">Hermann et al., 2015)</ref>, and the Children's Book Test (CBT), consisting of short excerpts from books available through Project Gutenberg ( <ref type="bibr">Hill et al., 2016)</ref>. The size of these datasets (on the order of 10 5 distinct questions) makes them amenable to data-intensive deep learning techniques. Both corpora use Cloze- style questions <ref type="bibr" target="#b6">(Taylor, 1953)</ref>, which are formulated by replacing a word or phrase in a given sentence with a placeholder token. The task is then to find the answer that "fills in the blank".</p><p>In tandem with these corpora, a host of neu- ral machine comprehension models has been devel- oped ( <ref type="bibr" target="#b9">Weston et al., 2015b;</ref><ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr">Hill et al., 2016;</ref><ref type="bibr">Kadlec et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2016)</ref>. We compare EpiReader to these earlier mod- els through training and evaluation on the CNN and CBT datasets. <ref type="bibr">1</ref> EpiReader factors into two components. The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text; we call this the Extractor. The sec- ond component reranks the proposed answers based on deeper semantic comparisons with the text; we call this the Reasoner. We can summarize this pro- cess as Extract → Hypothesize → Test 2 . The se- mantic comparisons implemented by the Reasoner are based on the concept of recognizing textual en- tailment (RTE) ( <ref type="bibr" target="#b3">Dagan et al., 2006</ref>), also known as natural language inference. This process is computa- tionally demanding. Thus, the Extractor serves the important function of filtering a large set of poten- tial answers down to a small, tractable set of likely candidates for more thorough testing. The two-stage process is an analogue of structured prediction cas- cades <ref type="bibr">(Weiss and Taskar, 2010)</ref>, wherein a sequence of increasingly complex models progressively filters the output space in order to trade off between model complexity and limited computational resources. We demonstrate that this cascade-like framework is appli- cable to machine comprehension and can be trained end-to-end with stochastic gradient descent.</p><p>The Extractor follows the form of a pointer net- work ( <ref type="bibr" target="#b7">Vinyals et al., 2015)</ref>, and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question. This approach was used (on its own) for question answering with the Attention Sum Reader ( <ref type="bibr">Kadlec et al., 2016)</ref>. The Extractor outputs a small set of answer candidates along with their estimated probabilities of correct- ness. The Reasoner forms hypotheses by inserting the candidate answers into the question, then esti- mates the concordance of each hypothesis with each sentence in the supporting text. We use these esti- mates as a measure of the evidence for a hypothesis, and aggregate evidence over all sentences. In the end, we combine the Reasoner's evidence with the Extractor's probability estimates to produce a final ranking of the answer candidates. This paper is organized as follows. In Section 2 we formally define the problem to be solved and give some background on the datasets used in our tests. In Section 3 we describe EpiReader, focusing on its two components and how they combine. Section 4 discusses related work, and Section 5 details our experimental results and analysis. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem definition, notation, datasets</head><p>EpiReader's task is to answer a Cloze-style question by reading and comprehending a supporting passage of text. The training and evaluation data consist of tuples (Q, T , a * , A), where Q is the question (a se- quence of words {q 1 , ...q |Q| }), T is the text (a se- quence of words {t 1 , ..., t |T | }), A is a set of possible answers {a 1 , ..., a |A| }, and a * ∈ A is the correct an- swer. All words come from a vocabulary V , and A ⊂ T . In each question, there is a placeholder token indicating the missing word to be filled in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>CNN This corpus is built using articles scraped from the CNN website. The articles themselves form the text passages, and questions are generated syn- thetically from short summary statements that ac- company each article. These summary points are (presumably) written by human authors. Each ques- tion is created by replacing a named entity in a sum- mary point with a placeholder token. All named entities in the articles and questions are replaced with anonymized tokens that are shuffled for each (Q, T ) pair. This forces the model to rely only on the text, rather than learning world knowledge about the en- tities during training. The CNN corpus (henceforth CNN) was presented by <ref type="bibr" target="#b5">Hermann et al. (2015)</ref>.</p><p>Children's Book Test This corpus is constructed similarly to CNN, but from children's books avail- able through Project Gutenberg. Rather than articles, the text passages come from book excerpts of 20 sentences. Since no summaries are provided, a ques- tion is generated by replacing a single word in the next (i.e. 21st) sentence. The corpus distinguishes questions based on the type of word that is replaced: named entity, common noun, verb, or preposition. Like <ref type="bibr">Kadlec et al. (2016)</ref>, we focus only on the first two classes since <ref type="bibr">Hill et al. (2016)</ref> showed that stan-dard LSTM language models already achieve human- level performance on the latter two. Unlike in the CNN corpora, named entities are not anonymized and shuffled in the Children's Book Test (CBT). CBT was presented by <ref type="bibr">Hill et al. (2016)</ref>.</p><p>The different methods of construction for ques- tions in each corpus mean that CNN and CBT assess different aspects of comprehension. The summary points of CNN are a condensed paraphrasing of infor- mation in the text; thus, determining the correct an- swer relies mostly on recognizing textual entailment. On the other hand, CBT is about story prediction. It is a comprehension task insofar as comprehension is likely necessary for story prediction, but comprehen- sion alone may not be sufficient. Indeed, there are some CBT questions that are unanswerable given the preceding context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EpiReader</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview and intuition</head><p>EpiReader explicitly leverages the observation that the answer to a question is often a word or phrase from the related text passage. This condition holds for the CNN and CBT datasets. EpiReader's first module, the Extractor, can thus select a small set of candidate answers by pointing to their locations in the supporting passage. This mechanism is detailed in Section 3.2, and was used previously by the At- tention Sum Reader ( <ref type="bibr">Kadlec et al., 2016</ref>). Pointing to candidate answers removes the need to apply a softmax over the entire vocabulary as in <ref type="bibr" target="#b9">Weston et al. (2015b)</ref>, which is computationally more costly and uses less-direct information about the context of a predicted answer in the supporting text.</p><p>EpiReader's second module, the Reasoner, begins by formulating hypotheses using the extracted answer candidates. It generates each hypothesis by replacing the placeholder token in the question with an answer candidate. Cloze-style questions are ideally-suited to this process, because inserting the correct answer at the placeholder location produces a well-formed, grammatical statement. Thus, the correct hypothesis will "make sense" to a language model. The Reasoner then tests each hypothesis individu- ally. It compares a hypothesis to the text, split into sentences, to measure textual entailment, and then ag- gregates entailment over all sentences. This compu- tation uses a pair of convolutional encoder networks followed by a recurrent neural network. The convo- lutional encoders generate abstract representations of the hypothesis and each text sentence; the recurrent network estimates and aggregates entailment. This is described formally in Section 3.3. The end-to- end EpiReader model, combining the Extractor and Reasoner modules, is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Throughout our model, words will be represented with trainable embeddings ( <ref type="bibr" target="#b0">Bengio et al., 2000</ref>). We represent these embeddings using a matrix W ∈ R D×|V | , where D is the embedding dimension and |V | is the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Extractor</head><p>The Extractor is a Pointer Network ( <ref type="bibr" target="#b7">Vinyals et al., 2015)</ref>. It uses a pair of bidirectional recurrent neural networks, f (θ T , T) and g(θ Q , Q), to encode the text passage and the question. θ T represents the param- eters of the text encoder, and T ∈ R D×N is a ma- trix representation of the text (comprising N words), whose columns are individual word embeddings t i . Likewise, θ Q represents the parameters of the ques- tion encoder, and Q ∈ R D×N Q is a matrix represen- tation of the question (comprising N Q words), whose columns are individual word embeddings q j .</p><p>We use a recurrent neural network with gated recur- rent units (GRU) ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) to scan over the columns (i.e. word embeddings) of the input ma- trix. We selected the GRU because it is computation- ally simpler than Long Short-Term Memory (Hochre- iter and Schmidhuber, 1997), while still avoiding the problem of vanishing/exploding gradients often encountered when training recurrent networks.</p><p>The GRU's hidden state gives a representation of the ith word conditioned on preceding words. To include context from proceeding words, we run a second GRU over T in the reverse direction. We refer to the combination as a biGRU. At each step the biGRU outputs two d-dimensional encoding vectors, one for the forward direction and one for the back- ward direction. We concatenate these to yield a vector f (t i ) ∈ R 2d . The question biGRU is similar, but we form a single-vector representation of the question by concatenating the final forward state with the final backward state, which we denote g(Q) ∈ R 2d .</p><p>As in <ref type="bibr">Kadlec et al. (2016)</ref>, we model the probabil- ity that the ith word in text T answers question Q  using</p><formula xml:id="formula_0">s i ∝ exp(f (t i ) · g(Q)),<label>(1)</label></formula><p>which takes the inner product of the text and question representations followed by a softmax. In many cases unique words repeat in a text. Therefore, we compute the total probability that word w is the correct answer using a sum:</p><formula xml:id="formula_1">P (w | T , Q) = i: t i =w s i .<label>(2)</label></formula><p>This probability is evaluated for each unique word in T . Finally, the Extractor outputs the set {p 1 , ..., p K } of the K highest word probabilities from 2, along with the corresponding set of K most probable an- swer words {â 1 , ..., ˆ a K }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Reasoner</head><p>The indicial selection involved in gathering {â 1 , ..., ˆ a K }, which is equivalent to a K-best arg max, is not a continuous function of its inputs. To construct an end-to-end differentiable model, we bypass this by propagating the probability estimates of the Extractor directly through the Reasoner.</p><p>The Reasoner begins by inserting the answer can- didates, which are single words or phrases, into the question sequence Q at the placeholder location. This forms K hypotheses {H 1 , ..., H K }. At this point, we consider each hypothesis to have proba- bility p(H k ) ≈ p k , as estimated by the Extractor. The Reasoner updates and refines this estimate.</p><p>The hypotheses represent new information in some sense-they are statements we have constructed, al- beit from words already present in the question and text passage. The Reasoner estimates entailment be- tween the statements H k and the passage T . We denote these estimates using e k = F (H k , T ), with F to be defined. We start by reorganizing T into a sequence of N s sentences: T = {t 1 , . . . , t N } → {S 1 , . . . , S Ns }, where S i is a sequence of words.</p><p>For each hypothesis and each sentence of the text, Reasoner input consists of two matrices: S i ∈ R D×|S i | , whose columns are the embedding vectors for each word of sentence S i , and H k ∈ R D×|H k | , whose columns are the embedding vectors for each word in the hypothesis H k . The embedding vectors themselves come from matrix W, as before.</p><p>These matrices feed into a convolutional architec- ture based on that of <ref type="bibr">Severyn and Moschitti (2016)</ref>. The architecture first augments S i with matrix M ∈ R 2×|S i | . The first row of M contains the inner prod- uct of each word embedding in the sentence with the candidate answer embedding, and the second row contains the maximum inner product of each sen- tence word embedding with any word embedding in the question. These word-matching features were inspired by similar approaches in <ref type="bibr" target="#b8">Wang and Jiang (2016)</ref> and <ref type="bibr" target="#b6">Trischler et al. (2016)</ref>, where they were shown to improve entailment estimates.</p><p>The augmented S i is then convolved with a bank of filters F S ∈ R (D+2)×m , while H k is convolved with filters F H ∈ R D×m , where m is the convolu- tional filter width. We add a bias term and apply a nonlinearity (we use a ReLU) following the convo- lution. Maxpooling over the sequences then yields two vectors: the representation of the text sentence, r S i ∈ R N F , and the representation of the hypothesis, r H k ∈ R N F , where N F is the number of filters.</p><p>We then compute a scalar similarity score between these vector representations using the bilinear form</p><formula xml:id="formula_2">ς = r T S i Rr H k ,<label>(3)</label></formula><p>where R ∈ R N F ×N F is a matrix of trainable parame- ters. We then concatenate the similarity score with the sentence and hypothesis representations to get a vector, x ik = [ς; r S i ; r H k ] T . There are more pow- erful models of textual entailment that could have been used in place of this convolutional architecture. We adopted the approach of Severyn and Moschitti (2016) for computational efficiency.</p><p>The resulting sequence of N s vectors feeds into yet another GRU for synthesis, of hidden dimension d S . Intuitively, it is often the case that evidence for a particular hypothesis is distributed over several sentences. For instance, if we hypothesize that the football is in the park, perhaps it is because one sen- tence tells us that Sam picked up the football and a later one tells us that Sam ran to the park. <ref type="bibr">3</ref> The Rea- soner synthesizes distributed information by running a GRU network over x ik , where i indexes sentences and represents the step dimension. <ref type="bibr">4</ref> The final hidden state of the GRU is fed through a fully-connected layer, yielding a single scalar y k . This value repre- sents the collected evidence for H k based on the text. In practice, the Reasoner processes all K hypotheses in parallel and the estimated entailment of each is normalized by a softmax, e k ∝ exp(y k ).</p><p>As pointed out in <ref type="bibr">Kadlec et al. (2016)</ref>, it is a strength of the pointer framework that it does not blend the representations that are being attended. Contrast this with typical attention mechanisms where such a blended representation is used down- stream to make similarity comparisons with, e.g., output vectors.</p><p>Differentiable attention mechanisms (as in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, for example) typically blend in- ternal representations together through a weighted sum, then use this 'blend' downstream for similarity comparisons. The pointer framework does not resort to this blending; <ref type="bibr">Kadlec et al. (2016)</ref> explain that this is an advantage, since in comprehension tasks the goal is to select the correct answer among semanti- cally similar candidates and more exact matching is necessary. The reranking function performed by the Reasoner entails this advantage, by examining the separate hypotheses individually without blending.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combining components</head><p>Finally, we combine the evidence from the Reasoner with the probability from the Extractor. We com- pute the output probability of each hypothesis, π k , according to the product</p><formula xml:id="formula_3">π k ∝ e k p k ,<label>(4)</label></formula><p>whereby the evidence of the Reasoner can be inter- preted as a correction to the Extractor probabilities, applied as an additive shift in log-space. We experi- mented with other combinations of the Extractor and Reasoner, but we found the multiplicative approach to yield the best performance. After combining results from the Extractor and Reasoner to get the probabilities π k described in Eq. 4, we optimize the parameters of the full EpiReader to minimize a cost comprising two terms, L E and L R . The first term is a standard negative log- likelihood objective, which encourages the Extractor to rate the correct answer above other answers. This is the same loss term used in <ref type="bibr">Kadlec et al. (2016)</ref>. It is given by:</p><formula xml:id="formula_4">L E = E (Q,T ,a * ,A) [− log P (a * | T , Q)] ,<label>(5)</label></formula><p>where P (a * | T , Q) is as defined in Eq. 2, and a * denotes the true answer. The second term is a margin- based loss on the end-to-end probabilities π k . We define π * as the probability π k corresponding to the true answer word a * . This term is given by:</p><formula xml:id="formula_5">L R = E (Q,T ,a * ,A)   ˆ a i ∈{â 1 ,...,ˆ a K }\a * [γ − π * + π ˆ a i ] +   ,<label>(6)</label></formula><p>where γ is a margin hyperparameter, {â 1 , ..., ˆ a K } is the set of K answers proposed by the Extractor, and <ref type="bibr">[x]</ref> + indicates truncating x to be non-negative. Intuitively, this loss says that we want the end-to-end probability π * for the correct answer to be at least γ larger than the probability π ˆ a i for any other answer proposed by the Extractor. During training, the cor- rect answer is occasionally missed by the Extractor, especially in early epochs. We counter this issue by forcing the correct answer into the top K set while training. When evaluating the model on validation and test examples we rely fully on the top K answers proposed by the Extractor.</p><p>To get the final loss term L ER , minus 2 regular- ization terms on the model parameters, we take a weighted combination of L E and L R :</p><formula xml:id="formula_6">L ER = L E + λL R ,<label>(7)</label></formula><p>where λ is a hyperparameter for weighting the rela- tive contribution of the Extractor and Reasoner losses. In practice, we found that λ should be fairly large (e.g., 10 &lt; λ &lt; 100). Empirically, we observed that the output probabilities from the Extractor of- ten peak and saturate the first softmax; hence, the Extractor term can come to dominate the Reasoner term without the weight λ (we discuss the Extractor's propensity to overfit in Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The Impatient and Attentive Reader models were proposed by <ref type="bibr" target="#b5">Hermann et al. (2015)</ref>. The Attentive Reader applies bidirectional recurrent encoders to the question and supporting text. It then uses the atten- tion mechanism described in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> to compute a fixed-length representation of the text based on a weighted sum of the text encoder's output, guided by comparing the question representation to each location in the text. Finally, a joint representa- tion of the question and supporting text is formed by passing their separate representations through a feed- forward MLP and an answer is selected by comparing the MLP output to a representation of each possible answer. The Impatient Reader operates similarly, but computes attention over the text after processing each consecutive word of the question. The two models achieved similar performance on the CNN and Daily Mail datasets. Memory Networks were first proposed by <ref type="bibr" target="#b9">Weston et al. (2015b)</ref> and later applied to machine compre- hension by <ref type="bibr">Hill et al. (2016)</ref>. This model builds fixed-length representations of the question and of windows of text surrounding each candidate answer, then uses a weighted-sum attention mechanism to combine the window representations. As in the previ- ous Readers, the combined window representation is then compared with each possible answer to form a prediction about the best answer. What distinguishes Memory Networks is how they construct the ques- tion and text window representations. Rather than a recurrent network, they use a specially-designed, trainable transformation of the word embeddings.</p><p>Most of the details for the very recent AS Reader are provided in the description of our Extractor mod- ule in Section 3.2, so we do not summarize it further here. This model ( <ref type="bibr">Kadlec et al., 2016</ref>) set the previ- ous state-of-the-art on the CBT dataset.</p><p>During the write-up of this paper, another very re- cent model came to our attention. <ref type="bibr" target="#b2">Chen et al. (2016)</ref> propose using a bilinear term instead of a tanh layer to compute the attention between question and pas- sage words, and also uses the attended word encod- ings for direct, pointer-style prediction as in <ref type="bibr">Kadlec et al. (2016)</ref>. This model set the previous state-of-the- art on the CNN dataset. However, this model used embedding vectors pretrained on a large external cor- pus ( <ref type="bibr" target="#b5">Pennington et al., 2014)</ref>.</p><p>EpiReader borrows ideas from other models as well. The Reasoner's convolutional architecture is based on <ref type="bibr">Severyn and Moschitti (2016)</ref> and . Our use of word-level matching was in- spired by the Parallel-Hierarchical model of <ref type="bibr" target="#b6">Trischler et al. (2016)</ref> and the natural language inference model of <ref type="bibr" target="#b8">Wang and Jiang (2016)</ref>. Finally, the idea of formu- lating and testing hypotheses for question-answering was used to great effect in IBM's DeepQA system for Jeopardy! <ref type="bibr" target="#b4">(Ferrucci et al., 2010</ref>) (although that was a more traditional information retrieval pipeline rather than an end-to-end neural model), and also resembles the framework of structured prediction cascades <ref type="bibr">(Weiss and Taskar, 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation and training details</head><p>To train our model we used stochastic gradient de- scent with the ADAM optimizer ( <ref type="bibr">Kingma and Ba, 2015)</ref>, with an initial learning rate of 0.001. The word embeddings were initialized randomly, draw- ing from the uniform distribution over [−0.05, 0.05). We used batches of 32 examples, and early stopping with a patience of 2 epochs. Our model was imple- mented in Theano ( <ref type="bibr" target="#b0">Bergstra et al., 2010</ref>) using the Keras framework <ref type="bibr" target="#b3">(Chollet, 2015)</ref>.</p><p>The results presented below for EpiReader were obtained by searching over a small grid of hyperpa- rameter settings. We selected the model that, on each dataset, maximized accuracy on the validation set, then evaluated it on the test set. We record the best settings for each dataset in <ref type="table" target="#tab_1">Table 1</ref>. As has been done previously, we train separate models on CBT's named entity (CBT-NE) and common noun (CBT- CN) splits. All our models used 2 -regularization at 0.001, λ = 50, and γ = 0.04. We did not use dropout but plan to investigate its effect in the future. <ref type="bibr">Hill et al. (2016)</ref> and <ref type="bibr">Kadlec et al. (2016)</ref> also present results for ensembles of their models. Time did not permit us to generate an ensemble of EpiReaders on the CNN dataset so we omit those measures; how- ever, EpiReader ensembles (of seven models) demon- strated improved performance on the CBT dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In <ref type="table">Table 5</ref>.2, we compare the performance of EpiReader against that of several baselines, on the validation and test sets of the CBT and CNN corpora. We measure EpiReader performance at the output of both the Extractor and the Reasoner. EpiReader achieves state-of-the-art performance across the board for both datasets. On CNN, we score 2.2% higher on test than the best previous model of <ref type="bibr" target="#b2">Chen et al. (2016)</ref>. Interestingly, an analysis of the CNN dataset by <ref type="bibr" target="#b2">Chen et al. (2016)</ref> suggests that approxi- mately 25% of the test examples contain coreference errors or questions which are "ambiguous/hard" even for a human analyst. If this estimate is accurate, then EpiReader, achieving an absolute test accuracy of 74.0%, is operating close to expected human perfor- mance. On the other hand, ambiguity is unlikely to be distributed evenly over entities, so a good model should be able to perform at better-than-chance levels even on questions where the correct answer is uncer- tain. If, on the 25% of "noisy" questions, the model can shift its hit rate from, e.g., 1/10 to 1/3, then there is still a fair amount of performance to gain.   On CBT-CN our single model scores 4.0% higher than the previous best of the AS Reader. The improve- ment on CBT-NE is more modest at 1.1%. Looking more closely at our CBT-NE results, we found that the validation and test accuracies had relatively high variance even in late epochs of training. We discov- ered that many of the validation and test questions were asked about the same named entity, which may explain this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head><p>We measure the contribution of several components of the Reasoner by ablating them. Results on the validation set of CBT-CN are presented in <ref type="table" target="#tab_4">Table 3</ref>. The word-match scores (cosine similarities stored in the first two rows of matrix M, see Section 3.3) make a contribution of 1.2% to the validation performance, indicating that they are useful. Similarly, the bilinear similarity score ς, which is passed to the final GRU network, contributes 1.5%.</p><p>Removing the Reasoner altogether reduces our model to the AS Reader, whose results we have reproduced to within negligible difference. Aside from achieving state-of-the-art results at its final out- put, the EpiReader framework gives a boost to its Extractor component through the joint training pro- cess. This can be seen by referring back to <ref type="table">Table 5</ref>.2, wherein we also provide accuracy scores evaluated at the output of the Extractor. These are all higher than the analogous scores reported for the AS Reader. Based on our own work with that model, we found it to overfit the training set rapidly and significantly, achieving training accuracy scores upwards of 98% after only 2 epochs. We suspect that the Reasoner module had a regularizing effect on the Extractor, but leave the confirmation for future work.</p><p>Although not exactly an ablation, we also tried bypassing the Reasoner's convolutional encoders al- together, along with the word-match scores and the bilinear similarity. This was done as follows: from the Extractor, we pass to the Reasoner's final GRU (i) the bidirectional hidden representation of the ques- tion; (ii) the bidirectional hidden representations of the end of each story sentence (recall that the Rea- soner operates on sentence representations). Thus, we reuse (parts of) the original biGRU encodings. This cuts down on the number of model parameters and on the length of the graph through which gra- dients must flow, potentially providing a stronger learning signal to the initial encoders. We found that this change yielded a relatively small reduction in per- formance on CBT-CN, perhaps for the reasons just discussed-only 0.5%, as given in the final line of Mr. Blacksnake grinned and started after him, not very fast because he knew that he wouldn't have to run very fast to catch old Mr. Toad, and he thought the exercise would do him good. … "Still, the green meadows wouldn't be quite the same without old Mr. Toad. I should miss him if anything happened to him. I suppose it would be partly my fault, too, for if I hadn't pulled over that piece of bark, he probably would have stayed there the rest of the day and been safe."</p><p>QUESTION: "Maybe he won't meet Mr. XXXXX," said a little voice inside of Jimmy.</p><p>EXTRACTOR: Toad REASONER: Blacksnake</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>18.</p><p>21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.">20.</head><p>Figure 2: An abridged example from CBT-NE demonstrating corrective reranking by the Reasoner. <ref type="table" target="#tab_4">Table 3</ref>. This suggests that competitive performance may be achieved with other, simpler architectures for the Reasoner's entailment system and this will be the subject of future research.</p><p>An analysis by <ref type="bibr">Kadlec et al. (2016)</ref> indicates that the trained AS Reader includes the correct answer among its five most probable candidates on approxi- mately 95% of test examples for both datasets. We verified that our Extractor achieved a similar rate, and of course this is vital for performance of the full system, since the Reasoner cannot recover when the correct answer is not among its inputs.</p><p>Our results show that the Reasoner often corrects erroneous answers from the Extractor. <ref type="figure">Figure 2</ref> gives an example of this correction. In the text passage, from CBT-NE, Mr. Blacksnake is pursuing Mr. Toad, presumably to eat him. The dialogue in the question sentence refers to both: Mr. Toad is its subject, re- ferred to by the pronoun "he", and Mr. Blacksnake is its object. In the preceding sentences, it is clear (to a human) that Jimmy is worried about Mr. Toad and his potential encounter with Mr. Blacksnake. The Extractor, however, points most strongly to "Toad", possibly because he has been referred to most re- cently. The Reasoner corrects this error and selects "Blacksnake" as the answer. This relies on a deeper understanding of the text. The named entity can, in this case, be inferred through an alternation of the entities most recently referred to. This kind alterna- tion is typical of dialogues, when two actors interact in turns. The Reasoner can capture this behavior because it examines sentences in sequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The complete EpiReader framework. The Extractor is above, the Reasoner below. Propagating the Extractor's probability estimates forward and combining them with the Reasoner's entailment estimates renders the model end-to-end differentiable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Hyperparameter settings for best EpiReaders. D is the embedding dimension, d is the hidden dimension in the Extractor GRUs, K is the number of candidates to consider, m is the filter width, NF is the number of filters, and dS is the hidden dimension in the Reasoner GRU.</head><label>1</label><figDesc></figDesc><table>Hyperparameters 

Dataset 
D 
d 
K m N F d S 

CBT-NE 300 128 5 
3 
16 32 
CBT-CN 300 128 5 
3 
32 32 
CNN 
384 256 10 3 
32 32 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Model comparison on the CBT and CNN datasets. Results marked with 1 are from Hill et al. (2016), with 2 are from 

Kadlec et al. (2016), with 3 are from Hermann et al. (2015), and with 4 are from Chen et al. (2016). 

Ablated component 
Validation accuracy (%) 

-
71.5 
Word-match scores 
70.3 
Bilinear similarity 
70.0 
Reasoner 
68.7 

Convolutional encoders 
71.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Ablation study on CBT-CN validation set. 

</table></figure>

			<note place="foot" n="1"> The CNN and Daily Mail datasets were released together and have the same form. The Daily Mail dataset is significantly larger; therefore, models consistently score higher when trained/tested on it. 2 The Extractor performs extraction, while the Reasoner both hypothesizes and tests.</note>

			<note place="foot" n="3"> This example is characteristic of the bAbI dataset (Weston et al., 2015a). 4 Note a benefit of forming the hypothesis: it renders bidirectional aggregation unnecessary, since knowing both the question and the putative answer &quot;closes the loop&quot; the same way that a bidirectional encoding would.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented the novel EpiReader framework for machine comprehension and evaluated it on two large, complex datasets: CNN and CBT. Our model achieves state-of-the-art results on these corpora, out-performing all previous approaches. In future work, we plan to test our framework with alternative models for natural language inference (e.g., <ref type="bibr" target="#b8">Wang and Jiang (2016)</ref>), and explore the effect of pretraining such a model specifically on an inference task.</p><p>As a general framework that consists in a two-stage cascade, EpiReader can be implemented using a variety of mechanisms in the Extractor and Reasoner stages. We have demonstrated that this cascade-like framework is applicable to machine comprehension and can be trained end-to-end. As more powerful machine comprehension models inevitably emerge, it may be straightforward to boost their performance using EpiReader's structure.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
	<note>Proc. of SciPy</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn / daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet ; Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
	<note>Dagan et al.2006</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ferrucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations. ICLR. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and Jürgen Schmidhuber</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<idno>arXiv:1604.01178</idno>
	</analytic>
	<monogr>
		<title level="m">Text understanding with the attention sum reader network</title>
		<editor>Kingma and Ba2015] Diederik Kingma and Jimmy Ba</editor>
		<meeting><address><addrLine>Matthew Richardson, Christopher JC Burges, and Erin Renshaw</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-01" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>EMNLP. Severyn and Moschitti2016] Aliaksei Severyn and Alessandro Moschitti. 2016. Modeling relational information in question-answer pairs with convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A parallel-hierarchical model for machine comprehension on sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson L Taylor ; Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1953" />
		</imprint>
	</monogr>
	<note>Cloze procedure: a new tool for measuring readability</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2674" to="2682" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang2016] Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<editor>NAACL. [Weiss and Taskar2010] David J Weiss and Benjamin Taskar</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="916" to="923" />
		</imprint>
	</monogr>
	<note>Structured prediction cascades</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<editor>Weston et al.2015b] Jason Weston, Sumit Chopra, and Antoine Bordes</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
