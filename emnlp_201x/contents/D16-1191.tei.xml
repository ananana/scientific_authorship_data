<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph Degeneracy-based Approach to Keyword Extraction *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><forename type="middle">J</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-P</forename><surname>Tixier</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fragkiskos</forename><forename type="middle">D</forename><surname>Malliaros</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">UC San Diego</orgName>
								<address>
									<addrLine>La Jolla</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ecole Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Graph Degeneracy-based Approach to Keyword Extraction *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1860" to="1870"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We operate a change of paradigm and hypothesize that keywords are more likely to be found among influential nodes of a graph-of-words rather than among its nodes high on eigenvector-related centrality measures. To test this hypothesis, we introduce unsuper-vised techniques that capitalize on graph de-generacy. Our methods strongly and significantly outperform all baselines on two datasets (short and medium size documents), and reach best performance on the third one (long documents).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Keyword extraction is a central task in NLP. It finds applications from information retrieval (notably web search) to text classification, summarization, and vi- sualization. In this study, we focus on the task of unsupervised single-document keyword extraction. Following ( <ref type="bibr" target="#b19">Mihalcea and Tarau, 2004</ref>), we concen- trate on keywords only, letting the task of keyphrase reconstruction as a post-processing step.</p><p>More precisely, while we capitalize on a graph representation of text like several previous ap- proaches, we deviate from them by making the as- sumption that keywords are not found among pres- tigious nodes (or more generally, nodes high on eigenvector-related centrality metrics), but rather among influential nodes. Those nodes may not have many important connections (like their prestigious counterparts), but they are ideally placed at the core * This research is supported in part by the OpenPaaS::NG project.</p><p>of the network. In other words, this switches the objective from capturing the quality and quantity of single node connections, to taking into account the density and cohesiveness of groups of nodes. To op- erate this change of paradigm, we propose several algorithms that leverage the concept of graph degen- eracy ( <ref type="bibr" target="#b15">Malliaros et al., 2016a</ref>).</p><p>Our contributions are threefold: (1) we propose new unsupervised keyword extraction techniques that reach state-of-the art performance, (2) we ap- ply the K-truss algorithm to the task of keyword ex- traction for the first time, and (3) we report new in- sights on the interplay between window size, graph- of-words structure, and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph-of-Words Representation</head><p>Many ways of encoding text as a graph have been explored in order to escape the very limiting term- independence assumption made by the traditional vector space model. In this study, we adopt the sem- inal Graph-of-Words representation (GoW) of <ref type="bibr" target="#b19">(Mihalcea and Tarau, 2004</ref>), for its simplicity, high his- torical success, and above all because it was recently used in several approaches that reached very good performance on various tasks such as information retrieval ( <ref type="bibr" target="#b21">Rousseau and Vazirgiannis, 2013)</ref>, doc- ument classification <ref type="bibr" target="#b14">(Malliaros and Skianis, 2015;</ref>, event detection ( <ref type="bibr" target="#b18">Meladianos et al., 2015)</ref>, and keyword extraction ( .</p><p>While sophisticated variants of the GoW model would be worth exploring (edge weights based on mutual information or word embeddings, adaptive window size, etc.), we aim here at making a bet-   ter use of an existing representation of text, not at proposing a new one. This is why, to demonstrate the additional skill brought by our proposed meth- ods, we stick to the basic GoW framework. As shown in <ref type="figure" target="#fig_1">Figure 1</ref> (a), the GoW representation of ( <ref type="bibr" target="#b19">Mihalcea and Tarau, 2004</ref>) encodes a piece of text as an undirected graph where nodes are unique nouns and adjectives in the document, and where there is an edge between two nodes if the terms they represent co-occur within a window of predeter- mined size W that is slided over the entire document from start to finish, overspanning sentences. Fur- thermore, edges are assigned integer weights match- ing co-occurrence counts. This fully statistical ap- proach is based on the Distributional Hypothesis <ref type="bibr" target="#b9">(Harris, 1954)</ref>, that is, on the premise that the re- lationship between words can be determined by the frequency with which they share local contexts of occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Degeneracy</head><p>The concept of graph degeneracy was introduced by <ref type="bibr" target="#b24">(Seidman, 1983)</ref> with the k-core decomposition technique and was first applied to the study of cohe- sion in social networks. Here, we consider it as an umbrella term also encompassing the K-truss algo- rithm <ref type="bibr" target="#b6">(Cohen, 2008)</ref>. In what follows, G(V, E) is a graph with |V | nodes and |E| edges. Note that for graphs-of-words, the nodes V are labeled according to the unique terms they represent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">k-core subgraph</head><p>A core of order k (or k-core) of G is a maximal con- nected subgraph of G in which every vertex v has at least degree k <ref type="bibr" target="#b24">(Seidman, 1983)</ref>. If the edges are unweighted, the degree of v is simply equal to the count of its neighbors, while in the weighted case, the degree of v is the sum of the weights of its in- cident edges. Note that with the definition of GoW previously given, node degrees (and thus, k) are in- tegers in both cases since edge weights are integers. As shown in <ref type="figure" target="#fig_2">Figure 2 (a)</ref>, the k-core decomposi- tion of G is the set of all its cores from 0 (G itself) to k max (its main core). It forms a hierarchy of nested subgraphs whose cohesiveness and size respectively increase and decrease with k <ref type="bibr" target="#b24">(Seidman, 1983)</ref>. The main core of G is a coarse approximation of its dens- est subgraph ( <ref type="bibr" target="#b25">Wang and Cheng, 2012)</ref>, and should be seen as a seedbed within which it is possible to find more cohesive subgraphs <ref type="bibr" target="#b24">(Seidman, 1983)</ref>. Fi- nally, the core number of a node is the highest order of a k-core subgraph that contains this node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">K-truss subgraph</head><p>K-truss is a triangle-based extension of k-core in- troduced by <ref type="bibr" target="#b6">(Cohen, 2008)</ref>. More precisely, the K- truss subgraph of G is its largest subgraph where ev- ery edge belongs to at least K − 2 triangles. In other words, every edge in the K-truss joins two vertices that have at least K − 2 common neighbors. Com- pared to k-core, K-truss thus does not iteratively prune nodes out based on the number of their direct links, but also based on the number of their shared connections. This more accurately captures cohe- siveness.</p><p>As a result, the K-trusses are smaller and denser subgraphs than the k-cores, and the maximal K- truss of G better approximates its densest subgraph. In essence, and as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> (b), the K-trusses can be viewed as the essential parts of the k-cores, i.e., what is left after the less cohesive elements have been filtered out ( <ref type="bibr" target="#b25">Wang and Cheng, 2012)</ref>. By analogy with k-core, the K-truss de- composition of G is the set of all its K-trusses from K = 2 to K max , and the truss number of an edge is the highest order of a truss the edge belongs to. By extension, we define the truss number of a node as the maximum truss number of its incident edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">k-shell</head><p>Depending on context, we will refer to the k-shell as the part of the k-core (or truss) that is not included in the (k + 1)-core (or truss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph degeneracy and spreading influence</head><p>In social networks, it has been shown that the best spreaders (i.e., the nodes able to propagate informa- tion to a large portion of the network at minimal time and cost) are not necessarily the highly connected individuals (i.e., the hubs), but rather those located at the core of the network (i.e., forming dense and cohesive subgraphs with other central nodes), as in- dicated by graph degeneracy algorithms ( <ref type="bibr" target="#b13">Kitsak et al., 2010)</ref>. Put differently, the spreading influence of a node is related to its structural position within the graph (density and cohesiveness) rather than to its prestige (random walk-based degree). More re- cently, ( <ref type="bibr" target="#b16">Malliaros et al., 2016b)</ref> found that the truss number is an even better indicator of spreading in- fluence than the core number. Motivated by these findings, we posit that taking cohesiveness into ac- count with the core and truss decomposition of a graph-of-words could improve keyword extraction performance. That way, by analogy with the notion of influential spreaders in social networks, we hy- pothesize that influential words in graphs-of-words will act as representative keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Complexity</head><p>( <ref type="bibr" target="#b1">Batagelj and Zaveršnik, 2002)</ref> proposed O(|V | + |E|) and O(|E| log |V |) time algorithms for k-core decomposition in the unweighted (resp. weighted) case. These algorithms are both O(|V |) in space. Computing the K-truss decomposition is more ex- pensive, and requires O(|E| 1.5 ) time and O(|V | + |E|) space ( <ref type="bibr" target="#b25">Wang and Cheng, 2012</ref>). Finally, build- ing a graph-of-words is linear in time and space: O(|V | W ) and O(|V | + |E|), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work and Point of Departure</head><p>TextRank. One of the most popular approaches to the task of unsupervised single-document keyword extraction is TextRank ( <ref type="bibr" target="#b19">Mihalcea and Tarau, 2004</ref>), or TR in what follows. In TR, the nodes of graphs- of-words are ranked based on a modified version of the PageRank algorithm taking edge weights into ac- count, and the top p% vertices are kept as keywords.</p><p>Limitations. TR has proven successful and has been widely used and adapted. However, PageR- ank, which is based on the concept of random walks and is also related to eigenvector centrality, tends to favor nodes with many important connections regardless of any cohesiveness consideration. For undirected graphs, it was even shown that PageR- ank values are proportional to node degrees ( <ref type="bibr" target="#b8">Grolmusz, 2015)</ref>. While well suited to the task of prestige-based ranking in the Web and social net- works (among other things), PageRank may thus not be ideal for keyword extraction. Indeed, a fun- damental difference when dealing with text is the paramount importance of cohesiveness: keywords need not only to have many important connections but also to form dense substructures with these con- nections. Actually, most of the time, keywords are n-grams ( . There- fore, we hypothesize that keywords are more likely to be found among the influential spreaders of a graph-of-words -as extracted by degeneracy-based methods -rather than among the nodes high on eigenvector-related centrality measures.</p><p>Topical vs. network coherence. Note that, un- like a body of work that tackled the task of key- word extraction and document summarization from a topical coherence perspective <ref type="bibr" target="#b3">(Celikyilmaz and Hakkani-Tür, 2011;</ref><ref type="bibr" target="#b4">Chen et al., 2012;</ref><ref type="bibr" target="#b5">Christensen et al., 2013)</ref>, we deal here with network coherence, a purely graph theoretic notion orthogonal to topical coherence.</p><p>Graph degeneracy. The aforementioned limita- tion of TR motivated the use of graph degeneracy to not only extract central nodes, but also nodes form- ing dense subgraphs. More precisely, (Rousseau and Vazirgiannis, 2015) applied both unweighted and weighted k-core decomposition on graphs-of-words and retained the members of the main cores as key- words. Best results were obtained in the weighted case, with small main cores yielding good precision but low recall, and significantly outperforming TR. As expected, (Rousseau and Vazirgiannis, 2015) ob- served that cores exhibited the desirable property of containing "long-distance n-grams". In addition to superior quantitative performance, another advantage of degeneracy-based techniques (compared to TR, which extracts a constant percent- age of nodes) is adaptability. Indeed, the size of the main core (and more generally, of every level in the hierarchy) depends on the structure of the graph- of-words, which by nature is uniquely tailored to the document at hand. Consequently, the distribu- tion of the number of extracted keywords matches more closely that of the human assigned keywords ( . Limitations. Nevertheless, while (Rousseau and Vazirgiannis, 2015) made great strides, it suffers the following limitations: (1) k-core is good but not best in capturing cohesiveness; (2) retaining only the main core (or truss) is suboptimal, as one cannot expect all the gold standard keywords to be found within a unique subgraph -actually, many valu- able keywords live in lower levels of the hierarchy (see <ref type="figure" target="#fig_1">Figure 1)</ref>; and (3) the coarseness of the k-core decomposition implies to work at a high granular- ity level (selecting or discarding a large group of words at a time), which diminishes the flexibility of the extraction process and negatively impacts perfor- mance.</p><p>Research objectives. To address the aforemen- tioned limitations, we investigate in this study (1) the use of K-truss to get a finer-grained hierarchy of more cohesive subgraphs, in order to filter unwanted words out of the upper levels and improve flexibil- ity; (2) the automated selection of the best level in the core (or truss) hierarchy to increase recall while preserving most of the precision; and (3) the conver- sion of node core (or truss) numbers into ranks, to decrease granularity from the subgraph to the node level, while still leveraging the valuable cohesive- ness information captured by degeneracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposed Methods</head><p>In what follows, we introduce the strategies we de- vised to implement our research objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Density</head><p>With the underlying assumption that keywords are found in cohesive subgraphs-of-words and are not all contained in the main core (or truss), an intu- itive, straightforward stopping criterion when going down the core (or truss) hierarchy is a density-based one. More precisely, it may be advantageous to go down the hierarchy as long as the desirable cohesive- ness properties of the main core or truss are main- tained, and to stop when these properties are lost. This strategy is more formally detailed in Algorithm 1, where G(V, E) is a graph-of-words, levels corre- sponds to the vector of the n levels unique k-core (or truss) numbers of V sorted in decreasing order, and the density of G is defined as:</p><formula xml:id="formula_0">density(G) = |E| |V | (|V | − 1)<label>(1)</label></formula><p>As can be seen in <ref type="figure" target="#fig_1">Figure 1</ref> (c) and as detailed in Algorithm 2, the elbow is determined as the most distant point from the line joining the first and last point of the curve. When all points are aligned, the top level is retained (i.e., main core or truss). When there are only two levels, the one giving the highest density is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: dens method</head><p>Input : core (or truss) decomposition of G Output: set of keywords 1 D ← empty vector of length n levels 2 for n ← 1 to n levels do 3 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inflexion</head><p>The Inflexion method (inf in what follows) is an empirically-grounded heuristic that relies on detect- ing changes in the variation of shell sizes (where size denotes the number of nodes). Recall from subsec- tion 3.3 than the k-shell is the part of the k-core (or truss) that does not survive in the (k + 1)-core (or truss), that is, the subgraph of G induced by the nodes with core (or truss) number exactly equal to k. In simple terms, the inf rule-of-thumb consists in going down the hierarchy as long as the shells in-crease in size, and stopping otherwise. More pre- cisely, inf is implemented as shown in Algorithm 3, by computing the consecutive differences in size across the shells and selecting the first positive point before the drop into the negative half (see <ref type="figure" target="#fig_1">Figure 1c)</ref>. If no point satisfies this requirement, the main core (or truss) is extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3: inf method</head><p>Input : core (or truss) decomposition of G Output: set of keywords 1 CD ← empty vector of length n − 1 2 for n ← 1 to (n levels − 1) do</p><formula xml:id="formula_1">3 k l ← levels[n + 1]; k u ← levels[n] 4 CD[n] ←size k l −shell −size k u −shell 5 end 6 if ∃n | (CD[n + 1] &lt; 0 ∧ CD[n] &gt; 0) then 7</formula><p>n best ← n 8 else 9 n best ← 1 10 end 11 k best ← levels[n best ] 12 return k best -core (or truss) as keywords Note that both dens and inf enjoy the same adaptability as the main core retention method of (Rousseau and Vazirgiannis, 2015) explained in Sec- tion 4, since the sizes of all the subgraphs in the hi- erarchy suit the structure of the graph-of-words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CoreRank</head><p>Techniques based on retaining the k best -core (or truss), such as dens and inf previously described, are better than retaining only the top level but lack flex- ibility, in that they can only select an entire batch of nodes at a time. This is suboptimal, because of course not all the nodes in a given group are equally good. To address this issue, our proposed CoreRank method (CR in what follows) converts nodes core (or truss) numbers into scores, ranks nodes in de- creasing order of their scores, and selects the top p% nodes (CRP) or the nodes before the elbow in the scores curve (CRE). Note that for simplicity, we still refer to the technique as CR even when dealing with truss numbers. Flexibility is obviously improved by decreasing granularity from the subgraph level to the node level. However, to avoid going back to the lim- itations of TR (absence of cohesiveness consider- ations), it is crucial to decrease granularity while retaining as much of the desirable information en- coded by degeneracy as possible. To accomplish this task, we followed ( <ref type="bibr" target="#b0">Bae and Kim, 2014</ref>) and assigned to each node the sum of the core (or truss) numbers of its neighbors.</p><p>Our CRE method is outlined in Algorithm 4, where N (v) denotes the set of neighbors of vertex v, and number(v) is the core (or truss) number of v. CRP implements the exact same strategy, the only difference being n best ← round(|V | * percentage) at step 8 (where percentage is a real number be- tween 0 and 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4: CRE method</head><p>Input : core (or truss) decomposition of G Output: set of keywords <ref type="bibr">7</ref> sort CR in decreasing order 8 n best ← elbow(n, CR[n]) 9 return names(CR[1 : n best ]) as keywords 6 Experimental Setup</p><formula xml:id="formula_2">1 CR ← empty vector of length |V | 2 for n ← 1 to |V | do 3 v ← V [n] 4 CR[n] ← u∈N (v) number(u) 5 name(CR[n]) ← label(v) 6 end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline methods</head><p>TextRank (TR). We used as our first benchmark the system of ( <ref type="bibr" target="#b19">Mihalcea and Tarau, 2004</ref>) discussed in Section 4. For the sake of fair comparison with our CRE and CRP methods, we considered two variants of TR that respectively retain nodes based on both the elbow (TRE) and percentage criteria (TRP). Main. Our second baseline is the main core re- tention technique of ( , also described in Section 4. This method is referred to as main in the remainder of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Datasets</head><p>To evaluate performance, we used three standard, publicly available datasets featuring documents of various types and sizes. <ref type="figure">Figure 3</ref> shows the distribu- tions of document size and manually assigned key- words for each dataset.</p><p>The Hulth2003 1 <ref type="bibr" target="#b11">(Hulth, 2003</ref>) dataset contains abstracts drawn from the Inspec database of physics and engineering papers. Following our baselines, we used the 500 documents in the validation set and the "uncontrolled" keywords assigned by human anno- tators. The mean document size is 120 words and on average, 21 keywords (in terms of unigrams) are available for each document.</p><p>We also used the training set of Marujo2012 1 , containing 450 web news stories of about 440 words on average, covering 10 different topics from art and culture to business, sport, and technology <ref type="bibr" target="#b17">(Marujo et al., 2012</ref>). For each story, the keyphrases assigned by at least 9 out of 10 Amazon Mechanical Turk- ers are provided as gold standard. After splitting the keyphrases into unigrams, this makes for an aver- age of 68 keywords per document, which is much higher than for the two other datasets, even the one comprising long documents (Semeval, see next).</p><p>The Semeval 2 dataset ( <ref type="bibr" target="#b12">Kim et al., 2010</ref>) offers parsed scientific papers collected from the ACM Digital Library. More precisely, we used the 100 articles in the test set and the corresponding author- and-reader-assigned keyphrases. Each document is approximately 1,860 words in length and is associ- ated with about 24 keywords.</p><p>Notes. In Marujo2012, the keywords were as- signed in an extractive manner, but many of them are verbs. In the two other datasets, keywords were freely chosen by human coders in an abstractive way and as such, some of them are not present in the orig- inal text. On these datasets, reaching perfect recall is therefore impossible for our methods (and the base- lines), which by definition all are extractive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Implementation</head><p>Before constructing the graphs-of-words and pass- ing them to the keyword extraction methods, we per- formed the following pre-processing steps:</p><p>Stopwords removal.  </p><note type="other">document size (in words) 0 20 40 60 80 100 140 0 20 40 60 80 100 140 number of manually assigned keywords Hulth2003 Semeval Marujo2012 Hulth2003</note><p>Semeval Marujo2012 <ref type="figure">Figure 3</ref>: Basic dataset statistics. SMART information retrieval system 3 were dis- carded.</p><p>Part-of-Speech tagging and screening using the openNLP (Hornik, 2015) R (R Core Team, 2015) implementation of the Apache OpenNLP Maxent POS tagger. Then, following <ref type="bibr" target="#b19">(Mihalcea and Tarau, 2004)</ref>, only nouns and adjectives were kept. For Marujo2012, as many gold standard keywords are verbs, this step was skipped (note that we did exper- iment with and without POS-based screening but got better results in the second case).</p><p>Stemming with the R SnowballC package (Bouchet-Valat, 2014) (Porter's stemmer). Gold standard keywords were also stemmed.</p><p>After pre-processing, graphs-of-words (as de- scribed in Section 2) were constructed for each doc- ument and various window sizes (from 3, increasing by 1, until a plateau in scores was reached). We used the R igraph package ( <ref type="bibr" target="#b7">Csardi and Nepusz, 2006</ref>) to write graph building and weighted k-core imple- mentation code. For K-truss, we used the C++ im- plementation offered by ( <ref type="bibr" target="#b25">Wang and Cheng, 2012)</ref>.</p><p>Finally, for TRP and CRP, we retained the top 33% keywords on Hulth2003 and Marujo2012 (short and medium size documents), whereas on Se- meval (long documents), we retained the top 15 key- words. This is consistent with our baselines. In- deed, the number of manually assigned keywords in- creases with document size up to a certain point, and stabilizes afterwards.</p><p>The code of the implementation and the datasets can be found here 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truss nu</head><p>q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluation</head><p>We computed the standard precision, recall, and F-1 measures for each document and averaged them at the dataset level (macro-averaging).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Precision/Recall trade-off</head><p>As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, our methods dens and inf outperform the baselines by a wide margin on the datasets containing small and medium size docu- ments (Hulth2003 and Marujo2012). As expected, this superiority is gained from a drastic improve- ment in recall, for a comparatively lower precision loss. TR and main exhibit higher precision than recall, which is in accordance with ( . The same observation can be made for our CR method. For TR, the unbalance is more severe on the Hulth2003 and Marujo2012 datasets (short/medium documents) when the elbow method is used (TRE), probably because it tends to retain only a few nodes. However, on Semeval (long documents), using the elbow method (TRE) gives the best trade-off between precision and recall. For CR, still on Semeval, using the elbow method (CRE) even gives better recall than precision.</p><p>Overall, compared to the baselines, the unbalance between precision and recall for our methods is less extreme or equivalent. On the Marujo2012 dataset for instance, our proposed inf method is almost per- fectly balanced and ranks second (significantly bet- ter than all baselines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Impact of window size</head><p>The performance of k-core does not dramatically increase with window size, while K-truss exhibits precision recall F1-score   <ref type="table">Table 2</ref>: Marujo2012, k-core, W = 13. *statistical significance at p &lt; 0.001 with respect to all baselines.</p><p>† baseline systems.</p><p>a surprising "cold start" behavior and only begins to kick-in for sizes greater than 4-5. A possible explanation is that the ability of K-truss (which is triangle-based) to extract meaningful information from a graph depends, up to a certain point, on the amount of triangles in the graph. In the case of graph-of-words, the number of triangles is pos- itively correlated with window size (see <ref type="figure" target="#fig_5">Figure 4)</ref>. It also appears that document size (i.e., graph-of- words structure) is responsible for a lot of perfor- mance variability. Specifically, on longer docu- ments, performance plateaus at higher window sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Best models comparison</head><p>For each dataset, we retained the degeneracy tech- nique and window size giving the absolute best per- formance in F1-score, and compared all methods un- der these settings (see <ref type="table">Tables 1-3</ref>). We tested for statistical significance in macro-averaged F1 scores using the non-parametric version of the t-test, the Mann-Whitney U test <ref type="bibr">5</ref> . On Hulth2003 and Marujo2012 (short and medium size documents), our methods dens and inf strongly and significantly outperform all baselines, with respective absolute improvements of more than 5.5% with respect to the best performing baseline q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q window size window size   <ref type="table">Table 3</ref>: Semeval, K-truss, W = 20. *statistical significance at p &lt; 0.001 w.r.t. main. † baseline systems.</p><p>(main). On Semeval, which features larger pieces of text, our CRP technique improves on TRP, the best performing baseline, by more than 1 %, altough the difference is not statistically significant. How- ever, CRP is head and shoulders above main, with an absolute gain of 6%. This suggests that converting the cohesiveness information captured by degener- acy into ranks may be valuable for large documents.</p><p>Finally, the poor performance of the dens and inf methods on Semeval <ref type="table">(Table 3)</ref> might be explained by the fact that these methods are only capable of selecting an entire batch of nodes (i.e., subgraph- of-words) at a time. This lack of flexibility seems to become a handicap on long documents for which the graphs-of-words, and thus the subgraphs corre- sponding to the k-core (or truss) hierarchy levels, are very large. This analysis is consistent with the ob- servation that conversely, approaches that work at a finer granularity level (node level) prove superior on long documents, such as our proposed CRP method which reaches best performance on Semeval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>Our results provide empirical evidence that spread- ing influence may be a better "keywordness" met- ric than eigenvector (or random walk)-based crite- ria. Our CRP method is currently very basic and leveraging edge weights/direction or combining it with other scores could yield better results. Also, more meaningful edge weights could be obtained by merging local co-occurrence statistics with exter- nal semantic knowledge offered by pre-trained word embeddings ( <ref type="bibr" target="#b26">Wang et al., 2014</ref>). The direct use of density-based objective functions could also prove valuable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Graph-of-words (W = 8) for document #1512 of Hulth2003 decomposed with k-core (non-(nouns and adjectives) in italic). (b) Keywords extracted by each proposed and baseline method (human assigned keywords in bold). (c) Selection criterion of each method except main (does not apply).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) k-core decomposition illustrative example. Note that while nodes * and ** have same degree (3), node ** makes a more influential spreader as it lies in a higher core than node *. (b) k-core versus K-truss. The main K-truss subgraph can be considered as the core of the main core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Stopwords from the 1 https://github.com/snkim/ AutomaticKeyphraseExtraction 2 https://github.com/boudinfl/centrality_ measures_ijcnlp13/tree/master/data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Triangle count versus window size (Hulth2003).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of window size on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>computer-aided share trading. We consider problems of statistical analysis of share prices and propose probabilistic characteristics to describe the price series. We discuss three methods of mathematical modelling of price series with given probabilistic characteristics.</head><label></label><figDesc></figDesc><table>Edge weights 

1 
2 
3 
4 
5 

Core numbers 

8 
9 
10 
11 

q 

q 

q 

inf 

shells 
difference in shell sizes 

−6 −4 −2 

0 

2 

(10 − 11) 
(9 − 10) 
(8 − 9) 

q 

q 

q 

q 

0.38 

0.42 

0.46 

0.50 

dens 

k−cores 

density 

11 
10 
9 
8 

q 

q q q 

q q 
q 

q q 
q 

q 

q 

q q 

2 
4 
6 
8 
10 
12 
14 

70 

90 

110 

130 

TR 

nodes 

TR scores 

elbow 
top 33% 

Mathematical aspects of Core numbers 
TR scores 

P 
R 
F1 
P 
R 
F1 
MAIN 0.86 0.55 0.67 ELB 
1 0.18 0.31 
INF 0.83 0.91 0.87 

PER 

1 0.45 0.63 
DENS 0.88 0.64 0.74 

mathemat 
11 price 
.1359 
price 
11 share 
.0948 
probabilist 
11 
.0906 
characterist 11 
.0870 
seri 
11 
.0860 
method 
11 mathemat 
.0812 
model 
11 analysi 
.0633 
share 
10 statist 
.0595 
trade 
9 method 
.0569 
problem 
9 problem 
.0560 
statist 
9 trade 
.0525 
analysi 
9 model 
.0493 
aspect 
8 computer-aid .0453 
computer-aid 8 aspect 
.0417 

MAIN 

INF 

DENS 

ELB 

probabilist 
characterist 
seri 

CR scores 

P 
R 
F1 

ELB 0.90 0.82 0.86 

PER 

1 0.45 0.63 

mathemat 
128 
price 
120 
analysi 
119 
share 
118 
probabilist 112 
characterist 112 
statist 
108 
trade 
97 
problem 
97 
seri 
94 
method 
85 
computer-aid 76 
model 
66 
aspect 
65 

PER 

ELB 

PER 

(a) 

(c) 

(b) 

q 

q 
q q q 

q 

q q q q q q q q 

2 
4 
6 
8 
10 
12 
14 

0.04 

0.08 

0.12 

CR 

elbow 
top 33% 

nodes 

CR scores 

</table></figure>

			<note place="foot" n="3"> http://jmlr.org/papers/volume5/ lewis04a/a11-smart-stop-list/english.stop 4 https://github.com/Tixierae/EMNLP_2016</note>

			<note place="foot" n="5"> https://stat.ethz.ch/R-manual/Rdevel/library/stats/html/wilcox.test.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identifying and ranking influential spreaders in complex networks by neighborhood coreness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonhyun</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page" from="549" to="559" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Batagelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matjaž</forename><surname>Zaveršnik</surname></persName>
		</author>
		<idno>cs/0202039</idno>
		<title level="m">Generalized cores</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SnowballC: Snowball stemmers based on the C libstemmer UTF-8 library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Bouchet-Valat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>R package version 0.5.1</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovery of topically coherent sentences for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="491" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised two-stage keyword extraction from spoken documents by topic coherence and support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5041" to="5044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards coherent multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Soderland</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1163" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Trusses: Cohesive subgraphs for social network analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Security Agency Technical Report</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The igraph software package for complex network research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Csardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Nepusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">InterJournal</title>
		<imprint>
			<biblScope unit="page">1695</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A note on the pagerank of undirected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Grolmusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="633" to="634" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<title level="m">openNLP: Apache OpenNLP Tools Interface</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>R package version 0.2-5</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olena</forename><surname>Su Nam Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identification of influential spreaders in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lazaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Gallos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Havlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Liljeros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Muchnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Makse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Physics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="888" to="893" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph-based term weighting for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fragkiskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skianis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting>the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Core decomposition in graphs: concepts, algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fragkiskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><forename type="middle">N</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Extending Database Technology, EDBT</title>
		<meeting>the 19th International Conference on Extending Database Technology, EDBT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locating influential nodes in complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Evgenia G</forename><surname>Fragkiskos D Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19307</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frederking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2012</title>
		<meeting>LREC 2012</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Degeneracy-based real-time sub-event detection in twitter stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polykarpos</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International AAAI Conference on Web and Social Media (ICWSM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Yannis Stavrakas, and Michalis Vazirgiannis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TextRank: bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>R Core Team</surname></persName>
		</author>
		<title level="m">R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph-of-word and tw-idf: new approach to ad hoc ir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on Information &amp; Knowledge Management (CIKM)</title>
		<meeting>the 22nd ACM international conference on Conference on Information &amp; Knowledge Management (CIKM)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Main core retention on graph-of-words for singledocument keyword extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="382" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text categorization as a graph classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanouil</forename><surname>Kiagias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">107</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network structure and minimum degree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen B Seidman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="287" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Truss decomposition in massive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="812" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Corpusindependent generic keyphrase extraction using word embedding vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software Engineering Research Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
