<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qile</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Large-scale Intelligent Systems Laboratory NSF Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Large-scale Intelligent Systems Laboratory NSF Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Large-scale Intelligent Systems Laboratory NSF Center for Big Learning</orgName>
								<orgName type="institution">University of Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">4663</biblScope>
							<biblScope unit="page" from="4663" to="4672"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Discovering the latent topics within texts has been a fundamental task for many applications. However, conventional topic models suffer different problems in different settings. The Latent Dirichlet Allocation (LDA) may not work well for short texts due to the data sparsity (i.e., the sparse word co-occurrence patterns in short documents). The Biterm Topic Model (BTM) learns topics by mod-eling the word-pairs named biterms in the whole corpus. This assumption is very strong when documents are long with rich topic information and do not exhibit the transitivity of biterms. In this paper, we propose a novel way called GraphBTM to represent biterms as graphs and design Graph Convolutional Networks (GCNs) with residual connections to extract transitive features from biterms. To overcome the data sparsity of LDA and the strong assumption of BTM, we sample a fixed number of documents to form a mini-corpus as a training instance. We also propose a dataset called All N ews extracted from (Thompson, 2017), in which documents are much longer than 20 Newsgroups. We present an amortized variational inference method for GraphBTM. Our method generates more coherent topics compared with previous approaches. Experiments show that the sampling strategy improves performance by a large margin.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic model ( <ref type="bibr" target="#b0">Blei et al., 2003</ref>) is one of the most popular approaches to learn hidden representa- tions of text. The broad applications of topic model range from recommender systems ( <ref type="bibr" target="#b30">Wang and Blei, 2011</ref>), computer vision <ref type="bibr" target="#b6">(Fei-Fei and Perona, 2005</ref>), to bioinformatics ( <ref type="bibr" target="#b26">Rogers et al., 2005)</ref>. Conventional topic models learning ap- proaches are based on Gibbs Sampling <ref type="bibr" target="#b8">(Griffiths and Steyvers, 2004</ref>) or Variational Expecta- tion Maximization (VEM) algorithm ( <ref type="bibr" target="#b0">Blei et al., 2003</ref>). Both Gibbs Sampling and VEM are not directly applicable to new variations of the topic model. Specifically, the inference algorithm re- quires re-deriving for any minor changes to the model.</p><p>Recently, a neural network based topic model inference approach, the Autoencoded Variational Inference for Topic Model (AVITM), was pro- posed by <ref type="bibr" target="#b27">(Srivastava and Sutton, 2017)</ref>. This ap- proach uses an inference network to directly map a document to its posterior distribution without any variational update steps. The proposed in- ference network is based on the Autoencoding Variational Bayes (AEVB) ( <ref type="bibr">Kingma and Welling, 2013)</ref>, a stochastic variational inference algorithm over neural networks. Compared with the sam- pling based approaches, AVITM can scale to large datasets. Although it improves the model's robust- ness and reduces the computational cost, it still suffers from the data sparsity in short texts.</p><p>Biterm Topic Model (BTM) proposed by (  and <ref type="bibr" target="#b32">(Yan et al., 2013</ref>) ad- dresses the shortcoming of data sparsity for mod- eling the corpus of short texts. It explicitly mod- els the patterns on top of word co-occurrence fea- ture (Biterm, the unordered word-pair occurs in texts) from the corpus. It holds one topic distri- bution for the whole corpus rather than one doc- ument. BTM, therefore, is suitable for modeling short documents like tweets, and online QA texts. It also achieves better results than LDA in specific scenarios of the normal texts ( <ref type="bibr" target="#b32">Yan et al., 2013)</ref>. However, using one topic distribution for all docu- ments limits the model's expressiveness when the documents contain diverse topics.</p><p>To address the issue of data sparsity of LDA when modeling the short texts and the insufficient corpus-wise topic representation in BTM for nor- mal texts, we strike a balance between these two approaches. Instead of modeling biterms in the whole corpus, we extract biterms inside a fixed- length text window for every document and sam- ple n documents to form an instance each time. As a result, we enhance the input feature with biterms that capture more word co-occurrence pat- terns than BOW and also avoid the insufficient corpus-wised topic representation in BTM. An- other advantage of biterms is the transitivity. For example, we have two biterms (A, B) and (A, C). It is natural to think that B and C may share some similarities. This transitivity is similar to the graph structure data. So we model the biterms in a graph where the words are taken as nodes and the counts of biterms are the weight of edges. We extract the information from biterms explicitly by the graph convolutional network <ref type="bibr">(Kipf and Welling, 2016)</ref>.</p><p>In this paper, we propose a novel Graph-based inference network for the biterm topic model (GraphBTM). To the best of our knowledge, GraphBTM is the first AEVB inference approach for the biterm topic model with graph enhanced feature. Our model also strikes a good balance be- tween LDA and BTM, leverages both advantages, and achieves better topics coherence scores than AVITM in two datasets. The main contributions of our work include:</p><p>• We are the first to apply the neural network based inference approach for the Biterm Topic Model, and achieve better results in topic coherence score than previous AEVB based inference method (AVITM) and online Variational Inference LDA.</p><p>• We propose a data argumentation method to enhance the input feature with word corre- lation from biterms in normal text and over- come the shortcoming of the data sparsity in LDA and the insufficient corpus-wise topic representation in BTM.</p><p>• We model the biterms as an undirected graph and adopt a novel graph convolutional net- work to encode word co-relationship in our inference network.</p><p>• We introduce a new dataset All N ews dataset containing 20,000 documents ex- tracted from 15 news publishers <ref type="bibr" target="#b28">(Thompson, 2017)</ref> for topic modeling. TThe documents are much longer compared with the 20 News- groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Biterm Topic Model</head><p>BTM ( ) is proposed to solve the data sparsity problem in the scenario of short texts. Instead of modeling a single document, BTM con- siders the whole corpus as a mixture of topics. BTM collects all unordered word-pairs (biterms) from each short text or a fixed-length text window of normal texts. The generative progress of BTM can be described as follows, where α and β are two parameters of Dirichlet priors.</p><p>1. For each topic z (a) draw a topic-specific word distribution</p><formula xml:id="formula_0">φ z ∼ Dir(β)</formula><p>2. Draw a topic distribution θ ∼ Dir(α) for the whole corpus</p><p>3. For each biterm b in the biterm set</p><formula xml:id="formula_1">(a) draw a topic assignment z ∼ M ulti(θ) (b) draw two words: w i , w j ∼ M ulti(φ z )</formula><p>With the procedure above, the joint probability of a biterm b = (w i , w j ) can be written as:</p><formula xml:id="formula_2">P (b) = z P (z)P (w i |z)P (w j |z) (1) = z θ z φ i|z φ j|z<label>(2)</label></formula><p>So the likelihood of the whole corpus B is:</p><formula xml:id="formula_3">P (B|α, β) = i,j z θ z φ i|z φ j|z (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Laplace Approximation of Dirichlet</head><p>Both LDA and BTM use the Dirichlet prior over the topic and word proportions. <ref type="bibr" target="#b29">Wallach et al. (2009)</ref> showed that the Dirichlet prior is important to producing interpretable topics. However, it is hard to apply the Dirichlet prior to AEVB directly. AEVB uses the reparameterization trick (RT) to obtain a differentiable Monte Carlo estimator for the variational lower bound (details can be found in next section), and it is difficult to propose an effective RT for the Dirichlet prior. Fortunately, we can approximate Dirichlet dis- tribution with a logistic normal in the sof tmax basis by Laplace approximation (Hennig et al., <ref type="figure">Figure 1</ref>: The overall structure of our proposed model GraphBTM. In this example, we sample 4 documents at a time and embed the aggregated biterm graph by GCNs. The graph embedding is sent to inference network E to produce the parameters for our variational distribution. We then use RT to generate the Monte Carlo samples. At last, we use the decoder network β to get the word probabilities and reconstruct the aggregated graph. 2012). <ref type="bibr" target="#b19">MacKay (1998)</ref> gives the Dirichlet prob- ability density function in the softmax basis over the variable x:</p><formula xml:id="formula_4">P (π|α) = Γ( K k (α k )) K k Γ(α k ) k π α k k g(1 T x) (4)</formula><p>where π = σ(x) (softmax) and g(1 T x) is an arbi- trary density for integrability. <ref type="bibr" target="#b10">Hennig et al. (2012)</ref> argued that the Eq. 4 could be approximately in- dependent for large K (number of topics). So the covariance matrix of the Dirichlet prior becomes a diagonal matrix for large K. By this way, we can approximate the Dirichlet distribution with a multivariate normal with mean µ k and covariance matrix Σ kk :</p><formula xml:id="formula_5">µ k = logα k − 1 K K i logα i (5) Σ kk = 1 α k (1 − 2 K ) + 1 K 2 K i 1 α i<label>(6)</label></formula><p>with this approximation in hand, we can easily ap- ply RT by sampling from ∼ N (0, I) and com- pute probability π k = σ(µ k + Σ 1/2 kk ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Biterm Topic Model</head><p>Before getting into details of GraphBTM, we give an overall structure of GraphBTM, as shown in <ref type="figure">Fig. 1</ref>. We extract the biterms from a mini-corpus (aggregated sampled documents) and embed the whole biterm graph into a fixed length vector with the dimension of vocabulary size. Then we use this graph embedding as the input of our inference network and get the topic proportion. At last, we use the decoder network to get the word probabil- ities and reconstruct the biterm graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Biterms as Graphs</head><p>Commonly used input feature of topic models is bag-of-words (BOW) which implicitly capture the word co-occurrence patterns. BTM models the word co-occurrence explicitly by directly counting the word-pairs in a text window. However, using one-hot encoding for biterms may lose the transi- tive co-relations. We model collected biterms as a graph G = (V, E), where V (words as nodes and |V | is the vocabulary size) and E (counts of corresponding biterms in the sample) are sets of nodes and edges, respectively. In this way, the ad- jacency matrix A (A ∈ R V ×V ) denotes the counts of biterms in the sample. We also leverage the ma- trix A as the node feature matrix (A i is the node feature for the word w i ). We use GCNs proposed by <ref type="bibr">(Kipf and Welling, 2016)</ref>, which is a framework used for learning the graph structure data. <ref type="bibr" target="#b7">Gilmer et al. (2017)</ref> presented a comprehensive overview. Consider an undirected graph G = (V, E) and a matrix X ∈ R n×m in where each row is a node feature x v ∈ R m (v ∈ V). One layer GCN encodes in- formation of a node with its immediate neighbors, defined as</p><formula xml:id="formula_6">h l+1 = f ˜ D −0.5 ˜ A ˜ D −0.5 (h l W l + b)<label>(7)</label></formula><p>where h 0 is the input features X, ˜ A = A + I N is the adjacency matrix of the graph with self- connections, I N is the identity matrix, ˜ D is the degree matrix of˜Aof˜ of˜A, W l is a trainable weight ma- trix in the layer and b is the bias. f denotes a non-linear activation function, such as ReLU. By stacking GCN layers, we can incorporate higher order neighborhoods. To represent the whole graph, we reduce the dimension of each node to one by using GCNs and concatenate them as the final representation of the biterm graph.</p><p>From another point of view, we can treat GCNs as a Laplacian smoothing. Repeatedly applying Laplacian smoothing may mix the features of ver- tices and make them indistinguishable ( <ref type="bibr">Li et al., 2018</ref>). On the other hand, the transitivity of words may not be meaningful when the number of hops (layers) of GCN increases. We solve this prob- lem by adding shortcut connections between dif- ferent layers inspired by Residual Networks ( <ref type="bibr" target="#b9">He et al., 2016</ref>). What's more, a recent study showed that adding the residual connection can help con- vergence (Li and Yuan, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AEVB for Biterm Graphs</head><p>Eq. 3 gives us the likelihood of the whole corpus based on M ulti(φ z ). Here we rewrite the Eq. 3 with latent variables as</p><formula xml:id="formula_7">p(B|α, β) = θ   (i,j) k z=1 π i π j p(z n |θ)   p(θ|α)dθ (8)</formula><p>where π i = p(w i |z n , β). The inference of poste- rior p(θ, z|B, α, β) over the hidden variables θ and z is intractable <ref type="bibr" target="#b5">(Dickey, 1983)</ref>. Many methods are proposed to solve this inference problem includ- ing Gibbs Sampling ( <ref type="bibr" target="#b8">Griffiths and Steyvers, 2004</ref>) and variational inference methods. Gibbs Sam- pling based approaches are computationally inef- ficient and varitional inference methods like mean field ( <ref type="bibr" target="#b0">Blei et al., 2003</ref>) scarify the topic quality for computational efficiency. Moreover, the ma- jor problem of these approximate inference algo- rithms is the inflexibility. Slight changes in model assumption may require designing a new infer- ence algorithm. To alleviate this problem, we de- sign an amortized approximate inference method similar to AVITM <ref type="bibr" target="#b27">(Srivastava and Sutton, 2017)</ref>. It is more flexible compared with other approxi- mate inference methods and can be applied to any biterm graphs.</p><p>In Eq. 8, there are two latent variables θ and z, we introduce two free variational parameters γ over θ and φ over z. Our goal is to approximate the true posterior p(θ, z|B, α, β) with variational distribution q(θ, z|γ, φ) = q γ (θ) k q φ (z k ). Then we can transfer the inference problem as an opti- mization problem ( <ref type="bibr" target="#b0">Blei et al., 2003)</ref>, which is to maximize</p><formula xml:id="formula_8">L(γ, φ|α, β) = logp(B|α, β) (9) − D KL [q(θ, z|γ, φ)||p(θ, z|B, α, β)]</formula><p>L is a lower bound to the marginal log likelihood (ELBO). Following AEVB ( <ref type="bibr">Kingma and Welling, 2013)</ref>, we rewrite the ELBO as</p><formula xml:id="formula_9">L(γ, φ|α, β) = −D KL + R<label>(10)</label></formula><p>where</p><formula xml:id="formula_10">R = E q [logp(B|z, θ, α, β)].</formula><p>This form is intuitive. The first term is the KL divergence be- tweent the variational distribution and the prior on the latent variables, and the second term ensures that the latent variables are good at explaining and reconstructing the input data. We use a neural network named inf erence network to compute the variational parameters. It takes the embedding of the biterm graph (sec. 3.1) as the input and outputs the parameters of the vari- ational distribution. So the inference network can be defined as (µ b , Σ b ) = f (b, γ), where µ b and Σ b are vectors of length k (topic numbers) and γ are the network parameters. In our setting, we use the logistic normal distribution which is an approxi- mation of the Dirichlet prior to the variational dis- tribution. We can choose the corresponding vari-</p><formula xml:id="formula_11">ational distribution q γ (θ) = LN (θ|µ b , diag(Σ b )),</formula><p>where diag(·) converts a column vector to a di- agonal matrix. One important advantage of using AEVB is that we couple the variational parame- ters for different inputs, unlike mean field varia- tional inference, because they are computed from the same network.</p><p>Next is how to compute the expectations respect to q in R (Eq. 10). <ref type="bibr">Kingma and Welling (2013)</ref> use a differential Monte Carlo estimator with the reparameterization trick. With RT, instead of sam- pling from the variational distribution directly, we sample from a simple distribution that is indepen- dent of all variational parameters. In this way, the gradient can be backpropagated through the vari- ational parameters. For the logistic normal distri- bution, we can sample from a standard normal dis- tribution ∈ N (0, I).</p><p>Although the reparameterization trick helps us deal with θ, it is hard to deal with the discrete variable z. Fortunately, we can collapse the dis- crete variables z and only infer θ with collapsed inference method ( <ref type="bibr">Kurihara et al.)</ref> as <ref type="formula">(11)</ref> where π i = p(w i |β, θ), which is the probability of one word in the biterm. Now we only need to sampling from θ.</p><formula xml:id="formula_12">p(B|α, β) = θ   (i,j) π i π j   p(θ|α)dθ</formula><p>We can now get our final variational objective function as (to minimize the negative ELBO)</p><formula xml:id="formula_13">L = D KL − E [ G b • log(P T P )]<label>(12)</label></formula><p>where G b is the input biterm graph, P = σ(β)σ(µ + Σ 1/2 ) is probabilities for all the words based on the input graph and • denotes the element-wise production. The KL divergence be- tween two logistic normal distritbutions are</p><formula xml:id="formula_14">D KL = 1 2 {tr(Σ −1 1 Σ 0 ) + (µ 1 − µ 0 ) T Σ −1 1 (µ 1 − µ 0 ) − K + log |Σ 1 | |Σ 0 | }<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sample Mini-corpus</head><p>To alleviate the data sparsity problem of LDA ( <ref type="bibr">Zhu and Xing, 2012;</ref><ref type="bibr" target="#b17">Lin et al., 2014</ref>), BTM learns topics from the aggregated patterns in the whole corpus. In our observation, this assumption is too strong for normal texts. Other than BTM, some approaches in the literature addressed this problem by aggregating documents into a mini- corpus before training the topic model. For ex- ample, in tweets analysis, <ref type="bibr" target="#b31">Weng et al. (2010)</ref> ag- gregated the tweets from one user into a docu- ment. <ref type="bibr">Hong and Davison (2010)</ref> combined the tweets containing the same word. Inspired by these strategies, we make the same assumption for normal text. We first extract all the biterms in each document and randomly select n documents in the dataset as a mini-corpus. The biterms of the mini- corpus simply merge all the biterms from the n documents. Experiments show that a proper sam- pling number achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unnormalize the β</head><p>The topic-word distribution β is a mixture of multinomials. One drawback of this formula- tion is that it cannot predict something that is sharper than the distributions being mixed <ref type="bibr" target="#b13">(Hinton and Salakhutdinov, 2009)</ref>. This problem may result in some poor quality topics. Previous re- search ( <ref type="bibr" target="#b27">Srivastava and Sutton, 2017)</ref> shows that unnormalizing the parameters β and changing the conditional distribution of w n as w n |β, θ ∼ M ultinomial(1, σ(βθ)) can solve this problem.</p><p>With the unnormalized β, we can model it as a decoder network whose weight matrix M = (m 1 , . . . , m K ) denotes the weight for all words under K topics. Applying softmax to row m i will give us the probabilities under topic i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>We demonstrate our model on two datasets: 20 Newsgroups and All N ews. For All News, we use the data from kaggle collection <ref type="bibr" target="#b28">(Thompson, 2017)</ref> 1 , which collects documents from 15 main news publishers between 2016 and July 2017. Among these, we randomly select 20,000 documents. In our preprocessing of the texts, we follow the steps of tokenization, filtering out stop words, and non- UTF-8 characters in <ref type="bibr" target="#b27">(Srivastava and Sutton, 2017)</ref>. From the statistics summarized in <ref type="table">Table 1</ref>. We can know that the 20 Newsgroups dataset is relatively sparse and the All News has rich information. The ratio of text lengths less or equal to 30 of the 20 Newsgroups dataset is 28%, which only 2% in the All News dataset. The average size differs a lot between these two datasets: 302 for the All News and 88 for the 20 Newsgroups.</p><p>For the generation of the matrix W g , the selec- tion of the window size of words is critical, a small size of window leads to very sparse W g . Here, we choose an experience value 30 for the window size follows the ( <ref type="bibr" target="#b32">Yan et al., 2013</ref>). For the logistic nor- mal approximation, we use the Dirichlet distribu- tion with parameter α as 0.02. Our GraphBTM approach, including the GCN layers and the in- ference network are implemented with Pytorch- v0.4.0 (Paszke et al.). Parameters in our imple- mented model are optimized by the stochastic op- timizer Adadelta (Zeiler, 2012) with learning rate 1. To embed the biterm graph, we use a 3-layer GCNs with size 1995-100, 100-100 and 100-1 for 20 Newsgroups and 5000-1000, 1000-100, 100-1 for ALL News. We use the edge dropout in GCN: when computing h l , we ignore each node with a probability of 0.6. We use batch normalization   ( <ref type="bibr">Ioffe and Szegedy, 2015</ref>) in inf erence network with batch size 100 as same in <ref type="bibr" target="#b27">(Srivastava and Sutton, 2017)</ref>. We run each model 10 times and take the average results. Code is available at https: //github.com/valdersoul/GraphBTM. The perplexity has been used in the past works to measure the quality of the generated topics. However, the perplexity is not shown to be a good evaluation metric for the topics <ref type="bibr" target="#b22">(Newman et al., 2010</ref>). What's more, our method models a mini- corpus instead of a real one and infer the top- ics through the pattern of biterms, so the perplex- ity is not suitable to measure the performance of our approach. To get a more objective measure- ment of the topics, we adopt "topic coherence" as our metric, proposed by <ref type="bibr" target="#b21">(Mimno et al., 2011)</ref> to evaluate the quality of the topics. For a vec- tor V (z) = (v z 1 , ..., v z T ) as the top T words of the topic z, which ordered by the probability p(w|z), the topic coherence is defined as:</p><formula xml:id="formula_15">C(z; V (z) ) = T t=2 t l=1 log D(v (z) m , v (z) l ) + 1 D(v (z) l )</formula><p>. <ref type="formula">(14)</ref> where D(v) is the number of the documents that <ref type="bibr">word v occurred, and D(v, v )</ref> is the number of the documents that both the words v and v oc- curred. The assumption of the topic coherence is the words with high frequency in a topic tend to appear in the same document. This measurement has been demonstrated to be highly consistent with the human evaluated quality of the topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussions</head><p>Comparison with other approaches. We com- pare our GraphBTM approach with the AVITM ( <ref type="bibr" target="#b27">Srivastava and Sutton, 2017</ref>) and the LDA model ( <ref type="bibr" target="#b0">Blei et al., 2003)</ref>. For AVITM, we use their results for 20 Newsgroups directly and run the model using the provided code 2 . We use the on- line variational inference for LDA <ref type="bibr" target="#b14">(Hoffman et al., 2010</ref>) implementation by gensim library <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref> as our LDA baseline. For both GraphBTM and AVITM, we run 200 iterations. <ref type="table" target="#tab_1">Table 2</ref> shows the average topic coherence for three models on the two datasets. The online VI LDA works worst in the three models and we find that on both datasets, the GraphBTM consis- tently outperforms other two models. We can ver- ify the quality of the learned topics by displaying the topic examples in <ref type="table">Table 4</ref>. The topics from GraphBTM are more coherent than the topics from both AVITM and LDA model. Effect of the mini-corpus. To study the ef- fect of our sampling strategy which has been dis- cussed in section 3.3. <ref type="table" target="#tab_2">Table 3</ref> shows the perfor- mance of our model with different sample size for a mini-corpus. For the 20 Newsgroups dataset, the best performance is achieved when the sam- ple size is 3. When we do not use our sample strategy (mini-corpus is 1), the performance drops by a large margin. From <ref type="table">Table 1</ref>, we see that the average size of documents size in 20 Newsgroups is relatively short (88 compared with 302 in All News). Therefore, the 20 Newsgroups dataset may suffer from the sparsity problem. The experiment shows that our sampling strategy can help to over- come this problem. When sample size increases, the performance drops again. The biterm graph with large sample size may bring the same prob- lem of the original BTM (insufficient topic repre- sentation). Compared to 20 Newsgroups dataset, documents in the All News dataset is longer and carried more topic information, so the best perfor- mance is achieved without sampling. We find that when the sample size is larger than an optimized value, the topic coherence starts to drop.</p><p>Effect of modeling biterms as graphs. To ver- ify the effect of the graph modeling of biterms. We also do experiments on AVITM with the same sampling strategy. We use the sampling size 3 which achieves the best performance by GraphBTM in 20 Newsgroups to train AVITM model. The performance does not change a lot, with an average score of 0.25 which is the same as the score without sampling. It is not surprising to us because AVITM models the topic directly on the individual document with BOW feature. The BOW feature captures the word co-occurrence im- plicitly. So aggregating documents in AVITM can not enhance the input feature. However, our model uses GCN to capture the transitivity of biterms and can benefit from the sampling strategy a lot.</p><p>Residual connection. We add the residual con- nection between the first and second layer of GCN. On the other hand, it can also help convergence by adding residual connection ( <ref type="bibr" target="#b16">Li and Yuan, 2017)</ref>. The residual can also help the network capture hierarchical information of the biterms. We re- move the residual connection with the same set- ting which achieves the best performance in these two datasets and results in a 0.1 drop in perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this section, we briefly summarize the related work of the topic model into two categories: nor- mal texts and short texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Normal Texts</head><p>The effort of uncovering the latent semantic repre- sentation of documents can be dated from the La- tent Semantic Analysis (LSA) <ref type="bibr" target="#b4">(Deerwester et al., 1990)</ref>, which used the singular value decomposi- tion of the document matrix to get the word pat- terns. The probabilistic latent semantic analy- sis (PLSA) <ref type="bibr" target="#b15">(Hofmann, 1999</ref>) improved the LSA model by adding a probabilistic model based on a mixture decomposition. It assumed that a docu- ment could be presented as a mixture of topics and a topic is a distribution over words. LDA added the Dirichlet priors on topic and word distributions and proposed a complete generative model. With the rising of deep learning ( <ref type="bibr">LeCun et al., 2015)</ref>, researchers achieve significant improve- ment in many areas including image classifica- tion ( <ref type="bibr" target="#b9">He et al., 2016)</ref>, speech recognition (Hin- ton et al., 2012) and named entity recognition <ref type="bibr" target="#b18">(Ma and Hovy, 2016;</ref><ref type="bibr">Zhu et al., 2018</ref>). Many at- tempts have been made for topic models based on neural networks <ref type="bibr" target="#b13">(Hinton and Salakhutdinov, 2009;</ref><ref type="bibr" target="#b1">Cao et al., 2015;</ref><ref type="bibr" target="#b20">Miao et al., 2016;</ref><ref type="bibr" target="#b27">Srivastava and Sutton, 2017)</ref>. <ref type="bibr" target="#b1">Cao et al. (2015)</ref> em- bedded multinomial relationships between docu- ments, topics, and words in differentiable func- tions. However, they lost the stochasticity and Bayesian inference of prior functions. <ref type="bibr" target="#b20">Miao et al. (2016)</ref> introduced the Neural Variational Docu- ment Model (NVDM), which used Gaussian dis- tribution over topics and averaged over topic-word distribution in the logit space. Although they used the black-box variational inference (VAE), they did not approximate the Dirichlet prior. Srivas- tava and Sutton (2017) approximated the Dirich- let prior with logistic Gaussian using the Laplace approximation of <ref type="bibr" target="#b10">Hennig et al. (2012)</ref> and col- lapsed the hidden topic value z with a mixture of experts <ref type="bibr" target="#b12">(Hinton, 2002)</ref>. This model (AVITM) sig- nificantly improved the topic coherence compared with the NVDM model. However, same as the LDA, AVITM suffers from the data sparsity prob- lem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Topics</head><p>GraphBTM attack ripem rsa encrypt cipher random key cryptography distribution encryption turkish turks greek greece armenian genocide turkey armenia armenians island season score player league game puck pitch win pitcher team israel lebanese israeli lebanon village attack zone arab territory civilian oname printf entry buf char contest stream output int remark AVITM ripem anonymous pgp rsa posting cipher atheism encrypt usenet atheist armenian genocide turks turkish muslim massacre turkey armenians armenia greek season nhl team hockey playoff puck league flyers defensive player israel israeli lebanese arab lebanon arabs civilian territory palestinian militia oname printf buf entry os char contest cpu stream remark LDA drug food health research medical test used development product computer system data software business personal ibm information technology offering common convertible proceeds co due used public filed agreement agreed acquisition acquire purchase sell subject subsidiary completed quarter earnings reported income expects fiscal loss per second <ref type="table">Table 4</ref>: Five selected topics from all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Short Texts</head><p>Early studies on short text topic model mainly fo- cused on adding external knowledge to enrich the information of short texts. <ref type="bibr" target="#b24">Phan et al. (2008)</ref> firstly learned hidden topics from substantial external re- sources to enrich the features in short text. Jin et al. (2011) leveraged the power of transfer learn- ing to learn topics on short texts from auxiliary long text data. However, external knowledge in some domain may not be available. Instead of adding external knowledge, one po- tential way is to add a sparse prior on the topic distribution. <ref type="bibr" target="#b3">Chien and Chang (2014)</ref> used a spike model to control the sparsity of selected topics. <ref type="bibr" target="#b17">Lin et al. (2014)</ref> used the same idea to add the sparsity on both topic and word distribution. Dif- ferent from these approaches, some researchers tried to enhance data without external knowledge. <ref type="bibr" target="#b31">Weng et al. (2010)</ref> aggregated the tweets from one user into a document. <ref type="bibr">Hong and Davison (2010)</ref> combined the tweets containing the same words. Some other used non-probability topic model to solve this problem. <ref type="bibr">Zhu and Xing (2012)</ref> pro- posed sparse topical coding, which relaxed the normalization constraint of admixture proportions and learned hierarchical latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We proposed a Graph Enhanced Autoencoding Variational inference for Biterm Topic Model (GraphBTM). Our model used a black-box ap- proximation inference approach to learn topics through the word co-occurrences (biterms). We modeled the biterms in the form of a graph where the nodes are the words and weighted edges are the counts of the corresponding biterms. On top of this graph representation, we designed a model by GCN layers with a residual connection to ef- fectively extract node representations that preserve the missing connectivity. To overcome the prob- lems of data sparsity in LDA and insufficient topic representation in BTM, we introduced a data ar- gumentation approach by producing a mini-corpus with sampled documents. By setting a proper hy- perparameter of sample size k, we achieved bet- ter topic coherence scores compared with previous works.</p><p>Our GCN model is based on spectral graph convolutions, which requires computing the graph Laplace for each sample. Compared to tasks with one graph input, we need to compute the graph Laplace for every input sample, causing substan- tial computational cost. It is critical to developing a memory efficient processing and storage strategy to handle the large-scale graph data when we gen- eralize GraphBTM to complex tasks. Recently, fastGCN ( <ref type="bibr">Zhang et al., 2018</ref>) interpreted graph convolutions as integral transforms of functions under probability measures. Our following work will consider adopting fastGCN to speed up the process. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Datasets Training instances Ratio of &lt;= 30 Avg size Vocabulary Avg</head><label>Datasets</label><figDesc></figDesc><table>Biterms # 
20 News 
11,259 
28% 
88 
1995 
1249 
All News 
20,000 
2% 
302 
5000 
5535 

Table 1: Datasets statistics. 

Dataset # topics GraphBTM AVITM LDA Online VI 

20 News 
50 
0.28 
0.25 
0.10 
100 
0.26 
0.23 
0.08 

All news 
50 
0.27 
0.24 
0.14 
100 
0.26 
0.23 
0.13 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Average topic coherence.</head><label>2</label><figDesc></figDesc><table>Datasets(# Topics) # Samples Score 

20 News (k=50) 

1 
0.24 
3 
0.28 
10 
0.25 

20 News (k=100) 

1 
0.21 
3 
0.26 
10 
0.25 

All news (k=50) 

1 
0.27 
3 
0.22 
5 
0.20 

All news (k=100) 

1 
0.26 
3 
0.20 
5 
0.17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for different sampling numbers in 
different setting for the two datasets. Score denotes the 
topic coherence score. 

</table></figure>

			<note place="foot" n="1"> https://www.kaggle.com/snapcrack/all-the-news</note>

			<note place="foot" n="2"> https://github.com/akashgit/ autoencoding vi for topic models</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by Na-tional Science Foundation (grant CNS-1842407), National Institutes of Health (grant R01GM110240), and industry mem-bers of NSF Center for Big Learning (http: //nsfcbl.org/index.php/partners/).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Btm: Topic modeling over short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2928" to="2941" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian sparse topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Tzung</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Lan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis. Journal of the American society for information science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple hypergeometric functions: Probabilistic interpretations and statistical uses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dickey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">383</biblScope>
			<biblScope unit="page" from="628" to="637" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Fei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Patrick F Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Replicated softmax: an undirected topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1607" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online learning for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. D. Lafferty, C. K. I. Williams, J. ShaweTaylor, R. S. Zemel, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convergence analysis of two-layer neural networks with relu activation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The dual-sparse topic model: mining focused topics and focused terms in short text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
		<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="539" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Choice of basis for laplace approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Mariethoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to classify short and sparse text &amp; web with hidden topics from largescale data collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susumu</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<publisher>Valletta</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The latent process decomposition of cdna microarray data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Breitling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on computational biology and bioinformatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="156" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01488</idno>
		<title level="m">Autoencoding variational inference for topic models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">All the news: 143,000 articles from 15 american publications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Thompson</surname></persName>
		</author>
		<ptr target="=https://www.kaggle.com/snapcrack/all-the-news" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking lda: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Hanna M Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientific articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Twitterrank: finding topic-sensitive influential twitterers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third ACM international conference on Web search and data mining</title>
		<meeting>the third ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A biterm topic model for short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1445" to="1456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
