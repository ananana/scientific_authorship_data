<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
							<email>saizheng.zhang@umontreal.ca, yoshua.bengio@gmail.com, wcohen@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University ♥ Stanford University ♣ Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal ♦ CIFAR Senior Fellow † Google AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2369" to="2380"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2369</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems&apos; ability to extract relevant facts and perform necessary comparison. We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make ex-plainable predictions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to perform reasoning and inference over natural language is an important aspect of in- telligence. The task of question answering (QA) provides a quantifiable and objective way to test the reasoning ability of intelligent systems. To this end, a few large-scale QA datasets have been pro- posed, which sparked significant progress in this direction. However, existing datasets have limita- tions that hinder further advancements of machine reasoning over natural language, especially in test- ing QA systems' ability to perform multi-hop rea- soning, where the system has to reason with in- formation taken from more than one document to arrive at the answer.</p><p>Paragraph A, Return to Olympus: <ref type="bibr">[1]</ref> Return to Olympus is the only album by the alterna- tive rock band Malfunkshun. <ref type="bibr">[2]</ref> It was released after the band had broken up and after lead singer Andrew Wood (later of Mother Love Bone) had died of a drug overdose in 1990. <ref type="bibr">[3]</ref> Stone Gossard, of Pearl Jam, had compiled the songs and released the album on his label, Loosegroove Records. Paragraph B, Mother Love Bone: <ref type="bibr">[4]</ref> Mother Love Bone was an American rock band that formed in Seattle, <ref type="bibr">Washington in 1987.</ref> [5] The band was active from 1987 to 1990. <ref type="bibr">[6]</ref> Frontman Andrew Wood's personality and compositions helped to catapult the group to the top of the burgeoning late 1980s/early 1990s Seattle music scene. <ref type="bibr">[7]</ref> Wood died only days be- fore the scheduled release of the band's debut album, "Apple", thus ending the group's hopes of success. <ref type="bibr">[8]</ref> The album was finally released a few months later. Q: What was the former band of the member of Mother Love Bone who died just before the release of "Apple"? A: Malfunkshun Supporting facts: 1, 2, 4, 6, 7</p><p>Figure 1: An example of the multi-hop questions in HOTPOTQA. We also highlight the supporting facts in blue italics, which are also part of the dataset.</p><p>First, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning. For example, in SQuAD ( <ref type="bibr" target="#b11">Rajpurkar et al., 2016</ref>) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph. As a result, it has fallen short at testing systems' ability to reason over a larger context. TriviaQA ( <ref type="bibr" target="#b3">Joshi et al., 2017)</ref> and SearchQA ( <ref type="bibr" target="#b2">Dunn et al., 2017</ref>) create a more challenging setting by using information retrieval to collect multiple documents to form the con- text given existing question-answer pairs. Nev- ertheless, most of the questions can be answered by matching the question with a few nearby sen- tences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., over multiple paragraphs).</p><p>Second, existing datasets that target multi-hop reasoning, such as QAngaroo ( <ref type="bibr" target="#b16">Welbl et al., 2018)</ref> and COMPLEXWEBQUESTIONS <ref type="bibr" target="#b14">(Talmor and Berant, 2018)</ref>, are constructed using existing knowl- edge bases <ref type="bibr">(KBs)</ref>. As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and an- swers is inherently limited.</p><p>Third, all of the above datasets only provide dis- tant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it. This makes it difficult for models to learn about the underlying reasoning process, as well as to make explainable predictions.</p><p>To address the above challenges, we aim at cre- ating a QA dataset that requires reasoning over multiple documents, and does so in natural lan- guage, without constraining itself to an existing knowledge base or knowledge schema. We also want it to provide the system with strong supervi- sion about what text the answer is actually derived from, to help guide systems to perform meaning- ful and explainable reasoning.</p><p>We present HOTPOTQA 1 , a large-scale dataset that satisfies these desiderata. HOTPOTQA is col- lected by crowdsourcing based on Wikipedia ar- ticles, where crowd workers are shown multiple supporting context documents and asked explic- itly to come up with questions requiring reason- ing about all of the documents. This ensures it covers multi-hop questions that are more natural, and are not designed with any pre-existing knowl- edge base schema in mind. Moreover, we also ask the crowd workers to provide the supporting facts they use to answer the question, which we also provide as part of the dataset (see <ref type="figure">Figure 1</ref> for an example). We have carefully designed a data collection pipeline for HOTPOTQA, since the col- lection of high-quality multi-hop questions is non- trivial. We hope that this pipeline also sheds light on future work in this direction. Finally, we also collected a novel type of questions-comparison questions-as part of HOTPOTQA, in which we require systems to compare two entities on some shared properties to test their understanding of both language and common concepts such as nu- merical magnitude. We make HOTPOTQA pub- licly available at https://HotpotQA.github.io. <ref type="bibr">1</ref> The name comes from the first three authors' arriving at the main idea during a discussion at a hot pot restaurant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection</head><p>The main goal of our work is to collect a diverse and explainable question answering dataset that requires multi-hop reasoning. One way to do so is to define reasoning chains based on a knowl- edge base <ref type="bibr" target="#b16">(Welbl et al., 2018;</ref><ref type="bibr" target="#b14">Talmor and Berant, 2018)</ref>. However, the resulting datasets are limited by the incompleteness of entity relations and the lack of diversity in the question types. Instead, in this work, we focus on text-based question an- swering in order to diversify the questions and an- swers. The overall setting is that given some con- text paragraphs (e.g., a few paragraphs, or the en- tire Web) and a question, a QA system answers the question by extracting a span of text from the context, similar to <ref type="bibr" target="#b11">Rajpurkar et al. (2016)</ref>. We additionally ensure that it is necessary to perform multi-hop reasoning to correctly answer the ques- tion.</p><p>It is non-trivial to collect text-based multi-hop questions. In our pilot studies, we found that sim- ply giving an arbitrary set of paragraphs to crowd workers is counterproductive, because for most paragraph sets, it is difficult to ask a meaning- ful multi-hop question. To address this challenge, we carefully design a pipeline to collect text-based multi-hop questions. Below, we will highlight the key design choices in our pipeline.</p><p>Building a Wikipedia Hyperlink Graph. We use the entire English Wikipedia dump as our cor- pus. <ref type="bibr">2</ref> In this corpus, we make two observations: (1) hyper-links in the Wikipedia articles often nat- urally entail a relation between two (already dis- ambiguated) entities in the context, which could potentially be used to facilitate multi-hop reason- ing; (2) the first paragraph of each article often contains much information that could be queried in a meaningful way. Based on these observations, we extract all the hyperlinks from the first para- graphs of all Wikipedia articles. With these hy- perlinks, we build a directed graph G, where each edge (a, b) indicates there is a hyperlink from the first paragraph of article a to article b.</p><p>Generating Candidate Paragraph Pairs. To generate meaningful pairs of paragraphs for multi- hop question answering with G, we start by considering an example question "when was the singer and songwriter of Radiohead born?" To answer this question, one would need to first rea- son that the "singer and songwriter of Radiohead" is "Thom Yorke", and then figure out his birth- day in the text. We call "Thom Yorke" a bridge entity in this example. Given an edge (a, b) in the hyperlink graph G, the entity of b can usually be viewed as a bridge entity that connects a and b. As we observe articles b usually determine the theme of the shared context between a and b, but not all articles b are suitable for collecting multi- hop questions. For example, entities like coun- tries are frequently referred to in Wikipedia, but don't necessarily have much in common with all incoming links. It is also difficult, for instance, for the crowd workers to ask meaningful multi- hop questions about highly technical entities like the IPv4 protocol. To alleviate this issue, we con- strain the bridge entities to a set of manually cu- rated pages in Wikipedia (see Appendix A). Af- ter curating a set of pages B, we create candidate paragraph pairs by sampling edges (a, b) from the hyperlink graph such that b ∈ B.</p><p>Comparison Questions. In addition to ques- tions collected using bridge entities, we also collect another type of multi-hop questions- comparison questions. The main idea is that com- paring two entities from the same category usu- ally results in interesting multi-hop questions, e.g., "Who has played for more NBA teams, Michael Jordan or Kobe Bryant?" To facilitate collecting this type of question, we manually curate 42 lists of similar entities (denoted as L) from Wikipedia. <ref type="bibr">3</ref> To generate candidate paragraph pairs, we ran- domly sample two paragraphs from the same list and present them to the crowd worker.</p><p>To increase the diversity of multi-hop questions, we also introduce a subset of yes/no questions in comparison questions. This complements the original scope of comparison questions by offer- ing new ways to require systems to reason over both paragraphs. For example, consider the en- tities Iron Maiden (from the UK) and AC/DC (from Australia). Questions like "Is Iron Maiden or AC/DC from the UK?" are not ideal, because one would deduce the answer is "Iron Maiden" even if one only had access to that article. With yes/no questions, one may ask "Are Iron Maiden and AC/DC from the same country?", which re- 3 This is achieved by manually curating lists from the Wikipedia "List of lists of lists" (https://wiki.sh/ y8qv). One example is "Highest Mountains on Earth".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Overall data collection procedure</head><p>Input: question type ratio r1 = 0.75, yes/no ratio r2 = 0.5 while not finished do if random() &lt; r1 then Uniformly sample an entity b ∈ B Uniformly sample an edge (a, b) Workers ask a question about paragraphs a and b else Sample a list from L, with probabilities weighted by list sizes Uniformly sample two entities (a, b) from the list if random() &lt; r2 then Workers ask a yes/no question to compare a and b else</p><p>Workers ask a question with a span answer to compare a and b end if end if Workers provide the supporting facts end while quires reasoning over both paragraphs.</p><p>To the best of our knowledge, text-based com- parison questions are a novel type of questions that have not been considered by previous datasets. More importantly, answering these questions usu- ally requires arithmetic comparison, such as com- paring ages given birth dates, which presents a new challenge for future model development.</p><p>Collecting Supporting Facts. To enhance the explainability of question answering systems, we want them to output a set of supporting facts nec- essary to arrive at the answer, when the answer is generated. To this end, we also collect the sentences that determine the answers from crowd workers. These supporting facts can serve as strong supervision for what sentences to pay at- tention to. Moreover, we can now test the explain- ability of a model by comparing the predicted sup- porting facts to the ground truth ones.</p><p>The overall procedure of data collection is illus- trated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Desc. Usage # Examples train-easy single-hop training 18,089 train-medium multi-hop training 56,814 train-hard hard multi-hop training 15,661 dev hard multi-hop dev 7,405 test-distractor hard multi-hop test 7,405 test-fullwiki hard multi-hop test 7,405 Total 112,779 <ref type="table">Table 1</ref>: Data split. The splits train-easy, train- medium, and train-hard are combined for training. The distractor and full wiki settings use different test sets so that the gold paragraphs in the full wiki test set remain unknown to any models.</p><p>their questions into the train-easy set if an over- whelming percentage in the sample only required reasoning over one of the paragraphs. We sam- pled these turkers because they contributed more than 70% of our data. This train-easy set contains 18,089 mostly single-hop examples.</p><p>We implemented a question answering model based on the current state-of-the-art architectures, which we discuss in detail in Section 5.1. Based on this model, we performed a three-fold cross validation on the remaining multi-hop examples. Among these examples, the models were able to correctly answer 60% of the questions with high confidence (determined by thresholding the model loss). These correctly-answered questions (56,814 in total, 60% of the multi-hop examples) are split out and marked as the train-medium subset, which will also be used as part of our training set.</p><p>After splitting out train-easy and train-medium, we are left with hard examples. As our ultimate goal is to solve multi-hop question answering, we focus on questions that the latest modeling tech- niques are not able to answer. Thus we constrain our dev and test sets to be hard examples. Specif- ically, we randomly divide the hard examples into four subsets, train-hard, dev, test-distractor, and test-fullwiki. Statistics about the data split can be found in <ref type="table">Table 1</ref>. In Section 5, we will show that combining train-easy, train-medium, and train- hard to train models yields the best performance, so we use the combined set as our default train- ing set. The two test sets test-distractor and test- fullwiki are used in two different benchmark set- tings, which we introduce next.</p><p>We create two benchmark settings. In the first setting, to challenge the model to find the true sup- porting facts in the presence of noise, for each ex- ample we employ bigram tf-idf ( <ref type="bibr" target="#b0">Chen et al., 2017)</ref> to retrieve 8 paragraphs from Wikipedia as dis- tractors, using the question as the query. We mix them with the 2 gold paragraphs (the ones used to collect the question and answer) to construct the distractor setting. The 2 gold paragraphs and the 8 distractors are shuffled before they are fed to the model. In the second setting, we fully test the model's ability to locate relevant facts as well as reasoning about them by requiring it to answer the question given the first paragraphs of all Wikipedia articles without the gold paragraphs specified. This full wiki setting truly tests the per- formance of the systems' ability at multi-hop rea- soning in the wild. <ref type="bibr">5</ref> The two settings present dif- ferent levels of difficulty, and would require tech- niques ranging from reading comprehension to in- formation retrieval. As shown in <ref type="table">Table 1</ref>, we use separate test sets for the two settings to avoid leak- ing information, because the gold paragraphs are available to a model in the distractor setting, but should not be accessible in the full wiki setting.</p><p>We also try to understand the model's good performance on the train-medium split. Manual analysis shows that the ratio of multi-hop ques- tions in train-medium is similar to that of the hard examples (93.3% in train-medium vs. 92.0% in dev), but one of the question types appears more frequently in train-medium compared to the hard splits (Type II: 32.0% in train-medium vs. 15.0% in dev, see Section 4 for the definition of Type II questions). These observations demonstrate that given enough training data, existing neural archi- tectures can be trained to answer certain types and certain subsets of the multi-hop questions. How- ever, train-medium remains challenging when not just the gold paragraphs are present-we show in Appendix C that the retrieval problem on these ex- amples are as difficult as that on their hard cousins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Analysis</head><p>In this section, we analyze the types of questions, types of answers, and types of multi-hop reasoning covered in the dataset.  yes/no questions, we consider as question words WH-words, copulas ("is", "are"), and auxiliary verbs ("does", "did"). Because questions often in- volve relative clauses beginning with WH-words, we define the CQW as the first question word in the question if it can be found in the first three to- kens, or the last question word otherwise. Then, we determine question type by extracting words up to 2 tokens away to the right of the CQW, along with the token to the left if it is one of a few com- mon prepositions (e.g., in the cases of "in which" and "by whom").</p><p>We visualize the distribution of question types in <ref type="figure" target="#fig_1">Figure 2</ref>, and label the ones shared among more than 250 questions. As is shown, our dataset cov- ers a diverse variety of questions centered around entities, locations, events, dates, and numbers, as well as yes/no questions directed at comparing two entities ("Are both A and B ...?"), to name a few.</p><p>Answer Types. We further sample 100 exam- ples from the dataset, and present the types of an- swers in <ref type="table" target="#tab_0">Table 2</ref>. As can be seen, HOTPOTQA covers a broad range of answer types, which matches our initial analysis of question types. We find that a majority of the questions are about en- tities in the articles (68%), and a non-negligible amount of questions also ask about various proper- ties like date (9%) and other descriptive properties such as numbers (8%) and adjectives (4%). Multi-hop Reasoning Types. We also sampled 100 examples from the dev and test sets and man- ually classified the types of reasoning required to answer each question. Besides comparing two en- tities, there are three main types of multi-hop rea- soning required to answer these questions, which we show in <ref type="table" target="#tab_2">Table 3</ref> accompanied with examples.</p><note type="other">Answer Type % Example(s) Person 30 King Edward II, Rihanna Group / Org 13 Cartoonito, Apalachee Location 10 Fort Richardson, California Date 9 10th or even 13th century Number 8 79.92 million, 17 Artwork 8 Die schweigsame Frau Yes/No 6 - Adjective 4 conservative Event 1 Prix Benois de la Danse Other proper noun 6 Cold War, Laban Movement Analysis Common noun 5 comedy, both men and women</note><p>Most of the questions require at least one sup- porting fact from each paragraph to answer. A ma- jority of sampled questions (42%) require chain reasoning <ref type="table">(Type I in the table)</ref>, where the reader must first identify a bridge entity before the second hop can be answered by filling in the bridge. One strategy to answer these questions would be to de- compose them into consecutive single-hop ques- tions. The bridge entity could also be used im- plicitly to help infer properties of other entities re- lated to it. In some questions (Type III), the entity in question shares certain properties with a bridge entity (e.g., they are collocated), and we can in- fer its properties through the bridge entity. An- other type of question involves locating the answer entity by satisfying multiple properties simultane- ously (Type II). Here, to answer the question, one could find the set of all entities that satisfy each of the properties mentioned, and take an intersection to arrive at the final answer. Questions comparing two entities (Comparison) also require the system to understand the properties in question about the two entities (e.g., nationality), and sometimes re- quire arithmetic such as counting (as seen in the table) or comparing numerical values ("Who is older, A or B?"). Finally, we find that sometimes the questions require more than two supporting facts to answer <ref type="bibr">(Other)</ref>. In our analysis, we also find that for all of the examples shown in the ta- ble, the supporting facts provided by the Turkers match exactly with the limited context shown here,</p><note type="other">Reasoning Type % Example(s)</note><p>Inferring the bridge entity to complete the 2nd-hop question <ref type="table">(Type I)</ref> 42 Paragraph A: The 2015 Diamond Head Classic was a college basketball tournament ... Buddy Hield was named the tournament's MVP. Paragraph B: Chavano Rainier "Buddy" Hield is a Bahamian professional basketball player for the Sacramento Kings of the NBA...  showing that the supporting facts collected are of high quality. Aside from the reasoning types mentioned above, we also estimate that about 6% of the sam- pled questions can be answered with one of the two paragraphs, and 2% of them unanswerable. We also randomly sampled 100 examples from train-medium and train-hard combined, and the proportions of reasoning types are: Type I 38%, Type II 29%, Comparison 20%, Other 7%, Type III 2%, single-hop 2%, and unanswerable 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Architecture and Training</head><p>To test the performance of leading QA systems on our data, we reimplemented the architecture described in <ref type="bibr" target="#b1">Clark and Gardner (2017)</ref> as our baseline model. We note that our implementa- tion without weight averaging achieves perfor- mance very close to what the authors reported on SQuAD (about 1 point worse in F 1 ). Our implemented model subsumes the latest techni- cal advances on question answering, including character-level models, self-attention ( <ref type="bibr" target="#b15">Wang et al., 2017)</ref>, and bi-attention ( <ref type="bibr" target="#b13">Seo et al., 2017)</ref>. Combin- ing these three key components is becoming stan- dard practice, and various state-of-the-art or com- petitive architectures ( <ref type="bibr" target="#b4">Liu et al., 2018;</ref><ref type="bibr" target="#b1">Clark and Gardner, 2017;</ref><ref type="bibr" target="#b15">Wang et al., 2017;</ref><ref type="bibr" target="#b13">Seo et al., 2017;</ref><ref type="bibr" target="#b9">Pan et al., 2017;</ref><ref type="bibr" target="#b12">Salant and Berant, 2018;</ref><ref type="bibr" target="#b17">Xiong et al., 2018</ref>) on SQuAD can be viewed as simi- lar to our implemented model. To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the prob- abilities of "yes", "no", and span-based answers. During decoding, we first use the 3-way output to determine whether the answer is "yes", "no", or a text span. If it is a text span, we further search for the most probable span.</p><p>Supporting Facts as Strong Supervision. To evaluate the baseline model's performance in pre- dicting explainable supporting facts, as well as how much they improve QA performance, we additionally design a component to incorporate such strong supervision into our model. For each sentence, we concatenate the output of the self- attention layer at the first and last positions, and use a binary linear classifier to predict the prob- ability that the current sentence is a supporting fact. We minimize a binary cross entropy loss for this classifier. This objective is jointly optimized with the normal question answering objective in a multi-task learning setting, and they share the same low-level representations. With this classi- fier, the model can also be evaluated on the task of supporting fact prediction to gauge its explainabil- ity. Our overall architecture is illustrated in <ref type="figure" target="#fig_2">Figure  3</ref>. Though it is possible to build a pipeline system, in this work we focus on an end-to-end one, which is easier to tune and faster to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We evaluate our model in the two benchmark set- tings. In the full wiki setting, to enable efficient tf- idf retrieval among 5,000,000+ wiki paragraphs, given a question we first return a candidate pool of at most 5,000 paragraphs using an inverted-index- based filtering strategy <ref type="bibr">6</ref> and then select the top 10 paragraphs in the pool as the final candidates using bigram tf-idf. <ref type="bibr">7</ref> Retrieval performance is shown in <ref type="bibr">6</ref> See Appendix C for details. <ref type="bibr">7</ref> We choose the number of final candidates as 10 to stay consistent with the distractor setting where candidates are 2 <ref type="table" target="#tab_5">Table 5</ref>. After retrieving these 10 paragraphs, we then use the model trained in the distractor setting to evaluate its performance on these final candi- date paragraphs.</p><p>Following previous work <ref type="bibr" target="#b11">(Rajpurkar et al., 2016)</ref>, we use exact match (EM) and F 1 as two evaluation metrics. To assess the explainability of the models, we further introduce two sets of met- rics involving the supporting facts. The first set fo- cuses on evaluating the supporting facts directly, namely EM and F 1 on the set of supporting fact sentences as compared to the gold set. The second set features joint metrics that combine the evalu- ation of answer spans and supporting facts as fol- lows. For each example, given its precision and recall on the answer span (P (ans) , R (ans) ) and the supporting facts (P (sup) , R (sup) ), respectively, we calculate joint F 1 as</p><formula xml:id="formula_0">P (joint) = P (ans) P (sup) , R (joint) = R (ans) R (sup) , Joint F 1 = 2P (joint) R (joint) P (joint) + R (joint) .</formula><p>Joint EM is 1 only if both tasks achieve an ex- act match and otherwise 0. Intuitively, these met- rics penalize systems that perform poorly on ei- ther task. All metrics are evaluated example-by- example, and then averaged over examples in the evaluation set.</p><p>The performance of our model on the bench- mark settings is reported in <ref type="table" target="#tab_4">Table 4</ref>, where all numbers are obtained with strong supervision over supporting facts. From the distractor setting to the full wiki setting, expanding the scope of the con- text increases the difficulty of question answering. The performance in the full wiki setting is sub- stantially lower, which poses a challenge to exist- ing techniques on retrieval-based question answer- ing. Overall, model performance in all settings is significantly lower than human performance as shown in Section 5.3, which indicates that more technical advancements are needed in future work.</p><p>We also investigate the explainability of our model by measuring supporting fact prediction performance. Our model achieves 60+ support- ing fact prediction F 1 and ∼40 joint F 1 , which in- dicates there is room for further improvement in terms of explainability.</p><p>In    <ref type="table" target="#tab_3">Table 6</ref>: Performance breakdown over different ques- tion types on the dev set in the distractor setting. "Br" denotes questions collected using bridge entities, and "Cp" denotes comparison questions.</p><p>than questions involving bridge entities (as defined in Section 2), which indicates that better mod- eling this novel question type might need better neural architectures. In the full wiki setting, the performance of bridge entity questions drops sig- nificantly while that of comparison questions de- creases only marginally. This is because both en- tities usually appear in the comparison questions, and thus reduces the difficulty of retrieval. Com- bined with the retrieval performance in <ref type="table" target="#tab_5">Table 5</ref>, we believe that the deterioration in the full wiki setting in <ref type="table" target="#tab_4">Table 4</ref> is largely due to the difficulty of retrieving both entities. We perform an ablation study in the distractor setting, and report the results in <ref type="table">Table 7</ref>. Both self- attention and character-level models contribute notably to the final performance, which is consis- tent with prior work. This means that techniques targeted at single-hop QA are still somewhat ef- fective in our setting. Moreover, removing strong supervision over supporting facts decreases per- formance, which demonstrates the effectiveness of our approach and the usefulness of the supporting facts. We establish an estimate of the upper bound of strong supervision by only considering the sup- porting facts as the oracle context input to our  <ref type="table">Table 7</ref>: Ablation study of question answering perfor- mance on the dev set in the distractor setting. "-sup fact" means removing strong supervision over support- ing facts from our model. "-train-easy" and "-train- medium" means discarding the according data splits from training. "gold only" and "sup fact only" refer to using the gold paragraphs or the supporting facts as the only context input to the model. model, which achieves a 10+ F 1 improvement over not using the supporting facts. Compared with the gain of strong supervision in our model (∼2 points in F 1 ), our proposed method of incorporating sup- porting facts supervision is most likely subopti- mal, and we leave the challenge of better model- ing to future work. At last, we show that combin- ing all data splits (train-easy, train-medium, and train-hard) yields the best performance, which is adopted as the default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Establishing Human Performance</head><p>To establish human performance on our dataset, we randomly sampled 1,000 examples from the dev and test sets, and had at least three additional Turkers provide answers and supporting facts for these examples. As a baseline, we treat the orig- inal Turker during data collection as the predic- tion, and the newly collected answers and support- ing facts as references, to evaluate human perfor- mance. For each example, we choose the answer and supporting fact reference that maximize the F 1 score to report the final metrics to reduce the effect of ambiguity ( <ref type="bibr" target="#b11">Rajpurkar et al., 2016</ref>  <ref type="table">Table 8</ref>: Comparing baseline model performance with human performance on 1,000 random samples. "Hu- man UB" stands for the upper bound on annotator per- formance on HOTPOTQA. For details please refer to the main body.</p><p>As can be seen in <ref type="table">Table 8</ref>, the original crowd worker achieves very high performance in both finding supporting facts, and answering the ques- tion correctly. If the baseline model were provided with the correct supporting paragraphs to begin with, it achieves parity with the crowd worker in finding supporting facts, but still falls short at finding the actual answer. When distractor para- graphs are present, the performance gap between the baseline model and the crowd worker on both tasks is enlarged to ∼30% for both EM and F 1 .</p><p>We further establish the upper bound of human performance in HOTPOTQA, by taking the maxi- mum EM and F 1 for each example. Here, we use each Turker's answer in turn as the prediction, and evaluate it against all other workers' answers. As can be seen in <ref type="table">Table 8</ref>, most of the metrics are close to 100%, illustrating that on most examples, at least a subset of Turkers agree with each other, showing high inter-annotator agreement. We also note that crowd workers agree less on supporting facts, which could reflect that this task is inher- ently more subjective than answering the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Various recently-proposed large-scale QA datasets can be categorized in four categories.</p><p>Single-document datasets. SQuAD ( <ref type="bibr" target="#b11">Rajpurkar et al., 2016</ref><ref type="bibr" target="#b10">Rajpurkar et al., , 2018</ref> questions that are relatively simple because they usually require no more than one sentence in the paragraph to answer.</p><p>Multi-document datasets. TriviaQA ( <ref type="bibr" target="#b3">Joshi et al., 2017</ref>) and SearchQA ( <ref type="bibr" target="#b2">Dunn et al., 2017</ref>) contain question answer pairs that are accompa- nied with more than one document as the context. This further challenges QA systems' ability to accommodate longer contexts. However, since the supporting documents are collected after the ques- tion answer pairs with information retrieval, the questions are not guaranteed to involve interesting reasoning between multiple documents. KB-based multi-hop datasets. Recent datasets like QAngaroo ( <ref type="bibr" target="#b16">Welbl et al., 2018)</ref> and COM- PLEXWEBQUESTIONS <ref type="bibr" target="#b14">(Talmor and Berant, 2018)</ref> explore different approaches of using pre-existing knowledge bases (KB) with pre-defined logic rules to generate valid QA pairs, to test QA models' ca- pability of performing multi-hop reasoning. The diversity of questions and answers is largely lim- ited by the fixed KB schemas or logical forms. Furthermore, some of the questions might be an- swerable by one text sentence due to the incom- pleteness of KBs.</p><p>Free-form answer-generation datasets. MS MARCO ( <ref type="bibr" target="#b7">Nguyen et al., 2016</ref>) contains 100k user queries from Bing Search with human generated answers. Systems generate free-form answers and are evaluated by automatic metrics such as ROUGE-L and BLEU-1. However, the reliabil- ity of these metrics is questionable because they have been shown to correlate poorly with human judgement ( <ref type="bibr" target="#b8">Novikova et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We present HOTPOTQA, a large-scale question answering dataset aimed at facilitating the devel- opment of QA systems capable of performing ex- plainable, multi-hop reasoning over diverse nat- ural language. We also offer a new type of fac- toid comparison questions to test systems' ability to extract and compare various entity properties in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Collection Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Data Preprocessing</head><p>We downloaded the dump of English Wikipedia of October 1, 2017, and extracted text and hyperlinks with WikiExtractor. <ref type="bibr">8</ref> We use Stanford CoreNLP 3.8.0 ( <ref type="bibr" target="#b5">Manning et al., 2014</ref>) for word and sen- tence tokenization. We use the resulting sentence boundaries for collection of supporting facts, and use token boundaries to check whether Turkers are providing answers that cover spans of entire to- kens to avoid nonsensical partial-word answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Further Data Collection Details</head><p>Details on Curating Wikipedia Pages. To make sure the sampled candidate paragraph pairs are intuitive for crowd workers to ask high-quality multi-hop questions about, we manually curate 591 categories from the lists of popular pages by WikiProject. <ref type="bibr">9</ref> For each category, we sample (a, b) pairs from the graph G where b is in the considered category, and manually check whether a multi-hop question can be asked given the pair (a, b). Those categories with a high probability of permitting multi-hop questions are selected.</p><p>Bonus Structures. To incentivize crowd work- ers to produce higher-quality data more efficiently, we follow <ref type="bibr" target="#b18">Yang et al. (2018)</ref>, and employ bonus structures. We mix two settings in our data collec- tion process. In the first setting, we reward the top (in terms of numbers of examples) workers every 200 examples. In the second setting, the workers get bonuses based on their productivity (measured as the number of examples per hour).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Crowd Worker Interface</head><p>Our crowd worker interface is based on ParlAI ( <ref type="bibr" target="#b6">Miller et al., 2017)</ref>, an open-source project that facilitates the development of dialog systems and data collection with a dialog interface. We adapt ParlAI for collecting question answer pairs by converting the collection workflow into a system- oriented dialog. This allows us to have more con- trol over the turkers input, as well as provide turk- ers with in-the-loop feedbacks or helpful hints to help Turkers finish the task, and therefore speed up the collection process.</p><p>Please see <ref type="figure">Figure 4</ref> for an example of the worker interface during data collection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Data Analysis</head><p>To further look into the diversity of the data in HOTPOTQA, we further visualized the distribu- tion of question lengths in the dataset in <ref type="figure" target="#fig_3">Figure  5</ref>. Besides being diverse in terms of types as is show in the main text, questions also vary greatly in length, indicating different levels of complexity and details covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Full Wiki Setting Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 The Inverted Index Filtering Strategy</head><p>In the full wiki setting, we adopt an efficient inverted-index-based filtering strategy for prelim- inary candidate paragraph retrieval. We provide details in Algorithm 2, where we set the control threshold N = 5000 in our experiments. For some of the question q, its corresponding gold para-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Inverted Index Filtering Strategy</head><p>Input: question text q, control threshold N , ngram-to- Wikidoc inverted index D Inintialize: Extract unigram + bigram set rq from q N cand = +∞ Cgram = 0 while N cands &gt; N do Cgram = Cgram + 1 Set S overlap to be an empty dictionary for w ∈ rq do</p><formula xml:id="formula_1">for d ∈ D[w] do if d not in S overlap then S overlap [d] = 1 else S overlap [d] = S overlap [d] + 1 end if end for end for S cand = ∅ for d in S overlap do if S overlap [d]</formula><p>≥ Cgram then S cand = S cand ∪ {d} end if end for N cands = |S cand | end while return S cand graphs may not be included in the output candidate pool S cand , we set such missing gold paragraph's rank as |S cand | + 1 during the evaluation, so MAP and Mean Rank reported in this paper are upper bounds of their true values. <ref type="table">Table 9</ref> shows the comparison between train- medium split and hard examples like dev and test under retrieval metrics in full wiki setting. As we can see, the performance gap between train- medium split and its dev/test is close, which im- plies that train-medium split has a similar level of difficulty as hard examples under the full wiki set- ting in which a retrieval model is necessary as the first processing step.  <ref type="table">Table 9</ref>: Retrieval performance comparison on full wiki setting for train-medium, dev and test with 1,000 ran- dom samples each. MAP and are in %. Mean Rank averages over retrieval ranks of two gold paragraphs. CorAns Rank refers to the rank of the gold paragraph containing the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Compare train-medium Split to Hard Ones</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Question Types.</head><label></label><figDesc>We heuristically identified question types for each collected question. To identify the question type, we first locate the cen- tral question word (CQW) in the question. Since HOTPOTQA contains comparison questions and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Types of questions covered in HOTPOTQA. Question types are extracted heuristically, starting at question words or prepositions preceding them. Empty colored blocks indicate suffixes that are too rare to show individually. See main text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our model architecture. Strong supervision over supporting facts is used in a multi-task setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8Figure 5 :</head><label>5</label><figDesc>Figure 4: Screenshot of our worker interface on Amazon Mechanical Turk.</figDesc><graphic url="image-1.png" coords="11,305.44,62.04,220.64,157.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Types of answers in HOTPOTQA. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Q: Which team does the player named 2015 Diamond Head Classic's MVP play for?</head><label></label><figDesc></figDesc><table>Comparing two enti-
ties (Comparison) 

27 Paragraph A: LostAlone were a British rock band ... consisted of Steven Battelle, Alan 
Williamson, and Mark Gibson... 
Paragraph B: Guster is an American alternative rock band ... Founding members Adam 
Gardner, Ryan Miller, and Brian Rosenworcel began... 
Q: Did LostAlone and Guster have the same number of members? (yes) 

Locating the answer 
entity by checking 
multiple 
properties 
(Type II) 

15 Paragraph A: Several current and former members of the Pittsburgh Pirates -... John 
Milner, Dave Parker, and Rod Scurry... 
Paragraph B: David Gene Parker, nicknamed "The Cobra", is an American former player 
in Major League Baseball... 
Q: Which former member of the Pittsburgh Pirates was nicknamed "The Cobra"? 

Inferring about the 
property of an entity 
in question through 
a bridge entity (Type 
III) 

6 Paragraph A: Marine Tactical Air Command Squadron 28 is a United States Marine Corps 
aviation command and control unit based at Marine Corps Air Station Cherry Point... 
Paragraph B: Marine Corps Air Station Cherry Point ... is a United States Marine Corps 
airfield located in Havelock, North Carolina, USA ... 
Q: What city is the Marine Air Control Group 28 located in? 

Other types of reason-
ing that require more 
than two supporting 
facts (Other) 

2 Paragraph A: ... the towns of Yodobashi, Okubo, Totsuka, and Ochiai town were merged 
into Yodobashi ward. ... Yodobashi Camera is a store with its name taken from the town and 
ward. 
Paragraph B: Yodobashi Camera Co., Ltd. is a major Japanese retail chain specializing in 
electronics, PCs, cameras and photographic equipment. 
Q: Aside from Yodobashi, what other towns were merged into the ward which gave the major 
Japanese retail chain specializing in electronics, PCs, cameras, and photographic equipment 
it's name? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Types of multi-hop reasoning required to answer questions in the HOTPOTQA dev and test sets. We show in orange bold italics bridge entities if applicable, blue italics supporting facts from the paragraphs that connect directly to the question, and green bold the answer in the paragraph or following the question. The remaining 8% are single-hop (6%) or unanswerable questions (2%) by our judgement.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 ,</head><label>6</label><figDesc>we break down the performance on different question types. In the distractor set- ting, comparison questions have lower F 1 scores gold paragraphs plus 8 distractors.</figDesc><table>Setting 
Split 
Answer 
Sup Fact 
Joint 

EM 
F1 
EM 
F1 
EM 
F1 

distractor dev 
44.44 58.28 21.95 66.66 11.56 40.86 
distractor test 
45.46 58.99 22.24 66.62 12.04 41.37 

full wiki 
dev 
24.68 34.36 
5.28 40.98 
2.54 17.73 
full wiki 
test 
25.23 34.40 
5.07 40.69 
2.63 17.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Main results: the performance of question answering and supporting fact prediction in the two benchmark 
settings. We encourage researchers to report these metrics when evaluating their methods. 

Set 
MAP Mean Rank Hits@2 Hits@10 

dev 43.93 
314.71 
39.43 
56.06 
test 43.21 
314.05 
38.67 
55.88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Retrieval performance in the full wiki setting. 
Mean Rank is averaged over the ranks of two gold para-
graphs. 

Setting 
Br EM Br F1 Cp EM Cp F1 

distractor 
43.41 
59.09 
48.55 
55.05 
full wiki 
19.76 
30.42 
43.87 
50.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Setting 
Answer 
Sp Fact 
Joint 

EM 
F1 
EM 
F1 
EM 
F1 

gold only 
65.87 74.67 59.76 90.41 41.54 68.15 
distractor 
60.88 68.99 30.99 74.67 20.06 52.37 

Human 
83.60 91.40 61.50 90.04 52.30 82.55 
Human UB 96.80 98.77 87.40 97.56 84.60 96.37 

</table></figure>

			<note place="foot" n="2"> https://dumps.wikimedia.org/</note>

			<note place="foot" n="3"> Processing and Benchmark Settings We collected 112,779 valid examples in total on Amazon Mechanical Turk 4 using the ParlAI interface (Miller et al., 2017) (see Appendix A).To isolate potential single-hop questions from the desired multi-hop ones, we first split out a subset of data called train-easy. Specifically, we randomly sampled questions (∼3-10 per Turker) from top-contributing turkers, and categorized all 4 https://www.mturk.com/</note>

			<note place="foot" n="5"> As we required the crowd workers to use complete entity names in the question, the majority of the questions are unambiguous in the full wiki setting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partly funded by the Facebook ParlAI Research Award. ZY, WWC, and RS are sup-ported by a Google grant, the DARPA grant D17AP00001, the ONR grants N000141512791, N000141812861, and the Nvidia NVAIL Award. SZ and YB are supported by Mila, Université de Montréal. PQ and CDM are supported by the Na-tional Science Foundation under Grant No. IIS-1514268. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">SearchQA: A new Q&amp;A dataset augmented with context from a search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugur</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic answer networks for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06476</idno>
		<title level="m">ParlAI: A dialog research software platform</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 30th Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why we need new evaluation metrics for NLG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">Cercas</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Bin Cao, Deng Cai, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for SQuAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contextualized word representations for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DCN+: Mixed objective and deep residual coattention for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mastering the dungeon: Grounded language learning by mechanical turker descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
