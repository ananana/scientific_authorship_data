<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Multiple System Summary Lengths: A Case Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Shapira</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<addrLine>Ramat-Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<addrLine>Ramat-Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ronen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<addrLine>Ramat-Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judit</forename><surname>Bar-Ilan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Information Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<addrLine>Ramat-Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Amsterdamer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<addrLine>Ramat-Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<addrLine>Ramat-Gan</addrLine>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Multiple System Summary Lengths: A Case Study</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="774" to="778"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>774</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Practical summarization systems are expected to produce summaries of varying lengths, per user needs. While a couple of early sum-marization benchmarks tested systems across multiple summary lengths, this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths. In this paper, we raise the research question of whether reference summaries of a single length can be used to reliably evaluate system summaries of multiple lengths. For that, we have analyzed a couple of datasets as a case study, using several variants of the ROUGE metric that are standard in summa-rization evaluation. Our findings indicate that the evaluation protocol in question is indeed competitive. This result paves the way to practically evaluating varying-length summaries with simple, possibly existing, summarization benchmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated summarization systems typically pro- duce a text that mimics a manual summary. In these systems, an important aspect is the output summary length, which may vary according to user needs. Consequently, output length has been a common tunable parameter in pre-neural sum- marization systems and has been incorporated re- cently in few neural models as well ( <ref type="bibr" target="#b4">Kikuchi et al., 2016;</ref><ref type="bibr" target="#b0">Fan et al., 2017;</ref><ref type="bibr" target="#b1">Ficler and Goldberg, 2017)</ref>.</p><p>It was originally assumed that summarization systems should be assessed across multiple sum- mary lengths. For that, the earliest Document Un- derstand Conference (DUC) (NIST, 2011) bench- marks, in 2001 and 2002, defined several tar- get summary lengths and evaluated each summary against (manually written) reference summaries of the same length.</p><p>However, due to the high cost incurred, subse- quent DUC and TAC <ref type="bibr" target="#b11">(NIST, 2018)</ref> benchmarks <ref type="bibr">(2003)</ref><ref type="bibr">(2004)</ref><ref type="bibr">(2005)</ref><ref type="bibr">(2006)</ref><ref type="bibr">(2007)</ref><ref type="bibr">(2008)</ref><ref type="bibr">(2009)</ref><ref type="bibr">(2010)</ref><ref type="bibr">(2011)</ref><ref type="bibr">(2012)</ref><ref type="bibr">(2013)</ref><ref type="bibr">(2014)</ref>, as well as the more recently popular datasets CNN/Daily Mail ( <ref type="bibr" target="#b7">Nallapati et al., 2016)</ref> and Gigaword ( <ref type="bibr" target="#b3">Graff et al., 2003)</ref>, included refer- ences and evaluation for just one summary length per input text. Accordingly, systems were asked to produce a single summary, of corresponding length. This decision was partly supported by an observation that system rankings tended to corre- late across different summary lengths <ref type="bibr" target="#b12">(Over et al., 2007)</ref>, even though, as we show in Section 2, this correlation is limited.</p><p>In this paper, we propose that the summariza- tion community should consider resuming evalu- ating summarization systems over multiple length outputs, as it would allow better assessment of length-related performance within and across sys- tems (illustrated in Section 3). To avoid the need in multiple-length reference summaries we raise the following research question: can reference sum- maries of a single length be used to evaluate sys- tem summaries of multiple lengths, as reliably as when using references of multiple lengths, with respect to different standard evaluation metrics? Recently, <ref type="bibr" target="#b4">Kikuchi et al. (2016)</ref> evaluated system summaries of three different lengths against ref- erence summaries of a single length. Yet, their evaluation methodology was not assessed through correlation to human judgment, as has been com- monly done for other automatic evaluation pro- tocols. Here, we provide a closer look into this methodology, given its potential value.</p><p>As a first accessible case study, we test our re- search question over the DUC 2001 and 2002 data (Section 2). To the best of our knowledge, these are the only two datasets that include multiple length reference and submitted system summaries, as well as manual assessment of the latter. Our analysis reveals that, for this data and with respect to various highly utilized automatic ROUGE met- rics, the answer to our question is affirmative, in  terms of correlation with human judgment.</p><p>Our promising results suggest repeating the as- sessment methodology presented here in future work, to test our question over more recent and broader summarization datasets and human eval- uation schemes. This, in turn, would allow the community to feasibly resume proper evaluation and deliberate development of systems that target effective summaries across a range of lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Case Study Analysis</head><p>Here, we first examine the relevance of our pro- posal to reinstitute summarization evaluation over multiple summary lengths. Then, we investigate our research question of whether using reference summaries of a single length suffices for evalu- ating system summaries of multiple lengths. We turn to the DUC 2001 and 2002 multi-document summarization datasets, which, to the best of our knowledge, are the only available datasets that provide the necessary requirements for this anal- ysis (see <ref type="table">Table 1</ref>).</p><p>The importance of evaluating and comparing systems at several lengths is demonstrated with the observation that system rankings can change quite significantly at different summary lengths. In 2001, the Spearman correlation between the available human rankings of systems at the 50- word and 400-word lengths is 0.61. For example, the system ranked first at length 50 ranks sixth at lengths 200 and 400. Even for the human sys- tem ranking at the 100-word length, which devi- ates the least from human rankings at the other lengths, the correlation with system ranking at the 400 length is only 0.73. Generally, the larger the difference between a pair of summary lengths, the greater the fluctuation in system rankings. Simi- lar trends were observed for DUC 2002, and when comparing system rankings by automatic ROUGE scoring (both rankings are elaborated below). Ob- viously, such performance differences are over- looked when evaluating systems over summaries of a single length.</p><p>Next, we turn to investigate our research ques- tion. In this paper, we examine it with respect to automatic summary evaluation, which has become most common for system development and evalua- tion, thanks to its speed and low cost. Specifically, we use several variants of the ROUGE metric <ref type="bibr" target="#b5">(Lin, 2004)</ref>, which is almost exclusively utilized as an automatic evaluation metric class for summariza- tion. ROUGE variants are based on word sequence overlap between a system summary and a refer- ence summary, where each variant measures a dif- ferent aspect of text comparison. Despite its pit- falls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods <ref type="bibr" target="#b5">(Lin, 2004;</ref><ref type="bibr" target="#b13">Over and James, 2004;</ref><ref type="bibr" target="#b12">Over et al., 2007;</ref><ref type="bibr" target="#b8">Nenkova et al., 2007;</ref><ref type="bibr" target="#b6">Louis and Nenkova, 2013;</ref><ref type="bibr" target="#b14">Peyrard et al., 2017)</ref>, such as SEE <ref type="bibr" target="#b5">(Lin, 2001</ref>), responsiveness <ref type="bibr" target="#b9">(NIST, 2006</ref>) and Pyramid ( <ref type="bibr" target="#b8">Nenkova et al., 2007</ref>).</p><p>We follow the same methodology of assessing the reliability of automatic evaluation scores by measuring their correlation to human evaluation scores. In our case, DUC 2001 and 2002 ap- plied the SEE manual evaluation method. NIST assessors compared systems' summaries to refer- ence summaries, which were all decomposed into a list of elementary discourse units (EDUs). Each reference EDU was marked throughout the sys- tem EDUs and was scored for how well it was expressed. The final manually evaluated scores, called the human mean content coverage scores, are provided in the DUC datasets. We can then correlate the human-based system ranking, at- tained from these provided scores, to the system ranking attained from the automatic scores that we calculate using our proposed methodology.</p><p>As a baseline, we consider the ROUGE Recall scores obtained by the standard reference sum- mary configuration (Standard, first row in Ta- ble 2), that is, when system summaries of each length (table columns) are evaluated against ref- erence summaries of the same length. This is the same configuration used by Lin (2004) when intro- ducing and assessing ROUGE. Then, looking into our research question, we consider reference sum- mary configurations in which system summaries of all lengths are evaluated against reference sum- maries of a single chosen length (OnlyNNN, sub- sequent rows of <ref type="table">Table 2</ref>). In each configuration (each row), we repeat the evaluation twice: once using the complete set of available reference sum-System <ref type="table">Summary Length  50  100  200  400  Avg. across lengths  3refs  1ref  3refs  1ref  3refs  1ref  3refs  1ref  3refs  1ref</ref> Reference Set  <ref type="table">Table 2</ref>: Pearson correlations between ROUGE-1 and human scores over DUC 2001 for different system summary lengths (column pairs) and different reference summary configurations (rows), when using one reference or three. The first baseline row presents absolute correlations while successive rows show relative differences to the baseline.  <ref type="table">Table 3</ref>: Averaged correlations (across system summary lengths, equivalent to the rightmost columns in <ref type="table">Table 2</ref>) for different ROUGE variants (column pairs) and reference summary configurations (rows), when using 1 reference or multiple. The first row presents absolute correlations, with relative differences in subsequent rows.</p><formula xml:id="formula_0">2001 2002 R-1 R-2 R-L R-1 R-2 R-L<label>3refs</label></formula><p>maries of the utilized reference length, and once with just one randomly chosen reference summary from that set (the 3refs and 1ref sub-columns). For each reference summary configuration, we compute ROUGE Recall system scores 1 for the three common ROUGE variants R-1, R-2 and R-L, which compare unigrams, bigrams and the longest common subsequence, respectively. Sys- tem scores, per summary length, are obtained by averaging across all summarized texts. We then calculate their Pearson correlation 2 with the avail- able human mean content coverage scores for the systems. The first row of <ref type="table">Table 2</ref> shows these cor- relations, considering the R-1 scores for the DUC 2001 systems, per summary length. The subse- quent rows show the corresponding figures for the single-reference-length configurations. For read- ability, we present in these rows the relative differ- ences to the Standard baseline row. Hence, pos- itive values indicate a configuration that is at least as good as the standard configuration. <ref type="table">Table 3</ref> presents correlations averaged over all summary lengths, for the three ROUGE variants over both datasets. We see in the tables that evalu- ating system summaries of all lengths against ref- erences of a single length often performs on par with the standard configuration. In particular, the single fixed set of 50-word reference summaries performs overall as well as the standard approach, and, although not substantially, is the most effec- tive configuration within the data analyzed. In other words, in this dataset, the 50-word reference summaries provide a "test sample" for evaluating the longer system summaries, which is as effective as the same length references used by the standard method.</p><p>We note that even when a single reference sum- mary is available, reasonable correlations with hu- man scores are obtained for the 50 word reference. This suggests that it may be possible to compare system summaries of multiple lengths even against a single reference summary, of a relatively short length. This observation seems to deserve further assessment over recent large scale datasets, such as CNN/DailyMail, which provide a single rela- tively short reference for each summarized text.</p><p>In addition to correlation to human assessment, we computed the correlations between system rankings calculated by Standard and those calcu- lated by Only50, at each system summary length. We find very high correlations (above 0.95 for all system summary lengths, in both datasets) when using multiple references and slightly lower (0.85 to 0.9) with one reference summary. These figures show that the Only50 configuration ranks systems very similarly to Standard.</p><p>To further verify our results, we computed cor- relations in two additional settings. First, we con- ducted the same analysis, excluding 2-3 of the worst systems, which might artificially boost the correlation ( <ref type="bibr" target="#b16">Rankel et al., 2013</ref>). Second, we com- puted score differences between all pairs of sys- tems, for both human and ROUGE scores, and computed the correlation between these two sets of differences <ref type="bibr" target="#b15">(Rankel et al., 2011</ref>). In both cases we observed rather consistent results, assessing that a single set of short reference summaries eval- uates system summaries of different lengths just as well as the standard configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-length Summary Evaluation</head><p>This section illustrates how system performances can be measured and compared when evaluating them on outputs of varying lengths against a sin- gle reference point. <ref type="figure" target="#fig_1">Figure 1</ref> presents the ROUGE scores of the Only50 configuration for three DUC- 01 submitted systems, and for ICSISumm ( <ref type="bibr" target="#b2">Gillick et al., 2008</ref>), a later competitive system.</p><p>As expected when measuring ROUGE Recall against a fixed reference length, longer system summaries typically cover more of the reference summaries content than shorter ones, yielding higher scores. Yet, it can be noted, for example, that the value of the 400-word summary of system R in the figure is lower than that of the 200-word summaries of the other systems. Such a compar- ison is impossible in the standard setup, as each system length is evaluated against different ref- erence summaries. We note that similar compar- isons are embedded in the evaluations of <ref type="bibr" target="#b17">Steinberger and Jezek (2004)</ref> and <ref type="bibr" target="#b4">Kikuchi et al. (2016)</ref>, who also evaluated multiple summary lengths.</p><p>Further, one can define the marginal value of longer summaries of a given system as the ROUGE score increase per number of additional words, namely the graph slope. This denotation allows measuring the effectiveness of producing longer summaries. For example, deploying sys- tem R, we might decide to output only summaries no longer than 200 words, since the marginal value of longer summaries becomes too small. The other systems, on the other hand, seem marginally effec- tive also in 400 word summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We proposed the potential value of evaluat- ing summarization systems at different summary lengths. Such evaluations would allow proper evaluation of systems' "length knob", track- ing how their ranking changes across summary lengths as well as tracking the cross-length be- havior of individual systems. Given that reference summaries of a single length are usually available in practice, we analyzed the potential use of refer- ence summaries of a single length for evaluating system summaries of multiple lengths. We found, on the only two datasets readily available for such analysis, that this configuration is as reliable as the standard configuration, which evaluates each sys- tem summary against a reference of a matching length.</p><p>To broadly substantiate our findings, we pro- pose future work that would follow our assess- ment methodology over test samples from cur- rent datasets (e.g. CNN/DailyMail), judging per- formance of current systems and utilizing current manual evaluation protocols. This would require preparing, for limited samples, additional manu- ally crafted summaries of several lengths and man- ually evaluating system summaries of correspond- ing lengths. Using such data, it will be possible to repeat our analysis and test the broader valid- ity of the single-reference-length configuration. If broadly assessed, it will be possible to start evalu- ating system summaries of multiple lengths over most currently available datasets, leveraging the available single-length reference summaries. Fu-ture benchmarks could require systems to produce different length outputs, while feasibly evaluating them using the existing, single length, reference summaries. This, in turn, is likely to drive research to better address the need for producing high qual- ity summaries flexibly across a range of summary lengths, a dimension that has been disregarded for long.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>#</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: R-1 scores of a few systems, evaluated against the 50-word reference set of DUC 01. Systems R, S and T are from DUC 01; ICSISumm is a later competitive system (Gillick et al., 2008).</figDesc></figure>

			<note place="foot" n="1"> Omitting stop words. 2 Following Lin (2004). Spearman ranking correlations provide similar results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their constructive comments. We thank Yin-fei Yang for his assistance in producing the IC-SISumm summaries that we utilized in our anal-ysis. This work was supported in part by grants from the MAGNET program of the Israel Innova-tion Authority; by the German Research Founda-tion through the German-Israeli Project Coopera-tion (DIP, grant DA 1600/1-1); by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Bureau in the Prime Ministers Office; and by the Israel Science Foundation (grants 1157/16 and 1951/17).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>abs/1711.05217</idno>
		<title level="m">Controllable abstractive summarization. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Controlling linguistic style aspects in neural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Stylistic Variation</title>
		<meeting>the Workshop on Stylistic Variation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="94" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The ICSI Summarization System at TAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tür</surname></persName>
			<affiliation>
				<orgName type="collaboration">TAC</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">English gigaword. Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www1.cs.columbia.edu/nlp/tides/SEEManual.pdf.Chin-YewLin" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Summary evaluation environment user guide. Text Summarization Branches Out</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatically assessing machine summary content without a gold standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="300" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pyramid method: Incorporating human content selection variation in summarization evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Responsiveness assessment instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
		<ptr target="www-nlpir.nist.gov/projects/duc/duc2006/responsiveness.assessment.instructions" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
		<ptr target="https://duc.nist.gov/" />
		<title level="m">Document Understanding Conferences</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
		<ptr target="https://tac.nist.gov/" />
		<title level="m">Text Analysis Conferences</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DUC in context. Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1506" to="1520" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An Introduction to DUC 2004 Intrisic Evaluation of Generic News Text Summarization Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><surname>James</surname></persName>
		</author>
		<ptr target="duc.nist.gov/pubs/2004slides/duc2004.intro.pdf" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to score system summaries for better content selection evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Peyrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Botschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2017 Workshop on New Frontiers in Summarization</title>
		<meeting>the EMNLP 2017 Workshop on New Frontiers in Summarization</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ranking human and machine summarization systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Rankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne P O&amp;apos;</forename><surname>Slud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="467" to="473" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A decade of automatic content evaluation of news summaries: Reassessing the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Peter A Rankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="136" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using latent semantic analysis in text summarization and summary evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Jezek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISIM</title>
		<meeting>ISIM</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
