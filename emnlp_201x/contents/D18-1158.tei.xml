<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Jia</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies Co</orgName>
								<address>
									<postCode>100085</postCode>
									<settlement>Ltd, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1267" to="1276"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1267</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierarchical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event detection (ED) is a crucial subtask of event extraction, which aims to identify event triggers and classify them into specific types from texts. According to the task defined in Automatic Con- text Extraction 1 (ACE), given the following sen- tence S1, a robust ED system should be able to rec- ognize two events: a Die event triggered by died and an Attack event triggered by fired.</p><p>S1: In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.</p><p>To this end, most methods <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b5">Hong et al., 2011;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref>) model ED as a multi- classification task and predict every word in the sentence separately to determine whether it trig- gers a specific type of event by using sentence- level information. However, they face two prob- lems: (1) Neglecting event interdependency by separately predicting each event; (2) Sentence- level information is usually insufficient to resolve ambiguities for some types of events. In the fol- lowing, we will use examples to illustrate these two problems specifically. S2: The project leader was fired for the bankruptcy of the subsidiary company.</p><p>Event interdependency: In S1, fired triggers an Attack event, while it triggers an End-Position event in S2. Because of the ambiguity, a tradi- tional approach may mislabel fired in S1 as a trig- ger of End-Position event. However, if we know died triggers a Die event in S1, which is easier to disambiguate, we tend to predict that fired triggers an Attack event. The reason is that the events men- tioned in the same sentence tend to be semanti- cally coherent and a Die event usually co-occurs with an Attack event. The similar phenomenon can be found in S2. We conduct a statistical anal- ysis on ACE 2005 dataset, and find that nearly 30% sentences contain multiple events which is a proportion we can not ignore. To give an in- tuitive illustration, the top 5 event types that co- occur with Attack event in the same sentence are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We call such clues as event interdependency. Some works ( <ref type="bibr" target="#b9">Li et al., 2013;</ref><ref type="bibr" target="#b19">Yang and Mitchell, 2016;</ref><ref type="bibr" target="#b13">Liu et al., 2016b</ref>) rely on a set of elaborately designed features and com- plicated natural language processing (NLP) tools to capture event interdependency. However, these methods lack generalization, take a large amount of human effort and are prone to error propaga- tion problem. Though  use a Recurrent Neural Networks (RNN) based classifi- cation model to capture the event interdependency between current event candidate and the former (left) predicted events, they miss the event interde- pendency between current event candidate and the later (right) predicted events, and the later events can not change the type of current event. The rea- son is that they classify the words of the sentence from left to right one by one and only use the for- mer events to predict the later event types. We claim that both of the former and later predicted events are important to predict the event type of current trigger candidate. For example in S1, the former predicted Die event can help us to predict that fired triggers an Attack event, while in S2 the later predicted bankruptcy event can help us to predict that fired triggers an End-Position event. Thus, how to use a neural-based model to capture all event interdependencies (the interdependencies between the current event candidate and its for- mer/later predicted events) in the whole sentence is a challenging problem.</p><p>Sentence-level and document-level informa- tion: Besides event interdependency, knowing that American tank is a weapon can also give us addi- tional evidence to predict that fired triggers an At- tack event in S1. Similarly in S2, knowing that project leader is a job title can also help us to predict that fired triggers an End-Position event.</p><p>We call such clues as sentence-level information. However, sometimes it is difficult even for peo- ple to classify event types from an isolated sen- tence. We must resort to document-level informa- tion. For example, considering the following sen- tence with an ambiguous word left: S3: He left the company.</p><p>It is hard to tell left triggers a Transport event which means that he left the place, or an End- Position event which means that he resigned from the company. However, if we read the whole doc- ument, a clue like "He planned to go shopping before he went home, because he got off work early today." would give us more confidence to believe that left triggers a Transport event, while a clue like "They held a party for his retire- ment." would indicate the aforementioned event is an End-Position event. We call such clues as document-level information. Moreover, the confi- dence of sentence-level and document-level infor- mation should be taken into consideration when using them together to construct a broader range of contextual information. For example in S3, document-level information will give us more ev- idence, while in S1 sentence-level information is enough to disambiguate the types of events. There have been some feature-based studies <ref type="bibr" target="#b7">(Ji and Grishman, 2008;</ref><ref type="bibr" target="#b10">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b6">Huang and Riloff, 2012</ref>) that construct rules to capture document-level information for improv- ing sentence-level ED. However, they suffer from two problems: (1) The features they used of- ten need to be manually designed and may in- volve error propagation from existing NLP tools; (2) Sentence-level and document-level informa- tion are integrated by a large number of fixed rules, which is complicated to construct and it will be far from complete. Thus, how to use a neural-based model to automatically extract sentence-level and document-level information and dynamically inte- grate them is another challenging problem.</p><p>In this paper, we propose a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to address the two problems stated above simultaneously. To capture event interdependency and collectively de- tect multiple events in one sentence, we propose a hierarchical and bias tagging networks for event detection. In which, we exploit a hierarchical RNN-based tagging layer to capture all event in- terdependencies in the whole sentence and de- vise a bias objective function to reinforce the in- fluence of trigger tags on the model 2 . To use a broader range of contextual information of the event candidate, we propose a gated multi-level at- tention, which can automatically extract sentence- level and document-level information and inte- grate them dynamically. In summary, the contri- butions of this paper are as follows:</p><p>• We propose a novel framework for event detection, which can automatically extract and dynamically integrate sentence-level and document-level information and collectively detect multiple events in one sentence.</p><p>• To capture event interdependency, we ex- ploit a hierarchical and bias tagging net- works to detect multiple events in one sen- tence collectively. To automatically extract and dynamically integrate contextual infor- mation, we devise a gated multi-level atten- tion Mechanisms. To our knowledge, this is the first work to jointly use event inter- dependency, sentence-level information and document-level information via a neural tag- ging schema for event detection task.</p><p>• We conduct extensive experiments on a widely used ACE 2005 dataset, and the ex- perimental results show that our approach significantly outperforms other state-of-the- art methods 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>Event detection (ED) is a crucial subtask of event extraction (EE). In this paper, we focus on ED task defined in ACE evaluation, where an event is defined as a specific occurrence involving one or more participants. Firstly, we introduce some ACE terminology to facilitate the understanding of this task: Event trigger: the main word or phrase that most clearly expresses the occurrence of an event. Event arguments: the mentions that are in- volved in an event (viz., participants). Event men- tion: a phrase or sentence within which an event is described, including a trigger and arguments. Given an English text document, an ED system should identify event triggers and categorize their event types for each sentence. For instance, in the sentence "He died in the hospital", an ED system is expected to detect a Die event along with the trigger word "died". The ACE 2005 evaluation defines 8 event types and 33 subtypes, such as At- tack or Die. Following previous works ( <ref type="bibr" target="#b9">Li et al., 2013;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref>, we categorize triggers into these 33 subtypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this paper, we formulate event detection as a sequence labelling task. As shown in <ref type="figure" target="#fig_9">Figure 2</ref>, we label all words in one sentence collectively via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBT- NGMA). We assign a tag for each word to indi- cate whether it triggers a specific type of event. We adopt the "BIO" tags schema, where tag "O" represent the "other" tag which means that the cor- responding word does not trigger any event, tags "B-EventType" and "I-EventType" represent the "Begin-EventType" and "Inside-EventType" tag respectively. "EventType" means that the word triggers a specific type of event. "B" and "I" rep- resent the position of the word in a trigger to solve the problem that a trigger word contains multi- ple words such as "take over", "go off" and so on. Thus, the total number of tags is N t = 2 ⇤ |N eventT ype | + 1, where |N eventT ype | is the size of the predefined event types and |N eventT ype | = 33 in this paper as stated above. <ref type="figure" target="#fig_9">Figure 2</ref> describes the architecture of HBT- NGMA, which primarily involves the following four components: (i) embedding layer, which transforms each word into a continuous vector; (ii) BiLSTM layer, which uses a Bidirectional Long Short Term Memory (BiLSTM) to encode the se- mantics of each word considering the forward and backward information; (iii) gated multi-level at- tention, in which we propose a sentence-level and document-level attention to automatically extract sentence-level and document-level information re- spectively, and we devise a fusion gate to dy- namically integrate them as context information; and (iv) Hierarchical tagging layer, in which we propose two Tagging LSTM (TLSTM1 and TL- STM2) and a tagging attention to automatically capture the event interdependency and tag all the words of the sequence collectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>This paper uses the learned word embeddings as the source of basic features. Specifically, we use the Skip-gram model ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) to learn word embeddings on the NYT corpus.</p><p>Given a document d = {s 1 , s 2 , ..., s i , ..., s Ns }, where N s is the number of sentences in the doc- ument. The i-th sentence s i can be represented as token sequence s i = {w 1 , w 2 , ..., w t , ..., w Nw }, where N w is length of the sentence and w t is the t-th token of the sentence. Assume that the word embedding for token w t is e t and we use it as the input of the following layer.          </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BiLSTM Layer</head><p>In sequence labelling problems, the BiLSTM has been proven effective to capture the semantic in- formation of each word ( <ref type="bibr" target="#b8">Lample et al., 2016)</ref>. In this paper, we use the LSTM unit as described in <ref type="bibr" target="#b20">(Zaremba and Sutskever, 2014</ref>). For each word w t , the forward LSTM encodes w t by consider- ing the contextual information from word w 1 to w t , which is marked as ! h t . Similarly, the back- ward LSTM encodes w t based on the contextual information from w Nw to w t , which is marked as h t . Finally, we concatenate ! h t and h t to rep- resent the information of the word w t , denoted as</p><formula xml:id="formula_0">h t = [ ! h t , h t ]</formula><p>, and we concatenate ! h Nw and h 1 to represent the encoding information of the whole sentence s i , denoted as</p><formula xml:id="formula_1">h s i = [ ! h Nw , h 1 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gated Multi-level Attention</head><p>Gated multi-level attention primarily involves the following three components: (i) sentence-level at- tention layer, which automatically captures im- portant sentence-level information by consider- ing the current word; (ii) document-level atten- tion layer, which automatically captures impor- tant document-level information by considering the current sentence; and (iii) fusion gate layer, which use a fusion gate to dynamically integrate sentence-level and document-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-level Attention Layer</head><p>Sentence-level attention layer aims to capture the important clues in sentence level. For each can- didate word w t in the sentence, its sentence-level semantic information sh t is calculated as follows:</p><formula xml:id="formula_2">sht = X Nw k=1 ↵ k s h k<label>(1)</label></formula><p>where ↵ k s is the weight of each word representa- tion h k . In this paper, we define ↵ k s as following:</p><formula xml:id="formula_3">↵ k s = exp(z k s ) P Nw j=1 exp(z j s )<label>(2)</label></formula><p>where z k s is the relatedness between the t-th word representation h t and the k-th word representation h k , modeled by bilinear attention as:</p><formula xml:id="formula_4">z k s = tanh(htWsah T k + bsa)<label>(3)</label></formula><p>where W sa is the weight matrix and b sa is the bias term. Following above sentence-level atten- tion mechanism, we can get the sentence-level in- formation for each word w t by considering the se- mantic information of the word w t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document-level Attention Layer</head><p>Similar to sentence-level attention, document- level attention captures the vital clues in the doc- ument level. The document-level semantic infor- mation dh i for i-th sentence calculated as follows:</p><formula xml:id="formula_5">dhi = X Ns k=1 ↵ k d hs k ↵ k d = exp(z k d ) P Ns j=1 exp(z j d ) z k d = tanh(hs i W da h T s k + b da )<label>(4)</label></formula><p>where ↵ k d is the weight of each sentence repre- sentation h s k , z k d is the relatedness between i-th sentence representation h s i and the k-th sentence representation h s k , W da is the weight matrix and b da is the bias term. Compared with sentence-level information, all words in the i-th sentence have the same document-level information dh i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fusion Gate Layer</head><p>We devise a fusion gate to dynamically integrate sentence-level information sh t and document- level information dh i for the t-th word w t in the i-th sentence s i , and calculate the contextual in- formation representation cr t as follows:</p><formula xml:id="formula_6">crt = (Gt sht) + ((1 Gt) dhi)<label>(5)</label></formula><p>where G t is a fusion gate aims to model the con- fidence of clues provided by sentence-level infor- mation sh t and document-level information dh i , which is calculated as follows:</p><formula xml:id="formula_7">G = (Wg[sht, dhi] + bg)<label>(6)</label></formula><p>where W g is the weight matrix and b g is the bias term, is a sigmoid function and denotes element-wise multiplication. Finally, the contex- tual information cr t of word w t and its word em- bedding e t are concatenated into a single vector xr t = [e t , cr t ] as the feature representation of w t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical Tagging Layer</head><p>In hierarchical tagging layer, we propose two Tag- ging LSTMs (TLSTM1 and TLSTM2) and a tag- ging attention to automatically capture the event interdependency and tag the sequence collectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The First Tagging Layer: TLSTM1</head><p>When detecting the tag of word w t in TLSTM1, the inputs are: the feature representation xr t ob- tained from embedding layer and gated multi-level attention layer, former predicted tag vector T 1 t1 , and former hidden vector h 1 t1 in TLSTM1. The detail operations are defined as follows:</p><formula xml:id="formula_8">i 1 t = (W 1 ix xrt + W 1 i h h 1 t1 + W 1 i T T 1 t1 ) f 1 t = (W 1 fx xrt + W 1 f h h 1 t1 + W 1 f T T 1 t1 ) o 1 t = (W 1 ox xrt + W 1 o h h 1 t1 + W 1 o T T 1 t1 ) u 1 t = '(W 1 ux xrt + W 1 u h h 1 t1 + W 1 u T T 1 t1 ) c 1 t = i 1 t u 1 t + f 1 t c 1 t1 h 1 t = o 1 t '(c 1 t ) T 1 t = W 1 T h 1 t + b 1 T<label>(7)</label></formula><p>Where i t is an input gate, u t is an input modulation gate, f t is a forget gate, o t is an output gate, c t is a memory cell and T 1 t is a predicted tagging vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Second Tagging Layer: TLSTM2</head><p>Though the TLSTM1 can capture the interdepen- dency between current event candidate and the for- mer predicted event tags, it can not capture the interdependency between the current event candi- date and the later predicted event tags. Thus, we devise a second tagging layer (TLSTM2) upon the LSTM1 to capture the interdependency between the current event candidate and both of former and later predicted event tags from TLSTM1. When detecting the tag of word w t in TLSTM2, the in- puts are: the feature representation xr t , former predicted tag vector T 2 t1 in TLSTM2, the pre- liminary predicted information T a t calculated from TLSTM1, and former hidden vector h 2 t1 in TL- STM2. The detail operations are defined as fol- lows:</p><formula xml:id="formula_9">i 2 t = (W 2 ix xrt + W 2 i h h 2 t1 + W 2 i T T 2 t1 + W 2 ia T a t ) f 2 t = (W 2 fx xrt + W 2 f h h 2 t1 + W 2 f T T 2 t1 + W 2 fa T a t ) o 2 t = (W 2 ox xrt + W 2 o h h 2 t1 + W 2 o T T 2 t1 + W 2 oa T a t ) u 2 t = '(W 2 ux xrt + W 2 u h h 2 t1 + W 2 u T T 2 t1 + W 2 ua T a t ) c 2 t = i 2 t u 2 t + f 2 t c 2 t1 h 2 t = o 2 t '(c 2 t ) T 2 t = W 2 T h 2 t + b 2 T<label>(8)</label></formula><p>The unit structure of TLSTM2 is similar to the unit of TLSTM1. The parts need to pay attention are follows: (1) the initial hidden input h 2 0 of the TLSTM2 is the last hidden vector h 1 Nw of the TL- STM1. (2) the preliminary predicted information T a t is calculated from TLSTM1 by using a tagging attention as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tagging Attention</head><p>Tagging attention aims to automatically encode the preliminary predicted information T a t for the word w t and the details are as follows:</p><formula xml:id="formula_10">T a t = X Nw k=1 ↵ k T T 1 k ↵ k T = exp(z k T ) P Nw j=1 exp(z j T ) z k T = tanh(T 1 t Wta(T 1 k ) T + bta)<label>(9)</label></formula><p>where ↵ k T is the weight of each preliminary pre- dicted tag T 1 k , z k T is the relatedness between t-th preliminary predicted tag T 1 t and the k-th prelim- inary predicted tag T 1 k , W ta is the weight matrix and b ta is the bias term.</p><p>The final normalized tag probability for word w t is based on the predicted tag vector T 2 t from TLSTM2 and computed as follows:</p><formula xml:id="formula_11">Ot = WyT 2 t + by p(O i t |sj, ✓) = exp(O i t ) P N t k=1 exp(O k t )<label>(10)</label></formula><p>where p(O i t |s j , ✓) is the probability that assigning the i-th tag to word w t in sentence s j when param- eters is ✓, and N t is the total number of tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training with Bias Objective Function</head><p>In one sentence, the number of "O" tags is much more than the number of trigger tags. Thus, we devise a bias objective function J(✓) to reinforce the influence of trigger tags on the model, which is defined as follows:</p><formula xml:id="formula_12">J(✓) = max X N ts j=1 X Nw t=1 (log p(O y t t |sj, ✓) · I(O) + ↵ log p(O y t t |sj, ✓) · (1 I(O)))<label>(11)</label></formula><p>where N ts is the number of training sentences, N w is the length of sentence s j , p(O yt t |s j , ✓) is the nor- malized probabilities of tags defined in Formula 10 and y t is the golden tag of word w t in sentence s j , ↵ is the bias weight and the larger ↵ will bring the greater influence of trigger tags on the model. Besides, I(O) is a switching function to distin- guish the loss of tag "O" and trigger tags, which is defined as follows:</p><formula xml:id="formula_13">I(O) = ⇢ 1, if tag = "O" 0, if tag 6 = "O"<label>(12)</label></formula><p>To compute the network parameter ✓, we maxi- mize the log likelihood J (✓) through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) update rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Evaluation Metrics</head><p>We conduct experiments on the widely used ACE 2005 dataset. For comparison, as the same as pre- vious works ( <ref type="bibr" target="#b10">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b9">Li et al., 2013;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref>, we used the same test set with 40 documents and the same development set with 30 documents and the rest 529 documents are used for training. Finally, we use Precision (P ), Recall (R) and F measure (F 1 ) as the evaluation metrics as the same as previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameter Setting</head><p>Hyper-parameters are tuned on the development dataset by grid search. We train the word embed- ding using Skip-gram algorithm 4 on the NYT cor- pus <ref type="bibr">5</ref> . We set the dimension of word embeddings as 100, the dimension of tag vector as 20, all the size of LSTM in BiLSTM layer, TLSTM1 and TL- STM2 layer as 100, the bias parameter ↵ in For- mula 11 as 5, the batch size as 20, the learning rate as 0.001, the dropout rate as 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Our Method vs. State-of-the-art Methods</head><p>We select the following state-of-the-art methods for comparison, which can be classified as two types: separate and collective methods: Collective methods: 1) Li's Structure: the method that collectively detects events by us- ing human-designed features ( <ref type="bibr" target="#b9">Li et al., 2013</ref>). 2) Yang's JointEE: the method that detects events and entities in one sentence jointly based on human-designed features <ref type="bibr" target="#b19">(Yang and Mitchell, 2016)</ref>. 3) Nguyen's JRNN: the method that ex- ploits a RNN model to collectively detects events by only using sentence-level information ). 4) Liu's PSL : the method that uses a probabilistic soft logic to detect events by using human-designed features ( <ref type="bibr" target="#b13">Liu et al., 2016b)</ref>.</p><p>Experimental results are shown in  <ref type="formula" target="#formula_2">(2013)</ref> 74.5 59.1 65.9 <ref type="bibr">Liao's CrossEvent (2010)</ref> 68.7 68.9 68.8 Hong's CrossEntity (2011) 72.9 64.3 68.3</p><p>Chen's DMCNN (2015) 75.6 63.6 69.1 Chen's DMCNN+ † (2017) 75.7 66.0 70.5 <ref type="bibr">Liu's FrameNet † (2016a)</ref> 77.6 65.2 70.7 Liu's ANN-Aug † <ref type="formula" target="#formula_2">(2017)</ref> 76.8 67.5 71.9 Li's Structure <ref type="formula" target="#formula_2">(2013)</ref> 73 3.9% (Liu's PSL) and improve the best sepa- rate method's F 1 by 1.4% (Liu's ANN-Aug) al- though Liu's ANN-Aug uses FrameNet as exter- nal resources. We also perform a t-test (p 6 0.05), which indicates that our method signifi- cantly outperforms all of the compared methods. value. The reason is that Nguyen's JRNN only uses sentence-level information while our model exploits multi-level information, and our model can capture the interdependencies between the current event candidate and its former/later pre- dicted events simultaneously.</p><note type="other">.7 62.3 67.5 Yang's JointEE (2016) 75.1 63.3 68.7 Nguyen's JRNN (2016) 66.0 73.0 69.3 Liu's PSL (2016b) 75.3 64.4 69.4 Ours HBTNGMA 77.9 69.1 73.3</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of The Hierarchical and Bias Tagging Networks</head><p>In this subsection, we prove the effectiveness of hierarchical and bias tagging networks for collec- tive ED. We select following methods as base- lines: 1) LSTM+Softmax: a simplified version of our HBTNGMA, which directly use a soft- max layer to separately detect events after we get the feature representation xr t of each word w t . 2) LSTM+CRF: the method is similar to our HBTNGMA, which uses a CRF layer to tag words instead of our Hierarchical TLSTM (HTL- STM) tagging layer. 3) LSTM+TLSTM: the method is similar to our HBTNGMA, which only use a TLSTM1 and takes all tags have same in- fluence in training loss (i.e. ↵ in is set as 1) . 4) LSTM+HTLSTM: the method is similar to our HBTNGMA, which use a HTLSTM (TL- STM1+TLSTM2) and do not use bias objective function. And LSTM+HTLSTM+Bias is our pro- posed HBTNGMA. Moreover, we divide the test- ing data into two parts according the event num- ber in a sentence (single event and multiple events) and perform evaluations separately.   Surprisingly, the LSTM+HTLSTM+Bias yields a 14.9% improve- ment on the sentence contains multiple events over the LSTM+Softmax. It proves neural tagging schema is effective for ED task especially for the sentences contain multiple events. 2) The LSTM+TLSTM achieve better performances than LSTM+CRF. And the LSTM+HTLSTM achieve better performances than LSTM+TLSTM. The results prove the effectiveness of the TLSTM layer and HTLSTM layer. 3) Compared with LSTM+HTLSTM, the LSTM+HTLSTM+Bias gains a 0.9% improvement on all sentence. It demonstrates the effectiveness of our proposed bias objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of The Gated Multi-level Attention</head><p>This subsection studies the effectiveness of our gated multi-level attention. We adopt same archi- tecture of our HBTNGMA as shown in <ref type="figure" target="#fig_9">Figure 2</ref> with different level clues as baselines: 1) Word Only is the method only uses word embedding e t to identify events. 2) Word+SA uses sentence- level attention to capture important sentence-level information as additional clues. 3) Word+DA uses document-level attention to capture important document-level information as additional clues. 4) Word+Average MA uses both of sentence-level and document-level attention to capture multi- level information and integrate them with a aver- age gate (all the dimension of the fusion gate are set as 0.5 ), which is a special case of our pro- posed HBTNGMA. And Word+Gated MA is our proposed HBTNGMA model.  Results are shown in <ref type="table" target="#tab_8">Table 3</ref>. From the re- sults, we have the following observations: 1) Compared with word only, Word+SA achieves a better performance. We can make the same ob- servation when comparing Word+DA with word only. It proves that both sentence-level and document-level information are helpful for ED task. 2) Compared with Word+DA, Word+SA achieves a better performance. It proves that in most of cases sentence-level information provides more clues than document-level information. 3) Word+Gated MA gains a 0.9% improvement than Word+Average MA. It demonstrates that the effec- tiveness of our fusion gate to dynamically inte- grate clues from multiple levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>Interesting Cases: Our neural tagging schema not only can model the interdependency between mul- tiple events in one sentence as proved in Subsec- tion 4.3, but also the "BIO" tagging schema can solve the multiple words trigger inherently. We conduct a statistical analysis on the experimental results, and find that nearly 50% cases with multi- ple word trigger was solved by our model. Exam- ple is shown in <ref type="figure" target="#fig_11">Figure 3</ref>. Attention Visualization: As limited of space, we take one sentence with high sentence-level gated weight (example 1) and one sentence with high document-level gated weight (example 2) as examples for attention visualization. As shown in <ref type="figure" target="#fig_12">Figure 4</ref>, in example 1, sentence-level informa- tion plays more important role in disambiguating fired, and the words (tank, died and Baghdad) give Sentence Document Sentence Document Example 1 (sentence-level attention) ： Example 2 (document-level attention) ： In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.</p><p>… this is when you were in the Senate --"less and less information was new, fewer and fewer arguments were fresh,and the repetitiveness of the old arguments became tiresome." "I was becoming almost as cynical as my constituents. I knew it was time to leave." Isn' t that a great argument for term limits? us ample evidence to predict that fired triggers an Attack event. While document-level information plays a more important role in example 2. The surrounding sentence "this is ... tiresome." gives us more confidence to predict that leave triggers an End-Position event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Event detection is an increasingly hot and chal- lenging research topic in NLP. Generally, exist- ing approaches could roughly be divided into two groups: separate and collective methods.</p><p>Separate methods: These methods regard mul- tiple events in one sentence as independent ones and recognize them separately. These meth- ods include feature-based methods which exploit a diverse set of strategies to convert classifica- tion clues into feature vectors <ref type="bibr" target="#b0">(Ahn, 2006;</ref><ref type="bibr" target="#b7">Ji and Grishman, 2008;</ref><ref type="bibr" target="#b10">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b5">Hong et al., 2011;</ref><ref type="bibr" target="#b6">Huang and Riloff, 2012)</ref>, and neural-based methods which use neural networks to automatically capture clues from plain texts <ref type="bibr" target="#b2">(Chen et al., 2015;</ref><ref type="bibr" target="#b16">Nguyen and Grishman, 2015;</ref><ref type="bibr" target="#b4">Feng et al., 2016;</ref><ref type="bibr" target="#b3">Duan et al., 2017;</ref>. Though effective these methods, they ne- glect event interdependency by separately predict- ing each event.</p><p>Collective methods: These methods try to model the event interdependency and detect mul- tiple events in one sentence collectively. How- ever, nearly all of these methods are feature-based methods <ref type="bibr" target="#b14">(McClosky et al., 2011;</ref><ref type="bibr" target="#b9">Li et al., 2013;</ref><ref type="bibr" target="#b19">Yang and Mitchell, 2016;</ref><ref type="bibr" target="#b13">Liu et al., 2016b</ref>), which rely on elaborately designed features and suffer er- ror propagation from existing NLP tools.  exploits a neural-based method to de- tect multiple events collectively. However, they only use the sentence-level information and ne-glect document-level clues, and can only capture the interdependencies between the current event candidate and its former predicted events. More- over, there method can not handle the multiple words trigger problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposes a novel framework for event detection, which can automatically extract and dy- namically integrate sentence-level and document- level information and collectively detect multi- ple events in one sentence. A hierarchical and bias tagging networks is proposed to detect mul- tiple events in one sentence collectively. A gated multi-level attention is devised to automatically extract and dynamically integrate contextual infor- mation. The experimental results on the widely used dataset prove the effectiveness of the pro- posed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top 5 event types that co-occur with Attack event in the same sentence in ACE 2005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of our proposed hierarchical and bias tagging networks with gated multi-level attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Separate methods: 1 )</head><label>1</label><figDesc>Li's MaxEnt: the method that detects events in one sentence sepa- rately by using human-designed features (Li et al., 2013). 2) Liao's CrossEvent : the method that uses cross event information (Liao and Grish- man, 2010). 3) Hong's CrossEntity: the method that uses cross entity information (Hong et al., 2011). 4) Chen's DMCNN: the dynamic multi- pooling convolutional neural networks method (Chen et al., 2015). 5) Chen's DMCNN+: the DMCNN method argumented with automati- cally labeled data (Chen et al., 2017). 6) Liu's FrameNet : the method that leverages FrameNet as extended training data to improve ED (Liu et al., 2016a). 7) Liu's ANN-Aug: the method that use the annotated argument information via a super- vised attention to improve ED (Liu et al., 2017).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Early Saturday，more units were waiting in Kuwait to smash through any Iraqi resistance.Figure 3 :</head><label>3</label><figDesc>Figure 3: The example of case solved by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention visualization. The heat map indicate the contextual attention. Blue for sentence-level and orange for document-level. The pie chart indicate the fusion gate weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>From the table, we have the following observa-
tions: (1) Among all the methods, our HBT-
NGMA achieves the best performance. It can 
improve the best collective method's F 1 by 

4 https://code.google.com/p/word2vec/ 
5 https://catalog.ldc.upenn.edu/LDC2008T19 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Overall performance on blind test data. The 
upper table illustrates the performance of separate ED 
systems and the lower illustrates collective ED systems. 
 † means the method that uses external resources. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Performance of different ED systems. 1/1 means one sentence that only has one event and 1/N means that one sentence has multiple events.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 shows the results.</head><label>2</label><figDesc></figDesc><table>And we have 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of gated multi-level attention. 

</table></figure>

			<note place="foot" n="2"> Compared with the task like Named Entities Recognition, the number of &quot;O&quot; tags is much more than the number of trigger tags in ED task, i.e. if we use the non-bias objective function and tag all words in one sentence as &quot;O&quot;, we will gain a low loss. Thus we devise a bias objective function.</note>

			<note place="foot" n="3"> Our source code, including all hyper-parameter settings and pre-trained word embeddings, is openly available at https://github.com/yubochen/NBTNGMA4ED</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work is supported by the Natural Science Foundation of China <ref type="bibr">(No.61533018 and No.61702512)</ref>, and the independent research project of National Laboratory of Pattern Recog-nition. This work is also supported in part by Huawei Technologies Co., Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 44th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>44th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatically labeled data generation for large scale event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistic (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting document level information to improve event detection via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifang</forename><surname>Shaoyang Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJNLP</title>
		<meeting>IJNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A languageindependent neural network for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using cross-entity inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling textual cohesion for event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Refining event extraction through cross-document inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leveraging framenet to improve automatic event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2134" to="2143" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A probabilistic soft logic based approach to exploiting latent and global information in event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event extraction as dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1626" to="1635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event detection and domain adaptation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Huu Thien Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistic (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling skip-grams for event detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint extraction of events and entities within a document context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
