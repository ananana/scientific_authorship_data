<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yftah</forename><surname>Ziser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="238" to="249"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>238</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While cross-domain and cross-language transfer have long been prominent topics in NLP research , their combination has hardly been explored. In this work we consider this problem, and propose a framework that builds on pivot-based learning, structure-aware Deep Neu-ral Networks (particularly LSTMs and CNNs) and bilingual word embeddings, with the goal of training a model on labeled data from one (language, domain) pair so that it can be effectively applied to another (language, domain) pair. We consider two setups, differing with respect to the unlabeled data available for model training. In the full setup the model has access to unlabeled data from both pairs, while in the lazy setup, which is more realistic for truly resource-poor languages, unlabeled data is available for both domains but only for the source language. We design our model for the lazy setup so that for a given target domain, it can train once on the source language and then be applied to any target language without retraining. In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms previous models even when it is trained in the lazy setup and previous models are trained in the full setup. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of Natural Language Processing (NLP) has made impressive progress in the last two decades and text processing applications are now performed in a quality that was beyond imagina- tion only a few years ago. With this success, it is only natural that researchers seek ways to apply NLP algorithms in as many languages and textual domains as possible. However, the success of NLP <ref type="bibr">1</ref> Our code is publicly available at https://github.com/yftah89/ PBLM-Cross-language-Cross-domain algorithms most often relies on the availability of non-trivial supervision such as corpora annotated with linguistic classes or structures, and for multi- lingual applications often also on parallel corpora. This resource bottleneck seriously challenges the world-wide accessibility of NLP technology.</p><p>To address this problem substantial efforts have been put into the development of cross-domain <ref type="bibr">(CD, (Daumé III, 2007;</ref><ref type="bibr" target="#b1">Ben-David et al., 2010)</ref>) and cross-language (CL) transfer methods. For both areas, while a variety of methods have been developed for many tasks throughout the years ( § 2), with the prominence of deep neural networks (DNNs) the focus of modern methods is shift- ing towards learning data representations that can serve as a bridge across domains and languages.</p><p>For CD, this includes: (a) pre-DNN work <ref type="bibr" target="#b3">((Blitzer et al., 2006</ref><ref type="bibr" target="#b2">((Blitzer et al., , 2007</ref>, known as structural correspondence learning (SCL)), that models the connections between pivot features -features that are frequent in the source and the target domains and are highly correlated with the task label in the source domain -and the other, non-pivot, fea- tures; (b) DNN work <ref type="bibr" target="#b12">(Glorot et al., 2011;</ref><ref type="bibr" target="#b9">Chen et al., 2012</ref>) which employs compress-based noise reduction to learn cross-domain features; and re- cently also (c) works that combine the two ap- proaches ( <ref type="bibr">Ziser and</ref><ref type="bibr">Reichart, 2017, 2018</ref>) (hence- forth ZR17 and ZR18). For CL, the picture is sim- ilar: multilingual representations (usually word embeddings) are prominent in the transfer of NLP algorithms from one language to another (e.g. ( <ref type="bibr">Upadhyay et al., 2016)</ref>).</p><p>In this paper we aim to take CL and CD trans- fer a significant step forward and design meth- ods that can adapt NLP algorithms simultane- ously across languages and domains. We consider this research problem fundamental to our field as manually annotated resources are often scarce in many domains, even for languages that are consid-ered resource-rich. With effective cross-language cross-domain (CLCD) methods it is sufficient to have training resources in a single domain of one language in order to solve the task in any other (language, domain) pair.</p><p>As a first step, our focus in this work is on the task of sentiment classification that has been ex- tensively researched in the CD literature. Surpris- ingly, even for this task we are aware of only one previous work that aims to perform CLCD learn- ing ( . However, this work does not employ modern DNN techniques and is substantially outperformed by our methods.</p><p>Our approach to CLCD learning is rooted in the family of methods that combine the power of both DNNs and pivot-based ideas, and is based on two principles. First, we build on the re- cent progress in learning multilingual word em- beddings ( <ref type="bibr" target="#b25">Ruder et al., 2017)</ref>. Such embeddings help close the lexical gap between languages as they map their different vocabularies to a shared vector space. Second, we follow <ref type="bibr">Stein, 2010, 2011;</ref>) and re- define the concept of pivot features for CLCD se- tups ( § 5). While these authors already employed this idea in order to design pivot-based methods in CL <ref type="bibr">Stein, 2010, 2011</ref>) and CLCD ( ) for text classifi- cation and sentiment analysis, their algorithms do not employ DNNs and multilingual embeddings. In this paper we show that it is the combination of bilingual word embeddings (BEs) and structure aware DNNs with the re-defined pivots that leads to high quality CLCD models.</p><p>Aiming to facilitate transfer to resource poor languages and domains, our methods rely on as little supervision as possible. Particularly, we explore two scenarios. In the first, full CLCD setup, models have access to manually annotated reviews from the source (language, domain) pair, and unannotated reviews from both the source and the target (language, domain) pairs. In the sec- ond, lazy CLCD setup, models have access only to source language reviews -annotated reviews from the source domain, and unannotated reviews from both the source and the target domains.</p><p>We consider the lazy setup to be the desired standard setup of CLCD learning for two reasons. First, in true resource-poor languages we expect it to be hard to find a sufficient number of reviews from many domains, even if they are unannotated (imagine for example trying to obtain 50K unla- beled spinner reviews in Swahili). Second, it al- lows a train once, adapt everywhere mode: instead of training a separate model for each target lan- guage, in this setup for each target domain only a single model is trained on the source language, and the target language is considered only at test time through BEs ( § 5). Notice that in order to allow the lazy setup, the BEs should be trained such that the source language embeddings have no knowledge about any particular target language. In § 5 we discuss the BEs we employ ( <ref type="bibr">Smith et al., 2017)</ref>, which have this property.</p><p>We create CLCD variants of DNN-and pivot- based methods originally designed to learn ef- fective representations for CD learning. To the best of our knowledge, there are three such meth- ods, which employ two types of DNNs ( § 4): (a) AE-SCL and AE-SCL-SR ( <ref type="bibr">Ziser and Reichart, 2017</ref>) that integrate pivot-based ideas (SCL) with autoencoder-based (AE) noise reduction; and (b) pivot-based language modeling <ref type="bibr">(PBLM, (Ziser and Reichart, 2018)</ref>) that combines pivot-based ideas with LSTMs for representation learning, and integrates this architecture with an LSTM or a CNN for task classification. In § 5 we discuss how to employ these methods for CLCD transfer where the lexical gap between languages is bridged by pivot translation and BEs, and show that PBLM allows for more effective transfer.</p><p>We address the task of binary sentiment classifi- cation and experiment with nine English-German and nine English-French domain pairs ( § 6, 7). Our PBLM-based models substantially outper- form all previous models, even when the PBLM model is trained in the lazy setup and the previous models are trained in the full setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>We briefly survey work on CL and CD learning and on multilingual word embeddings. We focus on aspects that are relevant to our work rather than on a comprehensive survey of the extensive previ- ous work on these problems.</p><p>Cross-language transfer CL has been explored extensively in NLP. Example applications include POS tagging <ref type="bibr">(Täckström et al., 2013</ref>), syntactic parsing ( <ref type="bibr" target="#b14">Guo et al., 2015;</ref><ref type="bibr" target="#b0">Ammar et al., 2016</ref>), text classification ( <ref type="bibr" target="#b28">Shi et al., 2010;</ref><ref type="bibr" target="#b22">Prettenhofer and Stein, 2010)</ref> and sentiment analysis <ref type="bibr">(Wan, 2009;</ref><ref type="bibr">Zhou et al., 2016</ref>) among others.</p><p>Our work is mostly related to two works: (a) Cross-lingual SCL (CL-SCL, <ref type="bibr">Stein, 2010, 2011)</ref>); and (b) Distributional Cor- respondence Indexing <ref type="bibr">(DCI, (Fernández et al., 2016)</ref>) -in both cases pivot features were re- defined to support CL (in (a)) and CLCD (in (b)) with non-DNN models, in order to perform senti- ment analysis. Below we show how we combine this idea with modern DNNs and BEs to substan- tially improve CLCD learning.</p><p>Cross-domain transfer In NLP, CD transfer (a.k.a domain adaptation) has been addressed for many tasks, including sentiment classification ( <ref type="bibr" target="#b7">Bollegala et al., 2011b</ref>), POS tagging ( <ref type="bibr" target="#b27">Schnabel and Schütze, 2013)</ref>, syntactic parsing <ref type="bibr" target="#b24">(Reichart and Rappoport, 2007;</ref><ref type="bibr" target="#b19">McClosky et al., 2010;</ref><ref type="bibr" target="#b26">Rush et al., 2012</ref>) and relation extraction <ref type="bibr" target="#b16">(Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b6">Bollegala et al., 2011a</ref>), if to name a handful of examples.</p><p>Several approaches to CD transfer have been proposed in the ML literature, including instance reweighting ( <ref type="bibr" target="#b15">Huang et al., 2007;</ref><ref type="bibr" target="#b18">Mansour et al., 2009)</ref>, sub-sampling from both domains <ref type="bibr" target="#b8">(Chen et al., 2011</ref>) and learning joint target and source feature representations. Representation learning, the latter, has become prominent in the DNN era, and is the approach we take here. As noted in § 1 we adopt CD models that integrate pivot-based learning with DNNs to perform CLCD.</p><p>Multilingual word embeddings Multilingual word embeddings learning is an active field of re- search. For example, <ref type="bibr" target="#b25">Ruder et al. (2017)</ref> compare 49 papers that have addressed the problem since 2011. Such embeddings are of importance as they provide means of bridging the lexical gap between languages, which supports CL transfer. Surveying this extensive literature is well be- yond our scope. Since our focus is on perform- ing CLCD with minimal supervision, we quote <ref type="bibr" target="#b25">Ruder et al. (2017)</ref> that categorize multilingual embedding methods with respect to two criteria on the data they require for their training: (a) type of alignment (word, sentence or document); and (b) comparability (parallel data: exact translation, vs. comparable data: data that is only similar). The BEs we use in our work are those of <ref type="bibr">Smith et al. (2017)</ref> that require several thousands trans- lated words as a supervision signal. That is, except from BEs induced using comparable word align- ment signals -words aligned through indirect sig- nals such as related images or through compara- bility of their features (e.g. POS tags) -the BEs we employ belong to the class of the most mini- mal supervision. In addition, as noted in § 1, in order to allow the lazy CLCD setup, we would like BEs where the source language embeddings are induced with no knowledge of the target lan- guage, and we indeed choose such BEs ( § 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>The task we address is cross-language cross- domain (CLCD) learning. Formally, we are given a set of labeled examples from language L s and domain D s (denoted as the pair (L s , D s )). Our goal is to train an algorithm that will be able to correctly label examples from language L t and do- main D t (L t , D t ). The same label set, T , is used across the participating source and target domains and languages.</p><p>The setup we consider is similar in spirit to the setup known as unsupervised domain adap- tation (e.g. ( <ref type="bibr" target="#b2">Blitzer et al., 2007;</ref><ref type="bibr">Ziser and</ref><ref type="bibr">Reichart, 2017, 2018)</ref>). When taking the represen- tation learning approach to CLCD learning, the training pipeline usually consists of two steps. In the first step, the representation learning model is trained on unlabeled data from the source and tar- get languages and domains, with the goal of gener- ating a joint representation for the source and the target. Below we describe the unlabeled data in the full and the lazy CLCD setups. In the second step, a classifier for the supervised task is trained on the (L s , D s ) labeled data. To facilitate lan- guage and domain transfer, every example that is fed to the task classifier in this second step is first represented by the representation model that was trained with unlabeled data at the first step. This is true both when the task classifier is trained and at test time when it is applied to data from (L t , D t ).</p><p>We consider two setups which differ with re- spect to the unlabeled examples available for the representation learning model. In the full CLCD setup, the training algorithm has access to unla- beled examples from both (L s , D s ) and (L t , D t ). Since for truly resource poor languages it may be challenging to find a sufficient number of unla- beled examples from (L t , D t ), we also consider the lazy setup where the training algorithm has ac- cess to unlabeled examples from (L s , D s ) and (L s , D t ) -that is, target domain unlabeled examples are available only in the source language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminaries</head><p>In this paper we aim to adapt CD models that inte- grate the power of DNNs and of pivot-based learn- ing so that they can be applied to CLCD learn- ing. In this section we hence briefly describe the works in this line. We start with the concept of do- main adaptation using pivot-based methods, con- tinue with works that are based on autoencoders and end with works that are based on sequence modeling with LSTMs.</p><p>Pivot based domain adaptation This approach was proposed by <ref type="bibr" target="#b3">Blitzer et al. (2006</ref><ref type="bibr" target="#b2">Blitzer et al. ( , 2007</ref>, through their SCL method. Its main idea is to divide the shared feature space of the source and the target domains to a set of pivot features that are: (a) frequent in both domains; and (b) have a strong correlation with the task label in the source domain labeled data. The features which do not comply with at least one of these criteria form a complementary set of non-pivot features.</p><p>In SCL, after the original feature set is divided into the pivot and non-pivot subsets, this divi- sion is utilized in order to map the original fea- ture space of both domains into a shared, low- dimensional, real-valued feature space. To do so, a binary classifier is defined for each of the pivot features. This classifier takes the non-pivot fea- tures of an input example as its representation, and is trained on the unlabeled data from both the source and the target domains, to predict whether its associated pivot feature appears in the example or not. Note that no human annotation is required for the training of these classifiers, the supervision signal is in the unlabeled data. The matrix whose columns are the weight vectors of the classifiers is post-processed with singular value decomposition (SVD) and the derived matrix maps feature vectors from the original space to the new.</p><p>Since the presentation of SCL, pivot-based cross-domain learning has been researched exten- sively (e.g. ( <ref type="bibr" target="#b21">Pan et al., 2010;</ref><ref type="bibr" target="#b13">Gouws et al., 2012;</ref><ref type="bibr" target="#b5">Bollegala et al., 2015;</ref><ref type="bibr">Yu and Jiang, 2016;</ref><ref type="bibr">Yang et al., 2017)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Autoencoder Based Methods</head><p>An autoencoder (AE) is comprised of an encoder function e and a decoder function d, and its output is a reconstruction of its input x: r(x) = d(e(x)). The model is trained to minimize a loss between x and r(x). Over the last decade AEs have become prominent in CD learning with methods such as Stacked Denoising Autoencoders <ref type="bibr">(SDA, (Vincent et al., 2008;</ref><ref type="bibr" target="#b12">Glorot et al., 2011</ref>) and marginalized SDA (MSDA, <ref type="bibr" target="#b9">(Chen et al., 2012)</ref>) outperforming earlier state-of-the-art methods that were based on the concept of pivots but did not employ DNNs ( <ref type="bibr" target="#b3">Blitzer et al., 2006</ref><ref type="bibr" target="#b2">Blitzer et al., , 2007</ref>. A survey of AE-based models in CD learning can be found in ZR17.</p><p>ZR17 combined AEs and pivot-based modeling for CD learning. Their basic model (AE-SCL) is a feed-forward NN where the non-pivot features of the input example are encoded into a hidden rep- resentation that is then decoded into the pivot fea- tures of the example. Their advanced model (AE- SCL-SR) is identical in structure but its recon- struction matrix is fixed and consists of pre-trained embeddings of the pivot features, so that input ex- amples with similar pivots are biased to have sim- ilar hidden representations. Since no CL learning was attempted in that work, the pre-trained em- beddings used in AE-SCL-SR are monolingual. Both models are illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>After one of the above representation models is trained with unlabeled data from the source and target domains, it is employed when training the task (sentiment analysis) classifier and when ap- plying this classifier to test data. ZR17 learned a standard linear classifier (logistic regression), and fed it with the hidden representation of AE-SCL or AE-SCL-SR. They demonstrated the superior- ity of their models (especially, AE-SCL-SR) over non-DNN pivot-based methods and a variety of AE-based methods that do not consider pivots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LSTM Based Methods</head><p>ZR18 observed that AE-based representation learning models do not exploit the structure of their input examples. Obviously, this can nega- tively impact text classification tasks, such as sen- timent analysis. They hence proposed a structure- aware representation learning model, named Pivot Based Language Modeling (PBLM, <ref type="figure" target="#fig_2">Figure 2a)</ref>.</p><p>PBLM is an LSTM fed with the embeddings of the input example words. As is standard in the LSTM literature, it is possible to feed the model with 1-hot word vectors and multiply them by a (randomly initialized) embeddings matrix (as done by ZR18) or to feed the model with pre-trained embeddings. In this paper we consider both op- tions, taking advantage of the second in order to feed the model with BEs.</p><p>In contrast to standard LSTM-based language   consider the example in <ref type="figure" target="#fig_2">Figure 2a</ref>, provided in ZR18 for adaptation of a sentiment classifier be- tween English book reviews and English reviews of kitchen appliances. PBLM learns the connec- tion between witty -an adjective that is often used to describe books, but not kitchen appliances -and great -a common positive adjective in both do- mains, and hence a pivot feature. Another exam- ple in ZR18 for the same domain pair (see <ref type="figure" target="#fig_1">Figure 1</ref> in their paper) is: "I was at first very excited with my new Zyliss salad spinner -it is easy to spin and looks great", from this sentence PBLM learns the connection between easy -an adjective that is often used to describe kitchen appliances, but not books -and great. That is, PBLM is able to learn the connection between witty and easy to facilitate adaptation between the domains. PBLM can naturally feed a structure-aware task classifier. Particularly, in the PBLM-CNN ar-  ZR18 demonstrated the superiority of PBLM- CNN over AE-SCL, AE-SCL-SR and a variety of other previous models, emphasizing the impor- tance of structure-awareness in CD transfer. We next discuss the adaptation of these models so that they can perform CLCD learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Cross-language Cross-domain Transfer</head><p>The models described in the previous section em- ploy pivot-based learning (all models) and allow a convenient integration of BEs (AE-SCL-SR and PBLM). Below we discuss how we adapt these models so that they can perform CLCD learning.</p><p>Pivot translation We follow <ref type="bibr">Stein, 2010, 2011;</ref>) and re- define pivot features to be features that: (a) are frequent in (L s , D s ) and that their translation is frequent in (L t , D t ) ; and (b) are highly correlated with the task label in (L s , D s ) . Note, that ex- cept for the translation requirement in (a) this is the classical definition of pivot features ( § 1).</p><p>Translated pivots are integrated into the mod- els in a way that creates a shared cross-lingual output space. For both PBLM and the AE-based models a source language pivot feature and its translation are considered to be the same pre- dicted class of the model. Consider, for exam- ple, a setup where we learn representations in or- der to adapt a classifier from (English, books) to (French, music). The pivot feature magnifi- cent(English)/magnifique(French) will be consid- ered the same PBLM prediction when trained on the unlabeled data from both (L s , D s ) and (L t , D t ). Similarly, in AE-SCL and AE-SCL-SR mag- nificent and magnifique will be assigned the same coordinate in the x p (gold standard pivot indica- tors) and o (model output) vectors. In the lazy setup, where training is done with unlabeled data from (English, books) and (English, music) pivot translation is irrelevant as the representation learn- ing model is trained only in the source language.</p><p>Note that when only pivot translation is used to make the CD methods address CLCD learning, the input space is not shared across languages. In- stead, 1-hot vectors are used to encode the vocab- ularies of both languages, whose overlap is lim- ited. This mismatch is somewhat reduced when training on unlabeled data from both (L s , D s ) and (L t , D t ). That is, we rely on the trained parame- ters of the models to align the input spaces when trained on unlabeled data from both (L s , D s ) and (L t , D t ).</p><p>In § 7 we show that this technique alone leads to improved CLCD results compared to existing methods. The lazy setup, however, is not sup- ported by this technique, as training is not per- formed on unlabeled data from the target lan- guage. We next describe how to integrate BEs into our models, which provides a shared input layer that is crucial for both full and lazy CLCD.</p><p>Multilingual word embeddings Translated pivot features provide the models with a shared output layer. But can we use the same mechanism in order to map the input layers of the models into a shared cross-lingual space ?</p><p>Unfortunately, word-level translation does not seem like the right solution to this problem, due to two reasons. First, word-level translation is in- herently ambiguous -it is very frequent that the set of senses associated with a given word in one language, is not identical to the set of senses asso- ciated with any other word in another given lan- guage. Moreover, large scale word-level trans- lation may impose prohibitively high costs -ei- ther financial or in human time. Hence word-level translation is feasible mostly when dealing with a relatively small number of pivot features. The in- put layers of the models, consisting of words from the entire vocabulary (PBLM) or of non-pivot un- igrams and bigrams (AE-SCL and AE-SCL-SR), require a cheaper and more stable mapping.</p><p>Our solution is hence based on BEs which em- bed words from the source and the target language in a shared vector space. As discussed in § 2 the BEs we use are those of <ref type="bibr">Smith et al. (2017)</ref> that require several thousands of translated word pairs as a supervision signal, which reflects a low super- vision level <ref type="bibr" target="#b25">(Ruder et al., 2017)</ref>. While bilingual word embedding models do not provide accurate word-level translation (to the level that such trans- lation is possible), they do embed words from the two languages that have similar meaning with sim- ilar vectors, in terms of euclidean distance.</p><p>The BEs of <ref type="bibr">Smith et al. (2017)</ref> also have the property required for our lazy setup: they are in- duced such that the source language embeddings have no knowledge of any particular target lan- guage. The embedding algorithm achieves that by learning two sets of monolingual embeddings and then aligning them with an SVD-based method.</p><p>Once we obtain the BEs, it is straightforward to integrate them into the PBLM model. We start by considering the full CLCD setup. When PBLM is applied to text from (L s , D s ) -both when it is trained with unlabeled data <ref type="figure" target="#fig_2">(Figure 2a</ref>) and when it is used as part of the task classifier, when this classifier is trained with labeled data <ref type="figure" target="#fig_2">(Figure 2b</ref>) - the BEs of the source language words are fed into the model. Likewise, when PBLM is applied to text from (L t , D t ) -both when it is trained with unlabeled data and when it is used as part of the task classifier when this classifier is applied to test data -it is fed with the bilingual representations of the target language words. In the lazy setup, the details are very similar except that PBLM is not trained with unlabeled data from (L t , D t ), only with unlabeled data from (L s , D s ) and (L s , D t ).</p><p>Unfortunately, BEs do not provide a sufficient solution for the AE-based models. In AE-SCL the input layer consists of a non-pivots indicator vector, x np , that cannot be replaced by embedding vectors in a straight forward manner. In AE-SCL- SR the input layer is identical to that of AE-SCL, but this model replaces the reconstruction matrix w r with a matrix whose rows consist of pre-trained word embeddings of the pivot features. Hence, similarly to PBLM we can construct a w r matrix with the source language BEs when this model is applied to source language data, and with target language BEs when this model is applied to target language data. This construction of w r provides an additional shared cross-lingual layer, added to the translated pivot features of the output layer.</p><p>Consequently, an inherent limitation of the AE- based models when it comes to CLCD transfer, is that they cannot be employed in the lazy setup. The intersection of their input spaces when ap- plied to the source and the target languages is lim- ited to the vectors representing the shared vocab- ulary items (see above in this section). Hence, these models have to be trained with unlabeled data from both languages in order to align the in- put layers of the two languages with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Task and data 3 As in our most related pre- vious work <ref type="bibr">Stein, 2010, 2011;</ref> we experiment with the Websis-CLS-10 dataset <ref type="bibr" target="#b22">(Prettenhofer and Stein, 2010)</ref> consisting of Amazon product reviews writ- ten in 4 languages (English, German, French and Japanese), from 3 product domains (Books (B), DVDs (D) and Music (M)). Due to our extensive experimental setup we leave Japanese for future. <ref type="bibr">4</ref> For each (language, domain) pair the dataset includes 2000 train and 2000 test documents, la- beled as positive or negative, and between 9,358 to 50,000 unlabeled documents. As in the afore- mentioned related works, we consider English as the source language, as it is likely to have labeled documents from the largest number of domains.</p><p>Following ZR18 we also consider a more chal- lenging setup where the English source domain consists of user airline (A) reviews <ref type="bibr" target="#b20">(Nguyen, 2015)</ref>. We use the dataset of ZR18, consisting of 1000 positive and 1000 negative reviews in the la- beled set, and 39396 reviews as the unlabeled set.</p><p>We employ a 5-fold cross-validation protocol. In all folds 1600 (English, D s ) train-set examples are randomly selected for training and 400 for de- velopment. The German and French test sets are used in all folds. All sets contain the same number of positive and negative reviews. For each model we report averaged performance across the folds.</p><p>The BEs were downloaded from the author's github. More details are in the appendix.</p><p>Models and baselines Our main model is PBLM+BE that is trained in the full setup and em- ploys both translated pivots for CL output align- ment and BEs for CL input alignment ( § 5). We also experiment with PBLM+BE+Lazy: the same model employed in the lazy setup, and with PBLM: a model similar to PBLM+BE except that BEs are not employed.</p><p>We further experiment with AE-SCL that em- ploys translated pivots for CL output alignment and AE-SCL-SR that does the same and also in- tegrates BEs into its fixed reconstruction matrix. Following ZR17 and ZR18 the linear classifier we use is logistic regression. To compare to previous work, we implemented the CL-SCL and the DCI models, for which we use the cosine kernel that performs best in .</p><p>To consider the power of BEs, we experiment with a classifier fed with the BEs of the input doc- ument's words. We consider both a CNN classi- fier (where the BEs are fed into the columns of the CNN input matrix) and logistic regression (where the embeddings of the document's words are av- eraged) and report results with CNN as they are superior. We denote this model with BE+CNN.</p><p>For reference, we also compare to a setup where L s = L t , and to a setup where L s = L t and D s = D t . For these setups we report results with a linear classifier with unigram and bigram fea- tures, as it outperforms both a linear classifier and a CNN with BE features. The models are denoted with Linear-IL and Linear-ILID, respectively (IL stands for in-language and ID for in-domain).</p><p>Pivot features For all models we consider un- igrams and bigrams as features. To divide these <ref type="bibr">Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)</ref>     <ref type="table">Table 1</ref>: Sentiment accuracy. Top: CLCD transfer in the product domains. Middle: CLCD transfer from the English airline domain to the French and German product domains. Bottom: within language learning for the target languages. "All" refers to the average over the setups. We shorten some abbreviations: P+BE stands for PBLM+BE, Lazy for PBLM+BE+Lazy, A-S-SR for AE-SCL-SR, A-SCL for AE-SCL, C-SCL for CL-SCL, CNN for BE+CNN, IL for Linear-IL and ILID for Linear-ILID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>), CLCD English-German English-French S-T D-B M-B B-D M-D B-M D-M All D-B M-B B-D M-D B-M D-M All</head><note type="other">A-S-SR 68.3 62.5 69.4 69.9 70.2 69 67.4 69.3 68.9 70.9 70.7 67 71.4 69.7 A-SCL 67.9 63.7 68.7 63.8 69.0 70.1 67.2 68.6 66.1 69.2 69.4 66.7 68.1 68.0 Pivot-based (no DNN) Models C-SCL 65.9 62.5 65.1 65.2 71.2 69.8 66.7 70.3 63.8 68.8 66.8 66.0 70.1 67.6 DCI 67.1 60.6 66.9 66.7 68.9 68.2 66.4 71.2 65.4 69.1 67.5 66.7 71.4 68.6 CLCD without CD Learning CNN 62.8 63.8 65.3 68.7 71.6 72.0 67.3 69.5 59.7 63.7 65.7 65.9 67.0 65.</note><formula xml:id="formula_0">), Within Language German-German French-French S-T D-B M-B B-D M-D B-M D-M All D-B M-B B-D M-D B-M D-M All In-language</formula><formula xml:id="formula_1">S-T B-B - D-D - M-M - All B-B - D-D - M-M - All</formula><p>features into pivots and non-pivots we follow ( <ref type="bibr" target="#b2">Blitzer et al., 2007;</ref><ref type="bibr">Ziser and</ref><ref type="bibr">Reichart, 2017, 2018)</ref>. Pivots are translated with Google translate. Pivot features are frequent in the unlabeled data of both the source and the target (language, domain) pairs: we require them to appear at least 10 times in each. Among those frequent features we select the ones with the highest mutual information with the task (sentiment) label in the source (language, domain) labeled data. For non-pivot features we consider unigrams and bigrams that appear at least 10 times in one of the (language, domain) pairs.</p><p>Hyper-parameter tuning For all models we follow the tuning process described in the original papers. Details are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>Our results <ref type="table">(Table 1</ref>) support the integration of structure-aware DNNs, translated pivots and BEs as advocated in this paper. Indeed, PBLM+BE which integrates all these factors and trained in the full setup is the best performing model in all 12 product setups (top table) and in 2 of 6 airline- product setups <ref type="table">(middle table)</ref>. PBLM+BE+lazy, the same model when trained in the lazy setup in which no target language unlabeled data is avail- able for training, is the second best model in 9 of 12 product-product setups (in the other three se- tups only PBLM+BE and PBLM perform better) and is the best performing model in 4 of 6 airline- product setup and on average across these setups. To better understand this last surprising result of the airline-product setups, we consider the pivot selection process ( § 6): (a) sort the source fea- tures by their mutual information with the source domain sentiment label; and (b) iterate over the pivots and exclude the ones whose translation fre- quency is not high enough in the target domain.</p><p>Let's examine the number of feature candidates that should be considered (in step (b)) from the list of criterion (a) in order to get 100 pivots. In product to product domain pairs: 182; In airline to product domain pairs: 304 (numbers are averaged across setups). In the lazy setup (where no pivot translation is performed) the corresponding num- bers are: product to product domain pairs: 148; airline to product domain pairs: 173.</p><p>Hence, for domain pairs that involve airline and product, in the full setup many good piv- ots are lost in translation which affects the rep- resentation learning quality of PBLM+BE. While PBLM+BE+lazy does not get access to target lan- guage data, many more of its pivot features are preserved. We hypothesize that this can be one reason to the surprising superior performance of PBLM+BE+lazy when adapting from airline to product domains.</p><p>The success of PBLM+BE+lazy provides a par- ticularly strong support to the validity of our ap- proach, as this model lacks a major source of su- pervision available to the other CLCD models. As noted in § 1, we believe that the lazy setup is cru- cial for the future of CLCD learning.</p><p>Excluding BEs (PBLM) or changing the model to not generate a shared cross-lingual input layer (AE-SCL-SR that is also unaware of the re- view structure) results in substantial performance degradation. PBLM is better on average for all four CLCD setups, which emphasizes the impor- tance of structure-awareness. Excluding both BEs and structure-awareness (AE) yields further degra- dation in most cases and on average. Yet, this degradation is minor (0.5% -1.7% in the averages of the different setups), suggesting that the way AE-SCL-SR employs BEs, which is useful for CD transfer (ZR17), is less effective for CLCD. CL-SCL and DCI, that employ pivot translation but neither DNNs nor BEs, lag behind the PBLM- based models and often also the AE-based mod- els, although they outperform the latter in some cases. Likewise, BE+CNN, where BEs are em- ployed but without any other CLCD learning tech- nique, is also substantially outperformed by the PBLM-based models, but it does better than the AE-based models with the airline source domain.</p><p>Finally, comparison to the within-language models of the bottom table allows us to quan- tify the gap between current CLCD models and standard models that do not perform CD and/or CL transfer. The averaged differences between our best product-product model, PBLM-BE, and Linear-ILID are 3.5% (English-German) and 6.5% (English-French). When adapting from the air- line domain the gap is much larger: averaged gaps of 17% and 16.2% from the best per- forming PBLM+BE-lazy, for English-German and English-French, respectively. This is not a surprise as ZR18 already demonstrated the challenging na- ture of within-language airline-product transfer. We consider our results to be encouraging, espe- cially given the improvement over previous work, and the smaller gaps in the product-product setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We addressed the problem of CLCD transfer in sentiment analysis and proposed methods based on pivot-based learning, structure-aware DNNs and BEs. We considered full and lazy training, and designed a lazy model that, for a given target do- main, can be trained with unlabeled data from the source language only and then be applied to any target language without re-training. Our models outperform previous models across 18 CLCD se- tups, even when ours are trained in the lazy setup and previous models are trained in the full setup.</p><p>In future work we wish to improve our results for large domain gaps and for more dissimilar lan- guages, particularly in the important lazy setup. As our airline-product results indicate, increasing the domain gap harms our results, and we expect the same with more diverse language pairs. <ref type="bibr">Samuel L Smith, David HP Turban, Steven Hamblin, and Nils Y Hammerla. 2017</ref>. Offline bilingual word vectors, orthogonal transformations and the inverted softmax. In proceedings of ICLR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyper-parameter Tuning</head><p>As promised in Section 6 of the main paper we detail here our hyper-parameter tuning process. For all models, we tune the number of pivot fea- tures among <ref type="bibr">[100,</ref><ref type="bibr">200,</ref><ref type="bibr">300,</ref><ref type="bibr">400,</ref><ref type="bibr">500]</ref>. For PBLM, the input embedding size (when no word embed- dings are used) is tuned among <ref type="bibr">[128,</ref><ref type="bibr">256]</ref>, and the hidden representation dimension is selected from <ref type="bibr">[128,</ref><ref type="bibr">256,</ref><ref type="bibr">512]</ref>. The size of the hidden layer of AE-SCL and AE-SCL-SR is set to 300.</p><p>The dimension of our bilingual embeddings is 300, as decided by <ref type="bibr">(Smith et al., 2017)</ref>. For all CNN models we use 256 filters of size 3 × |embedding| and perform max pooling for each of the 256 vectors to generate a single 1 × 256 vec- tor that is fed into the classification layer. In the SVD step of CL-SCL we tune the output dimen- sion among <ref type="bibr">[50,</ref><ref type="bibr">75,</ref><ref type="bibr">100,</ref><ref type="bibr">125,</ref><ref type="bibr">150]</ref>.</p><p>For AE-SCL and AE-SCL-SR, we follow ZR17 and represent each example fed into the sentiment classifier with its w h x np vector. Unlike ZR17 we do not concatenate this representation with a bag of unigrams and bigrams representation of the ex- ample -due to the cross-lingual nature of our task. As in the original papers, the input features of AE- SCL, AE-SCL-SR, CL-SCL and DCI are word un- igrams and bigrams.</p><p>All the algorithms in the paper that involve a CNN or a LSTM are trained with the ADAM algo- rithm ( <ref type="bibr" target="#b17">Kingma and Ba, 2015)</ref>. For this algorithm we follow ZR18 and use the parameters described in the original ADAM article:</p><p>• Learning rate: lr = 0.001.</p><p>• Exponential decay rate for the 1st moment es- timates: β 1 = 0.9.</p><p>• Exponential decay rate for the 2nd moment estimates: β 2 = 0.999.</p><p>• Fuzz factor: = 1e − 08.</p><p>• Learning rate decay over each update: decay = 0.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Code and Data</head><p>Here we provide the URLs of the code and data we used in this paper:</p><p>• The Websis-CLS-10 dataset <ref type="bibr" target="#b22">(Prettenhofer and Stein, 2010)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Input</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The AE-SCL and AE-SCL-SR models (figure imported from ZR17). x np is a binary vector indicating whether each of the non-pivot features appears in the input example. x p is a similar vector defined with respect to pivot features. o, the output vector of the model, provides the probability that each of the pivot features appears in the example. The loss function of both models is the cross-entropy loss between o and x p. While in AE-SCL both the encoding matrix w h and the reconstruction matrix w r are optimized, in AE-SCLSR w r consists of pre-trained word embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The PBLM model (figure imported form ZR18). (a) The PBLM representation learning model. (b) Adapting a classifier with PBLM: the PBLM-CNN model where PBLM representations are fed into a CNN task classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>http: //www.uni-weimar.de/en/media/ chairs/webis/research/corpora/ corpus-webis-cls-10/ • Bilingual word embeddings (Smith et al., 2017): https://github.com/ Babylonpartners/fastText_ multilingual. The authors employed their method to monolingual fastText em- beddings (Bojanowski et al., 2017) -the embeddings of 78 languages were aligned with the English embeddings. • The bilingual embeddings are based on the fastText Facebook embeddings (Bojanowski et al., 2017): https: //github.com/facebookresearch/ fastText/blob/master/ pretrained-vectors.md</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Airline (English, (Nguyen, 2015)) to Product Review Domains (German or French), CLCD 
English-German 
English-French 
Source-Target A-B A-D A-M All 
A-B A-D A-M All 
PBLM Models 
P+BE 
67.9 62.5 63.6 
64.6 63.5 66.9 64.8 
65.1 
PBLM 
60.9 59.6 60.1 
60.2 60.9 61.9 58.9 
60.5 
Lazy 
66.3 65.0 66.6 
66.0 65.7 65.6 69.0 
66.8 
Autoencoder+pivot Models 
A-S-SR 
55.8 57.5 60.8 
58 
55.8 52.9 56.3 
55.7 
A-SCL 
55.9 56.2 58.2 
56.8 55.8 52.9 56.4 
55.0 
Pivot-based (no DNN) Models 
C-SCL 
56.6 52.6 53.7 
54.3 52.7 54.5 53.1 
53.4 
DCI 
55.9 52.1 54.5 
54.1 53.1 53.7 53.9 
53.5 
CLCD without CD Learning 
CNN 
59.4 61.2 61.3 
60.6 57.9 55.3 56.2 
56.5 

Product Review Domains (Websis-CLS-10,(Prettenhofer and Stein, 2010)</table></figure>

			<note place="foot" n="2"> ZR18 also considered a PBLM-LSTM architecture where the PBLM representations feed an LSTM classifier. We focus on PBLM-CNN which demonstrated superior performance in 13 of 20 of their experimental setups.</note>

			<note place="foot" n="3"> The URLs of the code (previous models and standard packages) and data we used, are in the appendix. 4 We add an English domain to our experiments. Moreover, training the models we consider here is substantially more time consuming as we employ DNNs, as opposed to previous methods that use linear classifiers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers and the mem-bers of the Technion NLP group for their useful comments. We also thank Ivan Vuli´cVuli´c for guiding us in the world of multilingual word embeddings.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Many languages, one parser. Transactions of the Association for Computational Linguistics 4.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL (TACL)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relation adaptation: learning to extract novel relations with minimum supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic feature decomposition for single view co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributional correspondence indexing for cross-lingual and cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning structural correspondences across different linguistic domains with synchronous neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mih</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Medialab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the xLite Workshop on Cross-Lingual Technologies, NIPS</title>
		<meeting>of the xLite Workshop on Cross-Lingual Technologies, NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACL-IJCNLP)</title>
		<meeting>ACL-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Mehryar Mohri, and Afshin Rostamizadeh</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The airline review dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quang</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="https://github.com/quankiquanki/skytrax-reviews-dataset.Scrapedfromwww.airlinequality.com" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crosslingual adaptation using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>TIST)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A survey of cross-lingual word embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Sgaard</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1706.04902</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved parsing and pos tagging using inter-sentence consistency constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards robust cross-domain domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross language text classification by model translation and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjun</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<ptr target="http://scikit-learn.org/stable/" />
	</analytic>
	<monogr>
		<title level="j">• Logistic regression classifier</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Pblm</surname></persName>
		</author>
		<ptr target="https" />
		<title level="m">We use the code from the author&apos;s github</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<ptr target="https://github.com/yftah89/Neural-SCLDomain-Adaptation" />
		<title level="m">/yftah89/ PBLM-Domain-Adaptation • AE-SCL and AE-SCL-SR: We use the code from the author&apos;s github</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández</surname></persName>
		</author>
		<title level="m">• We reimplemented the CL-SCL (Prettenhofer and Stein, 2011) and the DCI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
