<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Fine-Grained Expressions to Solve Math Word Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangdong Key Laboratory of Big Data Analysis and Processing</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Guangdong Key Laboratory of Big Data Analysis and Processing</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Fine-Grained Expressions to Solve Math Word Problems</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="805" to="814"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a novel template-based method to solve math word problems. This method learns the mappings between math concept phrases in math word problems and their math expressions from training data. For each equation template , we automatically construct a rich template sketch by aggregating information from various problems with the same template. Our approach is implemented in a two-stage system. It first retrieves a few relevant equation system templates and aligns numbers in math word problems to those templates for candidate equation generation. It then does a fine-grained inference to obtain the final answer. Experiment results show that our method achieves an accuracy of 28.4% on the linear Dolphin18K benchmark, which is 10% (54% relative) higher than previous state-of-the-art systems while achieving an accuracy increase of 12% (59% relative) on the TS6 benchmark subset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The research topic of automatically solving math word problems dates back to the 1960s <ref type="bibr">(Bobrow, 1964a,b;</ref><ref type="bibr" target="#b3">Charniak, 1968)</ref>. Recently many sys- tems have been proposed to these types of prob- lems ( <ref type="bibr" target="#b8">Hosseini et al., 2014;</ref><ref type="bibr" target="#b10">Koncel-Kedziorski et al., 2015;</ref><ref type="bibr" target="#b20">Zhou et al., 2015;</ref><ref type="bibr" target="#b15">Roy and Roth, 2015;</ref><ref type="bibr" target="#b17">Shi et al., 2015;</ref><ref type="bibr" target="#b18">Upadhyay et al., 2016;</ref><ref type="bibr" target="#b13">Mitra and Baral, 2016)</ref>. On a re- cent evaluation conducted by <ref type="bibr" target="#b9">Huang et al. (2016)</ref>, current state-of-the-art systems only achieved an * Work done while this author was an intern at Microsoft Research.</p><p>18.3% accuracy on their published dataset Dol- phin18K. Their results indicate that math word problem solving is a very challenging task.</p><p>To solve a math word problem, a system needs to understand natural language text to extract in- formation from the problem as local context. Also, it should provide an external knowledge base, in- cluding commonsense knowledge (e.g. "a chicken has two legs") and mathematical knowledge (e.g. "the perimeter of a rectangle = 2 * length + 2 * width"). The system can then perform reasoning based on the above two resources to generate an answer. In this paper, we focus on the acquisition of mathematical knowledge, or deriving math con- cepts from natural language. Consider the first two problems P 1 and P 2 in <ref type="figure" target="#fig_0">Figure 1</ref>. The math con- cept in the problems tells you to take away a per- centage from one and get the resulting percentage of a total. Using mathematical language, it can be formulated as (1−n 1 ) * n 2 , where n 1 , n 2 are quan- tities. In this example, we can derive the concept of subtraction from the text " <ref type="bibr">[NUM]</ref> % off " and " <ref type="bibr">[NUM]</ref> % discount".</p><p>Acquisition of mathematical knowledge is non- trivial. Initial statistical approaches <ref type="bibr" target="#b8">(Hosseini et al., 2014;</ref><ref type="bibr" target="#b15">Roy and Roth, 2015;</ref><ref type="bibr">KoncelKedziorski et al., 2015</ref>) derive math concepts based on observations from their dataset of spe- cific types of problems, e.g. problems with one single equation. For example, <ref type="bibr" target="#b8">Hosseini et al. (2014)</ref> assumes verbs and only verbs embed math concepts and map them to addition/subtraction. <ref type="bibr" target="#b15">Roy and Roth (2015)</ref>; <ref type="bibr" target="#b10">Koncel-Kedziorski et al. (2015)</ref> assume there is only one unknown vari- able in the problem and cannot derive math con- cepts involving constants or more than one un- known variables, such as "the product of two un- known numbers".</p><p>Template-based approaches ( <ref type="bibr" target="#b20">Zhou et al., 2015;</ref><ref type="bibr" target="#b18">Upadhyay et al., 2016)</ref>, on the other hand, leverage the built-in composi- tion structure of equation system templates to for- mulate all types of math concepts seen in train- ing data, such as (1 − n 1 ) * n 2 = x in <ref type="figure" target="#fig_0">Figure 1</ref>. However, they suffer from two major shortcom- ings. First, the math concepts they learned, which is expressed as an entire template, fails to capture a lot of useful information with sparse training in- stances. We argue that it would be more expres- sive if the math concept is learned in a finer granu- larity. Second, their learning processes rely heav- ily on lexical and syntactic features, such as the dependency path between two slots in a template. When applied to a large-scale dataset, they create a huge and sparse feature space and it is unclear how these template-related features would contribute.</p><p>To alleviate the sparseness problem of math concept learning and better utilize templates, we propose a novel approach to capture rich informa- tion contained in templates, including textual ex- pressions that imply math concepts. We parse the template into a tree structure and define "template fragment" as any subtree with at least one opera- tor and two operands. We learn fine-grained map- pings between textual expressions and template fragments, based on longest common substring. For example, given the three problems in <ref type="figure" target="#fig_0">Figure 1</ref>, we can map "[NUM] % off" and " <ref type="bibr">[NUM]</ref> % dis- count" to 1 − n 1 , and "[NUM] % off <ref type="bibr">[NUM]</ref>" to (1 − n 1 ) * n 2 = x. In this way, we can decompose the templates and learn math concepts in a finer grain. Furthermore, we observe that problems of the same template share some common properties. By aggregating problems of the same template and capturing these properties, we automatically con- struct a sketch for each template in the training data.</p><p>Our approach is implemented in a two-stage system. We first retrieve a few relevant templates in the training data. This narrows our search space to focus only on those templates that are likely to be relevant. Then we align numbers in the prob- lem to those few returned templates, and do fine- grained inference to obtain the final answer. We show that the textural expressions and template sketch we propose are effective for both stages. In addition, our system significantly reduces the hy- pothesis space of candidate equations compared to previous systems, which benefits the learning pro- cess and inference at scale. We evaluate our system on the benchmark dataset provided by <ref type="bibr" target="#b9">Huang et al. (2016)</ref>. Experi- ments show that our system outperforms two state- of-the-art baselines with a more than 10% abso- lute (54% relative) accuracy increase in the linear benchmark and a more than 20% absolute (71% relative) accuracy increase for the dataset with a template size greater than or equal to 6.</p><p>In the remaining parts of this paper, we in- troduce related work in Section 2, describe tem- plate sketch and textual expression learning in Section 3, present our two-stage system in Sec- tion 4, summarize experiment setup and results in Section 5, and conclude this paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic math word problem solving meth- ods <ref type="bibr">(Bobrow, 1964a,b;</ref><ref type="bibr" target="#b3">Charniak, 1968</ref><ref type="bibr" target="#b4">Charniak, , 1969</ref><ref type="bibr" target="#b2">Briars and Larkin, 1984;</ref><ref type="bibr" target="#b6">Fletcher, 1985;</ref><ref type="bibr" target="#b5">Dellarosa, 1986;</ref><ref type="bibr">Bakman, 2007;</ref><ref type="bibr" target="#b19">Yuhui et al., 2010</ref>) devel- oped before 2008 are mostly rule-based. They ac- cept limited well-format input sentences and map them into certain structures by pattern matching. They usually focus on problems with simple math operations such as addition or subtraction. Please see <ref type="bibr" target="#b14">Mukherjee and Garain (2008)</ref> for a summary.</p><p>In recent years, symbolic and statistical meth- ods have been explored by various researchers. In the symbolic approach, systems transform math word problems to structured representations. <ref type="bibr">Bakman (2007)</ref> maps math problems to predefined schema with a table of textual formulas and chang- ing verbs. <ref type="bibr" target="#b12">Liguda and Pfeiffer (2012)</ref> uses aug- mented semantic networks to represent math prob- lems. <ref type="bibr" target="#b17">Shi et al. (2015)</ref> parses math problems to their pre-defined semantic language. However, these methods are only effective in their desig- nated math problem categories and are not scal- able to other categories. For example, the method used by <ref type="bibr" target="#b17">Shi et al. (2015)</ref> works extremely well for solving number word problems but not others.</p><p>In the statistical machine learning approach, <ref type="bibr" target="#b8">Hosseini et al. (2014)</ref> solves addition and subtrac- tion problems by extracting quantities as states and derive math concepts from verbs in the training data.  and <ref type="bibr" target="#b20">Zhou et al. (2015)</ref> generalize equations attached to problems with variable slots and number slots. They learn a probabilistic model for finding the best solution equation. <ref type="bibr" target="#b18">Upadhyay et al. (2016)</ref> follows their approach and leverage math word problems with- out equation annotation as external resources. <ref type="bibr" target="#b16">Seo et al. (2015)</ref> solves a set of SAT geometry ques- tions with text and diagram provided. <ref type="bibr">KoncelKedziorski et al. (2015)</ref> and <ref type="bibr" target="#b15">Roy and Roth (2015)</ref> target math problems that can be solved by one single linear equation. They map quantities and words to candidate equation trees and select the best tree using a statistical learning model. Mi- tra and Baral (2016) considers addition and sub- traction problems in three basic problem types: "Change", "Part Whole" and "Comparison". They manually design different features for each type, which is difficult to expand to more types.</p><p>In summary, previous methods can achieve high accuracy in limited math problem categories, (i.e. ( <ref type="bibr" target="#b17">Shi et al., 2015)</ref>), but do not scale or perform well in datasets contain- ing various math problem types as in <ref type="bibr" target="#b9">Huang et al. (2016)</ref>, as their designed features are becoming sparse. Their process of acquiring mathematical knowledge is either sparse or based on certain as- sumptions of specific problem types. To allevi- ate this problem, we introduce our template sketch construction and fine-grained expressions learning in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Template Sketch Construction</head><p>A template sketch contains template information. We define three categories of information for the sketch shown in this section. Next we describe how we construct a template sketch, via aggrega- tion of rich information from training problems. We group problems of the same template in train- ing set as one cluster and collect information. See <ref type="figure" target="#fig_2">Figure 2</ref> for the outline of our template sketch con- struction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition</head><p>Template: It is first introduced in . It is a unique form of an equation system. For example, given an equation system as follows:</p><formula xml:id="formula_0">2 · x 1 + 4 · x 2 = 34 x 1 + 4 = x 2</formula><p>This equation system is a solution for a specific math word problem. We replace the numbers with four number slots {n 1 , n 2 , n 3 , n 4 } and generalize the equations to the following template:</p><formula xml:id="formula_1">n 1 · x 1 + n 2 · x 2 = n 3 x 1 + n 4 = x 2</formula><p>Alignment: We align numbers in the math prob- lem with the number slots of a template. For the first math problem in <ref type="figure" target="#fig_0">Figure 1</ref> with its correspond- ing template (1 − n 1 ) * n 2 = x, there are two numbers 0.25 and 139.99 to align with two num- ber slots n 1 and n 2 , which results in two different alignments.  aligns nouns to variable slots {x 1 , x 2 , ...} which leads to a huge hypoth- esis space and does not perform as well as the number slot alignment only method proposed later by ( <ref type="bibr" target="#b20">Zhou et al., 2015</ref>). Therefore, we only con- sider number slot alignment in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Textual Expressions</head><p>For template fragments, there are usually some textual expressions. For example, "n 1 % off" and "n 1 % discount" are both mapped to the template fragment 1 − n 1 .</p><p>We employ a statistical framework to automat- ically mine textual expressions for template frag- ments from a training dataset. First we parse the equation to a hierarchical tree. In a bottom-up ap- proach, we obtain each possible subtree as a tem- plate fragment t k , which associates with at least one number slot. For each t k , we use the num- bers to anchor the number-related phrases in the problem, replace numbers with" <ref type="bibr">[NUM]</ref>" and noun phrases with " <ref type="bibr">[VAR]</ref>", and cluster the phrases P = {p 1 , p 2 , · · · } with the same t k across all data given all training problems. Then we compute the longest common substring lcs k ij between pairs p i and p j and calculate tf-idf score of lcs k ij . We keep the lcs k ij with scores above certain empirically de- termined threshold as the textual expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch for Template: (1-n 1 )*n 2 =x</head><formula xml:id="formula_2">[Unit Sequence] {%, $} … [Normalized Unit Sequence] {0, 1} … [Question Keyword] {price} … [Textual Expression] • 1.0-n 1 a discount of [NUM] % mark down [NUM] % [NUM] % less than ... • (1.0-n 1 )*n 2 [NUM] % of off [NUM] …</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem 2 Problem 1</head><p>Problem k …</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Aggregation</head><p>Wallace received a discount of 28% on an item priced at $275. What is the total price that he paid for it? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equation</head><p>Template Phrases </p><formula xml:id="formula_3">1-0.28 1-n 1 a discount of [NUM] % (1-0.28)*275 (1-n 1 )*n 2 a discount of [NUM] % on [VAR] priced at [NUM]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Slot Type</head><p>Number slots in templates have their own type of constraints. For example, in the template (1−n 1 ) * n 2 = x, usually n 1 represents a percentage quan- tity and n 2 is the quantity of an object. We model slot types with quantity units, and find the direct governing noun phrase as its 'owner'. For the problem in <ref type="figure" target="#fig_2">Figure 2</ref>, we ex- tract quantity unit sequence as {%, $}, normalized unit sequence as {0, 1} (because % and $ are of different quantity types), and quantity owners as {discount, item}. The slot type information pro- vides important clues to choose the correct tem- plate and alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Question Keyword</head><p>Question keyword decides which template we use. Given the following problem setting: "A rectangle has a width of 5cm and a length of 10cm.", we can ask either Q1:"What is the area of the rectangle?" or Q2: "What is the difference between width and length?". The question keywords area and differ- ence help our system to decide if is should apply template n 1 * n 2 = x for Q1 and apply template n 1 − n 2 = x for Q2.</p><p>We first detect the question sentence (containing keywords "what","how","figure out"...). Then we extract the question keyword on the dependency tree with simple rules that we observed in the dev set (e.g. retrieving nouns with "attr − nsubj" de- pendency relation with keyword "what"). Please note that we favor recall over precision of our de- tected question keywords since they are used as features instead of hard constrains on template de- cision. Simple rule-based extraction can already satisfy our need for detecting question keywords in math problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Two-Stage System</head><p>In this section, we describe our two-stage system for solving math problems, including template re- trieval and alignment ranking. We show how to apply textual expressions and template sketch to our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Template Retrieval</head><p>We use an efficient retrieval module to first narrow our search space and focus only on templates that are likely to be relevant. Let χ denote the set of test problems, and T = {t 1 , t 2 , . . . , t j } as the tem- plate set in the training data. For each test prob- lem x i , our goal is to select the correct template t j . We define the conditional probability of select- ing a template given a problem as follows:</p><formula xml:id="formula_4">p(t j |x i ; ν t ) = exp(ν t · f (x i , t j )) t j ∈T exp(ν t · f (x i , t j ))</formula><p>where ν t is the model parameter and f (x i , t j ) is the feature vector. We apply the Ranking SVM ( <ref type="bibr" target="#b7">Herbrich et al., 2000</ref>) to minimize a regu- larized margin-based pairwise loss. We then have the following objective function:</p><formula xml:id="formula_5">1 2 ν t 2 + C i l(ν T t f (x i , t j ) + − ν T t f (x i , t l ) − )</formula><p>where superscript "+" indicates the correct in- stance and "-" indicates the false ones. We use the loss function l(t) = max(0, 1 − t) 2 .</p><p>To construct the vector f (x i , t j ) for template t j , we use the three categories in the template sketch shown in <ref type="table">Table 1</ref>. Let Q(t j ) represent the cluster of training problems with template t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Features</head><p>Contains textual expressions in each template fragments? Average Word Overlap with Q(t j ) Max Word Overlap with Q(t j ) Quantity Features Unit sequence in Q(t j ) Normalized unit sequence in Q(t j ) Question Features Is Question keyword in Q(t j ) <ref type="table">Table 1</ref>: Features for template retrieval.</p><p>At the phrase level, as we have mined differ- ent expressions in 3.2 for slots in templates, we can extract the phrases related to each number or number pair in a test problem and match them with expressions. For example, given a test problem to match template (1 − n 1 ) * n 2 = x in <ref type="figure" target="#fig_2">Figure 2</ref>, we have two groups of patterns to match, correspond- ing to 1.0 − n 1 and (1.0 − n 1 ) * n 2 respectively.</p><p>Quantity types in a problem are important. We use the unit type sequences and normalized unit type sequence for describing number slot types in a template. In addition, if a number unit type can- not differentiate each number slot, we will make use of number "owner" as defined in subsection 3.3. For example, in the sentence "The width is 3cm and the length is 5cm", we extract two quanti- ties with unit type sequence {cm, cm}; and owner {width, length}.</p><p>In addition, we consider question keywords for templates. For example, if the question keyword is "difference", then x + n 1 = n 2 will have a higher probability of being selected than x = n 1 * n 2 .</p><p>We observe that in some cases, one word dif- ference can lead to two different templates. To consider cases in which some templates are very similar (e.g. x + n 1 = n 2 and n 1 + n 2 = x, part/whole unknown), we retrieve the top ranked N (N =3) templates as candidates for alignment in the next stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Alignment Ranking</head><p>For each top N templates from the previous stage, we generate possible alignments A = {a 1 , a 2 , . . . , a m } as the candidate equation sys- tem for the test problem x i . We train a ranking model to choose the alignment with the highest probability p(a k |x i , t j ; ν a ), where ν a is the model parameter vector.</p><formula xml:id="formula_6">p(a k ) = exp(ν a · f (x i , a k )) a k ∈A exp(ν a · f (x i , a k ))</formula><p>We use the same ranking model as in template se- lection stage and the objective function is changed to:</p><formula xml:id="formula_7">1 2 ν a 2 + C i l(ν T a f (x i , a k ) + − ν T a f (x i , a l ) − )</formula><p>We design more fine-grained features for each number slot to formulate the alignment feature vector f (x i , a k ). It contains the following features in <ref type="table" target="#tab_0">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Textual Features</head><p>Match textual expressions in template frag- ment aligned to each number slot (pair) Quantity Features Aligned unit sequence in Q(t j ) Aligned normalized unit sequence in Q(t j ) Relationship with noun phrase Optimal number 1 or 2 is used? Solution Features Is integer solution? Is positive solution? At the textual level, we want to capture textual expressions describing each number slot. For ex- ample, in the template (1 − n 1 ) * n 2 = x, we have mined patterns of 1 − n 1 in 3.2, such as "a dis- count of n 1 %", "mark down n 1 %", etc. Given the problem in <ref type="figure" target="#fig_2">Figure 2</ref> as the test problem, align- ment (1-0.28) * 275 = x matches textual expres- sions, while (1-275) * 0.28 = x does not.</p><p>For quantity features, we use the alignment- ordered unit sequence. For the problem in <ref type="figure" target="#fig_2">Figure  2</ref> mapping to template (1 − n 1 ) * n 2 = x, we have two different alignments: {n 1 :0.28, n 2 :275}, {n 1 :275,n 2 :0.28}. Their aligned unit sequences are {%, $} and {$,%} respectively. We also use the relations of quantities with noun phrases to dif- ferentiate number slot interaction with unknown variable slots and number slots, such as n 1 * x and n 1 * n 2 .</p><p>Some templates have numerical solution prop- erties while others do not. For example, tem- plate x 1 = (n 1 − n 2 )/(n 3 − n 4 ) would be less likely to have any strong indication of inte- ger solution properties. We count the percentage of integer/positive solutions from the correspond- ing problems as the probability that this template prefers an integer/positive solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Discussion</head><p>Our method has two main differences from pre- vious template-based methods ( <ref type="bibr" target="#b20">Zhou et al., 2015;</ref><ref type="bibr" target="#b18">Upadhyay et al., 2016)</ref>.</p><p>First, previous methods implicitly model map- ping from problem text to templates. We learn fine-grained textual expressions mapped to tem- plate fragments; and explicitly model the property of templates with template sketches. Second, pre- vious methods align numbers for all templates in a training set, while we only examine the N most probable templates. This significantly reduces the equation candidate search space. Given a prob- lem in which m numbers align with a template of n number slots, the number of possible equa- tion candidates would be A n m . The search space grows linearly with the number of templates in the training data. Suppose m = 5, n = 4 and we have 1000 templates, the total space would be (5 * 4 * 3 * 2) * 1000 = 120, 000 for one problem in <ref type="bibr" target="#b20">Zhou et al. (2015)</ref>, and will be much larger if it considers unknown variable alignment as in ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Settings As demonstrated in <ref type="bibr" target="#b9">Huang et al. (2016)</ref>, previous datasets for math problems are limited in both scale and diversity. We conduct our experi- ment on their dataset Dolphin18K. We use the lin- ear subset, containing 10,644 problems in total. We use two baseline systems for comparison: (1) ZDC ( <ref type="bibr" target="#b20">Zhou et al., 2015</ref>) is a statistical learning method that is an improved version of KAZB (  1 . (2) SIM ( <ref type="bibr" target="#b9">Huang et al., 2016</ref>) is a simple similarity based method. We do not compare other systems because they only solve one specific type of problem, e.g. <ref type="bibr" target="#b8">Hosseini et al. (2014)</ref> only handle addition/subtraction problems and Koncel- <ref type="bibr" target="#b10">Kedziorski et al. (2015)</ref> aim to solve problems with one single linear equation. Experiments are conducted using 5-fold cross- validation with 80% problems randomly selected as training data and the remaining 20% for testing. We report the solution accuracy.  From the table, we observe that our model consistently achieves better performance than the baselines on all template sizes. As the template size becomes larger, all three systems achieve bet- ter performance. When template size equals 6 (TS6, as a de-facto template size constrain adopted in ZDC), our model achieve an absolute increase of over 12% (59% relative). This demonstrates the effectiveness of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Evaluation Results</head><p>When including long tail problems with a tem- plate size less than 2, performance of all three systems drop significantly. This is because the templates of these problems are not seen in the training set, and thus are difficult to solve using these template-based methods. Still, we have at least 10% absolute (54% relative) accuracy in- crease on the whole test set compared to the two baselines. Previous template-based methods re- quire templates size larger than 6 in the data as constraints. From the result, we can see that our method relaxes the template size constraint and matches more problems with less frequent tem- plates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accuracy per Template</head><p>Here we investigate the performance of different templates. In <ref type="table" target="#tab_3">Table 4</ref>, we sample some domi- nant templates and report their accuracies. For our model, we report both template retrieval accuracy and final solution accuracy.</p><p>As we can see, our method performs better than the baselines for most dominant templates. Per- formance of the dominant templates can reach an accuracy level of 60%. This proves that our tem- plate sketch and textual expressions are effective in capturing rich template information.</p><p>To our surprise, some templates tend to perform better than others even with smaller template sizes. For example, x 1 = n 1 − n 2 , which represents the subtraction problem, has 63 problems but per- forms not as well as x 1 = (n 1 − n 2 )/(n 3 − n 4 ) which has 48 problems. We look into their corre- sponding problems and find out that x 1 = n 1 − n 2 are applied to more themes in natural language than x 1 = (n 1 − n 2 )/(n 3 − n 4 ), which are almost about the theme of "coordinate slope".</p><p>In our model, there is a gap between tem- plate retrieval accuracy and final solution accu- racy, which means that although we select the correct template candidates for the problem, the alignment model cannot rank the equations cor- rectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Two-Stage Evaluation</head><p>Next, we evaluate the performance of our two- stage system. Accuracy of template retrieval and alignment ranking is shown in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>For template retrieval accuracy, Hit@N means the correct template for a problem is included in the top N list returned by our model. We es- timate the best achievable performance by using oracle template retrieval. The result is 47.1% (Hit@ALL), which means 47.1% of the templates exist more than once in the problem set. Please note that our template retrieval evaluation may be underestimated, since in some cases, a test prob- lem can be solved by different templates.</p><p>We then use the top N templates as input for both our alignment ranking and ZDC. From the table, we have the following observations: (1) Hit@3 performs better compared to Hit@1 for both systems. This confirms our claim that some templates are similar and we need to incorporate more fine-grained features to differentiate in the alignment step; (2) It obtains the highest accu- racy when N = 3 and decreases when N gets larger. Both systems get benefits from our tem- plate retrieval which helps retrieve relevant tem- plates and reduce the hypothesis space of equa- tions; (3) Given the same N templates input, our alignment ranking achieves better performance than ZDC. This implies that our features are more indicative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Feature Ablation</head><p>This section describes our feature ablation study.</p><p>Template Retrieval In <ref type="table">Table 6</ref>, we conduct three configurations against our model (FULL). Each ablated configuration corresponds to one category of our template sketch. From the ta- ble, we can see that all three categories of fea- tures contribute to system performance. We re- move QUANTITY results in the worse perfor- mance comparing to the FULL model. Alignment Ranking In <ref type="table">Table 7</ref>, N means to select the top N templates in the previous stage for alignment. The column "Correct Template" represents how well the alignment model can per- form given the correct template input for align- ment. Our alignment model (FULL) performs the best compared to the three ablated settings, which confirms the effectiveness of template properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error Analysis</head><p>We have observed that template-based methods have difficulty solving problems with small tem- plate sizes, especially for cases that have a single problem instance (i.e. template size = 1). We sample 100 problems in which our system makes mistakes in the dev set of Dolphin18K and sum- marize them in <ref type="table" target="#tab_6">Table 8</ref>.</p><p>Quantity Type The types of quantities are dif- ficult to determined. For the example problem in Ours Template problems ZDC SIM Template retrieval Acc Final Acc (%) (%) (%) (%) n 1 * x 1 = n 2 548 26.3 23.9 87.0 58.7 n 1 /x 1 = n 2 /n 3 453 21.4 29.8 94.1 61.5 x 1 = n 1 * n 2 403 23.6 28.0 78.9 63.4 n 1 * x 1 + n 2 * x 2 = n 3 ; 300 86.3 69.7 94.9 85.8 x 1 + x 2 = n 4 x 1 = n 1 * n 2 * n 3 103 22.3 32.0 67.0 55.0 x 1 + x2 = n 1 80 39.7 48.8 79.4 65.1 x1 − x 2 = n2 x 1 = n 1 − n 2 63 11.7 15.9 50.7 23.4 x 1 = (n 1 − n 2 )/(n 3 − n 4 ) 48 14.9 18.8 95.7 89.4   <ref type="table">Table 6</ref>: Feature ablation of template retrieval. the table, if we can detect "24 male" is the same as "men", the problem can be solved.</p><p>Relation/State Detection If we can identify the changed states or mathematical relations between variables, we can solve this category of problems. In the example problem, it is important to under- stand that "commission is taken out" is my money state.</p><p>External Knowledge This requires specific mathematical models, such as permutation and combination, or commonsense knowledge, e.g. a dice has 6 sides.</p><p>Equation Decomposition The limitation of template-based approaches is that they require test problems belonging to one of the templates seen</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Correct  <ref type="table">Table 7</ref>: Feature ablation of alignment ranking.</p><formula xml:id="formula_8">N = N =</formula><p>in training. Thus, for problems corresponding to template sizes less than 2, we can decompose templates into smaller units and conduct learning more precisely. We then need to generate the equa- tions, which is also a challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose a novel approach to solv- ing math word problems with rich information of templates. We learn mappings between textual ex- pressions and template fragments. Furthermore, we automatically construct sketches for each tem- plate. We implement a two-stage system, includ- ing template retrieval and alignment ranking. Ex- periments show that our method performs signifi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Math Problem Quantity Type (10%)</p><p>The ratio of women to men in a certain club is 3 to 2. If there are 24 male club mem- bers, then how many female club members are there? Relation/State Detection (12%) If I am selling something for $25,000 and a 7% commis- sion is taken out, how much money will I be left with? External Knowledge (23%)</p><p>Find the probability that total score is 10 or more given at least one dice show 6 if 2 dice red &amp; blue thrown? Equation De- composition (55%)</p><p>The average weight of A, B and C is 45 kg. If the aver- age weight of A and B is 40 kg and that of B and C is 43 kg, the weight of B is? cantly better than two state-of-the-art systems.</p><p>Based on our error analysis, we plan to improve our model by detecting quantity types more accu- rately, learning relations and incorporating com- monsense knowledge. For long tail problems with a template size less 2, we want to utilize the fine- grained expressions we have learned and decom- pose equations for learning. Then we can reason with small equation units to generate a final equa- tion in testing. We would like to leverage seman- tic parsing and transform math problems to a more structured representation. Additionally, we plan to apply our findings to generating math problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Math Word Problem Examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Template Sketch Construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Features for alignment ranking.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 shows</head><label>3</label><figDesc>the overall performance of differ- ent systems. In the table, the size of a template is the number of problems corresponding to a tem- plate. For example, for templates with a size 100 or larger, their problem counts add up to 1,807.</figDesc><table>Template problems ZDC SIM Ours 
Size 
(%) 
(%) (%) 
&gt;=100 
1807 
34.2 29.7 64.5 
&gt;=50 
4281 
31.1 27.2 39.3 
&gt;=20 
5392 
29.4 25.8 36.9 
&gt;=10 
6216 
25.3 24.6 35.7 
&gt;=6 
6827 
21.7 20.2 34.6 
&gt;=5 
7081 
21.6 20.1 34.3 
&gt;=4 
7262 
21.1 19.8 33.8 
&gt;=3 
7466 
20.7 19.7 33.2 
&gt;=2 
8229 
20.6 20.3 32.2 
&gt;=1 
10644 
17.9 18.4 28.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Overall evaluation results.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy Per Template. Template retrieval acc reports percent of templates appears in one of 
the top 3 templates returned by our method. 

Hit@N 
1 
2 
3 
4 
5 
10 
20 
50 
ALL 
Template retrieval 17.5 22.4 26.3 27.2 28.0 30.2 32.7 35.2 47.1 
Acc (%) 
Final Acc (%) 
24.9 27.6 28.4 27.9 27.4 25.3 22.3 22.1 20.1 
ZDC (%) 
19.5 20.1 20.1 19.9 19.8 19.1 18.9 18.6 17.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results of template retrieval and final accuracy with different top N templates retrieved. 

Model 
Hit 
Hit 
Hit 
@1 @3 @10 
(%) (%) (%) 
FULL 
17.5 26.3 30.2 
-TEXTUAL 
14.1 24.7 28.4 
-QUANTITY 11.4 23.4 25.9 
-QUESTION 16.9 25.4 29.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 8 : Error Categorization.</head><label>8</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We ignore KAZB because it does not complete running on the dataset in three days</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work is supported by the National Nat-ural Science Foundation of China (61472453, U1401256, U1501252, U1611264). Thanks to the anonymous reviewers for their helpful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An integrated model of skill in solving elementary word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Briars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><forename type="middle">H</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Instruction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="296" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Carps, a program which solves calculus word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computer solution of calculus word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Joint Conference on Artificial Intelligence</title>
		<meeting>the 1st International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="page" from="303" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computer simulation of children&apos;s arithmetic word-problem solving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denise</forename><surname>Dellarosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="154" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding and solving arithmetic word problems: A computer simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="571" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large Margin Rank Boundaries for Ordinal Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Obermayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How well do computers solve math word problems? large-scale dataset construction and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing algebraic word problems into equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siena Dumas</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="585" to="597" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling math word problems with augmented semantic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Liguda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thies</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Information Systems. International Conference on Applications of Natural Language to Information Systems (NLDB-2012)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="247" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to use formulas to solve simple arithmetic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review of methods for automatic understanding of natural language mathematical problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utpal</forename><surname>Garain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="122" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1743" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving geometry problems: Combining text and diagram interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clint</forename><surname>Malcolm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically solving number word problems by semantic parsing and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from explicit and implicit supervision jointly for algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Frame-based calculus of solving arithmetic multistep addition and subtraction word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ma</forename><surname>Yuhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Guangzuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename><surname>Ronghuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Education Technology and Computer Science, International Workshop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="476" to="479" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learn to solve algebra word problems using quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
