<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Attentive Sentence Ordering Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baiyun</forename><surname>Cui</surname></persName>
							<email>baiyunc@yahoo.com,{yingming,funkyblack,zhongfei}@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Attentive Sentence Ordering Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4340" to="4349"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4340</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a novel deep attentive sentence ordering network (referred as ATTOrderNet) which integrates self-attention mechanism with LSTMs in the encoding of input sentences. It enables us to capture global dependencies among sentences regardless of their input order and obtains a reliable representation of the sentence set. With this representation, a pointer network is exploited to generate an ordered sequence. The proposed model is evaluated on Sentence Ordering and Order Discrimination tasks. The extensive experimental results demonstrate its effectiveness and superiority to the state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling a coherent text is one of the key prob- lems in natural language processing. A well- organized text with a logical structure is much easier for people to read and understand. Sen- tence ordering task ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref> has been proposed to cope with this problem. It aims to organize a set of sentences into a coherent text with a logically consistent order and has wide applications in natural language generation such as concept-to-text generation <ref type="bibr">Lapata, 2012a,b, 2013)</ref>, retrieval-based question an- swering ( <ref type="bibr" target="#b29">Yu et al., 2018;</ref><ref type="bibr" target="#b26">Verberne, 2011)</ref>, and ex- tractive multi-document summarization ( <ref type="bibr" target="#b1">Barzilay and Elhadad, 2002;</ref><ref type="bibr" target="#b7">Galanis et al., 2012;</ref><ref type="bibr" target="#b20">Nallapati et al., 2017)</ref>, where the improper ordering of sentences would introduce ambiguity and degrade readability. An example of this task is shown in <ref type="table">Table 1</ref>.</p><p>Traditional methods developed for this task em- ploy handcrafted linguistic features to model the document structure such as Entity Grid <ref type="bibr" target="#b2">(Barzilay and Lapata, 2008)</ref>, Content Model (Barzilay <ref type="table">Table 1</ref>: An example to illustrate the sentence ordering task. The set of sentences is confusing in its current order. We aim to reorganize them with a more coherent order.</p><p>and <ref type="bibr" target="#b3">Lee, 2004</ref>), and Probabilistic Model <ref type="bibr" target="#b15">(Lapata, 2003</ref>). However, manual feature engineer- ing heavily relies on linguistic knowledge and also limits these systems to be domain specific. Inspired by the success of deep learning, data- driven approaches based on neural networks have been proposed including Pairwise Ranking Model ( ) which learns the relative order of sentence pairs to predict the pairwise ordering of sentences, and Window network ( <ref type="bibr" target="#b16">Li and Hovy, 2014</ref>) sliding a window over the text to evaluate the coherence.</p><p>Recently, hierarchical RNN-based approaches ( <ref type="bibr" target="#b8">Gong et al., 2016;</ref><ref type="bibr" target="#b18">Logeswaran et al., 2018)</ref> have been proposed to deal with this task. Such meth- ods exploit LSTMs based paragraph encoder to compute a context representation for the whole se- quential sentences and then adopt a pointer net- work ( <ref type="bibr" target="#b27">Vinyals et al., 2015)</ref> as the decoder to pre- dict their order. However, since LSTM works sequentially, paragraph encoder only based on LSTMs suffers from the incorrect input sentence order and has difficulty in capturing a logically re- liable representation through the recurrent connec- tions, which makes trouble for the decoder to find the correct order.</p><p>To overcome the above limitation, in this work, we develop a novel deep attentive sentence order- ing network (referred as ATTOrderNet) by inte- grating self-attention mechanism ( <ref type="bibr" target="#b25">Vaswani et al., 2017</ref>) with LSTMs to learn a relatively reliable paragraph representation for subsequent sentence ordering. In particular, the bidirectional LSTM is first adopted as a sentence encoder to map the in- put sentences to the corresponding distributed vec- tors, and then a self-attention based paragraph en- coder is introduced to capture structural relation- ships across sentences and to obtain a hierarchi- cal context representation of the entire set of sen- tences. Consequently, based on the learned para- graph vector, a pointer network is applied to per- form sentence ordering by decoding an ordered se- quence. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of AT- TOrderNet, where self-attention mechanism is in- troduced to capture the dependencies among sen- tences.</p><p>In contrast to the previous paragraph encoders with LSTMs, self-attention mechanism is less sen- sitive to the input order of sentence sequence and is effective in modeling the accurate relationships across sentences, which reduces the influence of the original order of the input sentences and per- fectly meets the requirement of our task. Further, unlike Transformer ( <ref type="bibr" target="#b25">Vaswani et al., 2017</ref>), we do not add any positional encodings in our model to minimize the influence of the unclear order infor- mation.</p><p>Extensive evaluations are conducted on the sen- tence ordering task and order discrimination task to investigate the performances of ATTOrderNet. The experimental results on seven public sentence ordering datasets show the superior performances of the framework to the competing models. Mean- while, the visualization of the attention layer in paragraph encoder is provided for better under- standing of the effectivity of self-attention mecha- nism. Besides, in the Order Discrimination task, our model also achieves the state-of-the-art per- formance with remarkable improvements on two benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Attentive Sentence Ordering Network</head><p>In this section, we first formulate the sentence ordering problem and then describe the pro- posed model ATTOrderNet, which is based on the encoder-decoder architecture applying self- attention mechanism as paragraph encoder and a pointer network as the decoder. This combination effectively captures the intrinsic relations across a set of sentences with the desirable property of be- ing invariant to the sentence order, which directly helps address the difficulty of this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem formulation</head><p>The sentence ordering task aims to order a set of sentences as a coherent text. Specifically, a set of n sentences with the order</p><formula xml:id="formula_0">o = [o 1 , o 2 , · · · , o n ] can be described as s = [s o 1 , s o 2 , · · · , s o n ].</formula><p>The goal is to find the correct order o * for them,</p><formula xml:id="formula_1">o * = [o * 1 , o * 2 , · · · , o * n ]</formula><p>, with which the whole sentences have the highest coherence probability:</p><formula xml:id="formula_2">P(o * |s) &gt; P(o|s), ∀o ∈ ψ (1)</formula><p>where o indicates any order of these sentences and ψ denotes the set of all possible orders. For in- stance, in <ref type="table">Table 1</ref>, the current order o is <ref type="bibr">[4,</ref><ref type="bibr">1,</ref><ref type="bibr">3,</ref><ref type="bibr">2]</ref> and o * = [1, 2, 3, 4] is the correct order for these sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intuition and Model architecture</head><p>Given a set of sentences, the existing hierarchical RNN-based models first transform each sentence into a distributed vector with a sentence encoder and then these sentence embeddings are fed to a LSTMs-based paragraph encoder. Consequently, based on the learned paragraph vector, a pointer network is exploited to decode the order of the in- put sentences. However, since LSTM works se- quentially while the order of these sentences is un- known and quite possibly wrong in this problem, LSTMs-based paragraph encoder has difficulty in capturing a convincing representation through the recurrent connections, which influences the per- formance of sentence ordering.</p><p>In this paper, we use self-attention mechanism for paragraph encoder instead. In particular, we employ this mechanism without encoding any po- sitional information of the sentences. Ignoring the current order of the sentences, self-attention based paragraph encoder perfectly meets the re- quirement of the sentence ordering task and learns a logically reliable representation of the whole paragraph by globally capturing the relationships across sentences. With this learned representation vector, a decoder is then designed to generate a coherent order assignment for the input sentences.</p><p>In the following, we elaborate on the main building blocks of our ATTOrderNet in details: a sentence encoder, a self-attention mechanism based paragraph encoder, and a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Encoder</head><p>For a sentence, we first apply word embedding matrix to translate the raw words in the sentence into distributional representations, and then adopt bidirectional LSTMs to learn a sentence-level rep- resentation for summarizing its high level seman- tic concepts.</p><p>Specifically, assume that a sentence s o i contain- ing n w raw words as s o i = [w 1 , · · · , w n w ], these words are transformed to dense vectors through a word embedding matrix W e :</p><formula xml:id="formula_3">x t = W e w t , t ∈ [1, n w ]. The sequence of vectors [x 1 , · · · , x n w ]</formula><p>is then fed into bidirectional LSTMs sequentially to compute a semantic representation of the sentence.</p><p>Long Short-term Memory networks (LSTMs) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) is capable of learning long-term dependencies and alleviat- ing the problems of gradient vanishment and ex- ploding. Here, we adopt bidirectional LSTMs (Bi-LSTMs) to take full advantages of additional backward information and enhance the memory capability. In particular, the Bi-LSTMs contain the forward LSTMs which process the sentence s o i from w 1 to w n w and backward LSTMs which read s o i in the reversed direction:</p><formula xml:id="formula_4">− → h t , − → c t = LSTM( − → h t−1 , − → c t−1 , x t ) ← − h t , ← − c t = LSTM( ← − h t+1 , ← − c t+1 , x t ) h t = [ − → h t , ← − h t ]<label>(2)</label></formula><p>where h t denotes the representation of position t by concatenating the forward hidden state − → h t and backward hidden state ← − h t together. The out- put of the last hidden state of the Bi-LSTMs is taken to be the sentence representation vector as s o i = h n w , which incorporates the contextual in- formation from both directions in the sentence.</p><p>So far, we have obtained a syntactic and seman- tic representation for a single sentence. In the fol- lowing, a self-attention based paragraph encoder is proposed to obtain a high level representation for all given sentences by capturing sequential struc- tures and logical relationships among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Paragraph Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Self-attention mechanism</head><p>We start by introducing the scaled dot-product at- tention, which is the foundation of self-attention mechanism used in ATTOrderNet. Given a matrix of n query vectors Q ∈ R n×d , keys K ∈ R n×d , and values V ∈ R n×d , the scaled dot-product attention computes the output matrix as:</p><formula xml:id="formula_5">Attention(Q, K, V) = softmax( QK T √ d )V (3)</formula><p>The multi-head attention with h parallel heads is employed, where each head is an independent scaled dot-product attention. The mathematical formulation is shown below:</p><formula xml:id="formula_6">M i = Attention(QW Q i , KW K i , VW V i ) (4) MH(Q, K, V) = Concat(M 1 , · · · , M h )W (5)</formula><p>where</p><formula xml:id="formula_7">W Q i , W K i , W V i ∈ R d×d a with d a = d/h</formula><p>are the projection matrices for the i-th head and W ∈ R hd a ×d .</p><p>Self-attention ( <ref type="bibr" target="#b25">Vaswani et al., 2017;</ref><ref type="bibr" target="#b24">Tan et al., 2017;</ref><ref type="bibr" target="#b22">Shen et al., 2017</ref>) is a special case of attention mechanism that only requires a sin- gle sequence to compute its representation where queries, keys, and values are all from the same place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Self-attention based Paragraph Encoder</head><p>The paragraph encoder is composed of multiple self-attention layers followed by an average pool- ing layer. Sentence vectors encoded by the sentence en- coder are first packed together into a paragraph</p><formula xml:id="formula_8">matrix S = [s o 1 , s o 2 , · · · , s o n ]</formula><p>as E 0 . This para- graph matrix S ∈ R n×d is then fed forward to L self-attention layers, where each layer learns a representation E l+1 = U(E l ) by taking the output from the previous layer l:</p><formula xml:id="formula_9">U(E l ) = Φ(FN(D(E l )), D(E l )) (6) D(E l ) = Φ(MH(E l , E l , E l ), E l ) (7) Φ(v, w) = LayerNorm(v + w) (8) FN(x) = ReLU(xW l 1 + b l 1 )W l 2 + b l 2 (9)</formula><p>where Φ(·) performs layer normalization ( <ref type="bibr">Ba et al., 2016</ref>) on the residual output to preserve the auto- regressive property, and FN(·) represents the fully connected feed-forward networks which consists of two linear layers with ReLU nonlinearity in the middle.</p><formula xml:id="formula_10">W l 1 ∈ R d×d f , b l 1 ∈ R d f , W l 2 ∈ R d f ×d , and b l 2 ∈ R d are trainable parameters. We set d f = 1024 in all our experiments.</formula><p>Self-attention mechanism adopted in the para- graph encoder directly relate sentences at different positions from the text by computing the attention score (relevance) between each pair of sentences. This allows each sentence to build links with all other sentences in the text, which enables the en- coder to exploit latent dependency relationships among sentences without regarding to their input order. Then, attention mechanism uses weighted sum operation to establish a higher level repre- sentation for the entire sentence set. As we see, there is no order information used in the encoding process which prevents the model from being af- fected by the incorrect sentence order. Therefore, self-attention based paragraph encoder is efficient in modeling of dependencies while being invariant to the sentence order.</p><p>The final paragraph representation v is obtained in the average pooling layer by averaging the out- put matrix E L ∈ R n×d from the last self-attention layer: v = 1 n n i=1 e L i , where n is the number of sentences and e L i denotes the i-th row in E L . This learned representation vector can be viewed as a hierarchical encoding of the entire set of sentences which will then be used as the input of the decoder to perform sentence ordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Decoder</head><p>The aim of decoder is to predict a consistent order for the input set of sentences.</p><p>Following the previous approaches ( <ref type="bibr" target="#b8">Gong et al., 2016;</ref><ref type="bibr" target="#b18">Logeswaran et al., 2018)</ref>, the coherence probability of given sentences s with the order o is formalized as:</p><formula xml:id="formula_11">P(o|s) = n i=1 P(o i |o i−1 , · · · , o 1 , s)<label>(10)</label></formula><p>The higher the probability, the more coherent sen- tences assignment is. To calculate P(o|s), we employ the pointer net- work architecture ( <ref type="bibr" target="#b27">Vinyals et al., 2015)</ref> as our de- coder which consists of LSTMs cells <ref type="bibr">(Equation 11-13)</ref>. The LSTM takes the embedding of the previous sentence as the input to decoder step. During training, the correct order of sentences o * is known, so the input sequence</p><formula xml:id="formula_12">[x 1 , x 2 , · · · , x n ] = [s o * 1 , s o * 2 , · · · , s o * n ]</formula><p>. For step i, the input to the de- coder is</p><formula xml:id="formula_13">x i−1 = s o * i−1</formula><p>. At test time, the predicted sentence assignment s ˆ o i−1 is used instead. The initial state of the decoder LSTM is initial- ized with the final paragraph vector from the en- coder: h 0 = v T . And the input at the first step in decoder x 0 ∈ R d is a vector of zeros. The mathe- matical formulation for the i-th step in decoder is as follows:</p><formula xml:id="formula_14">h i , c i = LSTM(h i−1 , c i−1 , x i−1 )<label>(11)</label></formula><formula xml:id="formula_15">u i j = g T tanh(W 1 s o j + W 2 h i )<label>(12)</label></formula><formula xml:id="formula_16">P(o i |o i−1 , · · · , o 1 , s) = softmax(u i ) (13)</formula><p>where g ∈ R d , W 1 ∈ R d×d , and W 2 ∈ R d×d are learnable parameters and j ∈ (1, · · · , n). The softmax function normalizes the vector u i ∈ R n to produce an output distribution over all input sentences. And P(o i |o i−1 , · · · , o 1 , s) can be inter- preted as the coherence probability for the cur- rent output sequence when s o i being the sentence choice at position i conditioned on the previous sentences assignment.</p><formula xml:id="formula_17">Order Prediction: The predicted orderôorderˆorderô = [ ˆ o 1 , ˆ o 2 , · · · , ˆ o n ]</formula><p>is the one with the highest coher- ence probability:</p><formula xml:id="formula_18">ˆ o = argmax o P(o|s)<label>(14)</label></formula><p>In this work, we use beam search strategy to find a sub optimal result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Training</head><p>For each ordered document, we use one random permutation of sentences as the input sample at each epoch during the training and testing process. Assume that there are K documents in the training set. We define (q j , y j ) K j=1 , where y j is in the cor- rect order o * of original document j and q j denotes the set of sentences with a specific permutation of y j . P(y j |q j ) = P(o * |s = q j ) can be interpreted as the probability that sentences are assigned in the correct order when given sentences q j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Length <ref type="table" target="#tab_3">statistics  Data split  Vocabulary  min  mean  max  train  valid  test  Accident  6  11.5  19  100  - 100  4501  Earthquake  3  10.4  32  100  - 99  3022  NIPS abstract  2  6  15  2248  409  402  16721  AAN abstract  1  5  20  8569  962  2626  34485  NSF abstract  2  8.9  40  96070  10185  21580  334090  arXiv abstract  2  5.38  35  884912  110614  110615  64557  SIND caption  5  5  5  40155  4990  5055  30861   Table 2</ref>: Statistics of seven datasets used in our experiments.</p><p>We aim to train the overall model to maximize this probability by minimizing the loss function:</p><formula xml:id="formula_19">L = − 1 K K j=1 log P(y j |q j ; θ) + λ 2 ||θ|| 2 2<label>(15)</label></formula><p>where θ represents all trainable parameters in the networks and λ is a regularization parameter.  <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training setup</head><p>We use pre-trained 100 dimensional GloVe word embeddings ( <ref type="bibr" target="#b21">Pennington et al., 2014)</ref>. And all the out-of-vocabulary words are replaced with &lt;UNK&gt;, whose embeddings are updated during training process. The nltk sentence tokenizer is used for word tokenization. 1 Parameter optimiza- tion is performed using stochastic gradient de- scent. We adopt Adadelta <ref type="bibr" target="#b30">(Zeiler, 2012)</ref> as the op- timizer with = 10 6 and ρ = 0.95. The learning rate is initialized to 1.0, the batch size is 16, and the beam size is set to 64. The hidden layer size of LSTMs in sentence encoder is 256, and is 512 in the decoder. The number of attention layers in the paragraph encoder is 6 for AAN abstract, 4 for NSF abstract and arXiv abstract, and 2 for the rest of datasets. We employ 8 parallel heads through- out all self-attention layers and use L2 weight de- cay on the trainable variables with regularization parameter λ = 10 −5 . The model is implemented with TensorFlow 2 . Hyperparameters are chosen using the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Ordering</head><p>We first evaluate our model on the sentence or- dering task, as proposed by <ref type="bibr" target="#b2">Barzilay and Lapata (2008)</ref>. Given a set of permuted sentences, our goal is to return the original order for them which is considered to be the most coherent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Baselines</head><p>We compare ATTOrderNet against a random base- line and all the competing models. These baseline methods can be categorized into three classes and results are reported in <ref type="bibr" target="#b23">(Soricut and Marcu, 2006;</ref><ref type="bibr" target="#b8">Gong et al., 2016;</ref><ref type="bibr" target="#b18">Logeswaran et al., 2018</ref>    <ref type="bibr">2016</ref>). These architectures adopt RNN based ap- proaches to obtain the representation for the input set of sentences and employ the pointer network as the decoder to predict order. The main differ- ence between ATTOrderNet and these models lies in the design of paragraph encoder. For thorough comparison, besides the models proposed in the existing literature, we further im- plement two variants of ATTOrderNet. ATTOrderNet (ATT): The sentence encoder in this model is also entirely based on self-attention mechanism with 4 self-attention layers and 5 heads. Different from the paragraph encoder, the positional encoding method proposed by <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref> is applied here to encode temporal in- formation of each input word. ATTOrderNet (CNN): This model employs con- volutional neural networks to model sentences. In experiment, the number of feature maps is set to 512 and the width of convolution filter is 4.</p><formula xml:id="formula_20">- - - - - - - - - - Content Model 0.44 0.81 - - - - - - - - - - Utility-Train Model 0.50 0.47 - - - - - - - - - - Entity Grid</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Evaluation Metrics</head><p>To provide assessments on the quality of the order- ings we predict in this task, we use the following three metrics: Kendall's tau (τ): Kendall's tau is one of the most frequently used metrics for the automatic evalu- ation of document coherence <ref type="bibr" target="#b15">(Lapata, 2003;</ref><ref type="bibr" target="#b18">Logeswaran et al., 2018;</ref><ref type="bibr" target="#b17">Li and Jurafsky, 2017)</ref>. It could be formalized as:</p><formula xml:id="formula_21">τ = 1 − 2× (number of inversions) / n 2</formula><p>, where n is the length of the se- quence and the number of inversions denotes the number of pairs in the predicted sequence with in- correct relative order. This metric ranges from -1 (the worst) to 1 (the best). Accuracy (Acc): We follow ( <ref type="bibr" target="#b18">Logeswaran et al., 2018</ref>) in employing Accuracy to measure how of- ten the absolute position of a sentence was cor- rectly predicted. Compared to τ, it penalizes cor- rectly predicted subsequences that are shifted. Perfect Match Ratio (PMR): Perfect match ra- tio ( <ref type="bibr" target="#b8">Gong et al., 2016</ref>) is the most stringent mea- surement in this task. It calculates the radio of ex- actly matching orders: PMR= 1</p><formula xml:id="formula_22">K K i=1 1( o i = o i * ),</formula><p>where o i and o i * are predicted and correct orders of the i-th text respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Results</head><p>The experimental results on all datasets are re- ported in <ref type="table" target="#tab_3">Table 3</ref>. Results show that ATTOrderNet gives the best performance across most datasets and under most evaluation measurements.</p><p>The improvement is regardless of data sizes. In particular, for smaller datasets such as Acci- dent and Earthquake datasets, ATTOrderNet out- performs the previous best baseline methods by 6% and 7% tau score respectively. As for medium size datasets including NIPS abstract and AAN abstract, ATTOrderNet shows absolute improve- ments of 4.54% and 5.03% accuracy score over the previous state-of-the-art. Such finding is con- sistent across larger datasets. ATTOrderNet out- performs the previous state-of-the-art systems by 4.50% accuracy score with 3% tau score on NSF abstract, 1.75% PMR score with 1% tau score on arXiv abstract, and 1.67% PMR score with 1% tau score on SIND caption. Interestingly, ATTOrder- Net reaches 42.19% PMR score on arXiv abstract, which means that more than 2/5 texts in the test set can be ordered exactly right. This performance clearly demonstrates the adaptability and flexibil- ity of the proposed model. As shown in <ref type="table" target="#tab_3">Table 3</ref>, ATTOrderNet performs much better than data-driven methods by a sig- nificant margin on all corresponding datasets. It proves the importance of exploiting the context by self-attention mechanism as these competing mod- els only consider the local coherence in the text. Among the traditional ordering approaches, Con- tent Model ( <ref type="bibr" target="#b3">Barzilay and Lee, 2004</ref>) representing topics as states and capturing possible orderings for global coherence performs better than other methods with the tau score of 0.81 on Earthquake dataset, which also demonstrates that global con- text is important to sentence ordering. However, Content Model requires manual feature engineer- ing that costs great human efforts. In contrast, the self-attention mechanism used in ATTOrder- Net directly captures the global dependences for the whole text while requiring no linguistic knowl- edge anymore and enables ATTOrderNet to fur- ther improve tau score to 0.92 on the same dataset.</p><p>In addition, hierarchical RNN-based models capture the global coherence among sentences with LSTMs and outperform the traditional meth- ods and data-driven approaches in most cases. However, these models still suffer from the per- mutation of sentences within the document since LSTM works sequentially. ATTOrderNet achieves superior performances to them by adopting the self-attention mechanism to reduce the influence of the permutation of sentences.</p><p>Further, ATTOrderNet (CNN) has better per- formances than ATTOrderNet (ATT) on most of the datasets. We conjecture that this is due to the limitation of data size. Since ATTOrderNet (ATT) applies self-attention mechanism in both sentence and paragraph encoders requiring more data to train the model, however the size of the  datasets used in this task is smaller than those in other tasks such as document classification <ref type="bibr" target="#b28">(Yang et al., 2016)</ref>. Given larger datasets in the future, we believe ATTOrderNet (ATT) would perform much better. Among three sentence encoders, AT- TOrderNet presents a superior performance across the board. This indicates that LSTM is more effi- cient in learning semantic representation for sen- tence level in this task. ATTOrderNet becomes more competitive through combining both advan- tages of LSTMs and self-attention mechanism.</p><p>Since the first and the last sentences of the text are more special to discern <ref type="bibr" target="#b8">Gong et al., 2016)</ref>, we also evaluate the ratio of correctly predicting the first and the last sentences. <ref type="table" target="#tab_5">Table 4</ref> summarizes our performances on arXiv abstract and SIND caption. As we see, all models show fair well in predicting the first sentence, and the prediction accuracy declines for the last one. It is observed that ATTOrderNet still achieves a boost in predicting two positions compared to the previous state-of-the-art system on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Visualization of attention</head><p>The aim of this section is to visualize the relation- ship between sentences captured by self-attention mechanism and understand how it helps perform the sentence ordering task. A technique for vi- sualizing attention mechanism in neural networks is proposed by <ref type="bibr" target="#b25">Vaswani et al. (2017)</ref>  <ref type="bibr">3</ref> . Inspired by this work, we select a text from AAN abstract dataset to visualize the hierarchical attention layer from the paragraph encoder of ATTOrderNet in <ref type="figure" target="#fig_1">Figure 2</ref>. Different from visualizing the depen- dencies of one word with the other words in the sentence ( <ref type="bibr" target="#b25">Vaswani et al., 2017</ref>), our visualization shows the dependencies between each sentence and all other sentences in the text.</p><p>For the example in <ref type="figure" target="#fig_1">Figure 2</ref>, the left text is the input sample to our model which contains a set of permuted sentences with the correct order be- sides it. The right side is a copy of the input text, which is presented for showing the relevance be- tween each pair of sentences more clearly. The line with grey color on the right text is an example sentence chosen to visualize the attention weights with other sentences. On the top of the text are 8 colored squares representing 8 different atten- tion heads used in the paragraph encoder. Colored columns on the left text show the performance of their corresponding heads. The darkness of the color in column denotes the normalized distribu- tion of the attention weight for the example sen- tence in the head. Sentences in darker shades show more attention weight which reflects stronger links they have with the example sentence.</p><p>In particular, <ref type="figure" target="#fig_1">Figure 2</ref> shows the attention dis- tribution for the first sentence in the original doc- ument. We present the weight distribution in four heads as an instance. It is interesting to see that all of them showing significant higher atten- tion weights on the true second sentence "How- ever, text use changes ..." than other sentences in the text. This indicates that these heads are able to learn the latent dependency relationships from sentences and can successfully distinguish which one is the true next following among all sentence candidates. These heads build much stronger links between this sentence with the chosen one in order to keep structural information for higher level rep- resentation, such as paragraph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Order Discrimination</head><p>In this section, we assess ATTOrderNet on an- other common evaluation task which is usually adopted in the existing literature: order discrim- ination task.</p><p>Order discrimination ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008;</ref><ref type="bibr">Charniak, 2011, 2008</ref>) aims to compare a document to a randomly permuted ver- sion of it. Models are evaluated with Pairwise Ac- curacy: the ratio of correctly identifying the orig- inal document with higher coherence probability (defined in Equation 10) than the probability of its permutation.</p><p>Among seven datasets mentioned above, we use two of them to assess the performance of ATTOrderNet on the order discrimination task: Accident and Earthquake datasets. These two have been widely used for this task in the pre- vious literature ( <ref type="bibr" target="#b16">Li and Hovy, 2014;</ref><ref type="bibr" target="#b18">Logeswaran et al., 2018)</ref>. This gives us the convenience of di- rectly comparing the result of the proposed model against the reported results. Following the setup in ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, a maximum of 20 random permutations were generated for each training and testing article to create the pairwise data. There are 1986 and 1956 test pairs in Acci- dent and Earthquake datasets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Baselines</head><p>To demonstrate that ATTOrderNet truly improves the order discrimination performance, we com- pare ATTOrderNet with the following representa- tive models: Graph from ( <ref type="bibr" target="#b9">Guinaudeau and Strube, 2013)</ref>, HMM and HMM+Entity from (Louis and Nenkova, 2012), Entity Grid from ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref>, Recurrent and Recursive from ( <ref type="bibr" target="#b16">Li and Hovy, 2014</ref>), Discriminative model from ( <ref type="bibr" target="#b17">Li and Jurafsky, 2017)</ref>, Varient-LSTM+PtrNet from <ref type="bibr" target="#b18">(Logeswaran et al., 2018</ref>), CNN+PtrNet and LSTM+PtrNet from <ref type="bibr" target="#b8">(Gong et al., 2016)</ref>. The re- sults of the last two methods were obtained by training their models on two datasets.  <ref type="table">Table 5</ref>: Experimental results of Pairwise Accuracy for different approaches on two datasets in the Order Dis- crimination task. <ref type="table">Table 5</ref> reports the results of ATTOrderNet and currently competing architectures in this evalua- tion task. ATTOrderNet also achieves the state- of-the-art performance, showing a remarkable ad- vancement of about 1.8% gain on Accident dataset and further improving the pairwise accuracy to 99.8 on Earthquake dataset. LSTM+PtrNet and CNN+ PtrNet ( <ref type="bibr" target="#b8">Gong et al., 2016</ref>) fall short of Varient-LSTM+PtrNet ( <ref type="bibr" target="#b18">Logeswaran et al., 2018</ref>) in performance. This could also be blamed for their paragraph encoder. Docu- ments in both datasets are much longer than those in others, which brings more trouble for LSTMs in paragraph encoder to build logical representations. Compared to the result in the sentence order- ing task, Entity Grid ( <ref type="bibr" target="#b2">Barzilay and Lapata, 2008)</ref> achieves a good performance in this task and even outperforms Recurrent neural networks and Re- cursive neural networks ( <ref type="bibr" target="#b16">Li and Hovy, 2014</ref>) on Accident dataset. However, Entity Grid requires hand-engineered features and heavily relies on lin- guistic knowledge which restrain the model to be adapted to other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we develop a novel deep attentive sentence ordering model (referred as ATTOrder- Net) integrating self-attention mechanism with LSTMs. It enables us to directly capture logical relationships among sentences regardless of their input order and obtain a reliable representation of the sentence set. With this representation, a pointer network is applied to generate an ordered sequence. ATTOrderNet is evaluated on Sentence Ordering and Order Discrimination tasks. The experimental results demonstrate its effectiveness and show promising improvements over existing models across most datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of deep attentive Sentence Ordering Network.</figDesc><graphic url="image-1.png" coords="2,72.00,62.81,218.27,162.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the attention mechanism describing sentence dependencies in the encoder self-attention in layer 5 of 6 for the text from AAN abstract. Four attention heads attend to a dependency of the selected sentence "Temporal variations of text are ...", providing important clues for the correct order prediction. Attention here shown is only for this sentence. Different color indicate different heads. Best viewed in color.</figDesc><graphic url="image-2.png" coords="8,72.00,62.81,453.55,86.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Experimental results for different methods on the Sentence Ordering task. 

2017); Pairwise Ranking Model (Chen et al., 
2016). These three approaches capture the local 
coherence of text based on the neural networks. 
(3) Hierarchical RNN-based models: Varient-
LSTM+PtrNet, RNN Decoder (Logeswaran et al., 
2018); CNN+PtrNet, LSTM+PtrNet (Gong et al., 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The performance of correctly predicting the 
first and the last sentences on arXiv abstract and SIND 
caption datasets. 

</table></figure>

			<note place="foot" n="1"> NLTK implementation: http://www.nltk.org/ 2 https://www.tensorflow.org/</note>

			<note place="foot" n="3"> https://github.com/tensorflow/tensor2tensor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by National Natural Science Foundation of China (No. 61702448, 61672456) and the Fundamental Research Funds for the Central Universities <ref type="bibr">(No. 2017QNA5008, 2017FZA5007)</ref>. We thank all reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inferring strategies for sentence ordering in multidocument news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="35" to="55" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Catching the drift: Probabilistic content models, with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06952</idno>
		<title level="m">Neural sentence ordering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extending the entity grid with entity-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extractive multi-document summarization with integer linear programming and support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<biblScope unit="page" from="911" to="926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Gerasimos Lampouras, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end neural sentence ordering using pointer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04953</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based local coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Guinaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao Kenneth</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual storytelling. In NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1233" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conceptto-text generation via discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised concept-to-text generation with hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="752" to="761" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inducing document plans for concept-to-text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1503" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic text structuring: Experiments with sentence ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2039" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural net models of open-domain discourse coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sentence ordering and coherence modeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCONLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Summarunner: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Disan: Directional self-attention network for rnn/cnnfree language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discourse generation using utility-trained coherence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="803" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep semantic role labeling with self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yidong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01586</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Retrieval-based question answering for machine reading evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF (Notebook Papers/Labs/Workshop)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
