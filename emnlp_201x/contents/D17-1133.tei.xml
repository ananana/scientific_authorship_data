<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Contextually Informed Representations for Linear-Time Discourse Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<email>Yang.Liu2@ed.ac.uk,mlap@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Language, Cognition and Computation School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Contextually Informed Representations for Linear-Time Discourse Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1289" to="1298"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent advances in RST discourse parsing have focused on two modeling paradigms: (a) high order parsers which jointly predict the tree structure of the discourse and the relations it encodes; or (b) linear-time parsers which are efficient but mostly based on local features. In this work, we propose a linear-time parser with a novel way of representing discourse constituents based on neural networks which takes into account global contextual information and is able to capture long-distance dependencies. Experimental results show that our parser obtains state-of-the art performance on benchmark datasets, while being efficient (with time complexity linear in the number of sentences in the document) and requiring minimal feature engineering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The computational treatment of discourse phe- nomena has recently attracted much attention, due to their increasing importance for potential ap- plications. Knowing how text units can be com- posed into a coherent document and how they re- late to each other e.g., whether they express con- trast, cause, or elaboration, can usefully aid down- stream tasks such summarization ( <ref type="bibr" target="#b33">Yoshida et al., 2014</ref>), question answering ( <ref type="bibr" target="#b2">Chai and Jin, 2004)</ref>, and sentiment analysis <ref type="bibr" target="#b26">(Somasundaran, 2010)</ref>.</p><p>Rhetorical Structure Theory (RST, <ref type="bibr" target="#b19">Mann and Thompson 1988)</ref>, one of the most influential frameworks in discourse processing, represents texts by trees whose leaves correspond to Elemen- tary Discourse Units (EDUs) and whose nodes specify how these and larger units (e.g., multi- sentence segments) are linked to each other by rhetorical relations. Discourse units are further</p><p>The projections are in the neighborhood of 50 cents a share to 75 cent, compared with a restated $1.65 a share a year earlier, when profit was $107.8 million on sales of $435.5 million.</p><p>nucleus satellite <ref type="bibr">[Cray Research Inc. said]</ref> Figure 1: Example text (bottom) composed of two sentences (three EDUs) and its RST discourse tree representation (top).</p><p>characterized in terms of their importance in tex- t: nuclei denote central segments, whereas satel- lites denote peripheral ones. <ref type="figure">Figure 1</ref> shows an example of a discourse tree representing two sen- tences with three EDUs (e 1 , e 2 , and e 3 ). EDUs e 1 and e 2 are connected with a mononuclear relation (i.e., Consequence), where e 1 is the nucleus and e 2 the satellite (indicated by the left pointing ar- row in the <ref type="figure">figure)</ref>. Span e 1:2 is related to e 3 via List, a multi-nuclear relation, expressing the fact that both spans are equally important and there- fore both nucleus. Given such tree-based representations of dis- course structure, it is not surprising that RST-style document analysis is often viewed as a parsing task. State-of-the-art performance on RST parsing is achieved by cubic-time parsers <ref type="bibr" target="#b17">(Li, Li, and Hovy, 2014;</ref><ref type="bibr" target="#b18">Li, Li, and Chang, 2016)</ref>, with O(n 3 ) time complexity (where n denotes the number of sen-tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to gen- erate a global optimal tree. The high order com- plexity renders such parsers inefficient in prac- tice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed <ref type="bibr" target="#b7">(Feng and Hirst, 2014;</ref><ref type="bibr" target="#b13">Ji and Eisenstein, 2014</ref>) which make local de- cisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a smal- l window of discourse constituents) without con- sidering document-level information, which has been previously found useful in discourse analy- sis <ref type="bibr" target="#b6">(Feng and Hirst, 2012)</ref>.</p><p>In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we first parse each sentence in a document into a tree whose leaves correspond to EDUs, and then parse the document into a tree whose leaves correspond to already pre- processed sentences. The feature learning process for both stages is based on neural network model- s. At the sentence level, Long-Short Term Mem- ory Networks (LSTMs; Hochreiter and Schmid- huber 1997) learn representations for EDUs and larger constituents, whereas at the document level, LSTMs learn representations for entire sentences. Treating a sentence as a sequence of EDUs and a document as a sequence of sentences allows to incorporate important contextual information on both levels capturing long-distance dependencies.</p><p>Recurrent neural networks excel at modeling sequences, but cannot capture hierarchical struc- ture which is important when analyzing multi- sentential discourse. We therefore adopt a more structure-aware representation at the documen- t level which we argue is complementary to the flat representations obtained from the LSTM. We rep- resent documents as trees using recursive neural networks <ref type="bibr" target="#b25">(Socher et al., 2012)</ref>. Experimental eval- uation on the RST Treebank shows that our parser yields comparable performance to previous linear- time systems, without requiring extensive manu- al feature engineering and improves upon related neural models ( <ref type="bibr" target="#b17">Li et al., 2014</ref><ref type="bibr" target="#b18">Li et al., , 2016</ref>) on discourse relation classification, while being more efficient.</p><p>The rest of this paper is organized as follows.</p><p>We overview related work in the following sec- tion. We describe the general flow of our pars- er in Section 3 and provide details on our pars- ing algorithm and feature learning method in Sec- tion 4. Experimental results are reported in Sec- tion 5. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent advances in discourse modeling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT; <ref type="bibr" target="#b1">Carlson et al. 2003</ref>) and the Penn Discourse Treebank (PDTB, <ref type="bibr" target="#b23">Prasad et al. 2008)</ref>. In this work, we fo- cus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated most- ly between adjacent sentences and no global tree structure is provided.</p><p>Early approaches to discourse parsing <ref type="bibr" target="#b21">(Marcu, 2000;</ref><ref type="bibr" target="#b16">LeThanh et al., 2004</ref>) have primarily fo- cused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentence- level parsing. <ref type="bibr" target="#b0">Baldridge and Lascarides (2005)</ref> and <ref type="bibr" target="#b24">Sagae (2009)</ref> used probabilistic head-driven parsing techniques. <ref type="bibr" target="#b28">Subba and Di Eugenio (2009)</ref> were the first to incorporate rich compositional semantics into sentence-and document-level dis- course parsing.</p><p>HILDA <ref type="bibr" target="#b11">(Hernault et al., 2010</ref>) has been one of the most influential document-level dis- course parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vec- tor machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM predicts their discourse rela- tion. Subsequent work <ref type="bibr" target="#b7">(Feng and Hirst, 2014;</ref><ref type="bibr" target="#b14">Joty et al., 2013</ref>) has shown that two-stage systems are not only efficient but can also achieve competitive performance. CKY-based parsers which guarantee globally optimal results have also been developed ( <ref type="bibr" target="#b14">Joty et al., 2013;</ref><ref type="bibr" target="#b17">Li et al., 2014)</ref>. <ref type="bibr" target="#b13">Ji and Eisenstein (2014)</ref> were the first to ap- ply neural network models to RST discourse pars- ing; their shift-reduce parser uses a feedforward neural network to learn the representations of the transition stack and queue. <ref type="bibr" target="#b17">Li et al. (2014)</ref> pro- posed a CKY-based parser which uses recursive neural networks to learn representations for EDUs and their composition during the tree-building pro- cess. More recently, <ref type="bibr" target="#b18">Li et al. (2016)</ref> designed a CKY-based parser which uses LSTMs to model text spans and a tensor-based transformation to compose adjacent spans.</p><p>Our own work joins others <ref type="bibr" target="#b7">(Feng and Hirst, 2014;</ref><ref type="bibr" target="#b14">Joty et al., 2013</ref>) in adopting a two-stage architecture for our discourse parser. However, rather than considering each discourse constituen- t independently, we learn contextually-aware rep- resentations capturing long-range dependencies across sentences and documents. Discourse con- stituents at all levels are modeled with recurrent neural networks adopting a relatively simple, yet efficient, architecture compared to previously pro- posed neural systems ( <ref type="bibr" target="#b18">Li et al., 2016)</ref>. Finally, we experimentally assess whether sequence-based representations are expressive enough by compar- ing them to those obtained (with recursive neural networks) from structured inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discourse Parser Overview</head><p>In RST discourse parsing, a document is first seg- mented into EDUs and a parser then builds a dis- course tree with the EDUs as leaves. The first sub- task is considered relatively easy with state-of-art accuracy at above 90% ( <ref type="bibr" target="#b11">Hernault et al., 2010)</ref>. As a result, recent research focuses on the second sub- task and often uses manual EDU segmentation. <ref type="bibr" target="#b14">Joty et al. (2013)</ref> found that a two-stage pars- ing strategy, which separates intra-sentential from multi-sentential parsing, has some advantages for document-level discourse parsing, since the distri- bution of discourse relations and useful features are different in the two stages. Based on their findings, our model also follows a two-stage ap- proach and is composed by two components: an intra-sentential parser and a multi-sentential pars- er. Given a document pre-segmented into EDUs, our intra-sentential parser first builds a sentence- level discourse tree for individual sentences. Then, our multi-sentential parser creates a discourse tree for the entire document.</p><p>To guarantee linear-time complexity, both parsers adopt a greedy bottom-up tree-building process <ref type="bibr" target="#b11">(Hernault et al., 2010)</ref> and are based on two conditional random field (CRF) models, one for creating discourse structure and another one for assigning relations. The intra-sentential pars- er considers adjacent EDUs and decides whether they should be connected (based on the scores pre- dicted by the first CRF) and their relation (based on predictions of the second CRF). The multi- sentential parser follows the same procedure while operating over sentences.</p><p>The CRFs employ feature representations which we obtain using neural networks. Specifi- cally, discourse constituents at all levels are mod- eled with recurrent neural networks (see Sec- tion 4.2 for details). In addition, for inter-sentential constituents, we complement the flat text span rep- resentation with recursive neural networks (see Section 4.4). We argue that the combination is ad- vantageous; a sequential text span representation is in principle unsuitable for capturing hierarchi- cal discourse structure, whereas a tree-based rep- resentation can be more precise, albeit less robust (due to the accumulation of errors from the recur- sive tree-building process).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parsing Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intra-sentential Parser</head><p>To parse a sentence, we start with EDUs, which can be viewed as discourse constituents at the first level. As mentioned earlier, our intra- sentential parser is based on two linear-chain CRFs, the structure CRF decides which pair of constituents should be merged at the current lev- el and the relation CRF assigns discourse rela- tions to non-leaf constituents. For example, let C 1 = {e 1 , e 2 , · · · , e m } denote a sentence with a se- quence of EDUs, where e i is the i th EDU in the sentence. Suppose the structure CRF decides to merge together e 2 and e 3 , then the next-level se- quence is C 2 = {e 1 , e 2:3 , e 4 , · · · , e m } and the rela- tion CRF will assign a discourse relation to e 2:3 , the only non-leaf constituent so far. This process iterates until all EDUs are merged and a discourse subtree is generated for the entire sentence.</p><p>A linear-chain CRF for intra-sentential dis- course parsing is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Here, C = {c 0 , · · · , c t , · · · , c n } are observed discourse constituents and L = {l 1 , · · · , l t , · · · , l n } are hidden structure nodes; label l t ∈ {1, 0} denotes whether constituents c t and c t+1 should be connected. Our model differs from standard linear-chain CRFs in that the score between adjacent hidden nodes is not calculated based on a transition matrix, but is learned from observations instead. Specifically, given a constituent sequence C, the probability of forming the structure sequence Y str is written as:</p><formula xml:id="formula_0">p(Y str |C) = 1 Z n ∏ t=1 exp(u str t + b str t,t+1 )<label>(1)</label></formula><formula xml:id="formula_1">u str t = θ(c t , c t+1 , y t )<label>(2)</label></formula><formula xml:id="formula_2">b str t,t+1 = γ(c t−1 , c t , c t+1 , y t , y t+1 )<label>(3)</label></formula><p>where the u str t represents the unary potential s- core of structure node l t = y t , depending on con- stituents c t and c t+1 . The binary potential score b str t,t+1 for s t = y t , s t+1 = y t+1 is calculated based on c t−1 , c t and c t+1 . The binary potential pro- vides information for discriminating between s t = 0, s t+1 = 1 and s t = 1, s t+1 = 0. Also, we impose the constraint that one constituent can be merged with at most one other adjacent constituent. <ref type="figure" target="#fig_1">Figure 3</ref> depicts the relation CRF for intra- sentential parsing. Similar to the structure CRF, C = {c 0 , · · · , c t , · · · , c n } are observed constituents and R = {r 1 , · · · , r t , · · · , r n } hidden nodes, corre- sponding to discourse relations. The CRF will as- sign a relation to a non-leaf constituent c t based on its right and left children, c t,L and c t,R , respec- tively. If a constituent is a single EDU, we force its hidden node to be the special label LEAF, whereas hidden nodes for constituents which are the prod- uct of merging cannot be LEAF. Given a sequence of constituents C, the probability of the relation la- bel sequence Y rel can be written as:</p><formula xml:id="formula_3">p(Y rel |C) = 1 Z n ∏ t=1 exp(u rel t + b rel t,t+1 )<label>(4)</label></formula><formula xml:id="formula_4">u rel t = θ(c t , y t )<label>(5)</label></formula><formula xml:id="formula_5">b rel t,t+1 = γ(c t , c t+1 , y t , y t+1 )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intra-sentential Feature Learning</head><p>Instead of adopting a high order parsing model, we use neural networks to capture contextual in- formation and recover the meaning of discourse constituents. Aside from modeling long distance dependencies, our representation learning process , inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embed- dings of text spans, which has achieved compet- itive performance in both dependency and con- stituency parsing ( <ref type="bibr" target="#b32">Wang and Chang, 2016;</ref><ref type="bibr" target="#b3">Cross and Huang, 2016)</ref>. We describe below how we ex- tend this method which is based on subtraction be- tween LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of word embeddings [w w w sos , w w w 1 , · · · , w w w i , · · · , w w w n , w w w eos ] and insert a special embedding w E to indicate the boundaries of EDUs. We run a bidirectional LST- M over the sentence and obtain the output vector </p><formula xml:id="formula_6">sequence [h h h 0 , · · · , h h h i , · · · , h h h t ],</formula><formula xml:id="formula_7">s s sp p p = [ h h h b+1 − h h h a , h h h a−1 − h h h b ]<label>(7)</label></formula><p>As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, spans are represented using output from both backward and forward L- STM components. Intuitively, this allows to obtain representations for EDUs and larger constituents in context, as embeddings are learned based on in-   </p><p>where s s sp p p j is the span vector for the j th constituent in the current sequence; and W W W str u ∈ R d×2 ,W W W str b ∈ R d×4 are weight matrices. b b b str t is reshaped into a 2 × 2 matrix, where the (i, j) th entry indicates the score of the transition from label i to label j be- tween constituent c t and c t+1 .</p><p>Analogously, unary and binary potential scores for the relation CRF are calculated as: number of discourse relations; b b b rel t is also reshaped into a n r × n r matrix. For a constituent that is a s- ingle EDU, s s sp p p j,R and s s sp p p j+1,R are special vectors w w w LEAF L and w w w LEAF R .</p><formula xml:id="formula_9">u u u rel t = W W W rel u [s s sp p p j,L , s s sp p p j,R ]<label>(10</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-sentential Parser</head><p>The multi-sentential discourse parser treats sen- tences as the smallest possible discourse units, following a process similar to the intra-sentential model. Unfortunately, it is practically unfeasible to model all constituents with one CRF when pro- cessing entire documents. The forward-backward algorithm for calculating the CRF normalization factor on a sequence with T units has time com- plexity O(T M 2 ), where M is the number of labels, leading to O(T 2 M 2 ) time for parsing a document.</p><p>We therefore modify the two CRFs into sliding- window versions. Specifically, at each level, when decoding a constituent sequence, for each hidden structure node l i or relation node r i , we find al- l windows of constituents that contain the hidden node, and set the hidden node's label according to the window with the maximum joint probability.</p><p>We present the modified sliding-window CRFs in <ref type="figure">Figure 5</ref> (structure) and <ref type="figure">Figure 6</ref> (relation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-sentential Feature Learning</head><p>For multi-sentential parsing, we also use the LSTM-minus method to model constituents, how- ever, the minimum units here are sentences rather than EDUs. We represent individual sentences with the same bidirectional LSTM used for intra- sentential parsing. For sentence s i , the LSTM out- put vector of the first token h h h 0 and the last token h h h n are taken to form the sentence representation v v v i :</p><formula xml:id="formula_10">v v v i = [h h h 0 , h h h n ]<label>(12)</label></formula><p>In order to capture additional contextual infor- mation, we then build another bidirectional LSTM over a sequence of sentence vectors. We learn rep- resentations for constituents (aka text spans with- in a document) with the LSTM-minus method and use f f f l l l to denote the resulting vectors. Although this flat representation is relatively straightforward to obtain, it is perhaps overly simplistic for mod- eling documents where the number of sentences can be relative large. Moreover, it is not clear that it is discriminating enough for modeling relations between constituents. Such relations follow struc- tural regularities (e.g., they can be asymmetric or symmetric, left-or right-branching) which cannot be captured when adopting a sequence-based doc- ument view.</p><p>To inject structural knowledge in our represen- tation, we also model constituents as subtrees with recursive neural networks <ref type="bibr" target="#b25">(Socher et al., 2012</ref>). The latter operate over tree structures which we obtain during training from the RST Discourse Treebank ( <ref type="bibr" target="#b1">Carlson et al., 2003)</ref>. The representa- tion for each parent is computed based on its chil- dren iteratively, in a bottom-up fashion. More for- mally, let vectors h h h L and h h h R denote the left and right children of constituent c and dis their rhetor- ical relation. The vector for parent c is:</p><formula xml:id="formula_11">h h h c = tanh(W W W dis [h h h L , h h h R ] + b b b dis )<label>(13)</label></formula><p>where</p><formula xml:id="formula_12">[h h h L , h h h R ] is the concatenation of the chil- dren representations h h h L ∈ R d and h h h R ∈ R d , W W W dis ∈ R 2d×d</formula><p>, and b b b dis ∈ R d is the bias vector. We hence- forth use the term tree vector t t tr r r to refer to the rep- resentation in Equation <ref type="formula" target="#formula_0">(13)</ref>   </p><p>The representation above, attempts to capture rich- er semantic features during the tree building pro- cess while benefiting from the robustness afforded by the flat LSTM-based text spans. An example of this representation is given in <ref type="figure" target="#fig_5">Figure 7</ref>. Unary and binary potential scores for the struc- ture and relation CRFs are calculated as in intra- sentential parsing (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training</head><p>We first train the intra-sentential parser and use the learned LSTM as a component of the multi- sentential parser. During training, we maximize the log-likelihood of the correct label sequence Y given a constituent sequence C, which is p(Y str |C) for the structure CRFs and p(Y rel |C) for the rela- tion CRFs. Stochastic gradient descent with mo- mentum is used to update the parameters of the network. In our experiments, the momentum is set to 0.9 and the learning rate is 0.001. The LSTMs in our paper have one hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>In this section we present our experimental set- up for assessing the performance of the discourse parser described above. We give details on the datasets we used, evaluation protocol, and model training.</p><p>Evaluation We evaluated our model on the RST Discourse Treebank (RST-DT; <ref type="bibr" target="#b1">Carlson et al. 2003)</ref>, which is partitioned into 347 documents for training and 38 documents for testing. Following previous work <ref type="bibr" target="#b14">(Joty et al., 2013;</ref><ref type="bibr" target="#b18">Li et al., 2016)</ref>, we converted non-binary relations into a cascade of right-branching binary relations.</p><p>Predicted RST-trees are typically evaluated by computing F1 against gold standard trees <ref type="bibr" target="#b21">(Marcu, 2000</ref>). Evaluation metrics for RST-style dis- course parsing include: (a) span (S) which mea- sures whether the predicted subtrees match the goldstandard; (b) nucleus (N) which measures whether subtrees have the same nucleus as in the goldstandard and (c) relation (R) which measures whether discourse relations have been identified correctly. The three metrics are interdependent, er- rors on the span metric propagate to the nuclearity metric, and in turn to the relation metric. Follow- ing other RST-style discourse parsing systems <ref type="bibr" target="#b11">(Hernault et al., 2010)</ref>, we evaluate the relation met- ric using 18 coarse-grained relation classes, and with nuclearity attached, we have a total of 41 dis- tinct relations. <ref type="bibr">1</ref> Since EDU segmentation falls out- side the scope of this work, we evaluate our system on gold-standard EDUs. Comparison systems are also assessed in the same setting.</p><p>Training Details Word embeddings were pre- trained with the Gensim 2 implementation of word2vec ( <ref type="bibr" target="#b22">Mikolov et al., 2013</ref>) on the English GigaWord corpus (with case left intact). The di- mensionality of the word embeddings was set to 50. Following <ref type="bibr" target="#b18">Li et al. (2016)</ref>, the embed- dings were fine-tuned using a mapping matrix W W W ∈ R 50×50 trained with the following criterion:</p><formula xml:id="formula_14">min W W W ,b b b L L LT T T tuned − L L LT T T pre W W W + b b b (15)</formula><p>where L L LT T T tuned , and L L LT T T pre are lookup tables for fine-tuned and pre-trained word embeddings in the training set. Matrix W can be subsequently used to to estimate fine-tuned embeddings for words in the test set. Tokenization, POS tagging and sentence s- plitting were performed using the Stanford CIDER S N R Tree Span 79.5 68.1 56.6 Flat Span (−minus) 82.7 69.3 55.6 Flat Span (+minus) 83.6 70.1 55.4 Tree + Flat Span (+minus) 83.6 71.1 57.3 <ref type="table">Table 1</ref>: CIDER performance using different con- stituent representations (RST-DT test set).</p><p>CoreNLP toolkit ( <ref type="bibr" target="#b20">Manning et al., 2014</ref>). All neu- ral network parameters were initialized random- ly with Xavier's initialization <ref type="bibr" target="#b8">(Glorot and Bengio, 2010)</ref>. The hyper-parameters are tuned by cross- validation on the training set.</p><p>Additional Features Most existing state-of-the- art systems rely heavily on handcrafted features <ref type="bibr" target="#b11">(Hernault et al., 2010;</ref><ref type="bibr" target="#b7">Feng and Hirst, 2014;</ref><ref type="bibr" target="#b14">Joty et al., 2013</ref>) some of which have been also proved helpful in neural network models ( <ref type="bibr" target="#b17">Li et al., 2014</ref><ref type="bibr" target="#b18">Li et al., , 2016</ref>. In our experiments, we use the following basic features which have been widely adopted in various discourse parsing models: (1) the first three words and the last two words of each con- stituent; (2) the POS tags of the first three words and the last two words of each constituent; (3) the number of EDUs; and (4) the number of tokens of each constituent. We concatenate these features with the constituent vectors learned by our neural networks, and train new CRF models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In this paper we have presented two views for modeling discourse constituents, namely as trees or sequences. We experimentally assessed whether these two views are overlapping or complemen- tary. <ref type="table">Table 1</ref> reports the performance of our pars- er which we call CIDER (as a shorthand for Contextually Informed Discourse Parser) without the additional features introduced in Section 5. The first row presents a version of CIDER based solely on tree span representations. In the second row, CIDER uses flat representations without LST- M minus features. Specifically, each constituent is represented as the average of the LSTM output- s within it. In the third row, CIDER's representa- tions are computed with the LSTM minus method, while the fourth row shows results for the full sys- tem.</p><p>As can be seen, on the span (S) metric, CIDER with flat span representations is much better than CIDER with tree span representations; on the nu- clearity (N) metric tree representations are still in- ferior to flat representations but the performance Discourse Parsers S N R Speed Ji and Eisenstein <ref type="formula" target="#formula_0">(2014)</ref> 82.1 71.1 61.6 0.21 <ref type="bibr" target="#b7">Feng and Hirst (2014)</ref> 85.7 71.0 58.2 9.88 Heilman and Sagae (2015) 83.5 68.1 55.1 0.40 CIDER <ref type="table">(−AF)</ref> 83.6 71.1 57.3 3.80 CIDER (+AF) 85.0 71.1 59.0 3.80 <ref type="bibr" target="#b17">Li et al. (2014)</ref> 84.0 70.8 58.6 26.00 <ref type="bibr" target="#b18">Li et al. (2016)</ref> 85.8 71.7 58.9 - Human 88.7 77.7 65.8 - <ref type="table">Table 2</ref>: Comparison with state-of-the-art system- s (RST-DT test set). Speed indicates the average number seconds taken to parse a document.</p><p>gap is narrower, whereas on the the relation (R) metric, tree span representations are superior. We believe this can be explained by the fact that re- lation identification relies on more semantic in- formation while span identification relies on more shallow features, like the beginning and the end of the span <ref type="bibr" target="#b13">(Ji and Eisenstein, 2014)</ref>. Span features based on the LSTM minus method bring improve- ments over vanilla LSTM representations on the s- pan and nuclearity metrics. Perhaps unsurprising- ly, the combination of span and tree representa- tions achieves the best results overall.</p><p>In <ref type="table">Table 2</ref>, we compare our system with sev- eral state-of-the-art discourse parsers, which can be classified in two groups depending on their time complexity. Linear-time systems (first block in the table) include two transition-based parser- s ( <ref type="bibr" target="#b13">Ji and Eisenstein, 2014;</ref><ref type="bibr" target="#b10">Heilman and Sagae, 2015)</ref> and one CRF-based parser <ref type="bibr" target="#b7">(Feng and Hirst, 2014)</ref>, whereas cubic-time parsers (second block) include two neural network models <ref type="bibr" target="#b17">(Li, Li, and Hovy, 2014;</ref><ref type="bibr" target="#b18">Li, Li, and Chang, 2016)</ref>. CIDER falls in the first group as it is a liner-time parser, while it shares with parsers in the second group the use of neural architectures for automated fea- ture extraction. We report CIDER scores with and without the additional features (AF) discussed in the previous section. As an upper bound, we also report inter-annotator agreement on the discourse parsing task (last row in the table).</p><p>Amongst linear-time systems, our parser achieves comparable results on the span and relation metric, and best performance on the nuclearity metric. Note that the three metrics evaluate different aspects of a discourse parser, and CIDER achieves the most balanced results across all metrics. As far as other comparison systems are concerned, <ref type="bibr" target="#b13">Ji and Eisenstein (2014)</ref> employ a shift-reduce discourse parser. They represent EDUs with word-count vectors and use a projection matrix to combine them into text spans. A support vector machine classifier is used to decide the actions of the parser. <ref type="bibr" target="#b10">Heilman and Sagae (2015)</ref> also adopt a shift-reduce approach and use multi-class logistic regression to select the best parsing action. Their classifier considers a variety of lexical, syntactic, and positional features. <ref type="bibr" target="#b7">Feng and Hirst's (2014)</ref> system is closest to ours in their use of linear-chain CRFs, but their features are mainly extracted from local con- stituents. Furthermore, they adopt a post-editing method which modifies the discourse trees their parser creates with height features.</p><p>With regard to previously proposed cubic-time systems, CIDER outperforms <ref type="bibr" target="#b17">Li et al. (2014)</ref> across all metrics. Their CYK-based parser adopts a recursive deep model for composing EDUs hi- erarchically together with several additional fea- tures to boost performance. CIDER performs s- lightly worse on span and nuclearity compared to <ref type="bibr" target="#b18">Li et al. (2016)</ref>, but is better at identifying relation- s. Their system uses an attention-based hierarchi- cal neural network for modeling text spans and a tensor-based transformation for combining two s- pans. A CKY-like algorithm is used to generate the discourse tree structure. In comparison, CIDER is conceptually simpler, and more efficient.</p><p>We used paired bootstrap re-sampling ( <ref type="bibr" target="#b5">Efron and Tibshirani, 1993)</ref> to assess whether dif- ferences in performance are statistically signif- icant. CIDER is significantly better than Feng and Hirst's 2014 system on the relation metric (p &lt; 0.05); it is also significantly better (p &lt; 0.05) than Heilman and Sagae (2015) on all three met- rics and better than <ref type="bibr" target="#b13">Ji and Eisenstein (2014)</ref> on the span metric. Compared to <ref type="bibr" target="#b17">Li et al. (2014)</ref>, CIDER is significantly better on the span and relation met- rics (p &lt; 0.05). Unfortunately, we cannot perform significance tests against <ref type="bibr" target="#b18">Li et al. (2016)</ref> as we do not have access to the output of their system.</p><p>We also evaluated the speed of CIDER and comparison discourse parsers on a platform with Intel Core-i5-7200U CPU at 2.50GHz. We report the average number of seconds taken to parse a document in the RST-DT test set. The times shown in <ref type="table">Table 2</ref> do not include pre-processing, which for CIDER is only part-of-speech tagging, whereas all other linear-time systems rely on a syntactic pars- er. As can be seen CIDER is quite efficient com- pared to related systems <ref type="bibr" target="#b7">(Feng and Hirst, 2014;</ref><ref type="bibr" target="#b10">Heilman and Sagae, 2015)</ref> whilst requiring less feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we described CIDER, a simple and efficient discourse parser which adopts a two- stage parsing strategy, whilst exploiting a more global feature space. We proposed a novel way to learn contextually informed representations of constituents with the LSTM minus method, at the sentence and document level. We also demonstrat- ed that flat representations of text spans can be usefully complemented with tree-based ones lead- ing to a more accurate characterization of dis- course relations. Experimental results showed that CIDER performs on par with the state of the art ( <ref type="bibr" target="#b18">Li et al., 2016)</ref>, despite the greedy parsing algorith- m and relatively simple neural architecture. In the future, we would like to improve parsing accura- cy by leveraging unlabeled text rather than relying exclusively on human annotated training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Intra-sentential structure CRF with pairwise modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Intra-sentential relation CRF with pairwise modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where h h h i = [ h h h i , h h h i ] is the output vector for the i th word, and h h h i and h h h i are the output vectors from the forward and back- ward directions, respectively. We represent a con- stituent c from position a to b with a span vec- tor s s sp p p which is the concatenation of the vector d- ifferences h h h b+1 − h h h a and h h h a−1 − h h h b :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Modeling discourse constituents by LSTM-minus features. The feature vector s s sp p p j for a constituent covering EDU k and EDU k+1 is [ h h h k+1 − h h h i+3 , h h h k−1 − h h h i+6 ]. Blue nodes indicate word embeddings and LSTM outputs for words, while gray nodes represent EDU separators. Black nodes are learned LSTM-minus features for constituents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Document-level constituents with LSTM-minus features. Blue nodes are LSTM outputs for words, light green nodes represent vectors for sentences, dark green nodes are LSTM outputs for sentences, and black nodes are learned constituent representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>e 1 [it sold one of its newest and largest computer systems, the Cray Y-MP/832, to the United Kingdom Meteorological Office.]e 2</head><label></label><figDesc></figDesc><table>[The system is the first]e 3 [to be sold through the joint marketing agreement between 
Cray and Control Data Corp.]e 4 

[Only a few months ago, the 124-year-old securities 
firm seemed to be on the verge of a meltdown,]e 1 
[racked by internal squabbles and defections.]e 2 
[Its relationship with parent General Electric Co. 
had been frayed since a big Kidder insider-trading 
scandal two years ago.]e 3 

e 1 
e 2 

consequence 

e 1:2 
e 3 

list 

e 1:3 

</table></figure>

			<note place="foot" n="1"> For calculating the binary potential scores, 41 relations will lead to a large number of parameters; to avoid this, we only use 18 relations without nuclearity. 2 https://radimrehurek.com/gensim/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic head-driven parsing for discourse structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL conference</title>
		<meeting>the CoNLL conference</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="96" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current and new directions in discourse and dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="85" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discourse structure for context question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004: Workshop on Pragmatics of Question Answering</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An Introduction ot the Bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Text-level discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A lineartime bottom-up discourse parser with constraints and post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaitly</forename><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="293" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02425</idno>
		<title level="m">Fast rhetorical structure theory discourse parsing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>David A Duverle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Ishizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combining intra-and multisentential rhetorical parsing for document-level discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="486" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating discourse structures for written texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huong</forename><surname>Lethanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geetha</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Huyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">329</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discourse parsing with attention-based hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianshi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
		<title level="m">Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary Journal for the Study of Discourse</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="243" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference: System Demonstrations</title>
		<meeting>the ACL conference: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The rhetorical parsing of unrestricted texts: A surface-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="448" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Analysis of discourse structure with syntactic dependencies and data-driven shiftreduce parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Discourse-level Relation for Opinion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swapna Somasundaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL conference</title>
		<meeting>the NAACL conference</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An effective discourse parser that uses rich linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">Di</forename><surname>Eugenio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="566" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS conference</title>
		<meeting>the NIPS conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS conference</title>
		<meeting>the NIPS conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR conference</title>
		<meeting>the CVPR conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dependency-based discourse parser for single-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP conference</title>
		<meeting>the EMNLP conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1834" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
