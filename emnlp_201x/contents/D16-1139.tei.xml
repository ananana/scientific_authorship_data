<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequence-Level Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
							<email>yoonkim@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<email>srush@seas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University Cambridge</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sequence-Level Knowledge Distillation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1317" to="1327"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance , NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy de-coding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13× fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) <ref type="bibr">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b7">Cho et al., 2014;</ref><ref type="bibr">Sutskever et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2015</ref>) is a deep learning- based method for translation that has recently shown promising results as an alternative to statistical ap- proaches. NMT systems directly model the proba- bility of the next word in the target sentence sim- ply by conditioning a recurrent neural network on the source sentence and previously generated target words.</p><p>While both simple and surprisingly accurate, NMT systems typically need to have very high ca- pacity in order to perform well: <ref type="bibr">Sutskever et al. (2014)</ref> used a 4-layer LSTM with 1000 hidden units per layer (herein 4×1000) and <ref type="bibr">Zhou et al. (2016)</ref> ob- tained state-of-the-art results on English → French with a 16-layer LSTM with 512 units per layer. The sheer size of the models requires cutting-edge hard- ware for training and makes using the models on standard setups very challenging.</p><p>This issue of excessively large networks has been observed in several other domains, with much fo- cus on fully-connected and convolutional networks for multi-class classification. Researchers have par- ticularly noted that large networks seem to be nec- essary for training, but learn redundant representa- tions in the process <ref type="bibr" target="#b10">(Denil et al., 2013)</ref>. Therefore compressing deep models into smaller networks has been an active area of research. As deep learning systems obtain better results on NLP tasks, compres- sion also becomes an important practical issue with applications such as running deep learning models for speech and translation locally on cell phones.</p><p>Existing compression methods generally fall into two categories: (1) pruning and (2) knowledge dis- tillation. Pruning methods ( <ref type="bibr" target="#b16">LeCun et al., 1990;</ref><ref type="bibr">He et al., 2014;</ref><ref type="bibr" target="#b12">Han et al., 2016</ref>), zero-out weights or entire neurons based on an importance criterion: <ref type="bibr" target="#b16">LeCun et al. (1990)</ref> use (a diagonal approximation to) the Hessian to identify weights whose removal min- imally impacts the objective function, while <ref type="bibr" target="#b12">Han et al. (2016)</ref> remove weights based on threshold- ing their absolute values. Knowledge distillation ap- proaches ( <ref type="bibr" target="#b1">Bucila et al., 2006</ref>; <ref type="bibr" target="#b0">Ba and Caruana, 2014;</ref><ref type="bibr" target="#b13">Hinton et al., 2015</ref>) learn a smaller student network to mimic the original teacher network by minimiz- ing the loss (typically L 2 or cross-entropy) between the student and teacher output.</p><p>In this work, we investigate knowledge distilla- tion in the context of neural machine translation. We note that NMT differs from previous work which has mainly explored non-recurrent models in the multi- class prediction setting. For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence out- puts conditioned on previous decisions. With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to ap- proximately match the sequence-level (as opposed to word-level) distribution of the teacher network. This sequence-level approximation leads to a sim- ple training procedure wherein the student network is trained on a newly generated dataset that is the result of running beam search with the teacher net- work.</p><p>We run experiments to compress a large state-of- the-art 4 × 1000 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a 2 × 500 LSTM that roughly matches the per- formance of the full system. We see similar results compressing a 2 × 500 model down to 2 × 100 on a smaller data set. Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time. As a re- sult we are able to perform greedy decoding on the 2 × 500 model 10 times faster than beam search on the 4 × 1000 model with comparable performance. Our student models can even be run efficiently on a standard smartphone. 1 Finally, we apply weight pruning on top of the student network to obtain a model that has 13× fewer parameters than the origi- nal teacher model. We have released all the code for the models described in this paper. <ref type="bibr">2</ref> 1 https://github.com/harvardnlp/nmt-android 2 https://github.com/harvardnlp/seq2seq-attn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence-to-Sequence with Attention</head><p>Let s = [s 1 , . . . , s I ] and t = [t 1 , . . . , t J ] be (random variable sequences representing) the source/target sentence, with I and J respectively being the source/target lengths. Machine translation involves finding the most probable target sentence given the source:</p><formula xml:id="formula_0">argmax t∈T p(t | s)</formula><p>where T is the set of all possible sequences. NMT models parameterize p(t | s) with an encoder neural network which reads the source sentence and a de- coder neural network which produces a distribution over the target sentence (one word at a time) given the source. We employ the attentional architecture from <ref type="bibr">Luong et al. (2015)</ref>, which achieved state-of- the-art results on English → German translation. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>Knowledge distillation describes a class of methods for training a smaller student network to perform better by learning from a larger teacher network (in addition to learning from the training data set). We generally assume that the teacher has previously been trained, and that we are estimating parame- ters for the student. Knowledge distillation suggests training by matching the student's predictions to the teacher's predictions. For classification this usually means matching the probabilities either via L 2 on the log scale ( <ref type="bibr" target="#b0">Ba and Caruana, 2014</ref>) or by cross- entropy ( <ref type="bibr" target="#b17">Li et al., 2014;</ref><ref type="bibr" target="#b13">Hinton et al., 2015)</ref>.</p><p>Concretely, assume we are learning a multi-class classifier over a data set of examples of the form (x, y) with possible classes V. The usual training criteria is to minimize NLL for each example from the training data,</p><formula xml:id="formula_1">L NLL (θ) = − |V| k=1 1{y = k} log p(y = k | x; θ)</formula><p>where 1{·} is the indicator function and p the distribution from our model (parameterized by θ). is minimized between the student/teacher distributions (yellow) for each word in the actual target sequence (ECD), as well as between the student distribution and the degenerate data distribution, which has all of its probabilitiy mass on one word (black). In sequence-level knowledge distillation (center) the student network is trained on the output from beam search of the teacher network that had the highest score (ACF). In sequence-level interpolation (right) the student is trained on the output from beam search of the teacher network that had the highest sim with the target sequence (ECE).</p><p>This objective can be seen as minimizing the cross- entropy between the degenerate data distribution (which has all of its probability mass on one class) and the model distribution p(y | x; θ).</p><p>In knowledge distillation, we assume access to a learned teacher distribution q(y | x; θ T ), possibly trained over the same data set. Instead of minimiz- ing cross-entropy with the observed data, we instead minimize the cross-entropy with the teacher's prob- ability distribution,</p><formula xml:id="formula_2">L KD (θ; θ T ) = − |V| k=1 q(y = k | x; θ T )× log p(y = k | x; θ)</formula><p>where θ T parameterizes the teacher distribution and remains fixed. Note the cross-entropy setup is iden- tical, but the target distribution is no longer a sparse distribution. <ref type="bibr">4</ref> Training on q(y | x; θ T ) is attractive since it gives more information about other classes for a given data point (e.g. similarity between classes) and has less variance in gradients ( <ref type="bibr" target="#b13">Hinton et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge Distillation for NMT</head><p>The large sizes of neural machine translation sys- tems make them an ideal candidate for knowledge distillation approaches. In this section we explore three different ways this technique can be applied to NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-Level Knowledge Distillation</head><p>NMT systems are trained directly to minimize word NLL, L WORD-NLL , at each position. Therefore if we have a teacher model, standard knowledge distil- lation for multi-class cross-entropy can be applied. We define this distillation for a sentence as,</p><formula xml:id="formula_3">L WORD-KD = − J j=1 |V| k=1 q(t j = k | s, t &lt;j ) × log p(t j = k | s, t &lt;j )</formula><p>where V is the target vocabulary set. The student can further be trained to optimize the mixture of L WORD-KD and L WORD-NLL . In the context of NMT, we refer to this approach as word-level knowledge distillation and illustrate this in <ref type="figure" target="#fig_0">Figure 1</ref> (left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequence-Level Knowledge Distillation</head><p>Word-level knowledge distillation allows transfer of these local word distributions. Ideally however, we would like the student model to mimic the teacher's actions at the sequence-level. The sequence distri- bution is particularly important for NMT, because wrong predictions can propagate forward at test- time.</p><p>First, consider the sequence-level distribution specified by the model over all possible sequences t ∈ T ,</p><formula xml:id="formula_4">p(t | s) = J j=1 p(t j | s, t &lt;j )</formula><p>for any length J. The sequence-level negative log- likelihood for NMT then involves matching the one- hot distribution over all complete sequences,</p><formula xml:id="formula_5">L SEQ-NLL = − t∈T 1{t = y} log p(t | s) = − J j=1 |V| k=1 1{y j = k} log p(t j = k | s, t &lt;j ) = L WORD-NLL</formula><p>where y = [y 1 , . . . , y J ] is the observed sequence. Of course, this just shows that from a negative log likelihood perspective, minimizing word-level NLL and sequence-level NLL are equivalent in this model.</p><p>But now consider the case of sequence-level knowledge distillation. As before, we can simply replace the distribution from the data with a prob- ability distribution derived from our teacher model. However, instead of using a single word prediction, we use q(t | s) to represent the teacher's sequence distribution over the sample space of all possible se- quences,</p><formula xml:id="formula_6">L SEQ-KD = − t∈T q(t | s) log p(t | s)</formula><p>Note that L SEQ-KD is inherently different from L WORD-KD , as the sum is over an exponential num- ber of terms. Despite its intractability, we posit that this sequence-level objective is worthwhile. It gives the teacher the chance to assign probabilities to complete sequences and therefore transfer a broader range of knowledge. We thus consider an approxi- mation of this objective.</p><p>Our simplest approximation is to replace the teacher distribution q with its mode,</p><formula xml:id="formula_7">q(t | s) ∼ 1{t = argmax t∈T q(t | s)}</formula><p>Observing that finding the mode is itself intractable, we use beam search to find an approximation. The loss is then</p><formula xml:id="formula_8">L SEQ-KD ≈ − t∈T 1{t = ˆ y} log p(t | s) = − log p(t = ˆ y | s)</formula><p>wherê y is now the output from running beam search with the teacher model.</p><p>Using the mode seems like a poor approximation for the teacher distribution q(t | s), as we are ap- proximating an exponentially-sized distribution with a single sample. However, previous results showing the effectiveness of beam search decoding for NMT lead us to belief that a large portion of q's mass lies in a single output sequence. In fact, in experiments we find that with beam of size 1, q(ˆ y | s) (on aver- age) accounts for 1.3% of the distribution for Ger- man → English, and 2.3% for Thai → English (Ta- ble 1: p(t = ˆ y)). 5 To summarize, sequence-level knowledge distil- lation suggests to: (1) train a teacher model, (2) run beam search over the training set with this model, (3) train the student network with cross-entropy on this new dataset.</p><p>Step <ref type="formula">(3)</ref> is identical to the word-level NLL process except now on the newly-generated data set. This is shown in <ref type="figure" target="#fig_0">Figure 1</ref> (center). <ref type="bibr">5</ref> Additionally there are simple ways to better approximate q(t | s). One way would be to consider a K-best list from beam search and renormalizing the probabilities,</p><formula xml:id="formula_9">q(t | s) ∼ q(t | s) t∈T K q(t | s)</formula><p>where TK is the K-best list from beam search. This would increase the training set by a factor of K. A beam of size 5 captures 2.8% of the distribution for German → English, and 3.8% for Thai → English. Another alternative is to use a Monte Carlo estimate and sample from the teacher model (since</p><formula xml:id="formula_10">LSEQ-KD = E t∼q(t | s) [ − log p(t | s) ])</formula><p>. However in practice we found the (approximate) mode to work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequence-Level Interpolation</head><p>Next we consider integrating the training data back into the process, such that we train the student model as a mixture of our sequence-level teacher- generated data (L SEQ-KD ) with the original training data (L SEQ-NLL ),</p><formula xml:id="formula_11">L = (1 − α)L SEQ-NLL + αL SEQ-KD = −(1 − α) log p(y | s) − α t∈T q(t | s) log p(t | s)</formula><p>where y is the gold target sequence.</p><p>Since the second term is intractable, we could again apply the mode approximation from the pre- vious section,</p><formula xml:id="formula_12">L = −(1 − α) log p(y | s) − α log p(ˆ y | s)</formula><p>and train on both observed (y) and teacher- generated (ˆ y) data. However, this process is non- ideal for two reasons: (1) unlike for standard knowl- edge distribution, it doubles the size of the training data, and (2) it requires training on both the teacher- generated sequence and the true sequence, condi- tioned on the same source input. The latter concern is particularly problematic since we observe that y andˆyandˆ andˆy are often quite different.</p><p>As an alternative, we propose a single-sequence approximation that is more attractive in this setting. This approach is inspired by local updating ( <ref type="bibr" target="#b19">Liang et al., 2006</ref>), a method for discriminative train- ing in statistical machine translation (although to our knowledge not for knowledge distillation). Lo- cal updating suggests selecting a training sequence which is close to y and has high probability under the teacher model,</p><formula xml:id="formula_13">˜ y = argmax t∈T sim(t, y)q(t | s)</formula><p>where sim is a function measuring closeness (e.g. Jaccard similarity or BLEU ( <ref type="bibr" target="#b26">Papineni et al., 2002)</ref>). Following local updating, we can approximate this sequence by running beam search and choosing˜y</p><formula xml:id="formula_14">choosing˜ choosing˜y ≈ argmax t∈T K sim(t, y)</formula><p>where T K is the K-best list from beam search. We take sim to be smoothed sentence-level BLEU <ref type="bibr" target="#b4">(Chen and Cherry, 2014</ref>).</p><p>We justify training oñ y from a knowledge distil- lation perspective with the following generative pro- cess: suppose that there is a true target sequence (which we do not observe) that is first generated from the underlying data distribution D. And further suppose that the target sequence that we observe (y) is a noisy version of the unobserved true sequence: i.e. (i) t ∼ D, (ii) y ∼ (t), where (t) is, for ex- ample, a noise function that independently replaces each element in t with a random element in V with some small probability. <ref type="bibr">6</ref> In such a case, ideally the student's distribution should match the mixture dis- tribution,</p><formula xml:id="formula_15">D SEQ-Inter ∼ (1 − α)D + αq(t | s)</formula><p>In this setting, due to the noise assumption, D now has significant probability mass around a neighbor- hood of y (not just at y), and therefore the argmax of the mixture distribution is likely something other than y (the observed sequence) orˆyorˆ orˆy (the output from beam search). We can see that˜ythat˜ that˜y is a natural approx- imation to the argmax of this mixture distribution between D and q(t | s) for some α. We illustrate this framework in <ref type="figure" target="#fig_0">Figure 1</ref> (right) and visualize the distribution over a real example in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>To test out these approaches, we conduct two sets of NMT experiments: high resource (English → Ger- man) and low resource (Thai → English).</p><p>The English-German data comes from WMT 2014. <ref type="bibr">7</ref> The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set. We keep the top 50k most frequent words, and replace the rest with UNK. The teacher model is a 4 × 1000 LSTM (as in Lu- ong et al. <ref type="formula">(2015)</ref>) and we train two student models: 2 × 300 and 2 × 500. The Thai-English data comes from IWSLT 2015. 8 There are 90k sentences in the training set and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabu- lary size is 25k. Size of the teacher model is 2 × 500 (which performed better than 4×1000, 2×750 mod- els), and the student model is 2×100. Other training details mirror <ref type="bibr">Luong et al. (2015)</ref>.</p><p>We evaluate on tokenized BLEU with multi-bleu.perl, and experiment with the following variations:</p><p>Word-Level Knowledge Distillation (Word-KD) Student is trained on the original data and addition- ally trained to minimize the cross-entropy of the teacher distribution at the word-level. We tested α ∈ {0.5, 0.9} and found α = 0.5 to work better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-Level Knowledge Distillation (Seq-KD)</head><p>Student is trained on the teacher-generated data, which is the result of running beam search and tak- ing the highest-scoring sequence with the teacher model. We use beam size K = 5 (we did not see improvements with a larger beam).</p><p>Sequence-Level Interpolation (Seq-Inter) Stu- dent is trained on the sequence on the teacher's beam that had the highest BLEU (beam size K = 35). We adopt a fine-tuning approach where we begin train- ing from a pretrained model (either on original data or Seq-KD data) and train with a smaller learning rate (0.1). For English-German we generate Seq- Inter data on a smaller portion of the training set (∼ 50%) for efficiency.</p><p>The above methods are complementary and can be combined with each other. For example, we can train on teacher-generated data but still in- clude a word-level cross-entropy term between the teacher/student (Seq-KD + Word-KD in <ref type="table" target="#tab_3">Table 1</ref>), or fine-tune towards Seq-Inter data starting from the baseline model trained on original data (Baseline + Seq-Inter in <ref type="table" target="#tab_3">Table 1</ref>). <ref type="bibr">9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Results of our experiments are shown in <ref type="table" target="#tab_3">Table  1</ref>. We find that while word-level knowledge dis- tillation (Word-KD) does improve upon the base- line, sequence-level knowledge distillation (Seq- KD) does better on English → German and per- forms similarly on Thai → English. Combining them (Seq-KD + Word-KD) results in further gains for the 2 × 300 and 2 × 100 models (although not for the 2 × 500 model), indicating that these meth- ods provide orthogonal means of transferring knowl- edge from the teacher to the student: Word-KD is transferring knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge at the global (i.e. sequence) level.</p><p>Sequence-level interpolation (Seq-Inter), in addi- tion to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but fine- tuned towards Seq-Inter data (Baseline + Seq-Inter). In fact, greedy decoding with this fine-tuned model has similar performance (19.6) as beam search with the original model (19.5), allowing for faster decod- ing even with an identically-sized model. We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher's mode) instead of 'wasting' parameters on trying to model the entire</p><formula xml:id="formula_16">Model BLEU K=1 ∆ K=1 BLEU K=5 ∆ K=5 PPL p(t = ˆ y)</formula><p>English → German WMT 2014</p><p>Teacher Baseline 4 × 1000 (Params: 221m) 17   space of translations. Our results suggest that this is indeed the case: the probability mass that Seq- KD models assign to the approximate mode is much higher than is the case for baseline models trained on original data <ref type="table" target="#tab_3">(Table 1</ref>: p(t = ˆ y)). For example, on English → German the (approximate) argmax for the 2 × 500 Seq-KD model (on average) ac- counts for 16.9% of the total probability mass, while the corresponding number is 0.9% for the baseline. This also explains the success of greedy decoding for Seq-KD models-since we are only modeling around the teacher's mode, the student's distribution is more peaked and therefore the argmax is much easier to find. Seq-Inter offers a compromise be- tween the two, with the greedily-decoded sequence accounting for 7.6% of the distribution.</p><p>Finally, although past work has shown that mod- els with lower perplexity generally tend to have higher BLEU, our results indicate that this is not necessarily the case. The perplexity of the baseline 2 × 500 English → German model is 8.2 while the perplexity of the corresponding Seq-KD model is 22.7, despite the fact that Seq-KD model does sig- nificantly better for both greedy (+4.2 BLEU) and beam search (+1.4 BLEU) decoding.</p><note type="other">Model Size GPU CPU Android Beam = 1 (Greedy) 4 × 1000 425.5 15.0 − 2 × 500 1051.3 63.6 8.8 2 × 300 1267.8 104.3 15.8 Beam = 5 4 × 1000 101.9 7.9 − 2 × 500 181.9 22.1 1.9 2 × 300 189.1 38.4 3.4</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Decoding Speed</head><p>Run-time complexity for beam search grows linearly with beam size. Therefore, the fact that sequence- level knowledge distillation allows for greedy de- coding is significant, with practical implications for running NMT systems across various devices. To test the speed gains, we run the teacher/student mod- els on GPU, CPU, and smartphone, and check the average number of source words translated per sec- ond <ref type="table" target="#tab_2">(Table 2)</ref>. We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone. We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU (1051.3 vs 101.9 words/sec), with similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Weight Pruning</head><p>Although knowledge distillation enables training faster models, the number of parameters for the student models is still somewhat large (   2 × 500 English → German model the word em- beddings account for approximately 63% (50m out of 84m) of the parameters. The size of word em- beddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model. We therefore focus next on reducing the mem- ory footprint of the student models further through weight pruning. Weight pruning for NMT was re- cently investigated by <ref type="bibr" target="#b28">See et al. (2016)</ref>, who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in perfor- mance. We take our best English → German student model (2 × 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values. We then retrain the pruned model on Seq-KD data with a learning rate of 0.2 and fine-tune towards Seq-Inter data with a learning rate of 0.1. As observed by <ref type="bibr" target="#b28">See et al. (2016)</ref>, re- training proved to be crucial. The results are shown in <ref type="table">Table 3</ref>.</p><p>Our findings suggest that compression benefits achieved through weight pruning and knowledge distillation are orthogonal. 11 Pruning 80% of the weight in the 2 × 500 student model results in a model with 13× fewer parameters than the original teacher model with only a decrease of 0.4 BLEU. While pruning 90% of the weights results in a more appreciable decrease of 1.0 BLEU, the model is drastically smaller with 8m parameters, which is 26× fewer than the original teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Further Observations</head><p>• For models trained with word-level knowledge distillation, we also tried regressing the student network's top-most hidden layer at each time step to the teacher network's top-most hidden layer as a pretraining step, noting that <ref type="bibr">Romero et al. (2015)</ref> obtained improvements with a similar technique on feed-forward models. We found this to give comparable results to stan- dard knowledge distillation and hence did not pursue this further.</p><p>• There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables ( <ref type="bibr" target="#b21">Ling et al., 2015a;</ref><ref type="bibr" target="#b16">Kim et al., 2016;</ref><ref type="bibr" target="#b22">Ling et al., 2015b;</ref><ref type="bibr" target="#b15">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b8">Costa-Jussa and Fonollosa, 2016)</ref>. Combining such methods with knowledge dis- tillation/pruning to further reduce the memory footprint of NMT systems remains an avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Compressing deep learning models is an active area of current research. Pruning methods involve prun- ing weights or entire neurons/nodes based on some criterion. <ref type="bibr" target="#b16">LeCun et al. (1990)</ref> prune weights based on an approximation of the Hessian, while <ref type="bibr" target="#b12">Han et al. (2016)</ref> show that a simple magnitude-based pruning works well. Prior work on removing neurons/nodes include <ref type="bibr" target="#b30">Srinivas and Babu (2015)</ref> and <ref type="bibr">Mariet and Sra (2016)</ref>. <ref type="bibr" target="#b28">See et al. (2016)</ref> were the first to ap- ply pruning to Neural Machine Translation, observ- ing that that different parts of the architecture (in- put word embeddings, LSTM matrices, etc.) admit different levels of pruning. Knowledge distillation approaches train a smaller student model to mimic a larger teacher model, by minimizing the loss be- tween the teacher/student predictions ( <ref type="bibr" target="#b1">Bucila et al., 2006</ref>; <ref type="bibr" target="#b0">Ba and Caruana, 2014;</ref><ref type="bibr" target="#b17">Li et al., 2014;</ref><ref type="bibr" target="#b13">Hinton et al., 2015)</ref>. <ref type="bibr">Romero et al. (2015)</ref> addition- ally regress on the intermediate hidden layers of the student/teacher network as a pretraining step, while <ref type="bibr" target="#b24">Mou et al. (2015)</ref> obtain smaller word embeddings from a teacher model via regression. There has also been work on transferring knowledge across differ- ent network architectures: <ref type="bibr" target="#b3">Chan et al. (2015b)</ref> show that a deep non-recurrent neural network can learn from an RNN; <ref type="bibr" target="#b11">Geras et al. (2016)</ref> train a CNN to mimic an LSTM for speech recognition. <ref type="bibr" target="#b16">Kuncoro et al. (2016)</ref> recently investigated knowledge distil- lation for structured prediction by having a single parser learn from an ensemble of parsers.</p><p>Other approaches for compression involve low rank factorizations of weight matrices <ref type="bibr" target="#b11">(Denton et al., 2014;</ref><ref type="bibr" target="#b14">Jaderberg et al., 2014;</ref><ref type="bibr" target="#b22">Lu et al., 2016;</ref><ref type="bibr">Prabhavalkar et al., 2016)</ref>, sparsity-inducing regularizers <ref type="bibr">(Murray and Chiang, 2015)</ref>, binarization of weights ( <ref type="bibr">Courbariaux et al., 2016;</ref><ref type="bibr" target="#b20">Lin et al., 2016)</ref>, and weight sharing <ref type="bibr" target="#b5">(Chen et al., 2015;</ref><ref type="bibr" target="#b12">Han et al., 2016)</ref>. Finally, although we have motivated sequence-level knowledge distillation in the context of training a smaller model, there are other techniques that train on a mixture of the model's predictions and the data, such as local updating ( <ref type="bibr" target="#b19">Liang et al., 2006</ref>), hope/fear training <ref type="bibr" target="#b6">(Chiang, 2012)</ref>, SEARN <ref type="bibr" target="#b9">(Daumé III et al., 2009</ref>), <ref type="bibr">DAgger (Ross et al., 2011)</ref>, and minimum risk training <ref type="bibr" target="#b25">(Och, 2003;</ref><ref type="bibr" target="#b29">Shen et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work we have investigated existing knowl- edge distillation methods for NMT (which work at the word-level) and introduced two sequence-level variants of knowledge distillation, which provide improvements over standard word-level knowledge distillation.</p><p>We have chosen to focus on translation as this domain has generally required the largest capacity deep learning models, but the sequence-to-sequence framework has been successfully applied to a wide range of tasks including parsing ( <ref type="bibr">Vinyals et al., 2015a</ref>), summarization <ref type="bibr">(Rush et al., 2015)</ref>, dialogue <ref type="bibr">(Vinyals and Le, 2015;</ref><ref type="bibr" target="#b28">Serban et al., 2016;</ref><ref type="bibr" target="#b18">Li et al., 2016)</ref>, NER/POS-tagging ( <ref type="bibr">Gillick et al., 2016)</ref>, image captioning ( <ref type="bibr" target="#b31">Vinyals et al., 2015b;</ref>, video generation ( <ref type="bibr" target="#b30">Srivastava et al., 2015)</ref>, and speech recognition ( <ref type="bibr">Chan et al., 2015a</ref>). We antici- pate that methods described in this paper can be used to similarly train smaller models in other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the different knowledge distillation approaches. In word-level knowledge distillation (left) cross-entropy</figDesc><graphic url="image-1.png" coords="3,72.14,57.82,467.73,162.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of sequence-level interpolation on an example German → English sentence: Bis 15 Tage vor Anreise sind Zimmer-Annullationen kostenlos. We run beam search, plot the final hidden state of the hypotheses using t-SNE and show the corresponding (smoothed) probabilities with contours. In the above example, the sentence that is at the top of the beam after beam search (green) is quite far away from gold (red), so we train the model on a sentence that is on the beam but had the highest sim (e.g. BLEU) to gold (purple).</figDesc><graphic url="image-2.png" coords="6,72.02,57.83,226.77,159.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>Results on English-German (newstest2014) and Thai-English (2012/2013) test sets. BLEUK=1: BLEU score with beam size K = 1 (i.e. greedy decoding); ∆K=1: BLEU gain over the baseline model without any knowledge distillation with greedy decoding; BLEUK=5: BLEU score with beam size K = 5; ∆K=5: BLEU gain over the baseline model without any knowledge distillation with beam size K = 5; PPL: perplexity on the test set; p(t = ˆ y): Probability of output sequence from greedy decoding (averaged over the test set). Params: number of parameters in the model. Best results (as measured by improvement over the baseline) within each category are highlighted in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Performance of student models with varying % of the weights pruned. Top two rows are models without any pruning. Params: number of parameters in the model; Prune %: Percent- age of weights pruned based on their absolute values; BLEU: BLEU score with beam search decoding (K = 5) after retrain- ing the pruned model; Ratio: Ratio of the number of parameters versus the original teacher model (which has 221m parameters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Number of source words translated per second across 

GPU (GeForce GTX Titan X), CPU, and smartphone (Samsung 

Galaxy 6) for the various English → German models. We were 

unable to open the 4 × 1000 model on the smartphone. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Params), due to the word embeddings which dom-
inate most of the parameters. 10 For example, on the </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> Specifically, we use the global-general attention model with the input-feeding approach. We refer the reader to the original paper for further details.</note>

			<note place="foot" n="4"> In some cases the entropy of the teacher/student distribution is increased by annealing it with a temperature term τ &gt; 1 ˜ p(y | x) ∝ p(y | x) 1 τ After testing τ ∈ {1, 1.5, 2} we found that τ = 1 worked best. Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses, L(θ; θ T ) = (1 − α)L NLL (θ) + αL KD (θ; θ T ) where α is mixture parameter combining the one-hot distribution and the teacher distribution.</note>

			<note place="foot" n="6"> While we employ a simple (unrealistic) noise function for illustrative purposes, the generative story is quite plausible if we consider a more elaborate noise function which includes additional sources of noise such as phrase reordering, replacement of words with synonyms, etc. One could view translation having two sources of variance that should be modeled separately: variance due to the source sentence (t ∼ D), and variance due to the individual translator (y ∼ (t)). 7 http://statmt.org/wmt14 8 https://sites.google.com/site/iwsltevaluation2015/mt-track</note>

			<note place="foot" n="9"> For instance, &apos;Seq-KD + Seq-Inter + Word-KD&apos; in Table 1 means that the model was trained on Seq-KD data and finetuned towards Seq-Inter data with the mixture cross-entropy loss at the word-level.</note>

			<note place="foot" n="10"> Word embeddings scale linearly while RNN parameters scale quadratically with the dimension size.</note>

			<note place="foot" n="11"> To our knowledge combining pruning and knowledge distillation has not been investigated before.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caruana2014] Lei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bucila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<title level="m">Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2015a. Listen, Attend and Spell</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01483</idno>
		<title level="m">William Chan, Nan Rosemary Ke, and Ian Laner. 2015b. Transfering Knowledge from a RNN to a DNN</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherry2014] Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressing Neural Networks with the Hashing Trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hope and Fear for Discriminative Training of Statistical Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">A R</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00810.[Courbariauxetal.2016</idno>
		<idno>arXiv:1602.02830</idno>
		<title level="m">Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Character-based Neural Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search-based Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting Parameters in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting Linear Structure within Convolutional Neural Networks for Efficient Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<editor>Krzysztof J. Geras, Abdel rahman Mohamed, Rich Caruana, Gregor Urban, Shengjie Wang, Ozlem Aslan, Matthai Philipose, Matthew Richardson, and Charles Sutton</editor>
		<meeting>ICLR Workshop<address><addrLine>Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR. [He et al.2014] Tianxing He, Yuchen Fan</title>
		<editor>Yanmin Qian, Tian Tan, and Kai Yu</editor>
		<meeting>ICLR. [He et al.2014] Tianxing He, Yuchen Fan</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings of ICASSP</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.0253</idno>
		<title level="m">Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speeding up Convolutional Neural Networks with Low Rank Expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaderberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kalchbrenner and Blunsom2013] Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jozefowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Exploring the Limits of Language Modeling</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
	<note>Proceedings of NIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Small-Size DNN with Output-Distribution-Based Criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A DiversityPromoting Objective Function for Neural Conversational Models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An End-toEnd Discriminative Approach to Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Networks with Few Multiplications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding Function in Form: Composition Character Models for Open Vocabulary Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
	</analytic>
	<monogr>
		<title level="m">Character-based Neural Machine Translation</title>
		<editor>Luong et al.2015] Minh-Thang Luong, Hieu Pham, and Christopher D. Manning</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mariet and Sra2016] Zelda Mariet and Suvrit Sra. 2016. Diversity Networks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Auto-sizing Neural Networks: With Applications to N-Gram Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04488</idno>
	</analytic>
	<monogr>
		<title level="m">Distilling Word Embeddings: An Encoding Approach</title>
		<editor>Murray and Chiang2015] Kenton Murray and David Chiang</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the Compression of Recurrent Neural Networks with an Application to LVCSR Acoustic Modeling for Embedded Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Antoine Chassang, Carlo Gatta, and Yoshua Bengio. 2015. FitNets: Hints for Thin Deep Nets</title>
		<editor>Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, and Ian McGraw</editor>
		<meeting><address><addrLine>Nicolas Ballas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<editor>Alexander M. Rush, Sumit Chopra, and Jason Weston</editor>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of AAAI</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimum Risk Training for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Video Representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS. [Vinyals and Le2015] Oriol Vinyals and Quoc Le. 2015. A Neural Conversational Model</title>
		<meeting>NIPS. [Vinyals and Le2015] Oriol Vinyals and Quoc Le. 2015. A Neural Conversational Model</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proceedings of NIPS</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Samy Bengio, and Dumitru Erhan</title>
		<imprint>
			<publisher>Alexander Toshev</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of CVPR</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<editor>Ying Cao, Xuguang Wang, Peng Li, and Wei Xu</editor>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of TACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
