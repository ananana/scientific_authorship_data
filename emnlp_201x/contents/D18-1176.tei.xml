<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Meta-Embeddings for Improved Sentence Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 1466</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">CIFAR Global Scholar</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Meta-Embeddings for Improved Sentence Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1466" to="1477"/>
							<date type="published">October 31-November 4, 2018. 2018. 1466</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While one of the first steps in many NLP systems is selecting what pre-trained word em-beddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is no exaggeration to say that word embed- dings have revolutionized NLP. From early dis- tributional semantic models <ref type="bibr" target="#b29">(Turney and Pantel, 2010;</ref><ref type="bibr">Erk, 2012;</ref><ref type="bibr">Clark, 2015)</ref> to deep learning- based word embeddings ( <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Pennington et al., 2014;</ref><ref type="bibr">Bojanowski et al., 2016)</ref>, word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field <ref type="bibr">(Goldberg, 2016)</ref>.</p><p>A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn ( <ref type="bibr">Levy and Goldberg, 2014b</ref>), evaluating their per- formance ( <ref type="bibr" target="#b13">Milajevs et al., 2014;</ref><ref type="bibr" target="#b24">Schnabel et al., 2015;</ref><ref type="bibr">Bakarov, 2017)</ref>, specializing them for cer- tain tasks <ref type="bibr" target="#b8">(Maas et al., 2011;</ref><ref type="bibr">Faruqui et al., 2014;</ref><ref type="bibr">Kiela et al., 2015;</ref><ref type="bibr" target="#b15">Mrkši´Mrkši´c et al., 2016;</ref><ref type="bibr" target="#b32">Vuli´cVuli´c and Mrkši´Mrkši´c, 2017</ref>), learning sub-word level represen- tations ( <ref type="bibr" target="#b35">Wieting et al., 2016;</ref><ref type="bibr">Bojanowski et al., 2016;</ref><ref type="bibr">Lee et al., 2016)</ref>, et cetera.</p><p>One of the first steps in designing many NLP systems is selecting what kinds of word embed- dings to use, with people often resorting to freely available pre-trained embeddings. While this is of- ten a sensible thing to do, the usefulness of word embeddings for downstream tasks tends to be hard to predict, as downstream tasks can be poorly cor- related with word-level benchmarks. An alter- native is to try to combine the strengths of dif- ferent word embeddings. Recent work in so- called "meta-embeddings", which ensembles em- bedding sets, has been gaining traction <ref type="bibr" target="#b38">(Yin and Schütze, 2015;</ref><ref type="bibr">Bollegala et al., 2017;</ref><ref type="bibr" target="#b17">Muromägi et al., 2017;</ref><ref type="bibr">Coates and Bollegala, 2018)</ref>. Meta- embeddings are usually created in a separate pre- processing step, rather than in a process that is dy- namically adapted to the task. In this work, we ex- plore the supervised learning of task-specific, dy- namic meta-embeddings, and apply the technique to sentence representations.</p><p>The proposed approach turns out to be highly effective, leading to state-of-the-art performance within the same model class on a variety of tasks, opening up new areas for exploration and yielding insights into the usage of word embeddings.</p><p>Why Is This a Good Idea? Our technique brings several important benefits to NLP applica- tions. First, it is embedding-agnostic, meaning that one of the main (and perhaps most important) hyperparameters in NLP pipelines is made obso- lete. Second, as we will show, it leads to improved performance on a variety of tasks. Third, and per- haps most importantly, it allows us to overcome common pitfalls with current systems:</p><p>• Coverage One of the main problems with NLP systems is dealing with out-of- vocabulary words: our method increases lexical coverage by allowing systems to take the union over different embeddings.</p><p>are often trained on a single domain, such as Wikipedia or newswire. With our method, embeddings from different domains can be combined, optionally while taking into ac- count contextual information.</p><p>• Multi-modality Multi-modal information has proven useful in many tasks <ref type="bibr">(Baroni, 2016;</ref><ref type="bibr">Baltrušaitis et al., 2018</ref>), yet the ques- tion of multi-modal fusion remains an open problem. Our method offers a straight- forward solution for combining information from different modalities.</p><p>• Evaluation While it is often unclear how to evaluate word embedding performance, our method allows for inspecting the weights that networks assign to different embeddings, providing a direct, task-specific, evaluation method for word embeddings.</p><p>• Interpretability and Linguistic Analysis Different word embeddings work well on dif- ferent tasks. This is well-known in the field, but knowing why this happens is less well- understood. Our method sheds light on which embeddings are preferred in which linguistic contexts, for different tasks, and allows us to speculate as to why that is the case.</p><p>Outline In what follows, we explore dynamic meta-embeddings and show that this method out- performs the naive concatenation of various word embeddings, while being more efficient. We apply the technique in a BiLSTM-max sentence encoder ( <ref type="bibr">Conneau et al., 2017)</ref> and evaluate it on well- known tasks in the field: natural language infer- ence (SNLI and MultiNLI; §4), sentiment analysis (SST; §5), and image-caption retrieval (Flickr30k; §6). In each case we show state-of-the-art per- formance within the class of single sentence en- coder models. Furthermore, we include an exten- sive analysis ( §7) to highlight the general useful- ness of our technique and to illustrate how it can lead to new insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Thanks to their widespread popularity in NLP, a sprawling literature has emerged about learn- ing and applying word embeddings-much too large to fully cover here, so we focus on previ- ous work that combines multiple embeddings for downstream tasks. <ref type="bibr" target="#b8">Maas et al. (2011)</ref> combine unsupervised em- beddings with supervised ones for sentiment clas- sification. <ref type="bibr" target="#b4">Yang et al. (2016)</ref> and <ref type="bibr" target="#b14">Miyamoto and Cho (2016)</ref> learn to combine word-level and character-level embeddings. Contextual represen- tations have been used in neural machine trans- lation as well, e.g. for learning contextual word vectors and applying them in other tasks ( <ref type="bibr" target="#b9">McCann et al., 2017)</ref> or for learning context-dependent rep- resentations to solve disambiguation problems in machine translation <ref type="bibr">Choi et al. (2016)</ref>.</p><p>Neural tensor skip-gram models learn to com- bine word, topic and context embeddings ( <ref type="bibr" target="#b3">Liu et al., 2015</ref>); context2vec ( <ref type="bibr" target="#b10">Melamud et al., 2016</ref>) learns a more sophisticated context representation separately from target embeddings; and <ref type="bibr" target="#b1">Li et al. (2016)</ref> learn word representations with distributed word representation with multi-contextual mixed embedding. Recent work in "meta-embeddings", which ensembles embedding sets, has been gain- ing traction <ref type="bibr" target="#b38">(Yin and Schütze, 2015;</ref><ref type="bibr">Bollegala et al., 2017;</ref><ref type="bibr" target="#b17">Muromägi et al., 2017;</ref><ref type="bibr">Coates and Bollegala, 2018</ref>)-here, we show that the idea can be applied in context, and to sentence represen- tations. Furthermore, these works obtain meta- embeddings as a preprocessing step, rather than learning them dynamically in a supervised set- ting, as we do here. Similarly to <ref type="bibr" target="#b21">Peters et al. (2018)</ref>, who proposed deep contextualized word representations derived from language models and which led to impressive performance on a variety of tasks, our method allows for contextualization, in this case of embedding set weights.</p><p>There has also been work on learning multi- ple embeddings per word <ref type="bibr" target="#b7">(Chen et al., 2014;</ref><ref type="bibr" target="#b18">Neelakantan et al., 2015;</ref><ref type="bibr">Vu and Parker, 2016)</ref>, includ- ing a lot of work in sense embeddings where the senses of a word have their own individual embed- dings ( <ref type="bibr">Iacobacci et al., 2015;</ref><ref type="bibr" target="#b23">Qiu et al., 2016)</ref>, as well as on how to apply such sense embeddings in downstream NLP tasks <ref type="bibr" target="#b22">(Pilehvar et al., 2017</ref>).</p><p>The question of combining multiple word em- beddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined ( <ref type="bibr">Kiela and Bottou, 2014;</ref><ref type="bibr">Lazaridou et al., 2015)</ref>, see <ref type="bibr">Baltrušaitis et al. (2018)</ref> for an overview. In multi-modal semantics, for instance, word-level embeddings from differ- ent modalities are often mixed via concatenation r = [αu, (1 − α)v] ( <ref type="bibr">Bruni et al., 2014</ref>). Here, we dynamically learn the weights to combine repre- sentations. Recently, related dynamic multi-modal fusion methods have also been explored ( <ref type="bibr">Kiros et al., 2018)</ref>. There has also been work on unifying multi-view embeddings from different data sources <ref type="bibr" target="#b7">(Luo et al., 2014)</ref>.</p><p>The usefulness of different embeddings as ini- tialization has been explored ( <ref type="bibr">Kocmi and Bojar, 2017)</ref>, and different architectures and hyperpa- rameters have been extensively examined ( <ref type="bibr">Levy et al., 2015)</ref>. Problems with evaluating word em- beddings intrinsically are well known <ref type="bibr">(Faruqui et al., 2016)</ref>, and various alternatives for evaluat- ing word embeddings in downstream tasks have been proposed (e.g., <ref type="bibr" target="#b28">Tsvetkov et al., 2015;</ref><ref type="bibr" target="#b24">Schnabel et al., 2015;</ref><ref type="bibr">Ettinger et al., 2016)</ref>. For more related work with regard to word embeddings and their evaluation, see <ref type="bibr">Bakarov (2017)</ref>.</p><p>Our work can be seen as an instance of the well- known attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>, and its recent sentence-level incarnations of self-attention ( <ref type="bibr" target="#b2">Lin et al., 2017</ref>) and inner-attention ( <ref type="bibr">Cheng et al., 2016;</ref><ref type="bibr" target="#b4">Liu et al., 2016)</ref>, where the at- tention mechanism is applied within the same sen- tence instead of for aligning multiple sentences. Here, we learn (optionally contextualized) atten- tion weights for different embedding sets and apply the technique in sentence representations ( <ref type="bibr" target="#b40">Kiros et al., 2015;</ref><ref type="bibr" target="#b34">Wieting et al., 2015;</ref><ref type="bibr">Hill et al., 2016;</ref><ref type="bibr">Conneau et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic Meta-Embeddings</head><p>Commonly, NLP systems use a single type of word embedding, e.g., <ref type="bibr">word2vec (Mikolov et al., 2013)</ref>, <ref type="bibr">GloVe (Pennington et al., 2014</ref>) or Fast- Text ( <ref type="bibr">Bojanowski et al., 2016)</ref>. We propose giving networks access to multiple types of embeddings, allowing a network to learn which embeddings it prefers by predicting a weight for each embedding type, optionally depending on the context.</p><p>For a sentence of s tokens {t j } s j=1 , we have n word embedding types, leading to sequences</p><formula xml:id="formula_0">{w i,j } s j=1 ∈ R d i (i = 1, 2, . . . , n).</formula><p>We center each type of word embedding to zero mean.</p><p>Naive baseline We compare to naive concate- nation as a baseline. Concatenation is a sensible strategy for combining different embedding sets, because it provides the sentence encoder with all of the information in the individual embeddings:</p><formula xml:id="formula_1">w CAT j = [w 1,j , w 2,j , . . . , w n,j ].</formula><p>The downside of concatenating embeddings and giving that as input to an RNN encoder, however, is that the network then quickly becomes ineffi- cient as we combine more and more embeddings. DME For dynamic meta-embeddings, we project the embeddings into a common d - dimensional space by learned linear functions w i,j = P i w i,j + b i (i = 1, 2, . . . , n) where P i ∈ R d ×d i and b i ∈ R d . We then combine the projected embeddings by taking the weighted sum</p><formula xml:id="formula_2">w DM E j = n i=1 α i,j w i,j where α i,j = g({w i,j } s j=1</formula><p>) are scalar weights from a self-attention mechanism:</p><formula xml:id="formula_3">α i,j = g(w i,j ) = φ(a · w i,j + b) (1)</formula><p>where a ∈ R d and b ∈ R are learned parame- ters and φ is a softmax (or could be a sigmoid or tanh, for gating). We also experiment with an Un- weighted variant of this approach, that just sums up the projections. CDME Alternatively, we can make the self- attention mechanism context-dependent, leading to contextualized DME (CDME):</p><formula xml:id="formula_4">α i,j = g({w i,j } s j=1 ) = φ(a · h j + b) (2)</formula><p>where h j ∈ R 2m is the j th hidden state of a BiL- STM taking {w i,j } s j=1 as input, a ∈ R 2m and b ∈ R. We set m = 2, which makes the contextu- alization very efficient.</p><p>Sentence encoder We use a standard bidi- rectional LSTM encoder with max-pooling (BiLSTM-Max), which computes two sets of s hidden states, one for each direction:</p><formula xml:id="formula_5">− → h j = −−−− → LSTM j (w 1 , w 2 , . . . , w j ) ← − h j = ← −−− − LSTM j (w j , w j+1 , . . . , w s )</formula><p>The hidden states are subsequently concatenated for each timestep to obtain the final hidden states, after which a max-pooling operation is applied over their components to get the final sentence rep- resentation: <ref type="bibr">Conneau et al., 2017)</ref> 84.5 - NSE <ref type="bibr" target="#b16">(Munkhdalai and Yu, 2017)</ref> 84.6 - G-TreeLSTM ( <ref type="bibr">Choi et al., 2017)</ref> 86.0 - SSE ( <ref type="bibr" target="#b19">Nie and Bansal, 2017)</ref> 86.1 73.6 ReSan ( <ref type="bibr" target="#b25">Shen et al., 2018)</ref> 86 </p><formula xml:id="formula_6">h = max({[ − → h j , ← − h j ]} j=1,2,...,s ) Model SNLI MNLI InferSent (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Natural Language Inference</head><p>Natural language inference, also known as recog- nizing textual entailment (RTE), is the task of clas- sifying pairs of sentences according to whether they are neutral, entailing or contradictive. In- ference about entailment and contradiction is fun- damental to understanding natural language, and there are two established datasets to evaluate se- mantic representations in that setting: SNLI (Bow- man et al., 2015) and the more recent MultiNLI ( <ref type="bibr" target="#b36">Williams et al., 2017)</ref>. The SNLI dataset consists of 570k human- generated English sentence pairs, manually la- beled for entailment, contradiction and neutral. The MultiNLI dataset can be seen as an extension of SNLI: it contains 433k sentence pairs, taken from ten different genres (e.g. fiction, government text or spoken telephone conversations), with the same entailment labeling scheme.</p><p>We train sentence encoders with dynamic meta- embeddings using two well-known and often-used embedding types: FastText ( <ref type="bibr" target="#b11">Mikolov et al., 2018;</ref><ref type="bibr">Bojanowski et al., 2016</ref>) and GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>). Specifically, we make use of the 300-dimensional embeddings trained on a simi- lar WebCrawl corpus, and compare three scenar- ios: when used individually, when naively con- catenated or in the dynamic meta-embedding set- ting (unweighted, context-independent DME and contextualized CDME). We also compare our ap- proach against other models in the same class-in this case, models that encode sentences individ- ually and do not allow attention across the two sentences. <ref type="bibr">1</ref> We include InferSent ( <ref type="bibr">Conneau et al., 2017)</ref>, which also makes use of a BiLSTM-Max sentence encoder.</p><p>In addition, we include a setting where we combine not two, but six different embedding types, adding FastText wiki-news embeddings 2 , English-German and English-French embeddings from <ref type="bibr">Hill et al. (2014)</ref>, as well as the BOW2 em- beddings from Levy and Goldberg (2014a) trained on Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>The two sentences are represented individually us- ing the sentence encoder. As is standard in the literature, the sentence representations are subse- quently combined using</p><formula xml:id="formula_7">m = [u, v, u * v, |u−v|].</formula><p>We train a two-layer classifier with rectifiers on top of the combined representation. Notice that there is no interaction (e.g., attention) between the representations of u and v for this class of model.</p><p>We use 256-dimensional embedding projec- tions, 512-dimensional BiLSTM encoders and an MLP with 1024-dimensional hidden layer in the classifier. The initial learning rate is set to 0.0004 and dropped by a factor of 0.2 when dev accuracy stops improving, dropout to 0.2, and we use Adam for optimization <ref type="bibr">(Kingma and Ba, 2014</ref>). The loss is standard cross-entropy.</p><p>For MultiNLI, which has no designated valida- tion set, we use the in-domain matched set for vali- dation and report results on the out-of-domain mis- matched set. <ref type="table">Table 1</ref> shows the results. We report accuracy scores averaged over five runs with different ran- dom seeds, together with their standard deviation, for the SNLI and MultiNLI datasets. We include two versions of the naive baseline: one with a 512- dimensional BiLSTM encoder; and a bigger one with 2048 dimensions. Both naive baseline mod- els outperform the single encoders that have only GloVe or FastText embeddings. This shows how</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model SST</head><p>Const. Tree LSTM ( <ref type="bibr" target="#b27">Tai et al., 2015)</ref> 88.0 DMN ( <ref type="bibr">Kumar et al., 2016)</ref> 88.6 DCG ( <ref type="bibr" target="#b6">Looks et al., 2017)</ref> 89.4 NSE ( <ref type="bibr" target="#b16">Munkhdalai and Yu, 2017)</ref> 89.7</p><p>GloVe BiLSTM-Max (4.1M) 88.0±.1 FastText BiLSTM-Max <ref type="bibr">(4.1M)</ref> 86.7±.3 Naive baseline <ref type="bibr">(5.4M)</ref> 88.5±.4</p><p>Unweighted DME (4.1M) 89.0±.2 DME <ref type="bibr">(4.1M)</ref> 88.7±.6 CDME <ref type="bibr">(4.1M)</ref> 89.2±.4 CDME*-Softmax (4.6M) 89.3±.5 CDME*-Sigmoid <ref type="bibr">(4.6M)</ref> 89.8±.4 including more than one embeddings can help per- formance. Next, we observe that the DME embed- dings outperform the naive concatenation base- lines, while having fewer parameters. Differences between the three DME variants are small and not significant, although we do note that we found the highest maximum performance for the contextual- ized version, which adds very few additional pa- rameters. It is important to note that the imposi- tion of weighting thus is not detrimental to perfor- mance, which means that DME and CDME pro- vide additional interpretability without sacrificing performance. Finally, we obtain results for using the six dif- ferent embedding types (marked *), and show that adding in more embeddings increases perfor- mance further. To our knowledge, these numbers constitute the state of the art within the model class of single sentence encoders on these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sentiment</head><p>To showcase the general applicability of the pro- posed approach, we also apply it to a case where we have to classify a single sentence, namely, sen- timent classification. Sentiment analysis and opin- ion mining have become important applications for NLP research. We evaluate on the binary SST task <ref type="bibr" target="#b26">(Socher et al., 2013</ref>  negative) sentiment label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>We use 256-dimensional embedding projections, 512-dimensional BiLSTM encoders and an MLP with 512-dimensional hidden layer in the clas- sifier. The initial learning rate is set to 0.0004 and dropped by a factor of 0.2 when dev accu- racy stops improving, dropout to 0.5, and we use Adam for optimization. The loss is standard cross- entropy. We calculate the mean accuracy and stan- dard deviation based on ten random seeds. <ref type="table" target="#tab_1">Table 2</ref> shows a similar pattern as we observed with NLI: the naive baseline outperforms the single-embedding encoders; the DME methods outperform the naive baseline, with the contex- tualized version appearing to work best. Finally, we experiment with replacing φ in Eq. 1 and 2 with a sigmoid gate instead of a softmax, and ob- serve improved performance on this task, outper- forming the comparable models listed in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>These results further strengthen the point that hav- ing multiple different embeddings helps, and that we can learn to combine those different embed- dings efficiently, in interpretable ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Image-Caption Retrieval</head><p>An advantage of the proposed approach is that it is inherently capable of dealing with multi-modal information. Multi-modal semantics ( <ref type="bibr">Bruni et al., 2014</ref>) often combines linguistic and visual repre- sentations via concatenation with a global weight</p><formula xml:id="formula_8">α, i.e., v = [αv ling , (1 − α)v vis ].</formula><p>In DME we in- stead learn to combine embeddings dynamically, optionally based on context. The representation for a word then becomes grounded in the visual modality, and we encode on the word-level what things look like. We evaluate this idea on the Flickr30k image- caption retrieval task: given an image, retrieve the correct caption; and vice versa. The intuition is that knowing what something looks like makes it easier to retrieve the correct image/caption. While this work was under review, a related method was published by <ref type="bibr">Kiros et al. (2018)</ref>, which takes a similar approach but evaluates its effectiveness on COCO and uses Google images. We obtain word- level visual embeddings by retrieving relevant im- ages for a given label from ImageNet in the same manner as <ref type="bibr">Kiela and Bottou (2014)</ref>, taking the im- ages' ResNet-152 features ( <ref type="bibr">He et al., 2016</ref>) and subsequently averaging those. We then learn to combine textual (FastText) and visual (ImageNet) word representations in the caption encoder used for retrieving relevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>Our loss is a max-margin rank loss as in VSE++ <ref type="bibr">(Faghri et al., 2017)</ref>, a state-of-the-art method on this task. The network architecture is al- most identical to that system, except that we use DME (with 256-dimensional embedding projec- tion) and a 1024-dimensional caption encoder. For the Flickr30k images that we do retrieval over, we use random cropping during training for data aug- mentation and use a ResNet-152 for feature ex- traction. We tune the sizes of the encoders and use a learning rate of 0.0003 and a dropout rate of 0.1. <ref type="table" target="#tab_3">Table 3</ref> shows the results, comparing against VSE++. First, note that the ImageNet-only em- beddings don't work as well as the FastText ones, which is most likely due to poorer cover- age. We observe that DME outperforms naive and FastText-only, and outperforms VSE++ by a large margin. These findings confirm the intuition that knowing what things look like (i.e., having a word- level visual representation) improves performance in visual retrieval tasks (i.e., where we need to find relevant images for phrases or sentences)- something that sounds obvious but has not really been explored before, to our knowledge. This showcases DME's usefulness for fusing embed- dings in multi-modal tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion &amp; Analysis</head><p>Aside from improved performance, an additional benefit of learning dynamic meta-embeddings is that they enable inspection of the weights that the network has learned to assign to the respective em- beddings. In this section, we perform a variety of smaller experiments in order to highlight the usefulness of the technique for studying linguis- tic phenomena, determining appropriate training domains and evaluating word embeddings. We compute the contribution of each word embedding type as follows: <ref type="figure" target="#fig_0">Figure 1</ref> shows the attention weights for a CDME model trained on SNLI, using the aforementioned six embedding sets. The sentence is from the SNLI validation set. We observe that different em- beddings are preferred for different words. The figure is meant to illustrate possibilities for analy- sis, which we turn to in the next section.</p><formula xml:id="formula_9">β i,j = α i,j w i,j 2 n k=1 α k,j w k,j 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Visualizing Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Linguistic Analysis</head><p>We perform a fine-grained analysis of the behav- ior of DME on the validation set of SNLI. <ref type="figure" target="#fig_2">Fig- ure 3</ref> shows a breakdown of the average atten- tion weights per part of speech. <ref type="figure" target="#fig_3">Figure 4</ref> shows a similar breakdown for open versus closed class. The analysis allows us to make several interest- ing observations: it appears that this model prefers GloVe embeddings, followed by the two FastText embeddings (trained on Wikipedia and Common Crawl). For open class words (e.g., nouns, verbs, adjectives and adverbs), those three embedding types are strongly preferred, while closed class words get more evenly divided attention. The em- beddings from Levy and Goldberg (2014a) get low weights, possibly because the method is comple- mentary with FastText-wiki, which was trained on a more recent version of Wikipedia.</p><p>We can further examine the attention weights by analyzing them in terms of frequency and con- creteness. We use Norvig's Google N-grams cor- pus frequency counts 3 to divide the words into fre-    <ref type="figure" target="#fig_1">Figure 2 (right)</ref> shows the average attention weights per frequency bin, ranging from low to high. We observe a clear preference for GloVe, in particular for low-frequency words. For concreteness, we use the concreteness ratings from <ref type="bibr">Brysbaert et al. (2014)</ref>. <ref type="figure" target="#fig_1">Figure 2 (left)</ref> shows the average weights per concreteness bin for a model trained on Flickr30k. We can clearly see that vi- sual embeddings get higher weights as the words become more and more concrete.</p><p>There are of course intricate relationships be- tween concreteness, frequency, POS tags and open/closed class words: closed class words are often frequent and abstract, while open class words could be more concrete, etc. It is beyond the scope of the current work to explore these further, but we hope that others will pursue this direction in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Multi-domain Embeddings</head><p>The MultiNLI dataset consists of various gen- res. This allows us to inspect the applicability of source domain data for a specific genre. We train embeddings on three kinds of data: Wikipedia, the Toronto Books Corpus ( <ref type="bibr" target="#b40">Zhu et al., 2015</ref>) and the English OpenSubtitles <ref type="bibr">4</ref> . We examine the atten- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Levy LEAR SNLI CDME 0.33 0.67 85.3±.9</p><p>Model GloVe Refined SST CDME 0.59 0.41 89.0±.4 <ref type="table">Table 4</ref>: Accuracy and learned weights on SNLI using LEAR <ref type="bibr" target="#b32">(Vuli´cVuli´c and Mrkši´Mrkši´c, 2017)</ref> or SST using sentiment-refined embeddings using the specialization method from  tion weights on the five genres in the in-domain (matched) set, consisting of fiction; transcrip- tions of spoken telephone conversations; govern- ment reports, speeches, letters and press releases; popular culture articles from the Slate Magazine archive; and travel guides. <ref type="figure" target="#fig_4">Figure 5</ref> shows the average attention weights for the three embedding types over the five gen- res. We observe that Toronto Books, which con- sists of fiction, is very appropriate for the fiction genre, while Wikipedia is highly preferred for the travel genre, perhaps because it contains a lot of factual information about geographical locations. The government genre makes more use of Open- Subtitles. The spoken telephone genre does not appear to prefer OpenSubtitles, which we might have expected given that that corpus would con- tain spoken dialogue, but Toronto books, which does include written dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Specialization</head><p>The above shows that we can use DME to ana- lyze different embeddings on a task. Given the recent interest in the community in specializing, retro-fitting and counter-fitting word embeddings for given tasks, we examine whether the lexical- level benefits of specialization extend to sentence- level downstream tasks. After all, one of the main motivations behind work on lexical entail- ment is that it allows for better downstream tex- tual entailment. Hence, we take the LEAR embed- dings by <ref type="bibr" target="#b32">Vuli´cVuli´c and Mrkši´Mrkši´c (2017)</ref>, which do very well on the HyperLex lexical entailment evalua- tion dataset ). We compare their best-performing embeddings against the original embeddings that were used for specialization, de- rived from the BOW2 embeddings of <ref type="bibr">Levy and Goldberg (2014a)</ref>. Similarly, we use the technique of  for refining GloVe embeddings for sentiment, and evaluate model performance on the SST task. <ref type="table">Table 4</ref> shows that LEAR embeddings get high weights compared to the original source embeddings ("Levy" in the table). Our analy- sis showed that LEAR was particularly favored for verbs (with average weights of 0.75). The sentiment-refined embeddings were less useful, with the original GloVe embeddings receiving higher weights. These preliminary experiments show how DME models can be used for analyz- ing the performance of specialized embeddings in downstream tasks.</p><p>Note that different weighting mechanisms might give different results-we found that the normalization strategy and the depth of the net- work significantly influenced weight assignments in our experiments with specialized embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Examining Contextualization</head><p>We examined models trained on SNLI and looked at the variance of the attention weights per word in the dev set. If contextualization is important for getting the classification decision correct, then we would expect big differences in the attention weights per word depending on the context. Upon examination, we only found relatively few differ- ences. In part, this may be explained by the small size of the dev set, but for the Glove+FastText model we inspected there were only around twenty words with any variance at all, which suggests that the field needs to work on more difficult seman- tic benchmark tasks. The words, however, where characterized by their polysemy, in particular by having both noun and verb senses. The follow- ing words were all in the top 20 most context- dependent words: mob, boards, winds, trains, pitches, camp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We argue that the decision of which word em- beddings to use in what setting should be left to the neural network. While people usually pick one type of word embeddings for their NLP sys- tems and then stick with it, we find that dynami- cally learned meta-embeddings lead to improved results. In addition, we showed that the pro- posed mechanism leads to better interpretability and insightful linguistic analysis. We showed that the network learns to select different embeddings for different data, different domains and different tasks. We also investigated embedding specializa- tion and examined more closely whether contextu- alization helps. To our knowledge, this work con- stitutes the first effort to incorporate multi-modal information on the language side of image-caption retrieval models; and the first attempt at incorpo- rating meta-embeddings into large-scale sentence- level NLP tasks.</p><p>In future work, it would be interesting to ap- ply this idea to different tasks, in order to explore what kinds of embeddings are most useful for core NLP tasks, such as tagging, chunking, named en- tity recognition, parsing and generation. It would also be interesting to further examine specializa- tion and how it transfers to downstream tasks. Us- ing this method for evaluating word embeddings in general, and how they relate to sentence repre- sentations in particular, seems a fruitful direction for further exploration. In addition, it would be interesting to explore how the attention weights change during training, and if, e.g., introducing entropy regularization (or even negative entropy) might improve results or interpretability further. <ref type="bibr">Joshua Coates and Danushka Bollegala. 2018</ref>. Frus- tratingly easy meta-embedding-computing meta- embeddings by averaging source word embeddings. In Proceedings of NAACL-HLT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ronan</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example visualization of a sentence from the SNLI dev set.</figDesc><graphic url="image-3.png" coords="7,300.14,202.25,218.27,110.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Concreteness weights (left) for Flickr30k model and Frequency weights (right) for SNLI model with multiple embeddings. Visual ImageNet embeddings are preferred for more concrete words. GloVe is strongly preferred for low-frequency words.</figDesc><graphic url="image-2.png" coords="7,79.14,202.40,218.26,110.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pos tags and weights by embedding type.</figDesc><graphic url="image-4.png" coords="7,72.00,380.75,218.27,216.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Weights for open/closed class words.</figDesc><graphic url="image-5.png" coords="7,307.28,380.75,218.27,91.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Multi-domain weights on MultiNLI.</figDesc><graphic url="image-6.png" coords="8,72.00,62.81,218.26,143.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sentiment classification accuracy results on 
the binary SST task. For DCG we compare against 
their best single sentence model (Looks et al., 2017). 
*=multiple different embedding sets (see Section 4). 
Number of parameters included in parenthesis. Results 
averaged over ten runs with different random seeds. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Image and caption retrieval results (R@1 
and R@10) on Flickr30k dataset, compared to VSE++ 
baseline (Faghri et al., 2017). VSE++ numbers in the 
table are with ResNet features and random cropping, 
but no fine-tuning. Number of parameters included in 
parenthesis; averaged over five runs with std omitted 
for brevity. 

</table></figure>

			<note place="foot" n="1"> This is a common distinction, see e.g. the SNLI leaderboard at https://nlp.stanford.edu/projects/ snli/. 2 See https://fasttext.cc/</note>

			<note place="foot" n="3"> http://norvig.com/mayzner.html</note>

			<note place="foot" n="4"> http://opus.nlpl.eu/OpenSubtitles.php</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their com-ments. We also thank Marcus Rohrbach, Lau-rens van der Maaten, Ivan Vuli´cVuli´c, Edouard Grave, Tomas Mikolov and Maximilian Nickel for help-ful suggestions and discussions with regard to this work.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning distributed word representation with multi-contextual mixed embedding. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Abdul Masud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">Zhexue</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="220" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning context-sensitive word embeddings with neural tensor skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1284" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional LSTM model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1605.09090</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Looks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Herreshoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delesley</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02181</idno>
		<title level="m">Deep learning with dynamic computation graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pre-trained multi-view word embedding using two-side neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning generic context embedding with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Evaluating neural word representations in tensor-based compositional settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6179</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Miyamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01700</idno>
		<title level="m">Gated word-character recurrent language model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural semantic encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">397</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linear ensembles of word embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avo</forename><surname>Muromägi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kairit</forename><surname>Sirts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Laur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Nordic Conference on Computational Linguistics</title>
		<meeting>the 21st Nordic Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06654</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Shortcutstacked sentence encoders for multi-domain inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards a seamless integration of word senses into downstream NLP applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06632</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextdependent sense embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2049" to="2054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2016. k-embeddings: Learning conceptual embeddings for words using context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D Stott</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="1262" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperlex: A large-scale evaluation of graded lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="781" to="835" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Specialising word vectors for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06371</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning multimodal word representation via dynamic fusion methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00532</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02789</idno>
		<title level="m">Charagram: Embedding words and sentences via character n-grams</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01724</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning word meta-embeddings by using ensembles of embedding sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04257</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Refining word embeddings for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
