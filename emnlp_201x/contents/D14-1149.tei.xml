<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Clusters of Specialist Terms from Unstructured Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Gerow</surname></persName>
							<email>gerow@uchicago.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Computation Institute University of Chicago Chicago</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Clusters of Specialist Terms from Unstructured Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1426" to="1434"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language. This paper develops a corpus-based method of extracting coherent clusters of satellite terminology-terms on the edge of the lexicon-using co-occurrence networks of unstruc-tured text. Term clusters are identified by extracting communities in the co-occurrence graph, after which the largest is discarded and the remaining words are ranked by centrality within a community. The method is tractable on large corpora, requires no document structure and minimal normalization. The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size, content and structure. The findings also confirm that language consists of a densely connected core (ob-served in dictionaries) and systematic, semantically coherent groups of terms at the edges of the lexicon.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language consists of a number of rela- tional structures, many of which can be obscured by lexical idiosyncrasies, regional variation and domain-specific conventions. Despite this, pat- terns of word use exhibit loose semantic struc- ture, namely that proximate words tend to be re- lated. This distributional hypothesis has been op- erationalized in a variety of ways, providing in- sights and solutions into practical and theoretical questions about meaning, intention and the use of language. Distributional analyses rely primarily on observing natural language to build statistical representations of words, phrases and documents <ref type="bibr" target="#b30">(Turney and Pantel, 2010)</ref>. By studying dictio- naries and thesauri, lexicographic and terminolog- ical research has proposed that a core lexicon is used to define the remaining portions of vocabu- lary <ref type="bibr" target="#b14">(Itô and Mester, 1995;</ref><ref type="bibr" target="#b16">Massé et al., 2008)</ref>. Though many words that comprise general lan- guage use reside in this core lexicon, even the most general language contains specialist or so- called "satellite" words. This paper introduces a method of extracting this peripheral structure, with co-occurrence networks of unstructured text.</p><p>The core-periphery structure has been observed in dictionaries where definitions tend to use a re- stricted vocabulary, repetitively employing a core set of words to define others <ref type="bibr" target="#b27">(Sinclair, 1996;</ref><ref type="bibr" target="#b22">Picard et al., 2013</ref>). In the farther regions of the lex- icon, it is more difficult to find systematic seman- tic definition with corpus-based techniques due to the overwhelming number of infrequent words. Unfortunately, the fringe of the lexicon can be more important than the core because this is where domain-specific terminology resides -features that may be more important than frequent.</p><p>Examining dictionaries, ( <ref type="bibr" target="#b22">Picard et al., 2013)</ref> propose that the lexicon consists of four main parts: a core set of ubiquitous words used to de- fine other words, a kernel that makes up most of the lexicon, a minimal grounding set that includes most of the core and some of the kernel, leav- ing a set of satellites in the periphery. This to- pography, reproduced in <ref type="figure" target="#fig_0">Figure 1</ref>, has been found in the way dictionary entries use words to define one another. In networks of dictionary defini- tions, the core component tends to form a strongly connected component (SCC) leaving satellites in smaller SCCs with relatively weak links to the core. This paper explores whether these these satellites form systematic, cohesive groups and whether they are observable in natural language. Words with relatively specific definitions within subjects, referred to as terms in lexicographic re- search, are apparent in nearly all domains of dis- course. Here, the goal is to explore structure among these peripheral terms without a dictio- nary. To do this, a method based on commu- nity detection in textual co-occurrence networks is developed. Such graph-based methods have be- come increasingly popular in a range of language- related tasks such as word clustering, document clustering, semantic memory, anaphora resolution and dependency parsing (see Mihalcea and Radev, 2011 for a review). This paper seeks to address two important ques- tions about the observed landscape of the lexicon in natural language: to investigate whether satel- lite clusters found in dictionaries can be observed in text, and more importantly, to explore whether statistical information in co-occurrence networks can elucidate this peripheral structure in the lexi- con. We frame these questions as the task of ex- tracting clusters of related terms. If satellites are systematically organized, then we can expect to find cohesive clusters in this region. Moreover, if the networked structure of dictionary entries sup- ports the landscape in <ref type="figure" target="#fig_0">Figure 1</ref>, a similar structure may be present in co-occurrence patterns in natu- ral text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Word clustering, as a means to explore underly- ing lexical structure, should accommodate fuzzy and potentially contradictory notions of similarity. For example, red and green are at once similar, being colors, but as colors, they are very differ- ent. Alternatively, the words car, fast, wheel, ex- port and motorists share a thematic similarity in their relation to automobiles. One conception of word clustering is to construct a thesaurus of syn- onyms ( <ref type="bibr" target="#b3">Calvo et al., 2005</ref>), but clustering could allow other lexical semantic relationships. One such database, WordNet, defines specific seman- tic relationships and has been used to group words according to explicit measures of relatedness and similarity <ref type="bibr" target="#b18">(Miller, 1995;</ref><ref type="bibr" target="#b21">Pedersen et al., 2004</ref>). Distributional, corpus-based techniques that de- fine words as feature vectors (eg. word-document co-occurrences), can address many limitations of manually created lexicons (see <ref type="bibr" target="#b30">Turney et al., 2010</ref> for a review). Clustering nouns by argument struc- ture can uncover naturally related objects <ref type="bibr" target="#b12">(Hindle, 1990</ref>) and spectral methods can relate dis- tinct classes of nouns with certain kinds of verbs to induce selectional preferences <ref type="bibr" target="#b23">(Resnik, 1997;</ref><ref type="bibr" target="#b29">Sun and Korhonen, 2009;</ref><ref type="bibr" target="#b32">Wilks, 1975)</ref> and assist metaphor processing ( <ref type="bibr" target="#b26">Shutova et al., 2013)</ref>.</p><p>A pervasive weakness of many existing ap- proaches to word-clustering, is an underlying pri- oritization of frequent words. To help address this sparsity, many models collapse words into stems, preclude uncommon words, or underesti- mate the relevance of infrequent words <ref type="bibr" target="#b7">(Dagan et al., 1999)</ref>. Probabilistic topic models have emerged as a uniquely flexible kind of word- clustering used in content analysis <ref type="bibr" target="#b28">(Steyvers and Griffiths, 2007</ref>), text classification ( <ref type="bibr" target="#b31">Wei and Croft, 2006</ref>) and provide an extensible framework to ad- dress other tasks <ref type="bibr" target="#b1">(Blei, 2012)</ref>. Because the struc- ture of satellite terms is not likely to rely on spe- cific (much less consistent) lexical semantic re- lationships, we adopt a measure of semantic co- herence, commonly used to qualify the results of topic models, as an indirect measure of what peo- ple tend to view as a cohesive set of words. This measure, which is defined in the next section, is particularly attractive because it is corpus-based, does not assume any specific semantic relationship and correlates with expert evaluations <ref type="bibr" target="#b19">(Mimno et al., 2011;</ref><ref type="bibr" target="#b20">Newman et al., 2010)</ref>. Using semantic coherence provides a way of measuring the qual-ity of word-associations without appeal to a dic- tionary or assuming rigid relationships among the clustered words.</p><p>The first step is to construct a co-occurrence graph from which communities are extracted. Then the centrality of each word is computed within a community to generate cluster-specific rankings. The goal is not to categorize words into classes, nor to provide partitions that sepa- rate associated words across a corpus. Instead, the method is designed to extract qualifiable sets of specialist terms found in arbitrary text. Crucially, the method is designed to require no document structure and minimal pre-processing: stop-words and non-words are not removed and no phrasal, sentence or document structure is required. Al- though stemming or lemmatization could pro- vide more stream-lined interpretations, the mini- mal pre-processing allows the method to operate efficiently on large amounts of unstructured text of any language.</p><p>Co-occurrence networks have been used in a variety of NLP applications, the basic idea be- ing to construct a graph where proximate words are connected. Typically, words are connected if they are observed in an n-word window. We set this window to a symmetric 7 words on ei- ther side of the target and did not use any weight- ing 1 . In the resulting network, edge frequen- cies are set to the number of times the given co- occurrence is observed. The resulting networks are typically quite dense and exhibit small-world structure where most word-pairs are only a few edges apart ( <ref type="bibr" target="#b0">Baronchelli et al., 2013;</ref><ref type="bibr" target="#b9">Ferror i Cancho and Solé, 2001</ref>). To explore the effect of this density, different minimum node-and edge- frequencies were tested (analogous to the word- and co-occurrence frequencies in text). It was found that not setting any thresholds provided the best results (see <ref type="figure" target="#fig_1">Figure 2</ref>), supporting our minimal pre-processing approach.</p><p>To extract clusters from the co-occurrence ma- trix, the Infomap community detection algorithm was used. Infomap is an information-theoretic method that optimizes a compression dictionary using it to describe flow through connected nodes <ref type="bibr" target="#b24">(Rosvall and Bergstrom, 2008)</ref>. By minimizing a description of this flow, the algorithm can also ex- tract nested communities (Rosvall and Bergstrom,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Docs <ref type="table">Tokens Nodes  Edges  TASA  38,972  10.7M 58,357 1,319,534  NIPS  3,742  5.2M 28,936 1,612,659  enTenTen 92,327</ref> 72.2M 69,745 7,721,413 <ref type="table">Table 1</ref>: Co-occurrence networks of each corpus.</p><p>2011). In our experiments, we used the co- occurrence frequencies as edge-weights and ran 50 trials for each run of the algorithm. Co- occurrence networks tended to form one mono- lithic community, corresponding to the lexicon's core SCC, surrounded by a number of smaller communities. The monolithic community is dis- carded out-right, as it represents the core of the lexicon where few specialist terms reside. As we will see, the community detection algorithm nat- urally identifies this SCC, distinguishing satellite clusters of terminology. Though we do not explore its effect, the sensitivity of Infomap can be tuned to vary the relative size of the core SCC compared to the satellites, effectively allowing less modular communities to be considered satellites.</p><p>To compare and interpret the resulting clus- ters, various measures of centrality were tested for ranking words within their communities. The goal of this ranking is to find words that typify or de- fine their community without assuming its under- lying semantics. The results in the next section show that a number of common centrality mea- sures work comparably well for this task. The fi- nal output of the system is a set of communities, in which words are ranked by their centrality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results &amp; Analysis</head><p>Three corpora were used for evaluation: the TASA, NIPS and enTenTen collections. TASA consists of paragraph-length excerpts from high- school level, American English texts <ref type="bibr" target="#b15">(Landauer and Dumais, 1997</ref>). The NIPS collection contains 17 volumes of annual proceedings from the con- ference of the same name. The enTenTen corpus is a web-based collection of text-heavy, English web-sites. <ref type="table">Table 1</ref> summarizes the collections and their co-occurrence networks.</p><p>The extracted communities, which consist of word-centrality pairs, are similarly structured to the output of topic models. Because appeals to human judgement are expensive and can introduce issues of consistency ( <ref type="bibr" target="#b4">Chang et al., 2009;</ref><ref type="bibr" target="#b13">Hu et al., 2011</ref>), a corpus-based measure of semantic coher- ence has been proposed <ref type="bibr" target="#b19">(Mimno et al., 2011</ref>). Co-herence is used as a proxy for human judgments. A general form of semantic coherence can be de- fined as the mean pair-wise similarity over the top n words in a topic or cluster t</p><formula xml:id="formula_0">C(t) = 1 n n (w i ,w j )∈t i&lt;j S(w i , w j )</formula><p>where S is a symmetric measure of similarity. Newman, et al. <ref type="formula">(2010)</ref> surveyed a number of simi- larity metrics and found that mean point-wise mu- tual information (PMI) correlated best to human judgements. PMI is a commonly used measure of how much more information co-occurring words convey together compared to their independent contributions <ref type="bibr" target="#b5">(Church and Hanks, 1990;</ref><ref type="bibr" target="#b2">Bouma, 2009)</ref>. Using PMI as S, we can define a version of coherence, known as UCI Coherence:</p><formula xml:id="formula_1">C U CI (t) = 1 n n (w i ,w j )∈t i&lt;j log p(w i , w j ) p(w i )p(w j )</formula><p>where p(w) is estimated as relative frequency in a corpus: f (w) i f (w i ) . Using coherence to optimize topic models, <ref type="bibr" target="#b19">Mimno et al. (2011)</ref> found that a simplified measure, termed UMass Coherence, is more strongly correlated to human judgments than C U CI . For topic t, C U M ass is defined as follows:</p><formula xml:id="formula_2">C U M ass (t) = 1 n n (w i ,w j )∈t i&lt;j log D(w i , w j ) + 1 D(w j )</formula><p>where D(w) is the number of documents con- taining w, and D(w, w ) is the number of doc- uments containing both w and w . Note that D relies crucially on document segmentation in the reference corpus, which is not encoded in the co- occurrence networks derived by the method de- scribed above. Thus, though the networks be- ing analyzed and the coherence scores are both based on co-occurrence information, they are dis- tinct from one another. Following convention, we compute coherence for the top 10 words in a given community. C U M ass was used as the mea- sure of semantic coherence. and D was computed over the TASA corpus, which means the resulting scores are not directly comparable to <ref type="bibr" target="#b19">(Mimno et al., 2011</ref>), though comparisons to other published results are provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ranking Functions &amp; Frequency Thresholds</head><p>After communities are extracted from the co- occurrence graph, words are ranked by their cen- trality in a community. Six centrality measures were tested as ranking functions: degree centrality, closeness centrality, eigenvector centrality, Page- rank, hub-score and authority-score ( <ref type="bibr" target="#b10">Friedl et al., 2010)</ref>. Degree centrality uses a node's degree as its centrality under the assumption that highly connected nodes are central. Closeness centrality measures the average distance between a node and all other nodes, promoting nodes that are "close" to the rest of the network. Eigenvector centrality favors well-connected nodes that are themselves connected to well-connected nodes. Pagerank is similar to eigenvector centrality, but also promotes nodes that mediate connections between strongly connected nodes. Hub and authority scores mea- sure interconnectedness (hubs) and connectedness to interconnected nodes (authorities). <ref type="figure" target="#fig_1">Figure 2</ref> shows the average coherence, across all commu- nities extracted from the TASA corpus, for each centrality measure. The average coherence scores are highest using hub-score, though not signifi- cantly better than auth-score, eigenvector central- ity or closeness centrality. In the results that fol- low, hub-scores were used to rank nodes within communities. Imposing minimum node and edge frequencies in the co-occurrence graph was also tested. How- ever, applying no thresholds provided the high- est average coherence. <ref type="figure" target="#fig_2">Figure 3</ref> shows the aver- age coherence for eight threshold configurations. Though we used the TASA corpus for these tests, we have no reason to believe the results would dif- fer significantly for the other corpora.  <ref type="table">Table 2</ref> shows three communities of specialist terms extracted from each text collection, with their normalized hub-scores. Normalizing the scores preserves their rank-ordering and provides an indication of relative centrality within the com- munity itself. For example, compare the first and last words from the top TASA and NIPS clusters: the difference between thou and craven (TASA) is considerably more than model and net- work (NIPS). In general, higher ranked words ap- pear to typify their communities, with words like model, university and nuclear in the NIPS ex- amples. These clusters are typical of those pro- duced by the method, though in some cases, the communities contain less than 10 terms and were not included in the coherence analysis. Note that these clusters are not systematic in any lexical se- mantic sense, though in almost every case there are discernible thematic relations (middle-English words, Latin America and seafood in TASA).  <ref type="table">Table 2</ref>: Sample clusters from the TASA, NIPS and enTenTen collections. Shown are the clusters' top ten words, ranked by their normalized hub- score within the community. Note the differences in hub-score distributions between clusters. <ref type="figure">Figure 4</ref> shows the average coherence for our method, compared to that of a 20-topic latent Dirichlet allocation (LDA) model fit to the same corpora. Results from an LDA model fit to our corpora, as well as from a sample of published topics, are provided as a baseline to calibrate read- ers' intuitions about coherence 2 . Although topics from LDA do not necessarily consist of special- ist terms those in the current model, the expec- tation of coherence remains: probable or central words should comprise a cohesive group. In every case, coherence is calculated over the top 10 words ranked using within-community hub-scores, for every community of 10 or more words. The results show that LDA provides relatively consistent co- herence across collections, though with generally more variance than the communities of specialist terms. The term clusters are more coherent for the enTenTen collection than the others, which may be due to its larger size. This up-tick on the largest corpus may have to do with the proportional size of the monolithic community for the less struc- tured documents in enTenTen. <ref type="figure" target="#fig_3">Figure 5</ref> depicts how the proportional size of the core would effect the number and size of satellite clusters. It was found that the largest community (the core SCC) comprised 95% of TASA, 90% of NIPS and 97% of enTenTen. It may be that specialized language will have a proportionally smaller core and more satellite communities, whereas more general lan- guage will have a larger core and fewer satellites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Community Coherence</head><p>A critical question remains as to whether the method is actually observing the core-periphery structure of the lexicon or if it is an artifact. To test this, the frequencies of words in satellite com- munities were compared to those in the monolithic cases. If the monolithic community does indeed correspond to the core proposed in <ref type="figure" target="#fig_0">Figure 1</ref>, words in the satellites should have significantly lower fre- quencies. Indeed, the monolithic community in every corpus contained words that were signifi- cantly more frequent than those in the communi- ties (Wilcoxon rank-sum test; <ref type="table">Table 3</ref>). Taken with <ref type="figure">Figure 4</ref>: Mean coherence (C U M ass ) for satellite clusters and topics from LDA on the TASA, NIPS and enTenTen collections (top). Also shown are the mean coherence of topics found in published models (LDA, a dynamic topic model, DTM and a correlated topic model, CTM; bottom). Error-bars are ±2 SE of the mean. the coherence scores, these results show that there is coherent structure in the periphery of the lexi- con, that can be extracted from unstructured text.   <ref type="table">Table 3</ref>: Comparison of frequency for core words, f c , found in the monolithic community and spe- cialist terms, f s , found in the satellite communities (Wilcoxon rank-sum test). All differences were significant at p &lt; 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>The results of our method show that outlying structure in the lexicon can be extracted directly from large collections of unstructured text. The lexicon's topography, previously explored in dic- tionary studies, contains modular groups of satel- lite terms that are observable without appeal to ex- ternal resources or document structure and with minimal normalization. The contribution of this method is two-fold: it confirms the structure of the observed lexicon is similar to that apparent in the organization of dictionaries ( <ref type="bibr" target="#b22">Picard et al., 2013</ref>). Second, it offers a tractable, reliable means of ex- tracting and summarizing structure in the fringes of the lexicon.</p><p>The output of the model developed here is sim- ilar to topic models, but with some important differences. Topic models produce a probability distribution over words to define a topic, which can be summarized by the top 10 to 20 most likely words. Instead of probabilities, the within-community hub-scores were used to rank words in each cluster. This means that the actual structure of the community (to which topics have no ana- logue) is responsible for producing the scores that rate words' internal relevance. Another crucial difference is that topic size from a single sampling iteration tends to correlate with coherence <ref type="bibr" target="#b19">(Mimno et al., 2011</ref>), but in the current method, there is no correlation between cluster size and coherence (p = 0.98). The other important difference is that whereas topic models produce a topic-document mixture that can be used for posterior inference, to perform such inference with our method, the out- put would have to be used indirectly.</p><p>One understated strength of the community detection method is the minimal required pre- processing. Whereas many solutions in NLP (including topic models) require document seg- mentation, lexical normalization and statistical normalizations on the co-occurrence matrix it- self, the only variable in our method is the co- occurrence window size. However, lemmatiza- tion (or stemming) could help collapse morpho- syntactic variation among terms in the results, but stop-word removal, sentence segmentation and TF-IDF weighting appear unnecessary. What might be most surprising given the examples in <ref type="table">Ta- ble 2</ref> is that word-document occurrence informa- tion is not used at all. This makes the the method particularly useful for large collections with little to no structure.</p><p>One question overlooked in our analysis con- cerns the effect the core has on the satellites. It could be that the proportional size of a collection's core is indicative of the degree of specialist ter- minology contained in the collection. Also, the raw number of satellite communities might indi- cate the level of diversity in a corpus. Addressing these questions could yield measures of previously vague and latent variables like specialty or topi- cal diversity, without employing a direct semantic analysis. By measuring a collection's core size, relative to its satellites, one could also use mea- sure changes in specialization. The Infomap algo- rithm could accommodate such an experiment: by varying the threshold of density that constitutes a community, the core could be made smaller, yield- ing more satellites, the coherence of which could be compared to those reported here. One could ex- amine the position of individual words in the satel- lite(s) to explore what features signal important, emerging and dying terms or to track diachronic movement of terms like computer or gene from the specialized periphery to core of the lexicon.</p><p>At the level of inter-related term clusters, there are likely important or central groups that influ- ence other satellites. There is no agreed upon mea- sure of "community centrality" in a network sense <ref type="bibr" target="#b8">(Eaton and Mansbach, 2012)</ref>. One way to measure the importance of a community would be to use significance testing on the internal link mass com- pared to the external ( <ref type="bibr" target="#b6">Csardi and Nepusz, 2006</ref>). However, this approach discards some factors for which one might want to account, such as cen- trality in the network of communities and their composition. Future work could seek to com- bine graph-theoretic notions of centrality and in- tuitions about the defining features of term clus- ters. Another avenue for future research would be to use mixed membership community detection <ref type="bibr" target="#b11">(Gopalan and Blei, 2013)</ref>. Allowing terms to be represented in more than one community would accommodate words like nuclear, that might be found relating to weaponry, energy production and physics research at the same time. Using co- occurrence networks to extract clusters of special- ist terms, though an important task, is perhaps only a starting point for exploring the observed lexicon. Network-based analysis of language offers a gen- eral and powerful potential to address a range of questions about the lexicon, other NLP tasks and language more generally.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dictionary studies have proposed that the lexicon consists of a strongly connected core, around which there is a kernel, an asymmetric grounding set and satellites. Adapted from (Picard et al., 2013).</figDesc><graphic url="image-1.png" coords="2,77.32,62.80,207.64,205.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean coherence for six centrality measures. Error-bars are ±2 SE of the mean.</figDesc><graphic url="image-2.png" coords="4,308.84,482.57,215.15,177.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean coherence for different minimum node and edge frequencies, corresponding to thresholds for word and co-occurrence counts. Error-bars are ±2 SE of the mean.</figDesc><graphic url="image-3.png" coords="5,72.88,182.93,216.51,188.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A proportionally larger core SCC (right) would force satellite communities to be smaller, less numerous and more isolated. Alternatively, with a small core (left), satellite communities would be more numerous and prominent.</figDesc><graphic url="image-5.png" coords="6,319.10,114.34,194.62,106.68" type="bitmap" /></figure>

			<note place="foot" n="1"> 7 was found to be the optimal window-size in terms of coherence. These preliminary results are available at knowledgelab.org/docs/coherent clusters-data.xls.</note>

			<note place="foot" n="2"> Coherence was computed for the published results with CUMass using TASA as the reference corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Thanks to E. Duede and three reviewers for com-ments on earlier versions of this manuscript. This work was supported by a grant from the Templeton Foundation to the Metaknowledge Research Net-work and grant #1158803 from the National Sci-ence Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Networks in cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Baronchelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Ferrer I Cancho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romualdo</forename><surname>Pastor-Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Chater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="348" to="360" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Normalized (pointwise) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Society for Computational Linguistics &amp; Language Technology</title>
		<meeting>the German Society for Computational Linguistics &amp; Language Technology</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional thesaurus versus wordnet: A comparison of backoff techniques for unsupervised pp attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiram</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The igraph software package for complex network research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Csardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Nepusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">1695</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity-based models of word cooccurrence probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="43" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A spin-glass model for semi-supervised community detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachael</forename><surname>Mansbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The small world of human language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Ferror I Cancho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">V</forename><surname>Solé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Royal Society of London. Series B: Biological Sciences</title>
		<meeting>the Royal Society of London. Series B: Biological Sciences</meeting>
		<imprint>
			<date type="published" when="1482" />
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="2261" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A critical review of centrality measures in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipl-Math Bettina</forename><surname>Friedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Heidemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Business &amp; Information Systems Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="371" to="385" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient discovery of overlapping communities in massive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="14534" to="14539" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Noun classification from predicate-argument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Hindle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 28th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="268" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="248" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The core-periphery structure of the lexicon and constraints on reranking. University of Massachusetts occasional papers in linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junko</forename><surname>Itô</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Mester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How is meaning grounded in dictionary definitions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Blondin</forename><surname>Massé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Chicoisne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Gargouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odile</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing</title>
		<meeting>the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graphbased natural language processing and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet::similarity: measuring the relatedness of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Demonstration Papers at HLT-NAACL 2004</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="38" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hidden structure and function in the lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mélanie</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Blondinmassé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odile</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCS 2013: 10th International Workshop on Natural Language Processing and Cognitive Science</title>
		<meeting><address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selectional preference and sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How</title>
		<meeting>the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maps of random walks on complex networks reveal community structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1118" to="1123" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">T</forename><surname>Bergstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18209</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Statistical metaphor processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="353" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The empty lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sinclair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Corpus Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="119" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probabilistic topic models. Handbook of latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="424" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving verb clustering with automatically acquired selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="638" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lda-based document models for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A preferential, pattern-seeking, semantics for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="74" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
