<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S2SPMN:A Simple and Effective Framework for Response Generation with Relevant Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Wuhan University Hubei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University Hubei</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S2SPMN:A Simple and Effective Framework for Response Generation with Relevant Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="745" to="750"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>745</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>How to generate relevant and informative responses is one of the core topics in response generation area. Following the task formulation of machine translation, previous works mainly consider response generation task as a mapping from a source sentence to a target sentence. To realize this mapping, existing works tend to design intuitive but complex models. However, the relevant information existed in large dialogue corpus is mainly overlooked. In this paper, we propose Sequence to Sequence with Prototype Memory Network (S2SPMN) to exploit the relevant information provided by the large dialogue corpus to enhance response generation. Specifically, we devise two simple approaches in S2SPMN to select the relevant information (named prototypes) from the dialogue corpus. These prototypes are then saved into prototype memory network (PMN). Furthermore, a hierarchical attention mechanism is devised to extract the semantic information from the PMN to assist the response generation process. Empirical studies indicate the advantage of our model over several classical and strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue systems, or say, chatbots are usually con- sidered as the future of human-computer interac- tion and extensive works have been done in this area ( <ref type="bibr" target="#b22">Wen et al., 2016;</ref><ref type="bibr" target="#b15">Qiu et al., 2017;</ref><ref type="bibr" target="#b21">Wen et al., 2017;</ref><ref type="bibr" target="#b6">Kreyssig et al., 2018)</ref>.</p><p>As one of the main approaches for dialogue system design, response generation has attracted more and more attention from research commu- nity. Neural networks based models like Seq2Seq architecture <ref type="bibr" target="#b19">(Vinyals and Le, 2015;</ref><ref type="bibr" target="#b17">Shang et al., 2015</ref>) are proven to be effective to generate valid responses for a dialogue system. However, as re- vealed in many previous works ( <ref type="bibr" target="#b8">Li et al., 2016a</ref>; * *Chenliang Li is the Corresponding Author <ref type="bibr" target="#b23">Wu et al., 2018)</ref>, "safe reply" is still an open prob- lem and lots of efforts are made to generate more informative responses ( <ref type="bibr" target="#b8">Li et al., 2016a;</ref><ref type="bibr" target="#b13">Mou et al., 2016;</ref><ref type="bibr" target="#b9">Li et al., 2016b;</ref><ref type="bibr" target="#b15">Qiu et al., 2017;</ref><ref type="bibr" target="#b2">He et al., 2017;</ref>.</p><p>Note that in this paper when we say response generation, we focus on single turn chit-chat for that other tasks like multi-turn ( <ref type="bibr" target="#b26">Zhang et al., 2018)</ref> or goal-oriented ( )genera- tion could be partly considered as the extensions of single-turn generation.</p><p>Though existing works mentioned above are helpful in some ways, they all follow the task for- mulation proposed by <ref type="bibr" target="#b16">(Ritter et al., 2011</ref>), which considers response generation (RG) task as a map- ping from a source sentence to a target sentence like machine translation (MT). This task formu- lation ignores the natural difference between MT and RG: MT deals with sentence pairs of the same meanings while RG needs to realize the meaning transformation from a source post to the target re- sponse. In this sense, the meaning transformation is more difficult than machine translation. Hence, many researchers have designed more and more complex models. However, given a target post, the relevant information covered by the dialogue cor- pus is usually overlooked. It is intuitive that the responses for a similar post would provide more contextual information to guide the response gen- eration. To this end, we are interested in exploit- ing the relevant responses in the training set as soft prototypes to assist the response generation.</p><p>Specifically, in this paper, we propose Se- quence to Sequence with Prototype Memory Net- work (named S2SPMN). We introduce two Pro- totype Memory Networks (PMNs) to store the rel- evant responses extracted from the dialogue cor- pus: static PMN and dynamic PMN. Tested on a widely used benchmark dataset, the proposed  S2SPMN produces more informative responses than the standard and strong baselines. To the best of our knowledge, it is the first work lever- aging prototype information in dialogue corpus in response generation area.</p><p>The contributions of this paper could be sum- marized as follows:</p><p>(1) We propose S2SPMN, a simple yet effec- tive response generation model which could lever- age relevant information in dialogue corpus to as- sist response generation.</p><p>(2) Empirical studies indicate the superiority of proposed S2SPMN over other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Definition</head><p>Given a dialogue dataset</p><formula xml:id="formula_0">Γ = {X i , Y i } N i=1</formula><p>, where Y i is the response for a post X i , we aim to train a model with Γ such that the model can gener- ate an accurate and informative response for a new post X . Here, we propose to exploit the relevant information provided by Γ. Let T = (r 1 , r 2 , ..., r m ) refers to the prototype memory network constructed for post X , where r i is the i-th relevant response (named prototype) extracted from dialogue dataset Γ. The goal is to derive the model to generate the response Y :</p><formula xml:id="formula_1">p(Y |X ) = p(Y |T , X ).</formula><p>In following sections, we firstly introduce the generation framework with hierarchical attention mechanism assuming PMN is constructed. Then we will introduce two kinds of PMNs: static PMN and dynamic PMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sequence-to-Sequence with Prototype</head><p>Memory Network S2SPMN is built with a Seq2Seq encoder-decoder framework <ref type="bibr" target="#b18">(Sutskever et al., 2014</ref>) with the atten- tion mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>). We use LSTM <ref type="bibr" target="#b3">(Hochreiter and Schmidhuber, 1997</ref>) to materialize both encoder and decoder. The hid- den state at t-th encoding step is generated from previous hidden state h t−1 and current input x t as follows:</p><formula xml:id="formula_2">h t = lstm(x t , h t−1 )<label>(1)</label></formula><p>For decoder, at i-th timestep, s i is the decoder's hidden state and p i is the probability distribution of candidate words .</p><formula xml:id="formula_3">s i = lstm(y i−1 , s i−1 , c i , o i ) (2) p i = sof tmax(M LP (s i , y t−1 , o i , c i ))<label>(3)</label></formula><p>where M LP () is a one-layer perception, o i is the hierarchical attention over entire prototype mem- ory network which will be formalized in following sections. c i is the summarization for the post re- garding to the hidden state s i−1 :</p><formula xml:id="formula_4">c i = T j=1 α ij h j , α ij = exp(e ij ) T k=1 exp(e ik )<label>(4)</label></formula><formula xml:id="formula_5">e ij = v T 1 M LP (s i−1 , h j )<label>(5)</label></formula><p>where v 1 is the attention parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prototype Memory Network</head><p>Given a post X , a set of responses are selected from training set as prototypes and are then saved into the Prototype Memory Network(PMN). We propose two kinds of Prototype Memory Net- works.</p><p>Static PMN: For static PMN(SPMN), we ran- domly select m responses before training starts and the entire PMN remains unchanged during the training process. That is, we use the same proto- types for all the post-response pairs.</p><p>Dynamic PMN: In dynamic PMN(DPMN), prototypes are selected by retrieving the most rel- evant posts. We calculate the cosine similarity with TF-IDF weighting scheme between the given post and all the posts in training set. We consider top-m posts and put the associated responses into DPMN. This means that the prototypes are char- acteristic for each post-response pair.</p><p>In both SPMN and DPMN, m is a predefined hyper-parameter controlling the size of the PMN. Each prototype is represented with the concate- nation of word embeddings. We perform zero padding for both SPMN and DPMN with a pseudo word 1 , making the length for the representation of each prototype be the same. Here we de- note the prototype memory network as PMN = {r 1 , r 2 , ..., r m }, in which r m is the representation of m-th prototype and m is the size of the PMN. And r m = {w m,1 , w m,2 , ..., w m,l } where w m,i is the embedding of i-th word, and l is the maximum allowable length for a prototype.</p><p>For both SPMN and DPMN, we select re- sponses rather than posts although sometimes they have similar vocabularies and syntactic structure. We believe that using responses as prototypes could help with the meaning transformation from post to response. In DPMN, all the retrieved proto- types could be considered as responses to the tar- get post. It is intuitive that the generated response would have similar representation to these proto- types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Hierarchical Attention Mechanism</head><p>We use a two-stage hierarchical attention mech- anism to extract useful information in PMN and integrate it into the decoding process. The first stage is a sentence level attention over entire PMN to generate the abstractive prototypê r i at each timestep:</p><formula xml:id="formula_6">ˆ r i = M j=1 β ij r j , β ij = exp(f ij ) M k=1 exp(f ik )<label>(6)</label></formula><formula xml:id="formula_7">f ij = v T 2 M LP (s i−1 , r j )<label>(7)</label></formula><p>where v 2 is the attention parameter. The second stage is a word level attention o i over the generatedˆrgeneratedˆ generatedˆr i = { ˆ w 1 , ˆ w 2 , ..., ˆ w l } and is calculated as follows:</p><formula xml:id="formula_8">o i = l j=1 γ ijˆwijˆ ijˆw j , γ ij = exp(g ij ) l k=1 exp(g ik )<label>(8)</label></formula><formula xml:id="formula_9">g ij = v T 3 M LP (s i−1 , ˆ w j )<label>(9)</label></formula><p>where v 3 is the attention parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Setup</head><p>We use a subset of STC dataset (Shang et al., 2015) crawled from Weibo, the largest social me- dia in China. The vocabulary size is set to be 8, 000 for computational efficiency and words out of vocabulary are replaced by the symbol "unk". <ref type="bibr">1</ref> The embedding of the pseudo word is a zero vector.</p><p>We remove sentences longer than 25 words or containing more than 2 unk symbols. After pre- processing step, we have 315, 980 post-response pairs in training set, 3, 510 pairs in validation set and 300 in test set.</p><p>In our model, we use one-layer LSTM and the hidden size is set to be 600 in both encoder and de- coder. For all the words used in our model, the em- bedding size is 300. Mini-batch learning is used and batch size is set as 64. We use simple SGD for optimization and the initial learning rate is set to be 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We use two automatic evaluation metrics includ- ing Perplexity and Distinct. Human evaluation is also conducted as the only gold standard for re- sponse generation is human judgement.</p><p>Perplexity: Following (Vinyals and Le, 2015) and ( ), we use perplexity as one of our automatic evaluation metrics. Perplex- ity could measure the holistic condition of model learning. A lower perplexity score indicates bet- ter generalization performance. Perplexity on both validation set (PPL-V) and test set (PPL-T) are presented in table 2.</p><p>Distinct-1, Distinct-2: Distinct-1 and distinct- 2 calculate the ratios of distinct unigrams and bi- grams in the generated responses respectively ( <ref type="bibr" target="#b8">Li et al., 2016a;</ref><ref type="bibr" target="#b23">Wu et al., 2018</ref>). The higher score suggests that the generated re- sponse is more diverse and informative. Here, we report the distinct-1 and distinct-2 scores on entire test set.</p><p>Human Annatation: We further recruit human annotators to judge the quality of the generated an- swers for all the qa-pairs in test set. Responses generated by all the methods are pooled and ran- domly shuffled for each annotator. A score be- tween 0 and 2 is assigned to each generated answer based on the following criteria:</p><p>+2: the answer is natural and relevant to the question.</p><p>+1: the answer can be used as a reply, but is not informative enough (e.g. "我也是" (me too), "不 知道" (I don't know)).</p><p>+0: the answer is irrelevant and unclear in meaning (e.g. too many grammatical errors to un- derstand). </p><formula xml:id="formula_10">Model PPL-V PPL-T distinct-1 distinct-2<label>S2SA</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results Comparation</head><p>We use a standard baseline and a strong baseline for comparison. S2SA: The standard Seq2Seq model with an at- tention mechanism <ref type="bibr" target="#b19">(Vinyals and Le, 2015)</ref>.</p><p>TAS2S: One of the existing state-of-the-art neu- ral models based on Seq2Seq architecture. The topical words relevant to the post are considered via an attention mechanism when decoding ( .</p><p>As for our models, we use SPMN to denote the generating method with static prototype mem- ory networks and DPMN with dynamic prototype memory networks. The numbers following model names are the size of PMN.</p><p>Automatic Evaluation: <ref type="table">Table 1</ref> shows the au- tomatic evaluation results. We see that both SPMN and DPMN obtain huge improvements over the two baselines in terms of PPL-V and PPL-T. Also, we observe that SPMN1000 outperforms SPMN500 in all the four automatic metrics. Note that each post has the same prototypes provided by SPMN. This is reasonable that the relevant re- sponse is more likely to be covered by storing more prototypes in SPMN. As for the DPMN, we can see that DPMN achieves the best performance with only 100 prototypes in terms of PPL-T, com- pared with the other 4 methods. This suggests that using a retrieval mechanism to incorporate the relevant responses brings more useful information for better response generation. Note that S2SA outperforms the others in terms of distinct-1 and distinct-2. Further human evaluation indicates that many responses generated by S2SA are irrelevant and meaningless, which could inevitably increase the distinct scores.</p><p>Human Annotation: <ref type="table">Table 2</ref> shows human annotation results. It is clear that our mod- els (SPMN500, SPMN1000, DPMN100) gener- ate much more informative and valid responses and much less meaningless or "safe" responses than baseline models (S2SA, TAS2S). Specifi- cally, SPMN500, SPMN1000 and DPMN100 all</p><note type="other">Model 0 1 2 Kappa S2SA</note><p>76.83% 16.33% 6.83% 0.6124 TAS2S</p><p>69.83% 19.83% 10.33% 0.7425 SPMN500</p><p>21.67% 55.00% 23.33% 0.6534 SPMN1000</p><p>19.17% 52.50% 28.33% 0.7330 DPMN100</p><p>12.08% 56.67% 31.25% 0.6280 <ref type="table">Table 2</ref>: Human Annotation outperform S2SA and TAS2S by producing more informative and valid responses. Also, we can find that DPMN still outperforms SPMN500 and SPMN1000 with only 100 relevant responses, which is consistent with the observation made in automatic evaluation (in terms of PPL-V and PPL- T).</p><p>Case Study <ref type="table" target="#tab_1">Table 3</ref> shows several cases gen- erated by different models. Note that the size of training set and vocabulary used in our experi- ments are relatively small compared to millions of qa-pairs used in other works ( <ref type="bibr" target="#b23">Wu et al., 2018)</ref>, so it's reasonable that bad cases sometimes occur in results of baselines. How- ever, our models, no matter the static one or the dynamic one, could generate amazing responses which are not only grammatical and informative, but also have some emotional expressions like the use of punctuation and repetition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Natural language generation</head><p>How to generate grammatical and interesting sentences in different situations is one of the core topics in natural language processing area. Extensive works are proposed to generate po- ems ( , abstracts ( <ref type="bibr" target="#b20">Wang and Ling, 2016)</ref>, arguments (Hua and Wang, 2018), stories ( <ref type="bibr" target="#b14">Peng et al., 2018</ref>) and so on. Although existing approaches are useful in some ways, it's still difficult to generate natural sentences from scratch and integrating retrieved results has re- cently become a new fashion in this area. <ref type="bibr" target="#b4">Hua and Wang (2018)</ref> proposed an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia.  devised a Retrieve- Rerank-Rewrite model for abstractive summariza- tion which uses retrieved results as soft template to assist the decoding process.</p><formula xml:id="formula_11">Post 1 美 美 美国 国 国加 加 加州 州 州莫 莫 莫诺 诺 诺湖 湖 湖， ， ，奇 奇 奇异 异 异的 的 的美 美 美丽 丽 丽 (Mono Lake in California,the US, fantastically beautiful ) S2SA 有unk的地方,就是unk (It's unk if there's unk) TAS2S ，，， (speechless) SPMN500</formula><p>这是什么地方？我也去看看 (Where's the place? I'd like to go and see)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPMN1000</head><p>这是在哪？！ <ref type="formula">(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Response generation</head><p>Hand-craft rules, retrieval and generation are three main solutions for conversational AI and genera- tion is the most interesting one in current research community. <ref type="bibr" target="#b8">Li et al. (2016a;</ref><ref type="bibr" target="#b9">2016b;</ref> pro- posed a series of works in solving the "safe reply" problem using different approaches like redefining the objective function or leveraging GAN.  considered topic coherence issue by incorporating topical words. Dynamically restrict- ing the target vocabulary is also an interesting idea and <ref type="bibr" target="#b23">Wu et al. (2018)</ref> proposed to filter irrelevant words while achieving better computational effi- ciency . <ref type="bibr" target="#b2">He et al. (2017)</ref> introduced copy mecha- nism to simulate people's behaviors in real conver- sations and the proposed model could copy useful words from source sentences.  indicated that emotion is quite important in real dialogues thus an emotional chatting machine was devised to generate emotional responses.  proposed a neural knowledge diffu- sion (NKD) model to introduce knowledge into di- alogue generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose S2SPMN, a simple yet effective response generation model by exploiting relevant information contained in large dialogue dataset. Empirical studies indicate that simply se- lecting responses from training set as prototypes and integrating them into the generation process could dramatically improve the quality of gener- ated responses. Moreover, our model is very flex- ible and could be adapted to any other Seq2Seq based generation methods. Most importantly, we claim the intrinsic difference between RG and MT and propose a new way to define response genera- tion.</p><p>As the first work trying to help with the mean- ing transformation between source and target, we have obtained the encouraging progress. However, we know that there are still many directions to enrich the proposed framework. In future work, we would like to devise more sophisticated solu- tions to bridge the semantic gap in RG and explore linguistic patterns in conversations like what has been done in discourse analysis ( ) .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Encoder ··· Post Decoder Hierarchical Attention PMN Ytrain Static Dynamic Answer or 8/28/2018 //C:/Users/Li%20Jing/Desktop/PMN3.svg</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: S2SPMN Framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>What's the place?!) DPMN100 我也想去的地方 (That's exactly where I want to go) Post 2 看 看 看看 看 看能 能 能让 让 让妈 妈 妈妈 妈 妈疯 疯 疯掉 掉 掉的 的 的baby (Oh! Look at the baby! She's driving mom mad!) S2SA unk,unk! (speechless) TAS2S 我也喜欢 (Wow,I like her,too) SPMN500 好可爱啊！好可爱！ (She's so cute!) SPMN1000 我也是这样的 (I was like her when I was at her age) DPMN100 我也想养一个 (I want a baby like her) Post 3 装 装 装修 修 修以 以 以后 后 后很 很 很快 快 快后 后 后悔 悔 悔的 的 的80件 件 件事 事 事！ ！ ！还 还 还没 没 没装 装 装修 修 修的 的 的朋 朋 朋友 友 友们 们 们， ， ，一 一 一定 定 定要 要 要借 借 借鉴 鉴 鉴！ ！ ！ (80 things to regret after decorating your house! Look at this article if you haven't started decoration!) S2SA 有unk的时候，我会有一个unk的unk! (When I have unk, I will have a unk unk!) TAS2S ，的 (speechless) SPMN500 我也要去看看，一定要收藏！ (Ok,I will read and collect it!) SPMN1000 很实用，很实用，很实用 (very very very useful) DPMN100 很实用，很实用，很实用，很实用！ (very very very very useful!)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The answers generated by different models for the sampled questions.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical variational memory network for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><forename type="middle">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1653" to="1662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Shizhu He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural argument generation augmented with externally retrieved evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural user simulation for corpus-based policy optimisation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Kreyssig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linguistic properties matter for implicit discourse relation recognition: Combining semantic interaction, topic continuity and attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2157" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge diffusion for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuman</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3349" to="3358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards controllable story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Storytelling</title>
		<meeting>the First Workshop on Storytelling</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alime chat: A sequence to sequence and rerank based chatbot engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="498" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1506.05869</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural network-based abstract generation for opinions and arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A network-based end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-domain neural network language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural response generation with dynamic vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flexible and creative chinese poetry generation using neural memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling multiturn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018)</title>
		<meeting>the 27th International Conference on Computational Linguistics (COLING 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Emotional chatting machine: emotional conversation generation with internal and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01074</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
