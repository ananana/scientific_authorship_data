<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-label Text Categorization with Joint Learning Predictions-as-Features Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics(Peking University)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics(Peking University)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics(Peking University)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics(Peking University)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics(Peking University)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics(Peking University)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-label Text Categorization with Joint Learning Predictions-as-Features Method</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Key Laboratory on Machine Perception(Peking University)</term>
					<term>Ministry of Education</term>
					<term>China 2</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories. Recently , a series of methods have been developed , which train a classifier for each label, organize the classifiers in a partially ordered structure and take predictions produced by the former classifiers as the latter classifiers&apos; features. These predictions-as-features style methods model high order label dependencies and obtain high performance. Nevertheless, the predictions-as-features methods suffer a drawback. When training a classifier for one label, the predictions-as-features methods can model dependencies between former labels and the current label, but they can&apos;t model dependencies between the current label and the latter labels. To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. We conduct experiments using real-world tex-tual data sets, and these experiments illustrate the predictions-as-features models trained by our algorithm outperform the original models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories simultaneously. The multi-label setting is common and useful in the re- al world. For example, in the news categorization task, a newspaper article concerning global warm- ing can be classified into two categories simul- taneously, namely environment and science. For another example, in the task of classifying mu- sic lyrics into emotions, a song's lyrics can deliv- er happiness and excitement simultaneously. The research about the multi-label text categorization attracts increasing attention <ref type="bibr">(Srivastava and ZaneUlman, 2005;</ref><ref type="bibr" target="#b2">Katakis et al., 2008;</ref><ref type="bibr">Rubin et al., 2012;</ref><ref type="bibr">Nam et al., 2013;</ref><ref type="bibr" target="#b3">Li et al., 2014)</ref>.</p><p>Recently, a series of predictions-as-features style methods have been developed, which train a classifier for each label, organize the classifiers in a partially ordered structure and take prediction- s produced by the former classifiers as the latter classifiers' features. These predictions-as-features style methods model high order label dependen- cies ( <ref type="bibr" target="#b12">Zhang and Zhang, 2010)</ref> and obtain high performance. Classifier chain (CC) <ref type="bibr" target="#b6">(Read et al., 2011</ref>) and multi-label Learning by Exploiting lA- bel Dependency (Lead) ( <ref type="bibr" target="#b12">Zhang and Zhang, 2010)</ref> are two famous predictions-as-features method- s. CC organizes classifiers along a chain and LEAD organizes classifiers in a Bayesian net- work. Besides, there are other works on extend- ing the predictions-as-features methods <ref type="bibr" target="#b11">(Zaragoza et al., 2011;</ref><ref type="bibr" target="#b1">Gonçalves et al., 2013;</ref><ref type="bibr" target="#b10">Sucar et al., 2014)</ref>. In this paper, we focus on the predictions- as-features style methods.</p><p>The previous works of the predictions-as- features methods focus on learning the partial- ly ordered structure.</p><p>They neglect a draw- back. When training a classifier for one label, predictions-as-features methods can model depen- dencies between former labels and the current la- bel, but they can't model dependencies between the current label and the latter labels. Consider the case of three labels. We organize classifiers in a partially ordered structure shown in <ref type="figure">figure 1</ref>. When training the classifier for the second label, the feature (the bold lines in <ref type="figure">figure)</ref> consists of the origin feature and the prediction for the first la- bel. The information about the third label can't be incorporated. It means that we only model the dependencies between the first label and the sec- <ref type="figure">Figure 1</ref>: When training the classifier for the sec- ond label, the feature (the bold lines) consists of only the origin feature and the prediction for the first label. In this time, it is impossible to model the dependencies between the second label and the third label.</p><p>ond label and that the dependencies between the second label and the third label is missing.</p><p>To address this problem, we propose a nov- el joint learning algorithm that allows the feed- backs to be propagated from the classifiers for latter labels to the classifier for the current la- bel, so that the information about the latter labels can be incorporated. It means that the proposed method can model, not only the dependencies be- tween former labels and current label as the usual predictions-as-features methods, but also the de- pendencies between current label and latter labels. With not missing dependencies. Hence, the pro- posed method will improve the performance. Our experiments illustrate the models trained by our al- gorithm outperform the original models. You can find the code of this paper online 1 .</p><p>The rest of this paper is organized as follows. Section 2 presents the proposed method. We con- duct experiments to demonstrate the effectiveness of the proposed method in section 3. Section 4 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Joint Learning Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Let X denote the document feature space, and Y = {0, 1} m denote label space with m label- s. A document instance x x x ∈ X is associated with a label vector y y y = (y 1 , y 2 , ..., y m ), where y i = 1 denotes the document has the i-th label and 0 otherwise. The goal of multi-label learn- ing is to learn a function h : X → Y. In gener-al, h consists of m functions, one for a label, i.e.,</p><formula xml:id="formula_0">h(x x x) = [h 1 (x x x), h 2 (x x x), ..., h m (x x x)].</formula><p>In the predictions-as-features methods, the clas- sifiers are organized in a partially ordered structure and take predictions produced by the former clas- sifiers as features. We can describe the classifier in the predictions-as-features method as follows.</p><formula xml:id="formula_1">h j : x x x, h k∈pa j pa j pa j (x x x) → y j (1)</formula><p>where pa j pa j pa j denotes the set of parents of the j-th classifiers in the partially ordered structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture and Loss</head><p>In this subsection, we introduce architecture and loss function of our joint learning algorithm. As a motivating example, we employ logistic regres- sion as the base classifier in the predictions-as- features methods. The classification function is the sigmoid function, as shown in Eq. <ref type="formula" target="#formula_2">(2)</ref>.</p><formula xml:id="formula_2">p j = h j (x x x, p k∈pa j pa j pa j ) = exp([x x x, p k∈pa j pa j pa j ] T W j W j W j ) 1 + exp([x x x, p k∈pa j pa j pa j ] T W j W j W j )<label>(2)</label></formula><p>where p j denotes the probability the document has the j-th label, W j W j W j denotes the weight vector of the j-th model and [x x x, p k∈pa j pa j pa j ] denotes the feature vec- tor x extended with predictions [p k∈pa j pa j pa j ] produced by the former classifiers.</p><p>The joint algorithm learns classifiers in the par- tially ordered structure jointly by minimizing a global loss function. We use the sum of negative log likelihood losses of all classifiers as the global loss function.</p><formula xml:id="formula_3">L(y y y, h(x) h(x) h(x)) = m j=1 (p j , y j ) = − m j=1 (y j log(p j ) + (1 − y j )log(1 − p j ))<label>(3)</label></formula><p>The joint algorithm minimizes this global loss function, as Eq. <ref type="formula" target="#formula_4">(4)</ref> shows.</p><formula xml:id="formula_4">h * h * h * = argmin h h h L(y y y, h(x) h(x) h(x))<label>(4)</label></formula><p>Minimizing this global loss function is inequiv- alent to minimizing the loss function of each base classifier separately, since minimizing the global loss function results in feedbacks from latter clas- sifiers. In the predictions-as-features methods, the weights of the k-th classifier are the factors of not only the k-th classifier but also the latter classi- fiers. Consequently, when minimizing the global loss function, the weights of the k-th classifier are updated according to not only the loss of the k-th classifier but also the losses of the latter classifiers. In other words, feedbacks are propagated from the latter classifiers to the k-th classifier.</p><p>The predictions-as-features models trained by our proposed joint learning algorithm can model the dependencies between former labels and cur- rent label, since they take predictions by the for- mer classifiers to extend the latter classifiers' fea- tures, as the usual predictions-as-features methods do. Besides, they can also model the dependencies between current label and latter labels due to the feedbacks incorporated by the joint learning algo- rithm.</p><p>Here, we employ logistic regression as the mo- tivating example. If we want to employ other clas- sification models, we use other classification func- tion and other loss function. For example, if we want to employ L2 SVM as base classifiers, we resort to the linear classification function and the L2 hinge loss function.</p><p>We employ the Back propagation Through Structure (BTS) <ref type="bibr" target="#b0">(Goller and Kuchler, 1996)</ref> to minimize the global loss function. In BTS, par- ent node is computed with its child nodes at the forward pass stage; child node receives gradient as the sum of derivatives from all its parents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We perform experiments on four real world da- ta sets: 1) the first data set is Slashdot <ref type="bibr" target="#b6">(Read et al., 2011</ref>). The Slashdot data set is concerned about predicting multiple labels given science and technology news titles and partial blurbs mined from Slashdot.org. 2) the second data set is Med- ical ( <ref type="bibr" target="#b5">Pestian et al., 2007)</ref>. This data set involves the assignment of ICD-9-CM codes to radiology reports.</p><p>3) The third data set is Enron. The enron data set is a subset of the Enron Email Dataset, as labelled by the UC Berkeley Enron Email Analy- sis Project <ref type="bibr">2</ref> . It is concerned about classifying e- mails into some categories. 4) the fourth data set dataset n d <ref type="table" target="#tab_0">m  slashdot  3782  1079  22  medical  978  1449  45  enron  1702  1001  53  tmc2007  28596  500  22   Table 2</ref>: Multi-label data sets and associated statis- tics.</p><p>is Tmc2007 (Srivastava and Zane- <ref type="bibr" target="#b9">Ulman, 2005</ref>). It is concerned about safety report categorization, which is to label aviation safety reports with re- spect to what types of problems they describe. <ref type="table">Table 2</ref> shows these multi-label data sets and as- sociated statistics. n denotes the size of the entire data set, d denotes the number of the bag-of-words features, m denotes the number of labels. These data sets are available online 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>We use three common used evaluation metrics. The Hamming loss is defined as the percentage of the wrong labels to the total number of labels.</p><formula xml:id="formula_5">Hammingloss = 1 m |h(x x x)∆y y y| (5)</formula><p>where ∆ denotes the symmetric difference of two sets, equivalent to XOR operator in Boolean logic. The multi-label 0/1 loss is the exact match mea- sure as it requires any predicted set of labels h(x x x) to match the true set of labels S exactly. The 0/1 loss is defined as follows:</p><formula xml:id="formula_6">0/1loss = I(h(x x x) = y y y)<label>(6)</label></formula><p>Let p j and r j denote the precision and recall for the j-th label. The macro-averaged F score is a harmonic mean between precision and recall, de- fined as follows:</p><formula xml:id="formula_7">F score = 1 m m i=j 2 * p j * r j p j + r j<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Method Setup</head><p>In this paper, we focus on the predictions-as- features style methods, and use CC and LEAD as the baselines. Our methods are JCC and JLEAD. JCC(JLEAD) is CC(LEAD) trained by our join- t algorithm and we compare JCC(JLEAD) to C- C(LEAD) respectively. Put it another way, C- C/LEAD provide the partial order structure of Dataset BR CC LEAD JCC JLEAD hamming loss (lower is better) slashdot 0.046 ± 0.002 0.043 ± 0.001 0.045 ± 0.001• 0.043 ± 0.001 0.043 ± 0.001 medical 0.013 ± 0.001 0.013 ± 0.001• 0.012 ± 0.000• 0.011 ± 0.000 0.010 ± 0.001 enron 0.052 ± 0.001 0.053 ± 0.002• 0.052 ± 0.001• 0.049 ± 0.001 0.049 ± 0.001 tmc2007 0.063 ± 0.002 0.058 ± 0.001 0.058 ± 0.001 0.057 ± 0.001 0.057 ± 0.001 0/1 loss (lower is better) slashdot 0.645 ± 0.013 0.637 ± 0.015• 0.631 ± 0.017• 0.610 ± 0.014 0.614 ± 0.011 medical 0.398 ± 0.034 0.377 ± 0.032• 0.379 ± 0.033• 0.353 ± 0.030 0.345 ± 0.030 enron 0.856 ± 0.016 0.848 ± 0.017 0.853 ± 0.017 0.848 ± 0.018 0.850 ± 0.017 tmc2007</p><p>0.698 ± 0.004 0.686 ± 0.006 0.689 ± 0.009 0.684 ± 0.006 0.681 ± 0.006 F score (higher is better) slashdot 0.345 ± 0.016 0.354 ± 0.015• 0.364 ± 0.015• 0.385 ± 0.017 0.383 ± 0.017 medical 0.403 ± 0.012 0.416 ± 0.013• 0.426 ± 0.011• 0.444 ± 0.009 0.446 ± 0.013 enron 0.222 ± 0.014 0.224 ± 0.019 0.225 ± 0.018 0.223 ± 0.017 0.222 ± 0.015 tmc2007</p><p>0.524 ± 0.007 0.531 ± 0.009• 0.508 ± 0.017• 0.547 ± 0.007 0.546 ± 0.006 <ref type="table">Table 1</ref>: Performance (mean±std.) of each approach in terms of different evaluation metrics.</p><p>•/• indicates whether JCC/JLEAD is statistically superior to CC/LEAD respectively (pairwise t-test at 5% significance level).</p><p>classifiers, and train these classifiers one by one. JCC/LEAD train classifiers jointly in the partial order structure provided by CC/LEAD. For LEAD and JLEAD, we use the Banjo (Bayesian ANalysis with Java Objects) ( <ref type="bibr" target="#b8">Smith et al., 2006</ref>) package as the Bayesian structure learn- ing tool. Besides, we also perform experiments with Binary Relevance (BR), which is the baseline for the predictions-as-features methods. BR trains a classifier for a label independently, which does- n't model dependencies. The base classifier of all of them is set to logistic regression without regu- larization. Experiments are performed in ten-fold cross validation with pairwise t-test at 5% signifi- cance level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance</head><p>We reports the detailed results in terms of different evaluation metrics on different data sets in table 1. As shown in this figures, CC and LEAD outperfor- m BR, which shows the values of the prediction- as-features methods. JCC and JLEAD wins over CC and LEAD respectively, which shows the val- ues of the proposed joint learning algorithm.</p><p>The improvements are much smaller on the En- ron data set than other data sets. In fact, BR, the original prediction-as-features methods and our proposed methods share similar performance on the Enron data set. The reason may be that the label dependencies in the Enron dataset is weak. The label dependencies weakness can be validat- ed by the fact that the modeling-correlation C- C and LEAD can't obtain much higher perfor- mance than the not-modeling-correlation BR. Due  to the weak label dependencies, the modeling- correlation-better JCC(JLEAD) can't obtain much higher performance than CC(LEAD). We summarize the detailed results into <ref type="table" target="#tab_0">Table 3</ref>. JCC is significantly superior to CC in 7/12 cas- es, tie in 5/12 cases, inferior in zero case. JLEAD is significantly superior to LEAD in 8/12 cases, tie in 4/12 cases, inferior in zero case. The re- sults indicates that our proposed joint algorithm can improve the performance of the predictions- as-features methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Time</head><p>The training time (mean) of each approach is showed detailed in table 4. First, we find the train- ing time is related to the number of labels. The training time on the Tmc2007 dataset (28596 in- stances, 500 features and 22 labels) is less than that on the Enron dataset (1702 instances, 1001 features and 53 labels). This is very easy to un- derstand. We train more classifiers with respect to more labels, which leads to more training time. Second, LEAD/JLEAD have slightly less training time than CC/JCC. The Bayesian network struc-  ture learning tool limits that a node has five parent nodes at most. Hence, the partially order struc- ture of LEAD/JLEAD is much simpler. Third, the training time of the joint algorithm is slightly more than that of the original methods. Some time is spent on back-propagating feedbacks from latter classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The multi-label text categorization is a common and useful text categorization. Recently, a se- ries of predictions-as-features style methods have been developed, which model high order label de- pendencies and obtain high performance. The predictions-as-features methods suffer from the drawback that they methods can't model depen- dencies between current label and the latter labels.</p><p>To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the latter classifiers to the curren- t classifier. Our experiments illustrate the models trained by our algorithm outperform the original models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The win/tie/loss results for the joint learn-
ing algorithm against the original predictions-as-
features methods in terms of different evaluation 
metrics (pairwise t-test at 5% significance level). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 : The average training time (in seconds) of each approach</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/rustle1314/Joint_ Learning_Predictions_as_Features_for_ Multi_Label_Classification</note>

			<note place="foot" n="2"> http://bailando.sims.berkeley.edu/ enron_email.html</note>

			<note place="foot" n="3"> http://mulan.sourceforge.net/ datasets.html and http://mlkd.csd.auth. gr/multilabel.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledge</head><p>We sincerely thank all the anonymous reviewers for their valuable comments, which have helped to improve this paper greatly. Our work is supported by National High Technology Research and De-velopment Program of China <ref type="formula">(863 Program</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A genetic algorithm for optimizing the label ordering in multi-label classifier chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Corrêa Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Plastino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools with Artificial Intelligence (ICTAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="469" to="476" />
		</imprint>
	</monogr>
	<note>IEEE 25th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilabel text classification for automated tag suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grigorios Tsoumakas, and Ioannis Vlahavas</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Proceedings of the ECML/PKDD</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Muli-label text categorization with hidden components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1816" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5419</idno>
		<title level="m">Iryna Gurevych, and Johannes Fürnkranz. 2013. Large-scale multi-label text classification-revisiting neural networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A shared task involving multi-label classification of clinical free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>John P Pestian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweł</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matykiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Dj Hovermale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Włodzisław</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</title>
		<meeting>the Workshop on BioNLP 2007: Biological, Translational, and Clinical Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="333" to="359" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Padhraic Smyth, and Mark Steyvers. 2012. Statistical topic models for multi-label document classification. Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><surname>Timothy N Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chambers</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="157" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational inference of neural information flow networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich D</forename><surname>Hartemink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jarvis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">161</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discovering recurring anomalies in text reports regarding complex space systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zane-Ulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aerospace Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="3853" to="3862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-label classification with bayesian network-based chain classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concha</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><forename type="middle">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Larrañaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian chain classifiers for multidimensional classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Julio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">Enrique</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concha</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larranaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2192" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-label learning by exploiting label dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="999" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
