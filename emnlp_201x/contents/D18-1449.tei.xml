<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustratingly Easy Model Ensemble for Abstractive Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
							<email>hakobaya@yahoo-corp.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<country>RIKEN AIP</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frustratingly Easy Model Ensemble for Abstractive Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4165" to="4176"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4165</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Ensemble methods, which combine multiple models at decoding time, are now widely known to be effective for text-generation tasks. However, they generally increase computational costs, and thus, there have been many studies on compressing or distilling ensemble models. In this paper, we propose an alternative , simple but effective unsupervised ensemble method, post-ensemble, that combines multiple models by selecting a majority-like output in post-processing. We theoretically prove that our method is closely related to kernel density estimation based on the von Mises-Fisher kernel. Experimental results on a news-headline-generation task show that the proposed method performs better than the current ensemble methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent success in deep learning, especially encoder-decoder models <ref type="bibr" target="#b40">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, has dramatically improved the performance of various text-generation tasks, such as translation ( <ref type="bibr" target="#b19">Johnson et al., 2017)</ref>, summa- rization ( <ref type="bibr" target="#b0">Ayana et al., 2017)</ref>, question-answering ( <ref type="bibr" target="#b6">Choi et al., 2017)</ref>, and dialogue response genera- tion ( <ref type="bibr" target="#b11">Dhingra et al., 2017)</ref>. In these studies on neu- ral text generation, it has been known that a model- ensemble method, which predicts output text by averaging multiple text-generation models at de- coding time, is effective even for text-generation tasks, and many state-of-the-art results have been obtained with ensemble models. However, an en- semble method has a clear drawback in that it in- creases computational costs, i.e., the increase in time as the number of models increases, since it averages the word-prediction probabilities of all models in each decoding step. Therefore, there have been many studies on model compression or distillation for ensemble methods, each of which has successfully shrunk an ensemble model <ref type="bibr" target="#b18">(Hinton et al., 2015;</ref><ref type="bibr" target="#b4">Chebotar and Waters, 2016;</ref><ref type="bibr" target="#b26">Kuncoro et al., 2016;</ref><ref type="bibr" target="#b22">Kim and Rush, 2016;</ref><ref type="bibr" target="#b38">Stahlberg and Byrne, 2017;</ref><ref type="bibr" target="#b14">Freitag et al., 2017)</ref>. In this paper, we propose an alternative method for model ensemble inspired by the majority vote in classification tasks ( <ref type="bibr" target="#b29">Littlestone and Warmuth, 1994)</ref>. Majority vote is a method that selects the most frequent label from the predicted labels of multiple classifiers in post-processing. Simi- larly, our method involves selecting a majority- like output from the generated outputs of multi- ple text-generation models in post-processing as in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, instead of averaging models at decod- ing time as in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. The difference between a classification task and text-generation task is that we need to consider a sequence of labels for each model output in a text-generation task, although we consider only one label in a classification task. This means a majority output may not exist since each output will be basically different from other outputs, which are generated from different mod- els. To overcome this problem, we propose an unsupervised method for selecting a majority-like output close to the other outputs by using cosine similarity. The idea is quite simple, but experi- ments showed that our method is more effective than the current ensemble methods.</p><p>Our work can open up a new direction for two research communities: model ensemble and hy- potheses reranking (see Sec. 6 for detailed descrip- tions of the related studies). For the first, we sug- gest a new category of ensemble algorithms that corresponds to the output selection in classifica- tion tasks. In classification tasks, there are roughly three approaches for model ensemble: model se- lection in preprocessing, model average at run- time, and output selection in post-processing. In text generation studies, model selection by cross- validation and model average with an ensemble decoder have been frequently used, but output se- lection as typified by majority vote has received less attention because of the fact that a majority output may not exist, as described above. There- fore, there is enough room to study this direction in the future. Since our algorithm in this paper is quite simple, we expect that more sophisticated methods can improve the results even over our ap- proach.</p><p>For the hypotheses reranking research commu- nity, we suggest a new category of reranking tasks, where we need to select the best output from the generated outputs of multiple models, instead of the N-best hypotheses of a single model. Hy- potheses reranking for a text-generation model is related to our task, but in this case, a reranking method based on a language model is frequently used and is basically enough to correct the scoring of a beam search with a single model ( <ref type="bibr" target="#b5">Chen et al., 2006;</ref><ref type="bibr" target="#b45">Vaswani et al., 2013;</ref><ref type="bibr">Luong and PopescuBelis, 2016</ref>) since the purpose is to obtain a flu- ent output and remove erroneous outputs, assum- ing the model can generate good outputs. A clear difference between our task and the reranking task is that we should consider all outputs to decide the goodness of an output because a fluent output is not always appropriate in this task. This is simi- lar to extractive summarization ( <ref type="bibr" target="#b13">Erkan and Radev, 2004</ref>) but is significantly different from our task in that our output candidates have almost the same meaning.</p><p>Our contributions in this paper are as follows.</p><p>• We propose a simple, fast, and effective method for unsupervised ensembles of text generation models, where (i) the implementation is "frus- tratingly easy" without any modification of model code (Alg. 1), (ii) the computational time is enough for practical use (Sec. 5.3), i.e., an ensemble time of 3.7 ms per sentence against a decoding time of 44 ms, and (iii) the perfor- mance is competitive with the state-of-the-art results (Sec. 5.2), i.e., our method (ensemble of 32 models) for 37.52 ROUGE-1 against the state-of-the-art method (single model) for 37.27 ROUGE-1 on a news-headline-generation task.</p><p>• We prove that our method is an approximation of finding the maximum density point by ker- nel density estimation based on the von Mises- Fisher kernel <ref type="bibr">(Sec. 4)</ref>. In addition, we derive a formula of the error bound of this approxima- tion.</p><p>• We will release the 128 prepared models used in this paper (Sec. 5.1), each of which was trained for more than two days, as a new dataset to im- prove ensemble methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In Sec. 2.1, we briefly explain an encoder-decoder model for text generation, and in Sec. 2.2, we dis- cuss the current ensemble methods for combining multiple text generation models at decoding time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder-Decoder Model</head><p>An encoder-decoder model is a conditional lan- guage generation model, which can learn rules for generating an appropriate output sequence corre- sponding to an input sequence by using the statis- tics of many correct pairs of input and output se- quences, e.g., news articles and their headlines. When training this model, we calculate a condi- tional likelihood,</p><formula xml:id="formula_0">p(y | x) = T −1 t=1 p(y t+1 | y ≤t , x),<label>(1)</label></formula><p>with respect to each pair (x, y) of input sequence x = x 1 · · · x S and output sequence y = y 1 · · · y T , where y ≤t = y 1 · · · y t , and maximize its mean. The model p(y | x) in Eq. <ref type="formula" target="#formula_0">(1)</ref> is achieved by com- bining two recurrent neural networks, called an encoder and decoder. The former reads an input sequence x to recognize its content, and the latter predicts an output sequence y corresponding to the content. After training, we can obtain an output y from an input x by using a learned model p(y | x). Since the calculation of an optimal output is clearly intractable, most studies used a beam search, which is a greedy search algorithm that keeps a limited number of best partial solutions, whose size is called the beam size. Formally, a set of best partial solutions of beam size b at step t is represented as Y b ≤t , which is recursively defined as the top b elements with respect to p(y ≤t | x), where</p><formula xml:id="formula_1">y ≤t ∈ Y b ≤t−1 × Y .</formula><p>The Y is a set of avail- able elements for y i , or a target dictionary. Let start and goal meta symbols be &lt;s&gt; and &lt;/s&gt;, re- spectively. A beam search procedure starts from Y ≤0 = {&lt;s&gt;} and finishes when the last symbols of all elements in Y b ≤t are the goal element &lt;/s&gt; or when its length t becomes larger than some thresh- old.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Runtime-Ensemble</head><p>In a text-generation task, model ensemble is a method of predicting a next word by averaging the word-prediction probabilities of multiple text- generation models at decoding time. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows a flow chart of the current ensemble meth- ods, which we call runtime-ensemble to distin- guish them from our method. There are mainly two variants of runtime-ensemble using arithmetic mean p a and geometric mean p g , which are defined as</p><formula xml:id="formula_2">p a (y ≤t | x) = 1 |M | p∈M p(y ≤t | x),<label>(2)</label></formula><formula xml:id="formula_3">p g (y ≤t | x) = p∈M p(y ≤t | x) 1 |M | , (3)</formula><p>where M is a set of learned models. We call the former EnsSum and the latter EnsMul. Although there have been no comparative experiments, EnsMul is usually used since most decoding pro- grams keep log p and calculating p∈M log p is enough to obtain the top b words with respect to p g for a beam search procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Post-Ensemble</head><p>Our alternative ensemble method combines mul- tiple text-generation models by selecting a majority-like output close to the other outputs, which is calculated with a similarity function such as cosine similarity. We call this method post- ensemble since it is executed in post-processing, i.e., after a decoding process. <ref type="figure" target="#fig_0">Fig. 1(b)</ref> shows a flow chart of post-ensemble, and Alg. 1 shows its algorithm. When our method receives an input x, a normal decoder calculates the output s of each model p from the input in parallel (lines 2-4), and the output selector selects the majority-like output y from all outputs (lines 6-9). In line 7, we cal- culate the score c of each output s by using a sim- ilarity function K, where K(s, s ) represents the similarity between s and s . A higher score means that the output s is in a denser part in the output Input: Input text x, set M of learned models, and similarity function K, such as cos. Output: Output prediction y.</p><formula xml:id="formula_4">1 S ← ∅; 2 foreach p ∈ M do 3 s ← output of model p for input x; 4 S ← S ∪ {s}; 5 C ← {}; // as a hash map 6 foreach s ∈ S do 7 c ← 1 |S| s ∈S K(s, s ); 8 C[s] ← c; 9 y = argmax s∈S C[s]; 10 return y Algorithm 1: Post-ensemble procedure.</formula><p>space since the score c means the average similar- ity in other outputs.</p><p>The post-ensemble procedure has two main advantages compared with the current runtime- ensemble procedure. One is that we do not need to develop an ensemble decoder by modifying a decoding program on a deep learning framework. The concept of runtime-ensemble is simple, but its implementation is not that simple in recent so- phisticated open source software. For example, we need to modify about 100 lines to add an ensemble feature to the decoding program of an open source neural machine translator, OpenNMT 1 , which re- quires understanding the overall mechanism of the software. The other advantage is that we can eas- ily parallelize decoding processes in our method since each output can be calculated by using a sin- gle model. If we have a server program for text generation, we can improve its performance with all our machine resources (ideally) by assigning a server to each model and allowing the output se- lector to communicate with it.</p><p>One drawback of our method is that its expres- sive power is basically the same as that of each single model. However, this alternatively means that the lower bound of the quality of each output is guaranteed with the worst case of the outputs of single models, while the current runtime-ensemble method can perform worse than each single model for the worst case input. Furthermore, experiments showed our post-ensemble method is more effec- tive than the current runtime-ensemble methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we prove that when K(s, s ) = cos(s, s ), Alg. 1 is an approximation of find-ing the maximum density point by kernel den- sity estimation based on the von Mises-Fisher ker- nel. First, we briefly explain kernel density es- timation and how to apply it to our method in Sec. 4.1. Then, we introduce the von Mises-Fisher kernel used in this analysis and later experiments in Sec. 4.2. Finally, we prove a theorem that guar- antees the approximation error in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Kernel Density Estimation</head><p>Kernel density estimation is a non-parametric method for estimating the probability density function of a random variable. Let (X 1 , · · · , X n ) be an independent and identically distributed (i.i.d.) sample that was drawn from a distribution with an unknown density function f . The kernel density estimator based on the sample is defined as˜f as˜ as˜f (X) = 1</p><formula xml:id="formula_5">n n i=1 K(X, X i ).<label>(4)</label></formula><p>Using an appropriate kernel such as the Gaussian kernel, this estimator˜festimator˜ estimator˜f converges to the true den- sity f , and it can be proved that there is no non- parametric estimator that converges faster than this kernel density estimator <ref type="bibr" target="#b46">(Wahba, 1975)</ref>.</p><p>Here, let us consider our outputs (s 1 , · · · , s n ), which correspond to S in Alg. 1. They are gen- erated from text generation models (p 1 , · · · , p n ), which correspond to M in Alg. 1. We assume that these models are trained with randomly initialized parameters (θ 1 , · · · , θ n ), each of which includes a random seed for the optimizer, and the other set- tings are deterministic. In this case, we can con- struct a function F : P → O that maps the param- eter space P onto the output space O. In other words, if each parameter θ i is an i.i.d. random variable, the corresponding output s i = F (θ i ) is also an i.i.d. random variable. Therefore, Eq. (4) can be directly used for line 7 in Alg. 1.</p><p>Our method can be regarded as a heuristic ap- proach based on the characteristics of our encoder- decoder model, where there are many local solu- tions for optimization. We expect that our method can be applied to other models on the basis of a theoretical study <ref type="bibr" target="#b20">(Kawaguchi, 2016)</ref>, that showed that deep neural networks can have many local op- tima, but there are no poor local optima (formally, every local minimum of deep neural networks is a global minimum under a certain condition). We do not consider this direction since theoretical jus- tification is beyond our scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">von Mises-Fisher Kernel</head><p>The von Mises-Fisher kernel ( <ref type="bibr" target="#b16">Hall et al., 1987</ref>) is a natural extension of the Gaussian kernel to a unit hypersphere. This kernel is especially useful for directional or angular statistics, so it is expected to be compatible with the cosine similarity fre- quently used in natural language processing. The definition is</p><formula xml:id="formula_6">K vmf (s, s ) = C q (κ) exp(κ cos(s, s )),<label>(5)</label></formula><p>where κ is a smoothing factor called the concen- tration parameter, and cos is a cosine similarity, i.e., cos(s, s ) = s·s ||s|| 2 ||s || 2</p><p>. C q (κ) is the normal- ization constant, which is defined as</p><formula xml:id="formula_7">C q (κ) = κ q−1 2 (2π) q+1 2 I q−1 2 (κ) ,<label>(6)</label></formula><p>where I v is the modified Bessel function of the first kind at order v, and q is the dimension of di- rectional data (angular expression of data).</p><p>In the experiments described later, we im- plemented Alg. 1 with this kernel by us- ing the log-sum-exp trick <ref type="bibr" target="#b32">(Nielsen and Sun, 2016)</ref> to avoid overflow/underflow problems since argmax exp(x) = argmax log exp(x). In addition, we used Garcia-Portugues's rule <ref type="bibr" target="#b15">(Garcia-Portugues, 2013</ref>) to adjust the concentra- tion parameter κ = ˆ h −2 , defined asˆh</p><formula xml:id="formula_8">asˆ asˆh =   4π 1 2 I q−1 2 (˜ κ) 2 ˜ κ q+1 2 2qI q+1 2 (2˜κ2˜κ)+(2+q)˜ κI q+3 2 (2˜κ2˜κ) n   1 4+q (7)</formula><p>where˜κwhere˜ where˜κ is an approximation of κ derived from the maximum likelihood estimation (Sra, 2012), de- fined as˜κas˜ as˜κ = ˜ µ(q−˜ µ)</p><p>1−˜ µ 2 , where˜µwhere˜ where˜µ is the sample mean of the directional data in a unit hypersphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximation Error Analysis</head><p>We prove an approximation error bound of Alg. 1 when K(s, s ) = cos(s, s ), as shown in the fol- lowing theorem. </p><p>where the approximation error R * of the output y with respect to the true density estimator p, i.e,. R * = max s∈S p(s) − p(y), is bounded by</p><formula xml:id="formula_10">R * ≤ C q (κ)κ 2 exp(κ)(σ 2 + µ 2 ),<label>(9)</label></formula><p>where µ = max s∈S E s [cos(s, s )], and</p><formula xml:id="formula_11">σ 2 = max s∈S V s [cos(s, s )].</formula><p>Proof sketch. Eq. <ref type="formula" target="#formula_9">(8)</ref> can be obtained by using the first order Taylor series approximation at 0 of exp(x), i.e., exp(x) ≈ 1 + x, and the nature of argmax, i,e., argmax(1 + κx) = argmax x. Eq. <ref type="formula" target="#formula_10">(9)</ref> can be derived by the Lagrange error bound˜R bound˜ bound˜R(x) for exp(x) ≈ 1 + x, where x = κ cos(s, s ), and −κ ≤ x ≤ κ, as˜R</p><formula xml:id="formula_12">as˜ as˜R(x) = max x exp(x ) 2! x 2 ≤ exp(κ) 2 x 2 .<label>(10)</label></formula><p>See Appendix A for the complete proof.</p><p>This theorem implies that the approximation er- ror becomes smaller as κ becomes smaller. Since κ is the concentration parameter, the shape of the density estimation will be smooth when κ is small, while it will be a peak when κ is large. This means that, when κ is large, the density estimation is al- most the same as the majority vote. Therefore, we can naturally choose a small value for κ for our purpose. In fact, the concentration parameter was set as κ = 0.69 by using Garcia-Portugues's rule in our experiments. The normalization constant using κ was calculated as C q (κ) = 0.14, and the average values of µ and σ with respect to the set S of output candidates were E S [σ] = 0.30 and E S [µ] = 0.78, respectively. In this case, the theo- retical average approximation error was calculated as E S [R * ] ≤ 0.093 = 0.14 × 0.69 2 × exp(0.69) × (0.78 2 + 0.30 2 ). This is quite small in view of the approximation error for a probability. In addi- tion, the actual average approximation error can be much smaller, and it was about 1.95 × 10 −7 in our experiments. The accuracy defined by the rate at which the approximate maximum is the true maxi- mum, i.e., p(y) = max s∈S p(s), was 96.36%. De- tail on the settings of our experiments will be given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first explain the basic settings of our ex- periments in Sec. 5.1 and report a comparative experiment and analysis on the news-headline- generation task in Sec. 5.2. Then, we discuss the change in some of the settings to conduct an ex- periment by changing the number of models and the settings of model preparation in Sec. 5.3 and Sec. 5.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Basic Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>We used a well-known dataset Gigaword of a news-headline-generation task, which was prepared by <ref type="bibr" target="#b35">Rush et al. (2015)</ref>. This dataset has been extensively used in recent studies on abstractive summarization ( <ref type="bibr" target="#b42">Takase et al., 2016;</ref><ref type="bibr" target="#b7">Chopra et al., 2016;</ref><ref type="bibr" target="#b25">Kiyono et al., 2017;</ref><ref type="bibr" target="#b48">Zhou et al., 2017;</ref><ref type="bibr" target="#b3">Cao et al., 2018</ref>). The Gigaword dataset was created from the English Gigaword corpus 2 , in which the input is the first sentence in a news article, and the output is the headline of the article. The training, validation, and test sets included 3.8M, 189K, and 2K sentences, respectively. The preprocessed data are publicly available <ref type="bibr">3</ref> . The dataset is also used to train official pretrained models of OpenNMT <ref type="bibr">4</ref> . Model and Training: We basically used the de- fault PyTorch implementation of OpenNMT 5 on June 11, 2017 throughout our experiments, but the unidirectional long short-term memory (LSTM) for the encoder was replaced with a bidirectional one to obtain nearly state-of-the-art results. The basic settings are as follows. Our model con- sisted of a bidirectional LSTM for the encoder and a stacked LSTM with input feeding for the de- coder. These LSTMs had two layers with 500- dimensional hidden layers whose dropout rates were 0.3, and their input vectors were created by a 500-dimensional word-embedding layer.</p><p>The model was trained with a stochastic gra- dient descent method with a learning rate of 1.0, where the mini-batch size was set to 64. The learning process ended in 13 epochs, decaying the learning rate with a decay factor of 0.5 in each epoch after 8 epochs. These training settings are the same as the training of the official pretrained models of OpenNMT, and we confirmed that these settings performed better than training with Adam ( <ref type="bibr" target="#b24">Kingma and Ba, 2014</ref>) in our preliminary experi- ments. We prepared 10 learned models by random initialization for the ensemble methods in our ex- periments. Decoding and Evaluation: When decoding input sequences, we used a beam-search algorithm with a beam width of 5. The maximum size of decoded sequences was 100. The generated unknown token &lt;unk&gt; was replaced by the source word with the highest attention weight.</p><p>To evaluate decoded sequences, we calcu- lated ROUGE-1, ROUGE-2, and ROUGE-L <ref type="bibr" target="#b28">(Lin, 2004)</ref>, mainly used in the headline-generation- task ( <ref type="bibr" target="#b35">Rush et al., 2015)</ref>. ROUGE-1 and ROUGE-2 are the co-occurrence rates of unigrams and bi- grams, respectively, between a generated headline and its reference. ROUGE-L is the rate of the longest common subsequence between them to the reference length. We used a Python wrapper of the ROUGE-1.5.5.pl script <ref type="bibr">6</ref> and took the average value of 10 times, each of which used 10 models. Compared Methods: We compared the follow- ing methods. Single is a baseline with a single model. EnsSum and EnsMul are strong baselines with runtime-ensemble. MaxLik and MajVote are weak baselines with naive post-processing. LexRank and LMRank are simple unsupervised methods from two other related tasks, extrac- tive summarization and hypotheses reranking, re- spectively. PostCosE and PostCosB are vari- ants of the proposed method with post-ensemble. PostVmfE and PostVmfB are true density estima- tors corresponding to PostCosE and PostCosB, respectively. Their descriptions are listed below in detail.</p><p>• Single decodes an output by using the best sin- gle model with respect to the word level accu- racy on a validation set.</p><p>• EnsSum and EnsMul decode an output averag- ing multiple models with Eq. <ref type="formula" target="#formula_2">(2)</ref> and Eq. <ref type="formula">(3)</ref>, respectively.</p><p>• MaxLik selects an output with the maximum likelihood, which is calculated by the corre- sponding model p in Alg. 1, from candidate out- puts generated by multiple models.</p><p>• MajVote selects an output by major- ity vote based on exact matching, i.e., y = argmax s∈S |{s ∈ S | s = s }|.</p><p>• LexRank selects an output with the <ref type="bibr">LexRank algorithm (Erkan and Radev, 2004</ref>). We used a Python implementation <ref type="bibr">7</ref> , where a graph is con- structed on the basis of cosine similarities be- tween the tf-idf vectors (without stop-words) of candidate outputs. The idf weights are calcu- lated from the training set.</p><p>• LMRank selects an output that maximizes the likelihood of a (non-conditional) language model p LM , i.e., y = argmax s∈S p LM (s), as in ( <ref type="bibr" target="#b45">Vaswani et al., 2013)</ref>. We used the decoder part of the encoder-decoder model described in Sec. 5.1, which was trained with both source and target sentences in the training set. This allows this model to learn the fluency in both <ref type="bibr">6</ref> https://github.com/pltrdy/files2rouge 7 https://github.com/wikibusiness/lexrank normal and headline-like sentences.</p><p>• PostCosE and PostVmfE select an output on the basis of Alg. 1 with the cosine similarity i.e., K(s, s ) = cos(s, s ), and the von Mises- Fisher kernel, i.e., K(s, s ) = K vmf (s, s ) in Eq. <ref type="formula" target="#formula_6">(5)</ref>, respectively. The feature of each out- put is the average of pretrained 300-dimensional word embeddings 8 .</p><p>• PostCosB and PostVmfB are variants of PostCosE and PostVmfE with simple bag-of- words features (sparse vectors), respectively. In addition, we used the following measure- ments for analysis. MaxRef represents the upper bound for the performance of our method. Mean, Max, and Min represent the performance statistics of the single models.</p><p>• MaxRef selects the best output with respect to ROUGE-1, which is calculated by using the ref- erences in the test set.</p><p>• Mean, Max, and Min are the mean, maximum, and minimum of the (non-ensemble) ROUGE- 1 values for the 10 models, respectively. The difference between Single and Max is that the former uses the validation set, while the latter uses the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>We conducted a comparative experiment on the news-headline-generation task to verify the effec- tiveness of our post-ensemble method compared with the current runtime-ensemble methods. Tab. 1 shows the experimental results for the Gigaword dataset, including the results of our method with 32 models and other previous results. First of all, we can see that the variant of our post-ensemble method, PostCosB, clearly out- performed the runtime-ensemble methods (strong baselines), EnsSum and EnsMul, and the other baselines. The differences between our best method PostCosB and the best baseline EnsSum were all statistically significant on the basis of a one-tailed, paired t-test (p &lt; 0.05). Comparing with the recent results of <ref type="bibr" target="#b3">Cao et al. (2018)</ref> ob- tained with open information extraction and de- pendency parse technologies and the other pre- vious results 9 , our method with 32 models also performed better, although the algorithm of our  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.247</head><p>0: interpol asks members to devise rules for policing ... 1: interpol asks members to devise rules for policing 2: interpol asks members to devise rules for policing at ...</p><p>3: interpol asks members to devise rules on policing 4: interpol asks members to devise rules and procedures ... 5: interpol seeks rules for policing of global level 6: interpol seeks rules for policing at global level 7: interpol asks members to act against wanted fugitives 8: interpol asks members to help fight fugitives 9: interpol asks for legal status for red corner notices <ref type="figure">Figure 2</ref>: Left scatter-plot shows two-dimensional visualization of outputs generated from 10 models on basis of multi-dimensional scaling <ref type="bibr" target="#b8">(Cox and Cox, 2008)</ref>, and right list shows their contents. Each point in plot represents sentence embedding of corresponding output, and label indicates model ID and ROUGE-1, i.e., "ID (ROUGE)." Color intensity means score of kernel density estimation of PostCosE (see right color bar), and outputs are sorted by scores. Reference and input are as follows. Each bold word in above list means co-occurrence with reference below. Reference: interpol asks world govts to make rules for global policing Input: top interpol officers on wednesday asked its members to devise rules and procedures for policing at the global level and providing legal status to red corner notices against wanted fugitives .  <ref type="table">Table 1</ref>: F-measure ROUGE-1, ROUGE-2, and ROUGE-L scores (%) for news-headline-generation task. Bold and underlined scores represent best scores for ensembles of 10 models and all methods excluding measurements with " * ," respectively. Results with "" are taken from corresponding papers.</p><formula xml:id="formula_13">R-1 R-2 R-L<label>Single</label></formula><p>method is quite simple. Note that our method can be easily applied to their models to improve their results. Looking at the row for MaxRef, the results imply that our post-ensemble method still has room for improvement without any changes to model structure. Although we also conducted an experiment by changing the settings of model preparation, the results had a similar tendency to those of the main results (see Sec. 5.4). <ref type="figure">Fig. 2</ref> illustrates how our method worked with kernel density estimation (see the figure caption for detailed descriptions). The left scatter-plot shows a two-dimensional visualization of 10 out- puts generated from the 10 models and the esti- mated densities (represented by color intensity in the right bar). Looking at the center part of the plot, we can see that there are many good outputs with high ROUGE-1 results (noted in brackets in the plot) in the dense part. The right list shows the corresponding outputs of the points in the left plot, where these outputs are sorted by the estimated density. The list shows that our method success- fully obtained the majority-like output (model ID of 0) in the dense part of the output space, although there are no exact match outputs. Looking at the bottom part of the list, we can see that our method clearly eliminated unpromising outputs (model ID of 7, 8, and 9) with less information, since they are scattered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Number of Models</head><p>We compared the effect of changing the number of models on the performance of our best method PostCosB and the best baseline EnsSum. We pre-  <ref type="figure">Figure 3</ref>: F-measure ROUGE-1 performance (%) vs. number of models for news-headline-generation task. X-axis is log scale (2 1 -2 7 ). pared 128 models, in which each training took more than two days. The ROUGE-1 performance was measured by varying the number of models, i.e., 2, 4, · · · , 128. <ref type="figure">Fig. 3</ref> shows the performance of our best method PostCosB, the corresponding true esti- mator PostVmfB, the best baseline EnsMul, and the most widely-used baseline LexRank versus the number of models. Note that we could not calcu- late the results of EnsMul for more than 16 models due to out of memory errors. The figure shows that PostCosB performed better than EnsMul even for these 16 models. We obtained a 37.48 ROUGE- 1 score with 32 models, which was better than the state-of-the-art results in Tab. 1, but the per- formance seems to be saturated with more than 32 models. Looking at PostCosB and PostVmfB, we can see that the performances are almost the same, which also supports our theoretical analysis in Sec. 4. LexRank did not work well even though the number of models was large.</p><p>The complexity of the post-ensemble procedure in Alg. 1 is O(βν +δν 2 ), where ν is the number of models, δ is the dimension of the output space, and β is the number of operations of the beam-search. We can reduce it to O(β+δν) by simply paralleliz- ing lines 2-4 and 6-8 in Alg. 1 without any change to the model code on the deep learning framework. Since the operations of β includes all matrix cal- culations in the model, we can basically assume β δν. In fact, the actual calculation times of PostCosE and PostCosB with a naive implemen- tation in Python were 0.0097 and 0.0037 seconds per sentence when ν = 32, respectively. They are enough for practical use in comparison with the decoding speeds of 0.044 (GPU) and 0.49 (CPU) seconds per sentence. In addition, the complex- ity of the runtime-ensemble is O(βν), which can- not be parallelized without modifying more than a  <ref type="table">Table 2</ref>: F-measure ROUGE-1 scores (%) of random- ensemble (Random), self-ensemble (Self), hetero- ensemble (Hetero), and bagging-ensemble (Bagging) for news-headline-generation task. Bold scores rep- resent best scores for all methods excluding measure- ments with " * ."</p><p>hundred lines of code after understanding a whole system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Model Preparation</head><p>We conducted experiments to verify the effect of changing the model preparation on post-ensemble performance. In addition to random initializa- tion (random-ensemble), we address three varia- tions of model preparation: self-ensemble, hetero- ensemble, and bagging-ensemble. The first one, self-ensemble, is a method of extracting models from "checkpoints" saved in each epoch in a train- ing. We prepared the models of self-ensemble by using 10 checkpoints from 4-13 epochs. The sec- ond one, hetero-ensemble, is a method of train- ing models varying in model structure. We pre- pared 10 models for hetero-ensemble, consisting of 8 models prepared by changing the number of layers in the LSTM encoder/decoder in {2, 3}, the size of LSTM hidden states in {250, 500}, and the size of word embedding in {250, 500}, and two models prepared by replacing the bidi- rectional encoder with a unidirectional encoder and a bidirectional encoder with a different merge action, i.e., summation instead of concatenation. The third one, bagging-ensemble, is a method of training models by bagging of training data. We randomly extracted 80% of the training data 10 times and prepared 10 models for bagging- ensemble. We used the same dictionary of the original data for these models, since the runtime- ensemble methods, EnsSum and EnsMul, failed to average the models with different dictionaries.</p><p>Note that the outputs for self-ensemble and hetero- ensemble cannot be regarded as i.i.d samples, but we believe the basic idea can be practically ap- plied. Tab. 2 shows the F-measure ROUGE-1 scores for the Gigaword dataset of the above three vari- ations, self-, hetero-, and bagging-ensembles, as well as random-ensemble. The table indicates that all variants of our post-ensemble method per- formed better than the current runtime-ensemble methods, EnsSum and EnsMul, for all variations of model preparation. Looking at the row for PostCosE, random-ensemble was the most effec- tive, while self-ensemble was the worst, as ex- pected. Bagging-ensemble was relatively effective for post-ensemble according to the relative im- provement from Single, despite the fact that we trained the models with 80% of the training data. Hetero-ensemble performed worse than random- ensemble for these settings, but we expect that if the model structure can be randomly chosen, hetero-ensemble will perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Distillation techniques for an ensemble of mul- tiple models have been widely studied ( <ref type="bibr" target="#b26">Kuncoro et al., 2016;</ref><ref type="bibr" target="#b4">Chebotar and Waters, 2016;</ref><ref type="bibr" target="#b22">Kim and Rush, 2016;</ref><ref type="bibr" target="#b14">Freitag et al., 2017;</ref><ref type="bibr" target="#b38">Stahlberg and Byrne, 2017)</ref>, especially after a study by <ref type="bibr" target="#b18">Hinton et al. (2015)</ref>. <ref type="bibr" target="#b26">Kuncoro et al. (2016)</ref> and Chebotar and Waters (2016) studied distillation techniques for ensembles of multiple dependency parsers and speech recognition models, respectively. There are several ensemble methods for ensembles of ma- chine translation models <ref type="bibr" target="#b22">(Kim and Rush, 2016;</ref><ref type="bibr" target="#b14">Freitag et al., 2017;</ref><ref type="bibr" target="#b38">Stahlberg and Byrne, 2017)</ref>. For example, <ref type="bibr" target="#b38">Stahlberg and Byrne (2017)</ref> pro- posed a method of unfolding an ensemble of mul- tiple translation models into a single large model once and shrinking it down to a small one. How- ever, all methods require extra implementation on a deep-learning framework, and it is not easy to apply them to other models. Our post-ensemble method does not require such coding skills. In addition, since the predictions of post-ensemble can be regarded as a teacher model, these distilla- tion techniques should be combined with a teacher model based on post-ensemble.</p><p>Hypotheses reranking of language generation has been extensively studied, but most studies fo- cused on discriminative training using costly an- notated data <ref type="bibr" target="#b36">(Shen et al., 2004;</ref><ref type="bibr" target="#b47">White and Rajkumar, 2009;</ref><ref type="bibr" target="#b12">Duh et al., 2010;</ref><ref type="bibr" target="#b21">Kim and Mooney, 2013;</ref><ref type="bibr" target="#b31">Mizumoto and Matsumoto, 2016)</ref>. The main stream of our focused unsupervised ap- proach was a reranking method based on a lan- guage model <ref type="bibr" target="#b5">(Chen et al., 2006;</ref><ref type="bibr" target="#b45">Vaswani et al., 2013;</ref><ref type="bibr" target="#b30">Luong and Popescu-Belis, 2016)</ref>, and other approaches include reranking methods based on key phrase extraction <ref type="bibr" target="#b2">(Boudin and Morin, 2013)</ref>, dependency analysis ( <ref type="bibr" target="#b17">Hasan et al., 2010)</ref>, and search results ( <ref type="bibr" target="#b33">Peng et al., 2013</ref>). All of the above described studies were not used for model ensemble. <ref type="bibr" target="#b43">Tomeh et al. (2013)</ref> used an ensem- ble learning, but the purpose was to improve the performance of the reranking model for hypothe- ses reranking of a single model. <ref type="bibr" target="#b27">Li et al. (2009)</ref>, which work is the most related one, proposed a reranking algorithm for model ensemble. How- ever, their method was constructed to perform at decoding time, so it can be regarded as runtime- ensemble.</p><p>The term "frustratingly easy" in this paper is borrowed from "frustratingly easy" papers <ref type="bibr" target="#b9">(Daumé III, 2007;</ref><ref type="bibr" target="#b10">Daumé III et al., 2010;</ref><ref type="bibr" target="#b44">Tommasi and Caputo, 2013;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a simple but effective model- ensemble method, called post-ensemble, for abstractive-summarization models, i.e., encoder- decoder models. We verified the effectiveness of our method on the news-headline-generation task.</p><p>We will release the 128 prepared models used in this paper <ref type="bibr">10</ref> , each of which was trained for more than two days, as a new dataset for improving en- semble methods. For example, future research in- cludes applying learning-to-rank regarding all out- puts as features, conducting active learning to se- lect a new model setting online, and developing boosting-like-ensemble based on the bagging of training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flow charts of current runtime-ensemble (a) and our proposed post-ensemble (b).</figDesc><graphic url="image-2.png" coords="1,397.58,227.52,124.42,79.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 1 .</head><label>1</label><figDesc>The output y of Alg. 1 with K(s, s ) = cos(s, s ) is equivalent to the maxi- mization of the first order Taylor series approxi- matioñ p of the kernel density estimator p based on the von Mises-Fisher kernel, i.e., ˜ p(y) = max s∈S˜ps∈S˜ s∈S˜p(s),</figDesc></figure>

			<note place="foot" n="1"> https://github.com/OpenNMT/OpenNMT-py</note>

			<note place="foot" n="2"> https://catalog.ldc.upenn.edu/LDC2012T21 3 https://github.com/harvardnlp/sent-summary 4 http://opennmt.net/Models/ 5 SHA: c13a558767cbc19b612968eb4d01a1f26d5df688</note>

			<note place="foot" n="8"> https://github.com/mmihaltz/word2vec-GoogleNewsvectors 9 We did not present the results of Raffel et al. (2017) since Kiyono et al. (2017) pointed out that their settings are different from the previous studies.</note>

			<note place="foot" n="10"> https://research-lab.yahoo.co.jp/en/software/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The author would like to thank the anonymous re-viewers of all versions of this paper for sparing their valuable time and leaving many insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent Advances on Neural Headline Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Qi</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Kai</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cun-Chao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Yuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="768" to="784" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Morin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Faithful to the Original: Fact Aware Neural Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence (AAAI 2018)</title>
		<meeting>the ThirtySecond AAAI Conference on Artificial Intelligence (AAAI 2018)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4784" to="4791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2016, 17th Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3439" to="3443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reranking Answers for Definitional QA Using Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006)</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Question Answering for Long Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="209" to="220" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2016)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multidimensional Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2017)</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Semi-Supervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">N-best reranking by multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LexRank: Graph-based Lexical Centrality As Salience in Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ensemble Distillation for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<idno>abs/1702.01802</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exact risk improvement of bandwidth selectors for kernel density estimation with directional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Garcia-Portugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1655" to="1685" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel Density Estimation with Spherical Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Cabrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="751" to="762" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reranking Translation Hypotheses Using Structural Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL&apos;06 Workshop on Learning Structured Information in Natural Language Applications</title>
		<meeting>the EACL&apos;06 Workshop on Learning Structured Information in Natural Language Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Google&apos;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Learning without Poor Local Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS 2016)</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adapting Discriminative Reranking to Grounded Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="218" to="227" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SequenceLevel Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Neural Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING 2016)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Source-side Prediction for Neural Headline Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<idno>abs/1712.08302</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1744" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collaborative Decoding: Partial Hypothesis Re-ranking Using Translation Consensus between Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Ho</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP 2009)</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP 2009)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Text Summarization Branches Out</title>
		<meeting>the ACL Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Weighted Majority Algorithm. Information and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Littlestone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="212" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Contextual Language Model to Improve Machine Translation of Pronouns by Re-ranking Translation Hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Quang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Baltic Journal of Modern Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="292" to="304" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative reranking for grammatical error correction with statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoya</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2016)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1133" to="1138" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Guaranteed Bounds on Information-Theoretic Measures of Univariate Mixtures Using Piecewise Log-Sum-Exp Inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Search results based N-best hypothesis rescoring with maximum entropy classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shahshahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="422" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online and LinearTime Attention by Enforcing Monotonic Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2837" to="2846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative Reranking for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Short Note on Parameter Approximation for Von Mises-Fisher Distributions: And a Fast Implementation of I s (x)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unfolding and Shrinking Neural Machine Translation Ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1946" to="1956" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Return of Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016)</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence (AAAI 2016)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2058" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cutting-off redundant repeating generations for neural abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="291" to="297" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural Headline Generation on Abstract Meaning Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sho Takase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reranking with Linguistic and Semantic Features for Arabic Optical Character Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadi</forename><surname>Tomeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="549" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Frustratingly Easy NBNN Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV 2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Decoding with Large-Scale Neural Language Models Improves Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimal Convergence Properties of Variable Knot, Kernel, and Orthogonal Series Methods for Density Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Perceptron Reranking for CCG Realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Selective encoding for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
