<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">SCI Institute</orgName>
								<orgName type="institution" key="instit2">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">SCI Institute</orgName>
								<orgName type="institution" key="instit2">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Pascucci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Peer-Timo</roleName><surname>Bremer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Utah</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Lawrence Livermore National Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations)</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (System Demonstrations) <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="36" to="41"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>36</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural networks models have gained unprecedented popularity in natural language processing due to their state-of-the-art performance and the flexible end-to-end training scheme. Despite their advantages, the lack of inter-pretability hinders the deployment and refinement of the models. In this work, we present a flexible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the model internals (i.e., attention), and the output predictions , which in turn shed light on the model decision-making process.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have been successfully ap- plied to various natural language processing tasks. As the design of neural networks evolves, there is a clear trend of increasing model complexity in terms of both the architecture and the param- eter count. Although expressive models help im- prove the prediction performance, the fundamen- tal lack of interpretability leads many researchers to consider neural network models black boxes. The ability to interpret model internals and rea- son about predictions is essential for understand- ing the limitation of the models and improving upon them.</p><p>Recently, the attention networks have recently become widely-adopted ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b9">Seo et al., 2017;</ref><ref type="bibr">Parikh et al., 2016b;</ref><ref type="bibr" target="#b11">Vaswani et al., 2017)</ref>.</p><p>Attention not only improves the model performance but also yields inter- pretable intermediate representations (i.e., align- ment among words). As a result, attention can pro- vide a natural interface for analyzing the internals of neural networks. Conducting analysis directly on the raw attention values can be challenging for human users, and therefore, visual representations that highlight word alignments between sentences have been proposed, such as the bipartite graph and the heatmap of the attention matrix ( <ref type="bibr" target="#b2">Li et al., 2015</ref><ref type="bibr" target="#b3">Li et al., , 2016</ref><ref type="bibr" target="#b1">Lee et al., 2017)</ref>. However, many challenges remain. Firstly, for some NLP tasks, the word sequences pair for which the attention is computed can be highly asymmetrical (i.e., in ma- chine comprehension, the context paragraph can be much longer than the question sentence), which the standard visual encoding cannot adequately handle. Also, many previous works present the attention in a static setting. However, the ability to provide an interactive environment in which the user can instantaneously look at how changes in the input affect the attention, and how small varia- tions in the attention alter the prediction, is crucial for interpreting the model. Finally, many previ- ous visualization efforts, despite revealing many exciting results, focus more on illustrating poten- tially useful visualization techniques than on pro- viding a flexible software tool that can be easily integrated into an existing code base. As a result, applying these proposed techniques to real-world examples may involve substantial engineering ef- fort, which could prevent wide adoption.  <ref type="figure">Figure 1</ref>: Perturbation-driven interrogation of the end- to-end models that follow the encoder, attention, and classifier structure. The user can generate a small per- turbation of the input (i.e., replacing synonyms, para- phrasing), edit the attention, and observe the changes.</p><p>To address these challenges in interpreting attention-based models, we introduce a Python li- brary that allows users to create web-based in- teractive visual analytics environments, in which they can interrogate the model by perturbing the input text and observing the effects on the atten- tion and prediction, or modifying the attention and studying how it affects predictions (see <ref type="figure">Figure 1)</ref>. We demonstrate our library applied to two NLP tasks: natural language inference (NLI) and ma- chine comprehension (MC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Due to the increasing demand for model inter- pretability, many previous works have been pro- posed for making sense of NLP models by exam- ining individual predictions as well as the model mechanism as a whole. In recent work, <ref type="bibr" target="#b2">Li et al. (2015)</ref> investigated the composability of the vector-based text representations using instance- level attribution techniques that originated from the vision community (e.g., <ref type="bibr" target="#b12">Zeiler and Fergus, 2014)</ref>. In a study of the representation of erasure, <ref type="bibr" target="#b3">Li et al. (2016)</ref> explained neural model decisions by exploring the impact of altering or removing the components of the model (i.e., changing the dimension count of hidden units or input words) on the prediction performance. Besides interpreting the model via carefully de- signed experiments, several interactive demo/vi- sualization systems, such as AllenNLP's demos (http://demo.allennlp.org/), often rely on visual encodings to summarize the model pre- dictions. These systems provide a flexible envi- ronment in which the user can experiment with the various inputs and perform error analysis. The hidden state properties of the LSTM are visual- ized and investigated in the LSTMvis visualiza- tion system <ref type="bibr" target="#b10">(Strobelt et al., 2018)</ref>. <ref type="bibr" target="#b1">Lee et al. (2017)</ref> visualized the beam search and attention component in neural machine translation models, in which the user can dynamically change the probability for the next step of the search tree or change the weight of the attention. In the visual- ization work on question answering (Rücklé and Gurevych, 2017), the system shows the text con- text and highlights the critical phrase that is used to answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visualization System</head><p>As illustrated in <ref type="figure">Figure 1</ref>, many recent end-to-end NLP models follow a similar encoder-attention- classifier structure. The attention stage provides a window to peek into the model decision-making process. However, the static attention alone does not tell the whole story. Our proposed system uses a perturbation-driven exploration strategy that al- lows the user to manually or automatically perturb part of the pipeline and observe the changes down- stream. The system consists of several visualiza- tion components that can be selectively combined to form a functional interactive system for a given task. In the following section, we will cover some of the essential components; the usage of all com- ponents can be found in the accompanying video link. The user can manually edit the words or apply au- tomatic perturbation/paraphrasing of the inputs. In the "perturbed" drop-down menu, the blue background highlights words not in the original sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input Sentences Perturbation</head><p>Due to the discrete nature of natural language, automatically perturbing a sentence for sensitiv- ity analysis can be particularly challenging (com- pared to other domains such as images) -small changes in words can lead to drastic differences in the semantics of the sentences. To reduce the po- tential semantic deviation, we allow for a straight- forward perturbation method by replacing nouns and verbs by their synonyms in WordNet <ref type="bibr" target="#b5">(Miller, 1995)</ref>. However, synonym replacement does not guarantee that the meaning of the sentence re- mains the same. Furthermore, WordNet often pro- duces rare words or obscure usages that may lead to less meaningful sentences.</p><p>To improve the perturbation quality, we also al- low for a translation-based paraphrasing technique similar to that proposed by <ref type="bibr" target="#b4">Mallinson et al. (2017)</ref>. Here, we translate the original English sentence into several other languages and then pivot back to English. Provided the translation produces a good result (in our case, we use the Google Cloud Translation Platform), we can obtain paraphrases of the original sentence (see <ref type="figure" target="#fig_0">Figure 2</ref>, where the drop-down menu shows some of the perturbed sentences). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prediction Visualization</head><p>Efficient visual encoding for prediction is crucial for communicating the model behavior, and to fully support the input perturbation feature, the vi- sual encoding should also allow multiple predic- tions to be shown in the same visualization. For the natural language inference task, as il- lustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, the predicted probabilities are encoded via a triangular barycentric coordinate system, where the vertices represent the three pos- sible predictions (namely, entailment, contradic- tion, and neutral). The prediction for the original unperturbed input is illustrated by a larger yellow circle, whereas the prediction of perturbed inputs is represented by smaller gray circles.</p><p>A density contour of the prediction is computed to emphasize the highly cluttered areas and detect outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Visualization</head><p>As illustrated in <ref type="figure" target="#fig_2">Figures 4(a)(b)</ref>, the most widely adopted visual encodings for attention are bipartite graphs (a) and heatmaps (b). In the graph attention view <ref type="figure" target="#fig_2">(Figure 4(a)</ref>), the edge thickness corresponds to the attention value of the word pairs. The graph view is suitable for highlighting the most domi- nant alignments. However, the edges may become cluttered if multiple attention values are high. The matrix attention view <ref type="figure" target="#fig_2">(Figure 4(b)</ref>) resolves these issues, despite being more verbose and less ef- ficient in highlighting the most dominant align- ments. We also enable the linkage between high- lighted actions in both views (see <ref type="figure" target="#fig_2">Figure 4</ref>(a)(b), where one alignment relationship is highlighted).</p><p>We augment these standard visual encodings with grammar structure to address the challenge of long sentences. The text can be dynamically simplified based on the dependency tree (see <ref type="figure" target="#fig_2">Fig- ure 4(a1)</ref>). We also show how the dependency in- formation can potentially improve the prediction result in Section 4.1. To facilitate the perturbation of attention (see <ref type="figure">Figure 1</ref>), as illustrated in <ref type="figure" target="#fig_2">Fig- ure 4(c)</ref>, we implement an interface for directly manipulating the attention value.</p><p>However, when the text sequence becomes sig- nificantly longer, i.e., a full paragraph in the ma- chine comprehension task, even the simplified sentence cannot be meaningfully represented in the graph or matrix visual encoding. To address this visualization challenge, as illustrated in <ref type="figure" target="#fig_2">Fig- ure 4(d)</ref>, we introduce a hierarchical representa- tion. Here, a pure graphical encoding (the color bars marked by att2) is used to capture the ag- gregated attention information for the whole para- graph. The user can focus on localized attention information by selecting a pixel bar that repre- sents a single sentence (the colored blocks in the bar correspond to individual works). We also link this attention representation with the matrix form, such that whenever a sentence is selected the lo- cal attention is shown in the matrix view (see <ref type="figure">Fig- ure 6(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>The initial setup cost and unnecessary learning curve are often the barriers to broad adaptation of a tool. Therefore, instead of designing a vi- sualization system as a monolithic standalone ap- plication, we implement the proposed system as a Python library with modularity and ease of use in mind. The different pieces of the visualization (i.e., matrix-based attention encoding) can be ac- cessed individually or combined with other com- ponents to fit one's workflow via a simple API.</p><p>More importantly, the library-based design allows easy integration with the existing model imple- mented in Python. The code for creating an in- teractive exploration environment for a machine comprehension model is illustrated below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>from visPackage import MCModule</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Bipartite Graph Attention Representation (b) Matrix Attention Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications</head><p>We demonstrate the proposed visualization system on the decomposable attention network (Parikh et al., 2016a) for the NLI task and the BIDAF model ( <ref type="bibr" target="#b9">Seo et al., 2017)</ref> for the MC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Natural Language Inference</head><p>The NLI task predicts the entailment relationship between a premise sentence (P) and a hypothe- sis sentence (H). The attention matrix captures the alignment of words between these two sentences.</p><p>Here we give an example of how a wrong predic- tion can be corrected by editing attention values.</p><p>As illustrated in <ref type="figure" target="#fig_3">Figure 5</ref>(a), the input sentence pair (P:"A woman in a green jacket is drinking tea." H:"A woman is drinking green tea.") is pre- dicted to be entailment, which is incorrect. By ex- amining the attention, we can see the word green in "green jacket" is aligned to the green in "green tea." However, these two greens modify differ- ent nouns, which potentially leads to the wrong prediction. The grammatical structure is visually shown in the form of the dependency tree. How- ever, the model does not have access to the syn- tactic information and mistakenly assumes the two greens modify the same thing. To correct the mis- take, we can edit the attention value and remove the align between these two "greens" (see (c)(b)). As expected, the prediction label is corrected (neu- tral).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Comprehension</head><p>In the machine comprehension task, the goal is to select a span of text as the answer to a question sentence. The attention information is encoded Figure 6: In the machine comprehension visualization interface (a), the p1, p2 colored bar (in a3) illustrates the predicted start and end index of the answer in the context (the deeper the red, the higher the probability). The most likely answer is shown in (a1). The global attention and local attention are visualized by (a2, a3). We can evaluate the robustness of the prediction by perturbing the question sentence (b1, b3). As illustrated in (b1, b2), by removing the word "partial", the model still finds the correct answer (albeit different, as the sentence perturbation changes the exact meaning of the question).</p><p>as a bidirectional alignment (i.e., from context to question and vice versa). Here, we refer the con- text to question attention as att1 and the question to context attention as att2 (which is a vector in- stead of a matrix according to <ref type="bibr" target="#b9">Seo et al. (2017)</ref>).</p><p>In this demo, we apply a min max normalization for att2 after the softmax layer to better distinguish different attention values.</p><p>As illustrated in <ref type="figure">Figure 6</ref>, we represent att2 as colored bars with a yellow-green-blue colormap. Each rectangular bar corresponds to one sentence. The user can focus on individual sentences by clicking on the rectangular bar (see <ref type="figure">Figure 6</ref>). The p1, p2 colored bars (white-red colormap) illustrate the predicted probabilities of the start and the end index for the answer (the deeper the higher). In <ref type="figure">Figure 6</ref>(a3), the sentence containing the answer exhibits good alignment with the question (e.g., "Two" with "many"). Interestingly, the number "9" (in "Falcon 9") is also aligned with "many", which may lead to problems.</p><p>The user can explore the robustness of the model by examining how the prediction varies when the question is perturbed. As illustrated in (b1, and b2), the perturbation removes the word "partial" in the original sentence, which leads the model to produce a different yet correct answer ("No"). Referring to (a1), we can see the word "No" exhibits the second highest probability for the original question. The user can also manually edit the text. As shown in (b3), the model still pro- duces the correct answer when changing the ques- tion from "how many" to "which".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this work, we introduce a visualization library for creating customized environments that allows the user to interrogate the relationships among dif- ferent parts of the model pipeline via interactive queries. We demonstrate the usability and flexi- bility of the tool by configuring the visual com- ponents to investigate models for different NLP tasks (e.g., NLI, MC). We also conducted a small- scale user evaluation, in which five Ph.D.-level students with an NLP background spent 30 min- utes with the tool and then provided feedback. Most suggested the environment allowed them to refine queries iteratively and identify potential is- sues with the model, but some also mentioned the tool might not provide enough guidance for users who do not have an in-depth understanding of the model at hand.</p><p>Even though we designed the individual compo- nents with versatility in mind, due to a large num- ber of variants of attention networks, we found it difficult to ensure compatibility with all the avail- able configurations. In the future, we plan to im- prove upon the current attention interface, release the library as an open-source package, and expand the visualization components to handle tasks such as neural machine translation and more.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The interface for showing input sentences. The user can manually edit the words or apply automatic perturbation/paraphrasing of the inputs. In the "perturbed" drop-down menu, the blue background highlights words not in the original sentence.</figDesc><graphic url="image-1.png" coords="2,316.83,257.51,196.44,114.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Summary of the prediction results of the perturbed input for the natural language inference model. The prediction is encoded as a point in the barycentric coordinate system of the triangle, in which each vertex corresponds to one prediction label.</figDesc><graphic url="image-2.png" coords="3,94.17,129.16,174.27,163.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :bidaf_src import bidafModelInterface from NLPutility import translationPerturbation #initialize machine comprehension model model = bidafModelInterface( wordDict="data/bidaf/squad.word.dict", wordVec="data/bidaf/glove.hdf5", model="data/bidaf/bidaf.ema") gen = translationPerturbation() #visualization components visLayout = { "column":[{"row": ["Paragraph", "AttentionSubMatrix"]}, {"row": ["AttentionAsymmetric"]}] } #setup interface modelVis = MCModule(visLayout) modelVis.setPredictionHook(model.predict) modelVis.setAttentionHook(model.attention) modelVis.setSentenceHook(gen.perturbSentence) #open browser for the web-based visualization modelVis.</head><label>4</label><figDesc>Figure 4: Attention visualization. A bipartite graph encoding is adopted in the graph attention view (a), in which the edge thickness corresponds to the attention value. The same attention values can also be directly visualized in the matrix form (b). The user can edit the attention values via the pop-up interface illustrated in (c). We overlay the dependency tree (a 1 ) grammar structure to highlight important words and allow simplification of complex sentences (shown in the video). For highly asymmetric attention, we utilize a zoomable hierarchical visual representation (d). The user can focus on the individual sentence by selecting the summary visualization.</figDesc><graphic url="image-5.png" coords="4,87.82,207.57,415.99,103.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An illustration of the attention editing process. The dependency structure is shown in (a), where the two "greens" decorate different nouns. By removing the "wrong" alignment in (b), the original prediction entailment is corrected to neutral in (c).</figDesc><graphic url="image-12.png" coords="5,72.00,247.73,248.04,207.25" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was performed under the aus-pices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. This work is also supported in part by NSF: CGV: Award: 1314896, NSF: IIP: Award: 1602127, NSF: ACI: award: 1649923, DOE/SciDAC DESC0007446, CCMSC DE-NA0002375, PIPER: ER26142 DE-SC0010498, and NVIDIA Corporation. This material is based upon work supported by the Department of Energy, National Nuclear Secu-rity Administration, under Award Number(s) DE-NA0002375.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive visualization and manipulation of attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Hwi</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Seok</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01066</idno>
		<title level="m">Visualizing and understanding neural models in nlp</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<title level="m">Understanding neural networks through representation erasure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Täckström</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01933</idno>
		<title level="m">Dipanjan Das, and Jakob Uszkoreit. 2016b. A decomposable attention model for natural language inference</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Endto-end non-factoid question answering with an interactive visualization of neural attention weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="667" to="676" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
