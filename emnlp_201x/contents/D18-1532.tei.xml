<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kondratyuk</surname></persName>
							<email>dankondratyuk@gmail.com, gavento@kam.mff.cuni.cz, {straka,hajic}@ufal.mff.cuni.cz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Gavenčiak</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Mathematics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Physics † Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4921" to="4928"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4921</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present LemmaTag, a featureless neu-ral network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphologically rich languages are often difficult to process in many NLP tasks ( <ref type="bibr" target="#b17">Tsarfaty et al., 2010)</ref>. As opposed to analytical languages like English, morphologically rich languages encode diverse sets of grammatical information within each word using inflections, which convey char- acteristics such as case, gender, and tense. The addition of several inflectional variants across many words dramatically increases the vocabu- lary size, which results in data sparsity and out- of-vocabulary (OOV) issues.</p><p>Due to these issues, morphological part-of- speech (POS) tagging and lemmatization are heav- ily used in NLP tasks such as machine transla- tion ( <ref type="bibr">Fraser et al., 2012</ref>) and sentiment analysis ( <ref type="bibr" target="#b0">Abdul-Mageed et al., 2014</ref>). In morphologically rich languages, the POS tags typically consist of multiple morpho-syntactic subcategories provid- ing additional information (see <ref type="figure" target="#fig_0">Figure 1</ref>). Closely related to POS tagging is lemmatization, which in- volves transforming each word to its root or dic- tionary form. Both tasks require context-sensitive awareness to disambiguate words with the same form but different syntactic or semantic features and behavior. Furthermore, lemmatization of a word form can benefit substantially from the in- formation present in morphological tags, as gram- matical attributes often disambiguate word forms using context <ref type="bibr">(Müller et al., 2015)</ref>.</p><p>We address context-sensitive POS tagging and lemmatization using a neural network model that jointly performs both tasks on each input word in a given sentence. <ref type="bibr">1</ref> We train the model in a super- vised fashion, requiring training data containing word forms, lemmas, and POS tags. In addition, we incorporate the ideas from <ref type="bibr">Inoue et al. (2017)</ref> to optionally allow the network to predict the sub- categories of each tag to improve accuracy. Our model is related to the work of <ref type="bibr">Müller et al. (2015)</ref>, which use conditional random fields (CRF) to jointly tag and lemmatize words for morphologi- cally rich languages. The idea of jointly predict- ing several dimensions of categories has been ex- plored prior to this work, for example, joint mor- phological and syntactic analysis <ref type="bibr">(Bohnet et al., 2013)</ref> or joint parsing and semantic role labeling ( <ref type="bibr">Gesmundo et al., 2009)</ref>.</p><p>Our model consists of three parts: (1) The shared encoder, which creates an internal repre- sentation for every word based on its character se- <ref type="bibr">1</ref> The code for this project is available at https:// github.com/hyperparticle/LemmaTag V B -S ---3 P -A A --- quence and the sentence context. We adopt the encoder architecture of <ref type="bibr">Chakrabarty et al. (2017)</ref>, utilizing character-level ( <ref type="bibr">Heigold et al., 2017)</ref> and word-level embeddings ( <ref type="bibr" target="#b5">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b10">Santos and Zadrozny, 2014</ref>) processed through several layers of bidirectional recurrent neural networks (BRNN/BiRNN) <ref type="bibr" target="#b11">(Schuster and Paliwal, 1997;</ref><ref type="bibr">Chakrabarty et al., 2017)</ref>. <ref type="formula">(2)</ref> The tagger decoder, which applies a fully-connected layer to the outputs of the shared encoder to predict the POS tags. (3) The lemmatizer decoder, which applies an RNN sequence decoder to the combined outputs of the shared encoder and tagger decoder, producing a sequence of characters that predict each lemma (similar to <ref type="bibr" target="#b2">Bergmanis and Goldwater (2018)</ref>).</p><p>The main advantages over other proposed mod- els are: (i) The model is featureless, requiring little to no text preprocessing or morphological analysis postprocessing. (ii) The model shares the word embeddings, character embeddings, and RNN encoder weights in the tagger and lemma- tizer, improving both tagging and lemmatization accuracy while reducing the number of parameters required for both tasks. (iii) The model predicts tag subcategories and provides the output of the tagger as features for the input of the lemmatizer, further improving accuracy.</p><p>We evaluate the accuracy of our model in POS tagging and lemmatization across several lan- guages: Czech, Arabic, German, and English. For each language, we also compare the performance of a fully separate tagger and lemmatizer to the proposed joint model. Our results show that our joint model is able to improve the accuracy for both tasks, and achieves state-of-the-art perfor- mance in both POS tagging and lemmatization in Czech, German, and Arabic, while closely match- ing state-of-the-art performance for English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Joint LemmaTag Model</head><p>Given a sequence of words in a sentence w 1 , . . . , w k , the task of the model is to produce a sequence of associated tags t 1 , . . . , t k and lemmas 1 , . . . , k . For a word w i at position i, we denote c i,1 , c i,2 . . . c i,m i to be the sequence of characters that make up w i , where m i indicates the length of the word string at position i. Analogously, we de- fine l i,1 , . . . l i,λ i to be the sequence of characters that make up the lemma i . Our proposed model (shown in <ref type="figure" target="#fig_1">Figures 2 and 3)</ref> is split into three parts: the shared encoder, the tagger, and the lemmatizer. The initial layers of the model are shared between the tagger and lemmatizer, encoding the words, characters, and context in a given sentence. The encoder then passes its outputs to two networks, which perform a classification task to predict tags by the tagger and a sequence prediction task to output lemmas (character-by-character) in the lemmatizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Shared Encoder</head><p>In the encoder shown in <ref type="figure" target="#fig_1">Figure 2</ref>, each charac- ter c i,1 , c i,2 . . . c i,m i of a word w i is indexed into an embedding layer to produce fixed-length em- bedded vectors representing each character. These vectors are further passed into a layer of BRNNs composed of gated recurrent units (GRU) ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) producing outputs e c 1 , . . . , e c m , and whose final states are concatenated to produce the character-level embedding s c i of the word. Sim- ilarly, we index w i into a word-level embedding layer to compute vector e b i . Then we sum these re- sults to produce the final word embedding e w i = s c i + e b i . We repeat this process independently for all the words in the sentence and feed the resulting sequence e w 1 . . . e w k into another two BRNN lay- ers composed of long short-term memory units (LSTM) with residual connections. This pro- duces word-level outputs o w 1 , . . . o w k that encode sentence-level context for each word (we ignore the final hidden states).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Tagger</head><p>The task of the tagger is to predict a tag t i ∈ T given a word w i and its context, where T is a set of possible tags. As explained the introduction, morphologically rich languages typically subdi- vide tags further into several subcategories t i = (t i,1 , . . . , t i,τ ), where t i,j ∈ T j , the j-th subcate- gory. See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration taken from the Czech PDT tagset where τ = 15.</p><p>Having the encoded words of a sentence avail- able, the tagger consists of a fully-connected layer with |T | neurons whose input is the output of the word feature RNN o w i (see <ref type="figure" target="#fig_1">figure 2</ref>). This layer produces the logits t i of the tag values and the pre- dictions t i as the maximum-likelihood value (i.e., softmax).</p><p>To obtain the information about categorical na- ture of each tag, we also predict every category t i,j of the tag independently (if they exist in the are used in decoder attention, and the final states are summed with the word-level embedding (w-embed) to produce e w i . WD denotes word dropout. Top: Sentence-level encoder and tag classifier. Two BRNN layers with residual connections act on the em- bedded words e w i of a sentence, providing context. The output of the tag classification are the logits for both the whole tags t i and their components t i,j . Both: Thick slanted lines denote training dropout. dataset) with τ dense layers similar to <ref type="bibr">Inoue et al. (2017)</ref>. The j-th layer has |T j | neurons and out- puts the logits t i,j for the category values. While these values are trained for, their value is not used in tag prediction. All tag values T i = (t i , t i,1 . . . , t i,τ ) are concatenated into a flat vec- tor and fed into the lemmatizer as an additional set of potentially useful features.</p><formula xml:id="formula_0">e w 1 e w k BRNN ×2 o w 1 o w k t 1 t 1,1 t 1,2 + + t 1,τ t k t k,1 t k,2 t k,τ ----------- c i,1 c i,2 c i,m i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lemmatizer</head><p>The task of the lemmatizer is to produce a se- quence of characters l i,1 , . . . , l i,λ i and the lemma length λ i for each lemma i . We use a recurrent se- quence decoder, a setup typical of many sequence- to-sequence (seq2seq) tasks such as in neural ma- </p><formula xml:id="formula_1">T i = (t i , t i,1 , t i,2 , . . . , t i,τ )</formula><p>RNN decoder chine translation ( <ref type="bibr" target="#b15">Sutskever et al., 2014</ref>).</p><p>The lemmatizer consists of a recurrent LSTM layer whose initial state is taken from word-level output o w i and whose inputs consist of three parts. The first part is the embedding of the previous out- put character (initially a beginning-of-word char- acter BOW).</p><p>The second part is a character-level attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>) on the out- puts of the character-level BRNN e c i,1 , . . . , e c i,m i . We employ the multiplicative attention mecha- nism described in <ref type="bibr">Luong et al. (2015)</ref>, which al- lows the LSTM cell to compute an attention vec- tor that selectively weights character-level infor- mation in e c i,j at each time step j based on the input state of the LSTM cell.</p><p>The third and final part of the RNN input allows the network to receive the information about the embedding of the word, the surrounding context of the sentence, and the output of the tagger. This output is the same for all time steps of a lemma and is a concatenation of the following: the out- put of the encoder o w i , the embedded word e w i and processed tag features T f i . The tag features are ob- tained by projecting the concatenated outputs of the tagger T i through a fully connected layer with ReLU activation. During training, we do not pass the gradients back through T i to prevent the dis- tortion of the tagger output.</p><p>The decoder performs greedy decoding to pre- dict the character outputs. It runs until it produces the end-of-word character EOW or reaches a char- acter limit of m i + 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>We define the final loss function as the weighted sum of the losses of the tagger and the lemmatizer:</p><formula xml:id="formula_2">L(ˆ y, y) = αL(ˆ y t , y t ) + βL(ˆ y , y )</formula><p>where y are the predicted outputs, ˆ y the expected outputs, y t , the tag components and y are the lemma characters. The tagger and lemmatizer losses are separately computed as the softmax cross entropy of the output logits. The weight hy- perparameters α, β scale the training losses so that the subtag and lemmatizer losses do not overpower the unfactored tag predictor gradients. The vector α contains τ + 1 weights: one for the whole tag and one for every component. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we show the outcomes of evalu- ation when running our joint tagger and lemma- tizer and compare with the current state of the art in Czech, German, Arabic, and English datasets. Additionally, we evaluate the lemmatizer and tag- ger separately to compare the relative increase in tagging and lemmatization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Our datasets consist of the Czech Prague De- pendency Treebank (PDT) <ref type="bibr">(Hajič et al., 2006</ref><ref type="bibr">(Hajič et al., , 2018</ref>, the German TIGER corpus ( <ref type="bibr">Brants et al., 2004</ref>), the Universal Dependencies Prague Arabic Dependency Treebank (UD-PADT) ( <ref type="bibr">Hajic et al., 2004</ref>), the Universal Dependencies English Web Treebank (UD-EWT) ( <ref type="bibr" target="#b12">Silveira et al., 2014)</ref>, and the WSJ portion of the English Penn Treebank (tags only) <ref type="bibr" target="#b3">(Marcus et al., 1993)</ref>. In all datasets, we use the tags specific to their respective lan- guage. Of these datasets, only Czech and Ara- bic provide subcategorical tags, and we use unfac- tored tags for the rest. See <ref type="table">Table 1</ref> for tagger and lemmatizer accuracies.</p><p>Note that the PDT dataset disambiguates lem- mas with the same textual representation by ap- pending a number as lemma sense indicator. For example, the dataset contains disambiguated lem- mas moc-1 (as power) and moc-2 (as too much). About 17.5% of the PDT tokens have such sense- disambiguated lemmas. LemmaTag predicts the lemmas including the senses and the accuracies in <ref type="table">Table 1</ref> take that into account. Ignoring the sense ambiguity, the lemmatization accuracy of the joint LemmaTag model is 98.94% for Czech-PDT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperparameters</head><p>We use loss weights α 0 = 1.0 for the whole tags, α 1,...,τ = 0.1 for the tag component losses and β = 0.5 for the lemmatizer loss. <ref type="bibr">3</ref> The RNNs and word embedding tables have dimensionality 768 except for character-level embeddings and the character-level RNN, which are of dimension 384. The fully-connected layer whose inputs are T i is of dimension 256.</p><p>We train the models for 40 epochs with random permutations of training sentences and batches of 16 sentences. The starting learning rate is η = 0.001 and we scale this by 0.25 at epochs 20 and 30 to increase accuracy. We train the net- work using the lazy variant of the Adam optimizer ( <ref type="bibr">Kingma and Ba, 2014)</ref>, which only updates ac- cumulators for variables that appear in the cur- rent batch <ref type="bibr" target="#b16">(TensorFlow, 2018)</ref>, with parameters β 1 = 0.9 and β 2 = 0.99. We clip the global gra- dient norm to 3.0 to reduce the risk of exploding gradients.</p><p>To prevent the tagger from overfitting, we de- vise several strategies for regularization. We apply dropouts with rate 0.5 as indicated in <ref type="figure" target="#fig_1">Figures 2  and 3</ref>. The word dropout (WD) replaces 25% of words by the unknown token &lt;unk&gt; to force the network to rely more on context, combatting data sparsity issues. Lastly, we employ label smooth- ing ( <ref type="bibr" target="#b8">Pereyra et al., 2017)</ref> which is a way to pre- vent the network from being too confident in any one class. The label smoothing parameter is set to 0.1 for the tagger logits (both whole tags and the tag components).</p><p>Note that we did not perform any complex hy- perparameter search. For additional information on real-world performance and additional tech- niques which have not improved evaluation accu- racy, see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>The evaluation results show that performing lemmatization and tagging jointly by sharing en- coder parameters and utilizing tag features is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Czech   mutually beneficial in morphologically rich lan- guages. We have shown that incorporating these ideas results in excellent performance, surpass- ing state-of-the-art in Czech, German, and Ara- bic POS tagging and lemmatization by a substan- tial margin, while closely matching state-of-the- art English POS tagging accuracy.</p><note type="other">-PDT * German-TIGER Arabic-PADT * Eng-EWT Eng-WSJ tag</note><p>However, in languages with weak morphol- ogy such as English (and German to a lesser ex- tent), sharing the encoder parameters may even hurt the performance of the tagger. We believe this is a consequence of tags correlating less with word-level morphology, and more with sentence- level syntax in morphologically poor languages. Lemma prediction could benefit from the syntac- tic information in the tags, but the tag predictions rely more on syntactic structure (i.e., word or- der) rather than on root forms of individual words which could be ambiguous.</p><p>There are some possible performance improve- ments and additional metrics which we leave for future work. For simplicity, one improvement we intentionally left out is the use of additional data. We can incorporate word2vec (Mikolov et al., 2013a) or ELMo ( <ref type="bibr" target="#b9">Peters et al., 2018</ref>) word representations, which have shown to reduce out- of-domain issues and provide semantic informa- tion ( <ref type="bibr">Eger et al., 2016)</ref>. A second improvement is to integrate information from a morphological dictionary to resolve certain ambiguities <ref type="bibr">(Hajič et al., 2009;</ref><ref type="bibr">Inoue et al., 2017)</ref>. A third im- provement can be to replace the seq2seq lemma- tizer decoder with a classifier that chooses a cor- responding edit tree to modify (reduce) the word form to its lemma ( <ref type="bibr">Chakrabarty et al., 2017)</ref>. A fourth possible improvement would be to experi- ment with the Transformer model ( <ref type="bibr" target="#b18">Vaswani et al., 2017)</ref>, which utilizes non-recurrent multi-headed self-attention and has been shown to achieve state- of-the-art performance in several related sequence tasks <ref type="bibr">(Dehghani et al., 2018</ref> the standard greedy one, but the improvement was marginal (around 0.01%). Variational dropout. While the dropouts in the LemmaTag are completely random, variational dropout erases the same channels across the time steps of the RNN. While this generally improves training in convolutional networks and RNNs, we saw no significant difference.</p><p>Layer normalization. Layer normalization ap- plied to the encoding RNNs did not bring signifi- cant gain and also slowed down the training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The tag components of the PDT Czech treebank with the numbers of valid values. Around 1500 different tags are in use in the PDT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bottom: Word-level encoder. The characters of every input word are embedded with a look-up table (c-embed) and encoded with a BRNN. The outputs e c i,j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Lemma decoder, consisting of a standard seq2seq autoregressive decoder with Luong attention on character encodings, and with additional inputs of processed tagger features T i , embeddings e w i and sentence-level outputs o w i. Gradients from the lemmatizer are stopped from flowing into the tagger (denoted GradStop).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 :</head><label>1</label><figDesc>Final accuracies on the test sets comparing the LemmaTag architecture as described (joint), LemmaTag neither sharing the encoder nor providing tagger features (sep), and the state-of-the-art results (SoTA). The state- of-the-art results are taken from the following papers: (a) Hajič et al. (2009), (b) Straková et al. (2014), (c) Eger et al. (2016), (d) Inoue et al. (2017), (e) Straka et al. (2016), (f) Ling et al. (2015). The results marked with a plus + use additional resources apart from the dataset, and datasets marked with a star * indicate the availability of subcategorical tags.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> If no components are available, τ = 0.</note>

			<note place="foot" n="3"> These are reasonable values to prevent gradients from overpowering one another. The lemmatizer tends to influence the tagger heavily.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work described herein has been supported by the City of Prague under the "OP PPR" program, project No. CZ.07.1.02/0.0/0.0/16 023/0000108 and it has been using language resources devel-oped by the LINDAT/CLARIN project of the Min-istry of Education, Youth and Sports of the Czech Republic (project LM2015071).</p><p>Tomáš Gavenčiak has been supported by Czech Science Foundation (GACR) project 17-10090Y "Network optimization". Daniel Kondratyuk has been supported by the Erasmus Mundus pro-gram in Language &amp; Communication Technolo-gies (LCT).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 GPU Performance</head><p>We ran all the tests on an NVIDIA GTX 1080 Ti GPU. The joint LemmaTag training takes about 3 hours for Arabic PADT, 4.5 hours for English EWT, 12 hours for German TIGER, and 22 hours for Czech PDT. The separate models take about 50% more time. After training, the lemma and tag predictions of 219,000 test tokens of the Czech PDT take about 100 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Other Techniques</head><p>We briefly summarize some of the additional tech- niques we have tried but which do not improve the results. While some of those techniques do help on smaller models or earlier in the training, the effect on the fully trained network seems to be marginal or even detrimental.</p><p>Separate sense prediction. Instead of predict- ing the sense disambiguation with the lemmatizer (Czech only), we tried to predict sense as an addi- tional classification problem with one dense layer based on o w i and T i , but it seems to perform slightly worse (0.2%).</p><p>Beam search decoder. We have implemented a beam search decoder for the lemmatizer instead of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Samar: Subjectivity and sentiment analysis for arabic social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Abdul-Mageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Context sensitive neural lemmatization with lematus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toms</forename><surname>Bergmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint lemmatization and morphological tagging with lemming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06548</idno>
		<title level="m">Regularizing neural networks by penalizing confident output distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A gold standard dependency corpus for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Udpipe: Trainable pipeline for processing conll-u files performing tokenization, morphological analysis, pos tagging and parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Strakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open-Source Tools for Morphology, Lemmatization, POS Tagging and Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">tf.contrib.opt.lazyadamoptimizer: Class lazyadamoptimizer. TensorFlow documentation from tensorflow.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Statistical parsing of morphologically rich languages (spmrl): what, how and whither</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannick</forename><surname>Versley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Rehbein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamia</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
