<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learn to Solve Algebra Word Problems Using Quadratic Programming</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipu</forename><surname>Zhou</surname></persName>
							<email>zhoulipu@outlook.com, {daishuaixiang, chenliwei}@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaixiang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learn to Solve Algebra Word Problems Using Quadratic Programming</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a new algorithm to automatically solve algebra word problems. Our algorithm solves a word problem via analyzing a hypothesis space containing all possible equation systems generated by assigning the numbers in the word problem into a set of equation system templates extracted from the training data. To obtain a robust decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible. This results in a quadratic programming (QP) problem which can be efficiently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art base-line (Kushman et al., 2014).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An algebra word problem describes a mathemat- ical problem which can be typically modeled by an equation system, as demonstrated in <ref type="figure">Figure 1</ref>. Seeking to automatically solve word problems is a classical AI problem <ref type="bibr" target="#b3">(Bobrow, 1964)</ref>. The word problem solver is traditionally created by the rule- based approach ( <ref type="bibr" target="#b10">Lev et al., 2004;</ref><ref type="bibr" target="#b16">Mukherjee and Garain, 2008;</ref><ref type="bibr" target="#b14">Matsuzaki et al., 2013)</ref>. Recently, using machine learning techniques to construct the solver has become a new trend ( <ref type="bibr" target="#b6">Hosseini et al., 2014;</ref><ref type="bibr" target="#b0">Amnueypornsakul and Bhat, 2014;</ref><ref type="bibr" target="#b18">Roy et al., 2015)</ref>. This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend. <ref type="bibr">1</ref> To solve a word problem, our algorithm ana- lyzes all the possible ways to assign the numbers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our method</head><p>Kushman's method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word problem</head><p>An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50. Adult tickets cost $ 4. On a certain day, 278 people entered the park. On that same day the admission fees collected totaled $ 792. How many children were admitted on that day? How many adults were admitted?  <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17">5 4 3 2 10022520</head><p>Figure 1: Comparison between our algorithm and ( ). Nouns are boldfaced.</p><p>in the word problem to a set of equation system templates.  also consider filling the equation system templates to generate the candidate equations. But Kushman's template contains number slots (e.g. n 1 , n 2 , n 3 , n 4 in <ref type="figure">Fig- ure 1</ref>) and unknown slots (e.g. u 1 1 , u 2 1 , u 1 2 , u 2 2 in <ref type="figure">Figure 1</ref>). They separately consider assigning nouns into the unknown slots and numbers into the number slots, as demonstrated in <ref type="figure">Figure 1</ref>. As filling the unknown slots is closely related to the number slots assignment, we only consider assign- ing the number slots, and design effective features to describe the relationship between numbers and unknowns. This scheme significantly reduces the hypothesis space, as illustrated in <ref type="figure">Figure 1</ref>, which benefits the learning and inference processes. We use a log-linear model to describe the tem- plate selection and number assignment. To learn the model parameters of such problem, maxi- mizing the log-likelihood objective is generally adopted ( <ref type="bibr" target="#b9">Kwiatkowski et al., 2010;</ref>). The key difficulty of this method is that calculating the gradient of the objective func- tion needs to sum over exponentially many sam- ples. Thus, it is essential to approximate the gra- dient. For instance,  use beam search to approximately calculate the gra- dient. This method can not exploit all the training samples. Thus the resulting model may be sub- optimal. Motivated by the work ( <ref type="bibr" target="#b19">Taskar et al., 2005;</ref><ref type="bibr" target="#b11">Li, 2014)</ref>, we adopt the max-margin objec- tive. This results in a QP problem and opens the way toward an efficient learning algorithm <ref type="bibr" target="#b7">(Koller and Friedman, 2009)</ref>.</p><p>We evaluate our algorithm on the benchmark dataset provided by ). The experimental results show that our algorithm sig- nificantly outperforms the state-of-the-art base- line ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Our word problem solver is constructed by train- ing a log-linear model to find the correct mapping from a word problem to an equation. Notations: Let X denote the set of training word problems, and T denote the set of equation sys- tem templates abstracted from X as ( ). x i is the i-th word problem in X . Assume T j is the j-th equation system template in T , and</p><formula xml:id="formula_0">N T j = n 1 T j , n 2 T j , · · · , n m T j</formula><p>is the set of number slots of T j , where m represents the size of N T j . Denote the numbers in x i by</p><formula xml:id="formula_1">N x i = n 1 x i , n 2 x i , · · · , n l x i</formula><p>, where l represents the size of N x i . Assuming l ≥ m, we further de- fine π ijk a sequence of m numbers chosen from N x i without repetition. Given π ijk , we can map T j to an equation system e ijk by filling the num- ber slots N T j of T j sequently with the numbers in π ijk . Solving e ijk , we can obtain the correspond- ing solution s ijk . To simplify the notation, we de- fine y ijk = (T j , π ijk , e ijk , s ijk ) the k-th derivation give x i and T j , and let Y i denote the set of all pos- sible y ijk given x i and T . Therefore, to correctly solve x i is to find the correct y ijk ∈ Y i . Probabilistic Model: As ( ), we use the log-linear model to define the probabil- ity of y ijk ∈ Y i given x i :</p><formula xml:id="formula_2">p(y ijk |x i ; θ) = e θ·φ(x i ,y ijk ) y ijk ∈Y i e θ·φ(x i ,y ijk ) (1)</formula><p>where θ is the parameter vector of the model, and φ (x i , y ijk ) denotes the feature function. We adopt the max-margin objective <ref type="bibr" target="#b20">(Vapnik, 2013)</ref> to di- rectly learn the decision boundary for the correct derivations and the false ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning and Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning</head><p>Using <ref type="formula">(1)</ref>, we obtain the difference between the log-probability of a correct derivation y c ijk ∈ Y i and a false one y f ijl ∈ Y i as:</p><formula xml:id="formula_3">ln P y c ijk |x i ; θ − ln P y f ijl |x i ; θ =θ · φ x i , y c ijk − φ x i , y f ijl (2)</formula><p>Note that the subtraction in (2) cancels the denom- inator of <ref type="formula">(1)</ref> which contains extensive computa- tion. To decrease the generalization error of the learned model, we would like the minimal gap be- tween the correct derivations and the false ones as large as possible. In practice, we may not find a decision hyperplane to perfectly separate the cor- rect and the false derivations. Generally, this can be solved by introducing a slack variable</p><formula xml:id="formula_4">ξ ijkl ≥ 0 (Bishop, 2006) for each constraint derived from (2). Define ϕ x i , y c ijk , y f ijl = φ x i , y c ijk − φ x i , y f ijl .</formula><p>For ∀ x i ∈ X , the resulting optimiza- tion problem is:</p><formula xml:id="formula_5">arg min 1 2 θ 2 + C i,j,k,l ξ ijkl (3) s.t. θ · ϕ x i , y c ijk , y f ijl ≥ 1 − ξ ijkl , ξ ijkl ≥ 0</formula><p>The parameter C is used to balance the slack vari- able penalty and the margin. This is a QP problem and has been well studied <ref type="bibr" target="#b17">(Platt, 1999;</ref><ref type="bibr" target="#b4">Fan et al., 2008)</ref>. According to the Karush-Kuhn-Tucker (KKT) condition, only a part of the constraints is active for the solution of (3) ( <ref type="bibr" target="#b2">Bishop, 2006</ref>). This leads to an efficient learning algorithm called constraint generation ( <ref type="bibr" target="#b7">Koller and Friedman, 2009;</ref><ref type="bibr" target="#b5">Felzenszwalb et al., 2010)</ref>. Specifically, an initial model is trained by a randomly selected subset of the constraints. Next this model is used to check the constraints and at most N false deviations that are erroneously classified by this model are collected for each word problem. These constraints are then added to train a new model. This process repeats until converges. Our experimental results show that this process converges fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>When we obtain the model parameter θ, the infer- ence can be performed by finding the maximum</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single slot features Relation between numbers and the question sentence. Position of a number w.r.t a comparative word. Context of a number. Is one or two? Is a multiplier? Is between 0 and 1? Slot pair features</head><p>Relation between two numbers. Context similarity between two numbers. Does there exist coreference relationship? Are two numbers both multipliers? Are two numbers in the same sentence or con- tinuous sentences? Information of raw path and dependency path between two numbers One number is larger than another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution features Is integer solution? Is positive solution?</head><p>Is between 0 and 1? <ref type="table">Table 1</ref>: Features used in our algorithm.</p><p>value of (1). This can be simplified by computing arg max</p><formula xml:id="formula_6">y ijk ∈Y i θ · φ(x i , y ijk )<label>(4)</label></formula><p>As we only consider assigning the number slots of the templates in T , generally, the size of the possi- ble assignments per word problem is bearable, as shown in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features</head><p>A feature vector φ (x i , y ijk ) is calculated for each word problem x i and derivation y ijk pair. As Kushman (2014), a feature is associated with a sig- nature related to the template of y ijk . We extract three kinds of features, i.e., single slot features, slot pair features and solution features. Unless otherwise stated, single slot and slot pair features are associated with the slot and slot pair signature of the equation system template, respectively, and solution features are generated for the signature of the equation system template. <ref type="table">Table 1</ref> lists the fea- tures used in our algorithm. The detailed descrip- tion is as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single Slot Features</head><p>To reduce the search space, we only consider the assignment of the number slots of the template. It seems that our algorithm will lose the information about the unknown. But such information can be recovered by the features that include the infor- mation of the question sentence. Specifically, we associate a number with all the nouns in the same sentence sorted by the length of the dependence path between them. For instance, [$, tickets, chil- dren] is the sorted noun list for 1.5 in <ref type="figure">Figure 1</ref>. Assume the n-th noun of the nouns associated to a given number is the first noun that appears in the question sentence. We quantify the relationship between a number and a queried entity by the re- ciprocal of n. For instance, in <ref type="figure">Figure 1</ref>, "children" appears in the question sentence, and it is the third noun associated to 1.5. So the value of this feature is 1/3. A larger value of this feature means a num- ber more likely relates to the queried entity. The maximum value of this feature is 1. Thus we intro- duce a feature to indicate whether this special case occurs. We also use a feature to indicate whether a number appears in the question sentence. The comparative meaning is sensitive to both the comparative words and the position of a num- ber relative to them. For example, "one number is 3 less than twice another" is different to "one number is 3 more than twice another", but equal to "twice a number is 3 more than another". To ac- count for this, we use the comparative words cou- pled with the position of a number relative to them as features.</p><p>On the other hand, we use the lemma, part of speech (POS) tag and the dependence type related to the word within a widow [-5, +5] around a num- ber as features. Besides, if the POS tag or the named entity tag of a number is not labeled as a general number, we also import these tags together with the first noun and the dependence type related to the number as features.</p><p>Additionally, the numbers 1 and 2 are usually used to indicate the number of variables, such as "the sum of two numbers". To capture such usage, we use a feature to denote whether a number is one or two as ( ). Since such usage appears in various kinds of word problems, this feature does not contain the slot signature. We also generate features to indicate whether a num- ber belongs to (0, 1), and whether it is a multiplier, such as twice, triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Slot Pair Features</head><p>Assume n 1 and n 2 are two numbers in a word problem. Suppose N P 1 and N P 2 are the lists of nouns associated to n 1 and n 2 (described in sec- tion 4.1), respectively. We evaluate the relation- ship r (n 1 , n 2 ) between n 1 and n 2 by:</p><formula xml:id="formula_7">max noun i 1 ∈N P 1 , noun j 2 ∈N P 2 s.t. noun i 1 =noun j 2   2 ord noun i 1 + ord noun j 2  </formula><p>where ord(·) denotes the index of a noun in N P i (i = 1, 2), starting from 1. A larger r (n 1 , n 2 ) means n 1 and n 2 are more related. The maximum value of r (n 1 , n 2 ) is 1, which occurs when the first nouns of N P 1 and N P 2 are equal.</p><p>We use a feature to indicate whether r (n 1 , n 2 ) is 1. This feature helps to import some basic rules of the arithmetic operation, e.g., the units of sum- mands should be the same. If two slots are symmetric in a template (e.g., n 2 and n 3 in <ref type="figure">Figure 1</ref>), the contexts around both numbers are generally similar. Assume CT 1 and CT 2 are two sets of certain tags within a window around n 1 and n 2 , respectively. Then we calculate the contextual similarity between n 1 and n 2 by:</p><formula xml:id="formula_8">sim (ST 1 , ST 2 ) = |ST 1 ∩ ST 2 | |ST 1 ∪ ST 2 |</formula><p>In this paper, the tags include the lemma, POS tag and dependence type, and the window size is 5. Besides, we exploit features to denote whether there exists coreference relationship between any elements of the sentences where n 1 and n 2 locate, and whether two numbers are both multipliers. Fi- nally, according to ), we generate features related to the raw path and de- pendence path between two numbers, and use the numeric relation between them as a feature to im- port some basic arithmetic rules, such as the posi- tive summands are smaller than their sum. We also include features to indicate whether two numbers are in the same sentence or continuous sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Solution Features</head><p>Many word problems are math problems about the real life. This background leads the solutions of many word problems have some special numerical properties, such as the positive and integer prop- erties used by ). To capture such fact, we introduce a set of features to describe the solution properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset: The dataset used in our experiment is provided by ). Equiva- lent equation systme templates are automatically merged. The word problems are parsed by <ref type="bibr" target="#b12">(Manning et al., 2014</ref>). The version of the parser is the same as ( ). The performance of our algorithm is evaluated by comparing each number of the correct answer with the calculated one, regardless of the ordering. We report the av- erage accuracy of 5-fold cross-validation. Learning: We use liblinear <ref type="bibr" target="#b4">(Fan et al., 2008)</ref> to solve the QP problem. The parameter C in <ref type="formula">(3)</ref> is set to 0.01 in all the following experiments. We randomly select 300 false derivations of each word problem to form the initial training set. We add at most 300 false derivations for each word problem during the constraint generation step, and use 5- fold cross-validation to avoid overfitting. We stop iterating when the cross-validation error becomes worse or the training error converges or none new constraints are generated. Supervision Level: We consider the learning with two different levels of supervision. In the first case, the learning is conducted by providing the equation and the correct answer of every training sample. In the second case, the correct answer is available for every training sample but without the equation. Instead, all the templates are given, but the correspondence between the template and the training sample is not available. During learning, the algorithm should evaluate every derivation of each template to find the true one. Results: <ref type="table" target="#tab_1">Table 2</ref> lists the learning statistics for our algorithm and ( ). We can ob- serve that the number of possible alignments per word problem of our algorithm is much smaller than ( ). However, the num- ber of all the false alignments is still 80K. Us- ing the constraint generation algorithm <ref type="bibr" target="#b7">(Koller and Friedman, 2009)</ref>, only 9K false alignments are used in the quadratic programming. We trained our model on a Intel i5-3210M CUP and 4G RAM laptop. Kushman's algorithm (2014) needs much more memory than our algorithm and can not run on a general laptop. Therefore, we tested their al- gorithm on a workstation with Intel E5-2620 CPU and 128G memory. As shown in <ref type="table" target="#tab_1">Table 2</ref>, their al- gorithm takes more time than our algorithm. <ref type="table" target="#tab_3">Table 3</ref> lists the accuracy of our algorithm and Kushman's algorithm <ref type="bibr">(2014)</ref>. It is clear that our Mean negative samples 80K Mean negative samples used in learning 9K Mean time for feature extraction 22m Mean training time 7.3m Mean feature extraction and training time of ) 83m</p><p># Alignments per problem of ( ) 4M</p><p># Alignments per problem of our algo- rithm 1.9K   <ref type="table">Table 4</ref>: Ablation study for fully supervised data.</p><p>algorithm obtains better result. The result of the weakly supervised data is worse than the fully su- pervised one. But this result is still higher than Kushman's fully supervised result. <ref type="table">Table 4</ref> gives the results of our algorithm with different feature ablations. We can find that all the features are helpful to get the correct solution and none of them dramatically surpasses the others. Discuss: Although our algorithm gives a better re- sult than ( ), there still exist two main problems that need to be further investi- gated, as demonstrated in <ref type="table">Table 5</ref>. The first prob- lem is caused by our feature for semantic repre- sentation. Our current lexicalized feature can not generalize well for the unseen words. For exam- ple, it is hard for our algorithm to relate the word "forfeits" to "minus", if it does not appear in the training corpus. The second problem is caused by the fact that our algorithm only considers the sin- gle noun as the entity of a word problem. Thus when the entity is a complicated noun phrase, our algorithm may fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem</head><p>Example Lexicalized features can not gener- alize well for unseen words.</p><p>A woman is paid 20 dollars for each day she works and forfeits a 5 dollars for each day she is idle. At the end of 25 days she nets 450 dollars. How many days did she work?</p><p>Can not deal with compli- cated noun phrases.</p><p>The probability that San Francisco plays in the next super bowl is nine times the probability that they do not play in the next super bowl. The probability that San Francisco plays in the next super bowl plus the probabil- ity that they do not play is 1. What is the probability that San Francisco plays in the next super bowl? <ref type="table">Table 5</ref>: The problems of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>In this paper, we present a new algorithm to learn to solve algebra word problems. To reduce the possible derivations, we only consider filling the number slots of the equation system templates, and design effective features to describe the rela- tionship between numbers and unknowns. Addi- tionally, we use the max-margin objective to train the log-linear model. This results in a QP prob- lem that can be efficiently solved via the constraint generation algorithm. Experimental results show that our algorithm significantly outperforms the state-of-the-art baseline ( ).</p><p>Our future work will focus on studying the per- formance of applying nonlinear kernel function to the QP problem (3), and using the word embed- ding vector ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b15">Mikolov et al., 2013)</ref> to replace current lexicalized features. Be- sides, we would like to compare our algorithm with the algorithms designed for specific word problems, such as ( <ref type="bibr" target="#b6">Hosseini et al., 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Thus we simply evaluate all 
the y ijk ∈ Y i . The one with the largest score is 
considered as the solution of x i . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Learning statistics.</head><label>2</label><figDesc></figDesc><table>Algorithm 
Accuracy 
Our algorithm fully supervised 
79.7% 
Our algorithm weakly supervised 
72.3% 
Kushman's algorithm (2014) fully 
supervised 
68.7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Algorithm comparison.</head><label>3</label><figDesc></figDesc><table>Feature Ablation 
Accuracy 
Without single slot features 
70.4% 
Without slot pair features 
69.3% 
Without solution features 
71.8% 

</table></figure>

			<note place="foot" n="1"> Our code is available at http://pan.baidu.com/ s/1dD336Sx</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work is supported by the National Basic Research Program of <ref type="bibr">China (973 program No. 2014CB340505)</ref>. We would like to thank Hua Wu and the anonymous reviewers for their helpful comments that improved the work considerably.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Machine-guided solution to mathematical word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bussaba</forename><surname>Amnueypornsakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Natural language input for a computer problem solving system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bobrow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Pedro F Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to automatically solve algebra word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="281" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inducing probabilistic ccg grammars from logical form with higherorder unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Solving logic puzzles: From robust processing to precise semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iddo</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Text Meaning and Interpretation</title>
		<meeting>the 2nd Workshop on Text Meaning and Interpretation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval and natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="121" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The complexity of math problems-linguistic, or computational</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidenao</forename><surname>Iwane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokazu</forename><surname>Anai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Arai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous spaceword representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A review of methods for automatic understanding of natural language mathematical problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utpal</forename><surname>Garain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="122" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in kernel methods-Support vector learning</title>
		<editor>B. Scho04lkopf, C. Burges and A. Smola</editor>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="896" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
