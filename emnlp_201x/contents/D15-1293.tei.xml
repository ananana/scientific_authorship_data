<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<email>douwe.kiela@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">Computer Laboratory University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
							<email>stephen.clark@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Laboratory</orgName>
								<orgName type="institution" key="instit1">Computer Laboratory University of Cambridge</orgName>
								<orgName type="institution" key="instit2">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-modal semantics has relied on feature norms or raw image data for perceptual input. In this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics, including measuring conceptual similarity and related-ness. We also evaluate cross-modal map-pings, through a zero-shot learning task mapping between linguistic and auditory modalities. In addition, we evaluate multi-modal representations on an unsupervised musical instrument clustering task. To our knowledge, this is the first work to combine linguistic and auditory information into multi-modal representations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although distributional models <ref type="bibr" target="#b36">(Turney and Pantel, 2010;</ref><ref type="bibr" target="#b4">Clark, 2015)</ref> have proved useful for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the ground- ing problem <ref type="bibr" target="#b12">(Harnad, 1990)</ref>; i.e. they do not ac- count for the fact that human semantic knowledge is grounded in the perceptual system <ref type="bibr" target="#b22">(Louwerse, 2008)</ref>. Motivated by human concept acquisition, multi-modal semantics enhances linguistic repre- sentations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compo- sitionality <ref type="bibr" target="#b30">(Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b26">Roller and Schulte im Walde, 2013;</ref>). Al- though feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models.</p><p>However, if the objective is to ground seman- tic representations in perceptual information, why stop at image data? The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre. To understand how per- ceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary study by <ref type="bibr" target="#b21">Lopopolo and van Miltenburg (2015)</ref> found that it is possible to derive uni-modal semantic representations from sound data. Here, we explore taking multi-modal semantics beyond its current reliance on image data and experiment with grounding semantic rep- resentations in the auditory perceptual modality.</p><p>Multi-modal models that rely on raw image data have typically used "bag of visual words" (BoVW) representations <ref type="bibr" target="#b32">(Sivic and Zisserman, 2003)</ref>. We follow a similar approach for the auditory modality and construct bag of audio words (BoAW) representations. Following pre- vious work in multi-modal semantics, we evalu- ate these models on measuring conceptual simi- larity and relatedness, and inducing cross-modal mappings between modalities to perform zero- shot learning. In addition, we evaluate on an unsupervised musical instrument clustering task. Our findings indicate that multi-modal representa- tions enriched with auditory information perform well on relatedness and similarity tasks, particu- larly on words that have auditory assocations. To our knowledge, this is the first work to combine linguistic and auditory representations in multi- modal semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Information processing in the brain can be roughly described to occur on three levels: perceptual in- put, conceptual representation and symbolic rea- soning <ref type="bibr" target="#b11">(Gazzaniga, 1995)</ref>. While research in AI has made great progress in understanding the first and last of these, understanding the middle level is still more of an open problem: how is it that per-ceptual input leads to conceptual representations that can be processed and reasoned with?</p><p>A key observation is that concepts are, through perception, grounded in physical reality and sen- sorimotor experience <ref type="bibr" target="#b12">(Harnad, 1990;</ref><ref type="bibr" target="#b22">Louwerse, 2008)</ref>, and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn se- mantic representations from both textual and per- ceptual input, using either feature norms <ref type="bibr" target="#b30">(Silberer and Lapata, 2012;</ref><ref type="bibr" target="#b26">Roller and Schulte im Walde, 2013;</ref>) or raw image data <ref type="bibr" target="#b7">(Feng and Lapata, 2010;</ref><ref type="bibr" target="#b20">Leong and Mihalcea, 2011;</ref>) as the source of per- ceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT <ref type="bibr" target="#b23">(Lowe, 2004</ref>). These local descrip- tors are subsequently clustered into a set of "vi- sual words" using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of vi- sual words (BoVW) approach is transferring fea- tures from convolutional neural networks <ref type="bibr" target="#b15">(Kiela and Bottou, 2014</ref>).</p><p>Various ways of aggregating images into visual representations have been proposed, such as tak- ing the mean or the elementwise maximum. Ide- ally, one would jointly learn multi-modal repre- sentations from parallel multi-modal data, such as text containing images <ref type="bibr" target="#b31">(Silberer and Lapata, 2014)</ref> or images described with speech <ref type="bibr" target="#b35">(Synnaeve et al., 2014</ref>), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image repre- sentations are often learned independently. Ag- gregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion meth- ods ( ).</p><p>Cross-modal semantics, instead of being con- cerned with improving semantic representations through grounding, focuses on the problem of ref- erence. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects ( <ref type="bibr" target="#b18">Lazaridou et al., 2014</ref>). This problem is very much re- lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize tex- tual information <ref type="bibr" target="#b33">(Socher et al., 2014;</ref><ref type="bibr" target="#b10">Frome et al., 2013)</ref>. This allows for zero-shot learning, where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encoun- tered an image of that particular object ( <ref type="bibr" target="#b18">Lazaridou et al., 2014</ref>). Multi-modal and cross-modal ap- proaches have outperformed state-of-the-art text- based methods on a variety of tasks ( <ref type="bibr" target="#b31">Silberer and Lapata, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluations</head><p>Following previous work in multi-modal seman- tics, we evaluate on two standard similarity and re- latedness datasets: SimLex-999 ( ) and the MEN test collection ( ). These datasets consist of concept pairs together with a human-annotated similarity or relatedness score, where the former dataset focuses on gen- uine similarity (e.g., teacher-instructor) and the latter focuses more on relatedness (e.g., river- water). In addition, following previous work in cross-modal semantics, we evaluate on the zero- shot learning task of inducing a cross-modal map- ping to the correct label in the auditory modality from the linguistic one and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-modal Semantics</head><p>Evidence suggests that the inclusion of visual rep- resentations only improves performance for cer- tain concepts, and that in some cases the introduc- tion of visual information is detrimental to perfor- mance on similarity and relatedness tasks ( ). The same is likely to be true for other perceptual modalities: in the case of com- parisons such as guitar-piano, the auditory modal-  ity is certainly meaningful, whereas in the case of democracy-anarchism it is probably less so. Therefore, we had two graduate students annotate the datasets according to whether auditory percep- tion is relevant to the pairwise comparison. The annotation criterion was as follows: if both con- cepts in a pairwise comparison have a distinctive associated sound, the modality is deemed rele- vant. Inter-annotator agreement was high: κ = 0.93 for MEN and κ = 0.92 for SimLex-999.</p><p>Some examples of relevant pairs can be found in <ref type="table">Table 1</ref>. Hence, we now have four evaluation datasets: the MEN test collection MEN and its auditory-relevant subset AMEN; and the SimLex- 999 dataset SLex and its auditory-relevant sub- set ASLex. Due to the nature of the auditory data sources, it is not possible to build auditory representations for all concepts in the test sets. Hence, unless stated otherwise, we report results for the covered subsets (using the same subsets when comparing across modalities, to ensure a fair comparison). <ref type="table" target="#tab_2">Table 2</ref> shows how much of the test sets are covered for each modality. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-modal Semantics</head><p>In addition to evaluating our models on the MEN and SimLex tasks, we evaluate on the cross- modal task of zero-shot learning. In the case of vision, <ref type="bibr" target="#b18">Lazaridou et al. (2014</ref>) studied the possibility of predicting from "we found a cute, hairy wampimuk sleeping behind the tree" that a "wampimuk" will probably look like a small furry animal, even though a wampimuk has never been seen before. We evaluate zero-shot learning, using partial least squares regression (PLSR) to obtain cross-modal mappings from the linguistic to audi- tory space and vice versa. <ref type="bibr">2</ref> Thus, given a linguistic representation for e.g. guitar, the task is to map it to the appropriate place in auditory space without ever having heard a guitar; or map it to the appro- priate place in linguistic space without ever having read about a guitar (having only heard it).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>One reason for using raw image data in multi- modal models is that there is a wide variety of re- sources that contain tagged images, such as Im- ageNet ( <ref type="bibr" target="#b5">Deng et al., 2009</ref>) and the ESP Game dataset <ref type="bibr" target="#b37">(Von Ahn and Dabbish, 2004</ref>  <ref type="bibr">4</ref> . Because the database contains variable numbers of files, with varying duration per indi- vidual file, we restrict the search to a maximum of 50 files and a maximum of 1 minute duration per file. The Freesound API allows for various degrees of keyword matching: we opted for the strictest keyword matching, in that the audio file needs to have been purposely tagged with the given word (the alternative includes searching the text descrip- tion for matching keywords). For example, if we are searching for audio files of cars, we retrieve up to 50 files with a maximum duration of 1 minute per file that have been tagged with the label "car".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linguistic Representations</head><p>For the linguistic representations we use the con- tinuous vector representations from the log-linear skip-gram model of . Specifi- cally, we trained 300-dimensional vector represen- tations trained on a dump of the English Wikipedia plus newswire (8 billion words in total). <ref type="bibr">5</ref> These types of representations have been found to yield the highest performance on a variety of semantic similarity tasks ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Auditory Representations</head><p>A common approach to obtaining acoustic fea- tures of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) <ref type="bibr" target="#b25">(O'Shaughnessy, 1987)</ref>. MFCC features are abundant in a wide variety of applications in audio signal process- ing, ranging from audio information retrieval, to speech and speaker recognition, and music analy- sis <ref type="bibr" target="#b6">(Eronen, 2003)</ref>. Such features are derived from the mel-frequency cepstrum representation of an audio fragment ( <ref type="bibr" target="#b34">Stevens et al., 1937)</ref>. In MFCC, frequency bands are spaced along the mel scale, which has the advantage that it approximates hu- man auditory perception more closely than e.g. linearly-spaced frequency bands. Hence, MFCC takes human perceptual sensitivity to audio fre- quencies into consideration, which makes it suit- able for e.g. compression and recognition tasks, but also for our current objective of modelling au- ditory perception. We obtain MFCC descriptors for frames of audio files using librosa, a popu- lar library for audio and music analysis written in Python. <ref type="bibr">6</ref> After having obtained the descrip- tors, we cluster them using mini-batch k-means <ref type="bibr" target="#b29">(Sculley, 2010)</ref> and quantize the descriptors into a "bag of audio words" (BoAW) (Foote, 1997) rep- resentation by comparing the MFCC descriptors to the cluster centroids. This gives us BoAW rep- resentations for each of the audio files. Auditory representations are obtained by taking the mean of the BoAW representations of the relevant au- dio files, and finally weighting them using positive point-wise mutual information (PPMI), a standard weighting scheme for improving vector represen- tation quality <ref type="bibr" target="#b3">(Bullinaria and Levy, 2007)</ref>. We set k = 300, which equals the number of dimensions for the linguistic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-modal Fusion Strategies</head><p>Since multi-modal semantics relies on two or more modalities, there are several ways of combining or fusing linguistic and perceptual cues ( ). When computing similarity scores, for instance, we can either jointly learn the represen- tations; learn them independently, combine (e.g. concatenate) them and compute similarity scores; or learn them independently, compute similarity scores independently and combine the scores. We call these possibilities early, middle and late fu- sion, respectively, and evaluate multi-modal mod-els in each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Early Fusion</head><p>A good example of early fusion is the recently introduced multi-modal skip-gram model <ref type="bibr" target="#b19">(Lazaridou et al., 2015)</ref>. This model behaves like a nor- mal skip-gram, but instead of only having a train- ing objective for the linguistic representation, it in- cludes an additional training objective for the vi- sual context, which consists of an aggregated rep- resentation of images associated with the given target word. The skip-gram training objective for a sequence of training words w 1 , w 2 , ..., w T and a context size c is:</p><formula xml:id="formula_0">1 T T t=1 J θ (w t )</formula><p>where J θ is the log-likelihood −c≤j≤c log p(w t+j |w t ) and p(w t+j |w t ) is obtained via the softmax:</p><formula xml:id="formula_1">p(w t+j |w t ) = exp u w t+j vw t W w =1 exp u w vw t</formula><p>where u w and v w are the context and target vec- tor representations for the word w respectively, and W is the vocabulary size. The objective for the multi-modal skip-gram has an additional vi- sual objective J vis (in this case a margin criterion):</p><formula xml:id="formula_2">1 T T t=1 J θ (w t ) + J vis (w t )</formula><p>Here, we take a similar but more straightfor- ward approach by making the auditory context a part of the initial training objective, which is pos- sible because linguistic and auditory representa- tions have the same dimensionality. That is, we modified word2vec to predict additional auditory contexts that have been set to the corresponding BoAW representation. We jointly learn linguistic and audio representations by taking the aggregated mean µ a w of the auditory vectors for a given word w, and adding this mean vector to the context:</p><formula xml:id="formula_3">1 T T t=1 J θ (w t ) + log p(µ a wt |w t )</formula><p>The intuition is that the induced vector for the target word now has to predict an auditory vec- tor as part of its context, as well as the linguis- tic ones. As an alternative, we also investigate re-placing the mean µ a wt with an auditory vector ob- tained by uniformly sampling from the set of au- ditory representations for the target word. We re- fer to these two alternatives as MMSG-MEAN and MMSG-SAMPLED, respectively. For this model, auditory BoAW representations are built for the ten thousand most frequent words in our corpus, based on 10 audio files retrieved from FreeSound for each word (or fewer when 10 are not available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Middle and Late Fusion</head><p>Whereas early fusion requires a joint training ob- jective that takes into account both modalities, middle fusion allows for individual training ob- jectives and independent training data. Similarity between two multi-modal representations is calcu- lated as follows:</p><formula xml:id="formula_4">sim(u, v) = g(f (u l , u a ), f (v l , v a ))</formula><p>where g is some similarity function, u l and v l are linguistic representations, and u a and v a are the auditory representations. A typical formulation in multi-modal semantics for f (x, y) is α x(1−α)y, where is concatenation (see e.g.  and <ref type="bibr" target="#b15">Kiela and Bottou (2014)</ref>).</p><p>Late fusion can be seen as the converse of mid- dle fusion, in that the similarity function is com- puted first before the similarity scores are com- bined:</p><formula xml:id="formula_5">sim(u, v) = h(g(u l , v l ), g(u a , v a ))</formula><p>where g is some similarity function and h is a way of combining similarities, in our case a weighted average: h(x, y) = 1 2 (α x + (1 − α)y); and we use g = x·y |x||y| (cosine similarity). Since cosine simi- larity is the normalized dot-product, and the uni- modal representations are themselves normalized, middle and late fusion are equivalent if α = 0.5, which we call MM. However, when α = 0.5, we distinguish between the two models, calling them MM-MIDDLE and MM-LATE respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conceptual Similarity and Relatedness</head><p>We evaluate performance by calculating the Spear- man ρ s correlation between the ranking of the concept pairs produced by the automatic similar- ity metric (cosine between the derived vectors) and that produced by the gold-standard similarity scores. To ensure a fair comparison, we evaluate  on the common subsets for which there are repre- sentations in both modalities (see <ref type="table" target="#tab_2">Table 2</ref>).</p><p>The results are reported in <ref type="table" target="#tab_5">Table 3</ref>. We find that, while performance decreases for linguistic representations on the auditory-relevant subsets of the two datasets, performance increases for the uni-modal auditory representations on those sub- sets. This indicates that our auditory representa- tions are better at judging auditory-relevant com- parisons than they are at non-auditory ones, as we might expect.</p><p>For all datasets, the accuracy scores for multi- modal models are at least as high as those for the purely linguistic representations. In the case of the full datasets this difference is only marginal, which is to be expected given how few of the words in the datasets are auditory-relevant. How- ever, the results indicate that adding auditory in- put even for words that are not directly auditory- relevant is not detrimental to overall performance.</p><p>In the case of the auditory-relevant subsets, we see a large increase in performance when using multi-modal representations. It is also interesting that this performance increase is found in the sim- ple MM model, compared to the more complicated MMSG models, which seems to indicate that the latter models are still too reliant on linguistic in- formation, which harms their performance when performing auditory-specific comparisons. The model which performs consistently well across the four datasets is MM, the middle-late fusion model with α = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cross-modal Zero-shot Learning</head><p>We learn a cross-modal mapping between the linguistic and auditory spaces using partial least squares regression, taking out each concept, train- ing on the others, and then projecting from one  <ref type="table">Table 4</ref>: Cross-modal zero-shot learning accuracy.</p><p>space into the other. Zero-shot performance is evaluated using the average percentage correct at N (P@N ), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. Results are shown in <ref type="table">Table 4</ref>, with the chance baseline obtained by ran- domly ranking a concept's nearest neighbors. In- sofar as it is possible to make a direct comparison with linguistic-visual zero-shot learning (which uses entirely different data), it appears that the cur- rent task may be more difficult: <ref type="bibr" target="#b18">Lazaridou et al. (2014)</ref> report a P@1 of 2.4 and P@20 of 33.0 for their linguistic-visual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head><p>We also performed a small qualitative analysis of the BoAW representations for the words in MEN and SLex. As <ref type="table" target="#tab_8">Table 5</ref> shows, the nearest neighbors are remarkably semantically coherent. For exam- ple, the model groups together sounds produced by machines, or by water. It even finds that dinner, meal, lunch and breakfast are closely related. In contrast, nearest neighbors for the linguistic model tend to be of a more abstract nature: where we find mouth and throat as auditory neighbors for lan- guage, the linguistic model gives us concepts like word and dictionary; while auditory gossip sounds like maids and is something you might do in the corridor, it is linguistically associated with more abstract concepts like news and newspaper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parameter Tuning</head><p>There are many parameters that were left fixed in the main results that could have been adjusted to improve performance, particularly in the middle and late fusion models. It is useful to investigate some of the parameters that are likely to have an impact on performance: what the effect of the α mixing parameter is, whether a different k would have yielded better auditory representations, and whether the number and duration of the audio files from FreeSound has any effect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Mixing with α</head><p>The mixing parameter α plays an important role in the middle and late fusion models. We kept it fixed at 0.5 for the MM model above, but here we experiment with varying the parameter, yield- ing results for two different models, MM-MIDDLE and MM-LATE. The results are shown in <ref type="figure" target="#fig_0">Figure 1</ref>, where moving to the right on the x-axis uses more linguistic input and moving to the left uses more auditory input. The late model consistently out- performs the middle fusion model, which is prob- ably because it is less susceptible to any noise in the auditory representation. Optimal performance seems to be around α = 0.6 for both fusion strate- gies on all four datasets, indicating that it is bet- ter to include a little more linguistic than auditory input. It appears that any 0.5 ≤ α &lt; 1 (i.e., where we have more linguistic input but still some auditory signal), outperforms the purely linguis- tic representation, substantially in the case of the auditory-relevant subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Number of Auditory Dimensions</head><p>We experimented with different values for the number of audio words k (i.e. the number of clus- ters in the k-means clustering that determines the number of "audio words"). As <ref type="figure" target="#fig_1">Figure 2</ref> shows, the quality of the uni-modal auditory representations is highly robust to the number of dimensions. In fact, any choice of k in the range shown provides similar results across the datasets. <ref type="table">Linguistic   navy  language gossip  dinner  navy  language  gossip  dinner   army  mouth  maid  meal  army  word  news  lunch   aviation  man  guest  lunch  military  words  newspaper wedding   plane  father  elevator  writer  vessel  literature  cute  meal   jet  adult  danger  breakfast  sunk  dictionary  sexy</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auditory</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Number and Duration of Audio Files</head><p>We experimented with the number of audio files by querying FreeSound for up to 100 audio files per search word, while keeping k = 300. The results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>. It appears that "the more the better", although performance does not increase significantly after around 40 audio files. In order to examine the effect of audio file du- ration, we experimented with specifying the du- ration of audio files when querying the database, either taking very short (up to 5 seconds), medium length (up to 1 minute) or files of any duration. The results can be found in <ref type="figure" target="#fig_3">Figure 4</ref>, showing that performance generally increases as the files get longer (except on AMEN where a duration of 1 minute provides optimal performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Case Study: Musical Instruments</head><p>To strengthen the finding that multi-modal repre- sentations perform well on the auditory-relevant subsets of the datasets, we evaluate on an alto- gether different task, namely that of musical in- strument classification. We used Wikipedia to col- lect a total of 52 instruments and divided them into 5 classes: brass, percussion, piano-based, string and woodwind instruments. For each of the in- struments, we collected as many audio files from FreeSound as possible, and used the MM-MIDDLE model with parameter settings that yielded good results in the previous experiments (k = 300 and α = 0.6). We then performed k-means cluster- ing with five cluster centroids and compared re- sults between auditory, linguistic and multi-modal, evaluating the clustering quality using the standard V-measure clustering evaluation metric <ref type="bibr" target="#b27">(Rosenberg and Hirschberg, 2007)</ref>. This is an interesting problem because instru- ment classes are determined somewhat by conven- tion (is a saxophone a brass or a woodwind in- strument?). What is more, how instruments ac- tually sound is rarely described in detail in text, so corpus-based linguistic representations cannot take this information into account. The results are in <ref type="table">Table 6</ref>, clearly showing that the multi-modal representation which utilizes both linguistic infor- mation and auditory input performs much better on this task than the uni-modal representations. It is interesting to observe that the linguistic repre- sentations perform better than the auditory ones: a possible explanation for this is that audio files in FreeSound are rarely samples of a single individ- ual instrument, so if a bass is often accompanied by a drum this will affect the overall representa- tion. The table also shows, for the 5 clusters under both models, the nearest instruments to the cluster centroids, qualitatively demonstrating the greater cluster coherence for the multi-modal model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have studied grounding semantic representa- tions in raw auditory perceptual information, us- ing a bag of audio words model to obtain au- ditory representations, and combining them into multi-modal representations using a variety of fu- sion strategies. Following previous work in multi- modal semantics, we evaluated on conceptual sim- ilarity and relatedness datasets, and on the cross- modal task of zero-shot learning. We presented a short case study showing that multi-modal rep- resentations perform much better than auditory or linguistic representations on a musical instrument clustering task. It may well be the case that the  <ref type="table">Table 6</ref>: V-measure performance for clustering musical instruments, together with instruments closest to cluster centroid for linguistic and multi- modal.</p><p>auditory modality is better suited for other evalua- tions, but we have chosen to follow standard eval- uations in multi-modal semantics to allow for a di- rect comparison.</p><p>In future work, it would be interesting to inves- tigate different sampling strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the mid- dle and late fusion models, e.g. using the "audio dispersion" of a word to determine how much au- ditory input should be included in the multi-modal representation ( ). Another in- teresting possibility is to improve auditory repre- sentations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics. Lastly, now that the perceptual modalities of vi- sion, audio and even olfaction ( <ref type="bibr" target="#b17">Kiela et al., 2015)</ref> have been investigated in the context of distribu- tional semantics, the logical next step for future work is to explore different fusion strategies for multi-modal models that combine various sources of perceptual input into a single grounded model. for useful suggestions and thank the anonymous reviewers for their helpful comments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of middle and late multimodal fusion models compared to linguistic representations on the four datasets when varying the α mixing parameter on the x-axis.</figDesc><graphic url="image-1.png" coords="6,307.28,62.81,222.24,177.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of uni-modal auditory representations on the four datasets when varying the k parameter.</figDesc><graphic url="image-2.png" coords="7,72.00,293.98,222.24,89.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of uni-modal auditory representations on the four datasets when varying the number of audio files per target word.</figDesc><graphic url="image-3.png" coords="7,307.28,293.98,222.24,171.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of uni-modal auditory representations on the four datasets when varying the maximum duration.</figDesc><graphic url="image-4.png" coords="8,72.00,62.81,222.24,166.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Number of concept pairs for which repre- sentations are available in each modality.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Spearman ρ s correlation comparison of 
uni-modal and multi-modal representations. The 
MMSG models perform early fusion, MM repre-
sents middle and late fusion with α = 0.5 (see 
Section 4.3.2). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example nearest neighbors for auditory (BoAW) representations and linguistic representations. 

</table></figure>

			<note place="foot" n="1"> To facilitate further work in multi-modal semantics beyond vision, our code and data have been made publicly available at http://www.cl.cam.ac.uk/˜dk427/audio.html. 2 To avoid introducing another parameter, we set the number of latent variables in the cross-modal PLSR map to a third of the number of dimensions of the perceptual representation.</note>

			<note place="foot" n="3"> http://www.freesound.org. 4 http://www.vorbis.com. 5 We used the demo-train-big-model-v1.sh script from http://word2vec.googlecode.com to obtain this corpus.</note>

			<note place="foot" n="6"> http://bmcfee.github.io/librosa.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using visual information to predict lexical preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="399" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artifical Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting Semantic Representations from Word Cooccurrence Statistics: A computational study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vector Space Models of Lexical Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Contemporary Semantics, chapter 16</title>
		<editor>Shalom Lappin and Chris Fox</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-Blackwell</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Musical instrument recognition using ICA-based transform of features and discriminatively trained HMMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eronen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Symposium on Signal Processing and Its Applications</title>
		<meeting>the Seventh International Symposium on Signal Processing and Its Applications</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual information in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning object categories from Google&apos;s image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1816" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Content-based retrieval of music and audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jonathan T Foote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Voice, Video, and Data Communications</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DeViSE: A Deep Visual-Semantic Embedding Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Gazzaniga</surname></persName>
		</author>
		<title level="m">The Cognitive Neurosciences</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The symbol grounding problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning abstract concept embeddings from multi-modal data: Since you probably can&apos;t see what I mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SimLex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning image embeddings using convolutional neural networks for improved multi-modal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving multi-modal representations using image dispersion: Why less is sometimes more</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="835" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grounding semantics in olfactory perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1403" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skipgram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going beyond text: A hybrid image-text approach for measuring word relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Chee Wee Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1403" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Soundbased distributional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopopolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Miltenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Computational Semantics</title>
		<meeting>the 11th International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>IWCS 2015</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Symbol interdependency in symbolic and embodied cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">M</forename><surname>Louwerse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in Cognitive Science</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="617" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR<address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Speech communication: human and machine. Addison-Wesley series in electrical engineering: digital signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;shaughnessy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>Universities Press</publisher>
			<pubPlace>India) Pvt</pubPlace>
		</imprint>
	</monogr>
	<note>Limited</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A multimodal LDA model integrating textual, cognitive and visual modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1146" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<imprint>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1177" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grounded models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1423" to="1433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A scale for the measurement of the psychological magnitude pitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">Smith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Volkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><forename type="middle">B</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="190" />
			<date type="published" when="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning words from images and speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<meeting><address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artifical Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
