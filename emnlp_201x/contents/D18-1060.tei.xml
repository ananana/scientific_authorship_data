<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Metaphor Detection in Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for Artificial Intelligence †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for Artificial Intelligence †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for Artificial Intelligence †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename><forename type="middle">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for Artificial Intelligence †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for Artificial Intelligence †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Metaphor Detection in Context</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="607" to="613"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>607</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present end-to-end neural models for detecting metaphorical word use in context. We show that relatively standard BiLSTM models which operate on complete sentences work well in this setting, in comparison to previous work that used more restricted forms of linguistic context. These models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the metaphoricity of all words in a running text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Metaphors are pervasive in natural language, and detecting them requires challenging contextual reasoning about whether specific situations can ac- tually happen. ( <ref type="bibr" target="#b14">Lakoff and Johnson, 1980)</ref>. For example, in <ref type="table" target="#tab_0">Table 1</ref>, "examining" is metaphorical because it is impossible to literally use a "micro- scope" to examine an entire country. In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate inter- pretation of figurative language.</p><p>In contrast, most previous approaches focused on limited forms of linguistic context, for exam- ple by only providing SVO triples such as <ref type="bibr">(car, drink, gasoline)</ref> to the model ( <ref type="bibr" target="#b30">Tsvetkov et al., 2013;</ref><ref type="bibr" target="#b22">Rei et al., 2017;</ref>. While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Ta- ble <ref type="bibr">1</ref>. Even in the few cases when the full sentence is used <ref type="bibr" target="#b13">(Köper and im Walde, 2017;</ref><ref type="bibr" target="#b31">Turney et al., 2011;</ref><ref type="bibr" target="#b8">Jang et al., 2016</ref>) existing models have used unigram-based features with limited expressivity. We investigate two common task formulations: (1) given a target verb in a sentence, classify- ing whether it is metaphorical or not, and <ref type="formula">(2)</ref> The experts started examining the Soviet Union with a microscope to study perceived changes. Rockford teachers are honored for saving a drowning student. You're drowning in student loan debt. given a sentence, detecting all of the metaphor- ical words (independent of their POS tags). We find that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhu- ber, 1997) augmented with contextualized word embeddings ( <ref type="bibr" target="#b20">Peters et al., 2018</ref>) perform sur- prisingly well on both tasks, even with mod- est amount of training data. We improve the previous state-of-the-art by 7.5 F1 on the VU Amsterdam Metaphor Corpus (VUA) for the se- quence labeling task <ref type="bibr" target="#b24">(Steen et al., 2010)</ref>, by 2.5 F1 on the VUA verb clasificiation dataset, and by 4.9 F1 on the MOH-X dataset <ref type="bibr" target="#b15">(Mohammad et al., 2016)</ref>. Our code is publicly available at https://github.com/gao-g/ metaphor-in-context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task</head><p>We study two task formulations.</p><p>Sequence Labeling: Given a sentence x 1 ,. . . ,x n , predict a sequence of binary labels l 1 , . . . , l n to indicate the metaphoricity of each word.</p><p>Classification: Given a sentence x 1 , . . . , x n and a target verb index i, predict a binary label l to indicate the metaphoricity of the target x i .</p><p>While both formulations have been studied in pre- vious work, it is worth noting that the sequence labeling task generalizes the classification task in that the prediction for the target verb can be ex- tracted from the full sentence predictions. In ad- dition, as will be shown in Section 5, we find that given accurate annotations for all words in a sen- tence, the sequence labeling model outperforms the classification model even when the evaluation is set up as a classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Our models use a bidirectional LSTM to encode a sentence, and a feedforward neural network for classification, optimized for the log-likelihood of gold labels.</p><p>Sentence encoding For both sequence labeling and classification, we represent each token x i in the input sentence with a pre-trained word em- bedding w i . To further encode contextual infor- mation, we also concatenate ELMo (Embeddings from Language Models) vectors e i from <ref type="bibr" target="#b20">Peters et al. (2018)</ref>. These vectors have been shown to be useful for word sense disambiguation, a task closely related to metaphor detection ( <ref type="bibr" target="#b0">Birke and Sarkar, 2006</ref>). <ref type="figure" target="#fig_0">Figure 1</ref> shows the model architecture. We input the word representation [w i ; e i ] to a bidirectional LSTM, producing a contextualized representation h i for each token. Then we use a feedforward neu- ral network that takes h i to predict a label l i for each word x i . When the dataset does not contain annotations for every word, we make the simplifying assump- tion that every unannotated word is used literally.  <ref type="figure" target="#fig_1">Figure 2</ref> shows the model architecture. We con- catenate an index embedding n i , which indicates whether x i is the target verb. We use [w i ; e i ; n i ] as an input to a bidirectional LSTM, producing a contextualized representation h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence Labeling Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification Model</head><p>We add an attention layer by computing the at- tention weight a i for token x i , and compute the representation c as a weighted sum of LSTM out- put states where W a and b a are learned parameters.</p><formula xml:id="formula_0">a i = SoftMax i (W a h i + b a ) c = n i=1 a i h i</formula><p>Finally, we feed c to a feedforward network to compute the label scores for target verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>We evaluate performance on a number of bench- mark datasets, including two for classification (TroFi and MOH\MOH-X) and one for tagging (VUA). 1 <ref type="table" target="#tab_2">Table 2</ref> shows statistics for the verb clas- sification datasets. Despite being two times larger than the MOH dataset, the TroFi dataset contains only 50 unique verbs, and the larger VUA dataset contains over 2K unique verbs. The MOH dataset contains shorter and simpler sentences (example sentences in WordNet), compared to sentences in other datasets which come from resources such as   Classification Experiment Setup We perform 10 fold cross-validation on the MOH-X and TroFi datasets, following prior work. For the VUA dataset, we use the original training and test split ( <ref type="bibr" target="#b10">Klebanov et al., 2016)</ref>, and set aside 10% of the training set as a development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Labeling Experiment Setup</head><p>The VUA dataset contains annotations for all words in each sentence. We divide the data into training, development, and test set following the same split for the VUA verb classification task. While the la- bel classes are less balanced (only 11% metaphors at the token level), this dataset is much bigger. <ref type="table" target="#tab_3">Ta- ble 3</ref> shows the data statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Evaluation Metric We report precision, recall and F1 measure for the metaphor class as well as the overall accuracy. For the VUA dataset, we also report macro-averaged F1 score across four gen- res (conversation, academic writing, fiction and news).</p><p>Comparison Systems We propose a simple yet effective lexical baseline. It assigns the metaphor label if the word is annotated metaphorically more frequently than as literally in the training set, and the literal label otherwise. We also compare our</p><formula xml:id="formula_1">Model P R F1 Acc.</formula><p>Lexical Baseline 68.6 45.2 54.5 90.6 Wu (2018) ensemble 60.8 70.0 65.1 - Ours (SEQ) 71.6 73.6 72.6 93.1   <ref type="bibr">2</ref> We experiment with both sequence labeling model (SEQ) and classification model (CLS) for the verb classification task, and the sequence la- beling model (SEQ) for the sequence labeling task.</p><p>Implementation Details We used 300d GloVe vectors ( <ref type="bibr" target="#b19">Pennington et al., 2014</ref>) and 1024d ELMo vectors. We used additional 50d index embedding for the classification task. The LSTM module has a 300d hidden state. We applied dropout on the in- put to LSTM and on the input to the feedforward layer. We fine-tuned learning rate and dropout rate for each model on each dataset. We used SGD to optimize the CLS model and Adam <ref type="bibr" target="#b9">(Kingma and Ba, 2013</ref>) for the SEQ model. We used spaCy (Honnibal and Montani, 2017) for lemma- tization, tokenization, and part-of-speech tagging.    Sequence Labeling Results Performance on the sequence labeling task is reported in <ref type="table" target="#tab_4">Table 4</ref>. While prior work ( <ref type="bibr" target="#b11">Klebanov et al., 2014;</ref><ref type="bibr" target="#b18">¨ Ozbal et al., 2016</ref>) reported on the same dataset, the ex- periment setting is not comparable (they did cross validation on a smaller training set). <ref type="bibr">3</ref> Our lexical baseline performs strongly in terms of precision, as some words and POS tags are almost exclu- sively annotated as literal. Our sequence labeling model mainly improves recall. <ref type="table" target="#tab_5">Table 5</ref> reports the breakdown of performance by POS tags. Not surprisingly, tags with more data are easier to classify. Adposition is the easiest to identify as metaphorical and is also the most fre- quently metaphorical class (28%). On the other hand, particles are challenging to identify, since they are often associated with multi-word expres- sions, such as "put down the disturbances". <ref type="table" target="#tab_7">Table 6</ref> shows per- formance on the verb classification task for three datasets (MOH-X , TroFi and VUA). <ref type="bibr">4</ref> Our models achieve strong performance on all datasets, outperforming existing models on the MOH-X and VUA datasets. On the MOH-X dataset, the CLS model outperforms the SEQ model, likely due to the simpler overall sentence structure and the fact that the target verbs are the only words annotated for metaphoricity. For the VUA dataset, where we have annotations for all words in a sentence, the SEQ model significantly outperforms the CLS model. This result shows that predicting metaphor labels of context words helps to predict the target verb. We hypothesize that <ref type="bibr">Köper et al. (2017)</ref> outperforms our models on the TroFi dataset for a similar reason: their work uses concreteness labels, which highly correlate to metaphor labels of neighboring words in the sen- tence. Also, their best model uses the verb lemma as a feature, which itself provides a strong clue in the dataset of 50 verbs (see lexical baseline). <ref type="table" target="#tab_8">Table 7</ref> shows an ablation study on input repre- sentations (with or without ELMo vectors). Con- textualized word vectors improve the performance of both models by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verb Classification Results</head><p>Error Analysis We sampled 100 errors of our best model from the VUA verb classification de- velopment set for analysis. <ref type="table" target="#tab_9">Table 8</ref> shows exam- ples. Following the original annotation guideline, <ref type="bibr">5</ref> we classify metaphors into five categories: direct metaphor, indirect metaphor, implicit metaphor, personification, and borderline case. Indirect metaphor, the most common type for verbs, means that the basic meaning of a word is different from its contextual meaning. Implicit metaphor occurs due to an underlying link which points to a recov- erable metaphorical concept.</p><p>About half of the errors were false positives, and the other half were false negatives. Among the false negatives, 33% are indirect metaphors, 18% are personifications, and 2% are direct metaphors. Among 55 false positives, 31% of verbs have im-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLS SEQ Sentence</head><p>Metaphor Type To throw up an impenetrable Berlin Wall between you and them could be tactless. - In reality you just invent a tale, as if you were sitting round a fire in a cave. direct metaphor So they bought immunity.</p><p>indirect metaphor During the early states of the phased evacuation the logistical problem facing the po- lice was the street-by-street warning of the population to make ready for evacuation. indirect metaphor There are few things worse than being bludgeoned into reading a book you hate.</p><p>indirect metaphor He thought of thick, fat, hot motorways carving up that land. personification One might ask whether motorists are ever justified in knowingly taking risks with other people's lives.</p><p>- The abstract talk of commuting by rail or road being replaced by information technol- ogy finds a concrete expression in the idea of telecottages. - A fly landed on the empty, staring vizor, and crawled across it. - plicit arguments that are not explicitly mentioned in the context, 15% have long range dependencies (at least five words away) from core arguments, 10% have arguments with rare word senses, and 5% have anthropomorphic arguments. Finally, we found about half of false negatives and 20% of false positives to be borderline cases, showing the subjective nature of the task. We sampled 257 dev examples that the CLS model gets wrong but the SEQ model gets cor- rect. We found that the SEQ model outperforms the CLS model on detecting personifications, in- direct metaphors, and direct metaphors involving uncommon verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There has been significant work on studying dif- ferent features for metaphor detection, includ- ing concretenesss and abstractness <ref type="bibr" target="#b31">(Turney et al., 2011;</ref><ref type="bibr" target="#b29">Tsvetkov et al., 2014;</ref><ref type="bibr" target="#b13">Köper and im Walde, 2017)</ref>, imaginability ( <ref type="bibr" target="#b26">Broadwell et al., 2013;</ref><ref type="bibr" target="#b26">Strzalkowski et al., 2013)</ref>, feature norms ( , sensory features ( <ref type="bibr" target="#b28">Tekiroglu et al., 2015;</ref>, bag-of-words fea- tures <ref type="bibr" target="#b12">(Köper and im Walde, 2016)</ref>, and semantic class using WordNet ( <ref type="bibr" target="#b7">Hovy et al., 2013;</ref><ref type="bibr" target="#b29">Tsvetkov et al., 2014</ref>). More recently, embedding-based ap- proaches <ref type="bibr" target="#b13">(Köper and im Walde, 2017;</ref><ref type="bibr" target="#b22">Rei et al., 2017)</ref> showed gains on various benchmarks.</p><p>Many neural models with various features and architectures were introduced in the 2018 VUA Metaphor Detection Shared Task. They include LSTM-based models and CRFs augmented by lin- guistic features, such as WordNet, POS tags, con- creteness score, unigrams, lemmas, verb clusters, and sentence-length manipulation <ref type="bibr" target="#b27">(Swarnkar and Singh, 2018;</ref><ref type="bibr" target="#b21">Pramanick et al., 2018;</ref><ref type="bibr" target="#b16">Mosolova et al., 2018;</ref><ref type="bibr" target="#b1">Bizzoni and Ghanimifard, 2018;</ref><ref type="bibr" target="#b32">Wu et al., 2018)</ref>. Researchers also studied different word embeddings, such as embeddings trained from corpora representing different levels of lan- guage mastery <ref type="bibr" target="#b25">(Stemle and Onysko, 2018)</ref> and binarized vectors that reflect the General Inquirer dictionary category of a word ( <ref type="bibr" target="#b17">Mykowiecka et al., 2018)</ref>. We show that contextualized word embed- ding significantly improves metaphor detection. We also study both sequence labeling and classifi- cation approaches, suggesting that sequence label- ing approach enhances performance when used to jointly predict the metaphoricity of all words in a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present simple biLSTM models augmented with contextualized word representa- tion for metaphor detection. Our models estab- lish new state-of-the-arts across multiple existing benchmarks, and our error analysis shows remain- ing challenges for metaphor detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A sequence labeling model for metaphor detection. Every word in a sentence is classified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A classification model for metaphor detection. Only a single word per sentence is labeled as metaphorical or literal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Metaphorical usages of the target word are bold faced, and literal usages are italicized. Full sen- tence context is crucial for metaphor detection.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Verb classification dataset statistics. % 
Metaphor refers to sentence-level percentage. 

Train 
Dev 
Test 

# Unique tokens 
13,843 
7,458 
7,200 
# Tokens 
116,622 38,628 50,175 
# Unique sent. 
6,323 
1,550 
2,694 
% Metaphor 
11.2 
11.6 
12.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>VUA sequence labeling dataset statistics. % 
Metaphor refers to token-level percentage. 

news articles. The TroFi and MOH-X datasets 
are constructed to have higher percentages of 
metaphor, compared to the natural likelihood of 
metaphor in a running text, as seen in the VUA 
dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Performance on the VUA sequence labeling test set for all POS tags.</head><label>4</label><figDesc></figDesc><table>POS 
# % metaphor 
P 
R 
F1. 

VERB 
20K 
18.1 68.1 71.9 69.9 
NOUN 20K 
13.6 59.9 60.8 60.4 
ADP 
13K 
28.0 86.8 89.0 87.9 
ADJ 
9K 
11.5 56.1 60.6 58.3 
PART 
3K 
10.1 57.1 59.1 58.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The breakdown of performance on the VUA 
sequence labeling test set by POS tags. We show data 
statistics (count, % metaphor) on the training set. We 
only show POS tags whose % metaphor &gt; 10. 

models to previously published work, including: 
(1) a logistic regression classifier with features 
that indicate verb lemmas and the verbs' seman-
tic class from WordNet (Klebanov et al., 2016), 
(2) a neural similarity network with skip-gram 
word embeddings (Rei et al., 2017), (3) a bal-
anced logistic regression classifier on target verb 
lemma that uses a set of features based on multi-
sense abstractness rating (Köper and im Walde, 
2017), and (4) a CNN-LSTM ensemble model 
with weighted-softmax classifier which incorpo-
rates pre-trained word2vec, POS tags, and word 
cluster features (Wu et al., 2018). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Model performances for the verb classification task. Our models achieve strong performance on all 
datasets. The CLS model performs better than the SEQ model when only one word per sentence is annotated 
by human (TroFi and MOH-X). When all words in the sentence are accurately annotated (VUA), the SEQ model 
outperforms the CLS model. 

Model 
P 
R 
F1. Acc. 

SEQ 
68.3 72.0 70.4 83.5 
-ELMo 59.4 64.3 61.7 78.2 
CLS 
52.4 63.0 57.3 74.3 
-ELMo 52.0 48.7 50.8 74.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on VUA development set for the verb classification task.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Some examples from the VUA verb classification development set. Metaphorical usages of the target 
word are bold faced, and literal usages are italicized. Leftmost columns show the correctness of prediction. 

</table></figure>

			<note place="foot" n="1"> For detailed information about each dataset, please refer to original papers: TroFi (Birke and Sarkar, 2006), MOH (Mohammad et al., 2016), VUA (Steen et al., 2010). MOH-X refers to a subset of MOH dataset used in previous work (Shutova et al., 2016) where verb and its argument are extracted from each sentence.</note>

			<note place="foot" n="2"> The best performing model on the VUA Metaphor Detection Shared Task at the NAACL 2018 workshop on Figurative Language Processing.</note>

			<note place="foot" n="3"> As a point of reference, their macro-averaged F1 scores were 33.25 / 50.6 respectively. 4 We did not compare to Shutova et al. (2016) as their experiment setting is not comparable.</note>

			<note place="foot" n="5"> http://www.vismet.org/metcor/documentation/home.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful comments. This work was supported in part by the NSF (IIS-1714566 and IIS-1252835), the ARO (W911NF-16-1-0121), the DARPA CwC program through ARO (W911NF-15-1-0543), and gifts from Google and Facebook.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A clustering approach for nearly unsupervised recognition of nonliteral language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Birke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bigrams and bilstms two neural networks for sequential metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Bizzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ghanimifard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George Aaron</forename><surname>Broadwell</surname></persName>
		</author>
		<imprint>
			<pubPlace>Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah M</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using imageability and topic chaining to locate metaphors in linguistic corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SBP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Modelling metaphor with attribute-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In EACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">2017. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifying metaphorical word use with tree kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP, ACL</title>
		<meeting>the First Workshop on Metaphor in NLP, ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metaphor detection with topic transition, emotion and cognition in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeju</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Seungwhan Moon, and Carolyn Penstein Rosé</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic classifications for detection of verb metaphors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Wee</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Dario</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Gutiérrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Different texts, same metaphors: Unigrams and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Wee</forename><surname>Beata Beigman Klebanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Metaphor in NLP, ACL</title>
		<meeting>the Second Workshop on Metaphor in NLP, ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinguishing literal and non-literal usage of german particle verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving verb metaphor detection by propagating abstractness to words, phrases and individual senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Sense, Concept and Entity Representations and their Applications</title>
		<meeting>the First Workshop on Sense, Concept and Entity Representations and their Applications</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Metaphors we live by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>The University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Metaphor as a medium for emotion: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*Sem)</title>
		<meeting>the Fifth Joint Conference on Lexical and Computational Semantics (*Sem)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional random fields for metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Mosolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Fomin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="121" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting figurative word occurrences using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Mykowiecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Wawer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magorzata Aneta</forename><surname>Marciniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="124" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to identify metaphors from a corpus of proverbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gözdë</forename><surname>Ozbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serra</forename><surname>Sinem Tekiroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Pighin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An lstm-crf based approach to token-level metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malay</forename><surname>Pramanick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pabitra</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grasping the finer point: A supervised similarity network for metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Black holes and white rabbits: Metaphor identification with visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Metaphor in usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerard J Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">A</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Kaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krennmayr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">765796</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using language learner data for metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Egon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Stemle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Onysko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust extraction of metaphor from novel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomek</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">Aaron</forename><surname>Broadwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Yamrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP, ACL</title>
		<meeting>the First Workshop on Metaphor in NLP, ACL<address><addrLine>Umit Boz, Ignacio Cases, and Kyle Elliot</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Di-lstm contrast : A deep neural network for metaphor detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnkant</forename><surname>Swarnkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Kumar</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring sensorial features for metaphor identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tekiroglu</forename><surname>Serra Sinem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Metaphor in NLP, ACL</title>
		<meeting>the Third Workshop on Metaphor in NLP, ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Gözdë Ozbal, and Carlo Strapparava</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metaphor detection with cross-lingual model transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cross-lingual metaphor detection using common semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Mukomel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Metaphor in NLP, ACL</title>
		<meeting>the First Workshop on Metaphor in NLP, ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural metaphor detecting with cnn-lstm model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Figurative Language Processing, NAACL</title>
		<meeting>the Workshop on Figurative Language Processing, NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="110" to="114" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
