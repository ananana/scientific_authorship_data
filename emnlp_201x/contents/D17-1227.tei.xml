<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size)</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<settlement>Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<settlement>Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<settlement>Oregon</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When to Finish? Optimal Beam Search for Neural Text Generation (modulo beam size)</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2134" to="2139"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In neural text generation such as neural machine translation, summarization, and image captioning, beam search is widely used to improve the output text quality. However, in the neural generation setting , hypotheses can finish in different steps, which makes it difficult to decide when to end beam search to ensure op-timality. We propose a provably optimal beam search algorithm that will always return the optimal-score complete hypothesis (modulo beam size), and finish as soon as the optimality is established (finishing no later than the baseline). To counter neural generation&apos;s tendency for shorter hypotheses, we also introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on neural machine translation demonstrate that our principled beam search algorithm leads to improvement in BLEU score over previously proposed alternatives.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural text generation using re- current networks have witnessed rapid progress, quickly becoming the state-of-the-art paradigms in machine translation <ref type="bibr" target="#b2">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b9">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), summarization ( <ref type="bibr">Rush et al., 2015;</ref><ref type="bibr" target="#b5">Ranzato et al., 2016)</ref>, and image captioning ( <ref type="bibr" target="#b10">Vinyals et al., 2015;</ref><ref type="bibr" target="#b12">Xu et al., 2015</ref>). In the decoder of neu- ral generation, beam search is widely employed to boost the output text quality, often leading to sub- stantial improvement over greedy search (equiva- lent to beam size 1) in metrics such as BLEU or † Current address: Google Inc., New York, NY, USA. ROUGE; for example, <ref type="bibr" target="#b5">Ranzato et al. (2016)</ref> re- ported +2.2 BLEU (on single reference) in trans- lation and +3.5 ROUGE-2 in summarization, both using a beam of 10. Our own experiments on ma- chine translation (see Sec. 5) show +4.2 BLEU (on four references) using a beam of 5.</p><p>However, unlike traditional beam search in phrase-based MT or shift-reduce parsing where all hypotheses finish in the same number of steps, here in neural generation, hypotheses can finish in vastly different numbers of steps. Once you find a completed hypothesis (by generating the &lt;/s&gt; sym- bol), there are still other active hypotheses in the beam that can continue to grow, which might lead to better scores. Therefore when can you end the beam search? How (and when) can you guarantee that the returned hypothesis has the optimal score modulo beam size?</p><p>There have not been satisfying answers to these questions, and existing beam search strategies are heuristic methods that do not guarantee optimality. For example, the widely influential RNNsearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) employs a "shrinking beam" method: once a completed hypothesis is found, beam size shrinks by 1, and beam search would finish if beam size shrinks to 0 or if the number of steps hits a hard limit. The best scoring completed hypothesis among all completed ones encountered so far is returned. On the other hand, OpenNMT ( <ref type="bibr" target="#b3">Klein et al., 2017)</ref>, whose PyTorch version will be the baseline in our experiments, uses a very different strategy: beam search ter- minates whenever the highest-ranking hypothesis in the current step is completed (which is also the one returned), without considering any other com- pleted hypotheses. Neither of these two methods guarantee optimality of the returned hypothesis.</p><p>We therefore propose a novel and simple beam search variant that will always return the optimal- score complete hypothesis (modulo beam size), and finish as soon as the optimality is established.</p><p>However, another well-known problem remains, that the generated sentences are often too short, compared to previous paradigms such as SMT ( <ref type="bibr" target="#b8">Shen et al., 2016)</ref>. To alleviate this problem, pre- vious efforts introduce length normalization (as a switch in RNNsearch) or length reward ) borrowed from SMT ( <ref type="bibr" target="#b4">Koehn et al., 2007)</ref>. Unfortunately these changes will invali- date the optimal property of our proposed algo- rithm. So we introduce a bounded length reward mechanism which allows a modified version of our beam search algorithm to remain optimal. Ex- periments on neural machine translation demon- strate that our principled beam search algorithm leads to improvement in BLEU score over previ- ously proposed alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Generation and Beam Search</head><p>Here we briefly review neural text generation and then review existing beam search algorithms.</p><p>Assume the input sentence, document, or image is embedded into a vector x, from which we gen- erate the output sentence y which is a completed hypothesis: 1</p><formula xml:id="formula_0">y * = argmax y:comp(y) p(y | x) = argmax y:comp(y) ∏ i≤|y| p(y i | x, y &lt;i )</formula><p>where y &lt;i is a popular shorthand notation for the prefix y 0 y 1 ...y i−1 . We say that a hypothesis y is completed, notated comp(y), if its last word is</p><formula xml:id="formula_1">&lt;/s&gt;, i.e., comp(y) ∆ = (y |y| = &lt;/s&gt;)</formula><p>in which case it will not be further expanded.</p><p>A crucial difference in RNN-based neural gen- eration compared to previous paradigms such as phrase-based MT is that we no longer decom- pose p(y i | x, y &lt;i ) into the translation model, p(y i | x), and the language model, p(y i | y &lt;i ), and more importantly, we no longer approximate the latter by n-gram models. This ability to model arbitrarily-lengthed history using RNNs is an im- portant reason for NMT's substantially improved fluency compared to SMT.</p><p>To (approximately) search for the best output y * , we use beam search, where the beam B i at step <ref type="bibr">1</ref> For simplicity reasons we do not discuss bidirectional LSTMs and attentional mechanisms here but our algorithms still work with those encoders (we have tested them).</p><p>i is an ordered list of size (at most) b, and expands to the next beam B i+1 of the same size:</p><formula xml:id="formula_2">B 0 = [⟨&lt;s&gt;, p(&lt;s&gt; | x)⟩] B i = b top{⟨y ′ • y i , s·p(y i |x, y)⟩ | ⟨y ′ , s⟩ ∈ B i−1 }</formula><p>where the notation top b S selects the top b scoring items from the set S, and each item is a pair ⟨y, s⟩ where y is the current prefix and s is its accumu- lated score (i.e., product of probabilities).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimal Beam Search (modulo beam size)</head><p>We propose a very simple method to optimally fin- ish beam search, which guarantees the returned hypothesis is the highest-scoring completed hy- pothesis modulo beam size; in other words, we will finish as soon as an "optimality certificate" can be established that future hypotheses will never score better than the current best one.</p><p>Let best ≤i be the best completed hypothesis so far up to step i, i.e.,</p><formula xml:id="formula_3">best ≤i ∆ = max{y ∈ ∪ j≤i B j | comp(y)} (1)</formula><p>We update it every time we find a completed hy- pothesis (if there is none yet, then it remains unde- fined). Now at any step i, if best ≤i is defined, and the highest scoring item B i,1 in the current beam B i scores worse than or equal to best ≤i , i.e., when</p><formula xml:id="formula_4">B i,1 ≤ best ≤i (2)</formula><p>we claim the optimality certificate is established, and terminate beam search, returning best ≤i (here smaller means worse, since we aim for the highest- probability completed hypothesis).</p><p>Theorem 1 (optimality). When our beam search algorithm terminates, the current best completed hypothesis (i.e., best ≤i ) is the highest-probability completed hypothesis (modulo beam size).</p><p>Proof. If B i,1 ≤ best ≤i then B i,j ≤ B i,1 ≤ best ≤i for all items B i,j in beam B i . Future de- scendants grown from these items will only be no better, since probability ≤ 1, so all items in current and future steps are no better than best ≤i .</p><p>Theorem 2 (early stopping). Our beam search al- gorithm terminates no later than OpenNMT's ter- mination criteria (when B i,1 is completed).</p><p>Proof. When B i,1 is itself completed, best ≤i = max{B i,1 , · · · } ≥ B i,1 , so our stopping criteria is also met. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">4 6 8 10 114 16 18 20 total model score (dev) beam size</head><p>OpenNMT-py default optimal ending <ref type="figure">Figure 1</ref>: Comparison between optimal beam search and OpenNMT-py's default search, in terms of search quality (model score, ↑ is better).</p><p>This above Theorem shows that our search is stopping earlier once the optimality certificate is established, exploring fewer items than Open- NMT's default search. Also note that the latter, even though exploring more items than ours, still can return suboptimal solutions; e.g., when B i,1 is worst than best ≤i (they never stored best ≤i ). In practice, we noticed our search finishes about 3- 5 steps earlier than OpenNMT at a beam of 10, and this advantage widens as beam size increases, although the overall speedup is not too notice- able, given the target language sentence length is much longer. Also, our model scores (i.e., log- probabilities) are indeed better (see <ref type="figure">Fig. 1</ref>), where the advantage is also more pronounced with larger beams (note that OpenNMT baseline is almost flat after b = 10, while our optimal beam search still steadily improves). Combining these two Theo- rems, it is interesting to note that our method is not just optimal but also faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimal Beam Search for Bounded Length Reward</head><p>However, optimal-score hypothesis, though satis- fying in theory, is not ideal in practice, since neu- ral models are notoriously bad in producing very short sentences, as opposed to older paradigms such as SMT ( <ref type="bibr" target="#b8">Shen et al., 2016)</ref>. To alleviate this problem, two methods have been proposed: (a) length normalization, used in RNNsearch as an option, where the revised score of a hypothesis is divided by its length, thus favoring longer sen- tences; and (b) explicit length reward ( ) borrowed from SMT, rewarding each gen- erated word by a constant tuned on the dev set. Unfortunately, each of these methods breaks the optimality proof of our beam search algo- rithm in Section 3, since a future hypothesis, be- ing longer, might end up with a higher (revised) score. We therefore devise a novel mechanism called "bounded length reward", that is, we reward each word until the length of the hypothesis is longer than the "estimated optimal length". In ma- chine translation and summarization, this optimal length l can be ratio · |x| where |x| is the source sentence length, and ratio is the average ratio of reference translation length over source sentence length on the dev set (in our Chinese-to-English NMT experiments, it is 1.27 as the English side is a bit longer). Note that we use the same ratio estimated from dev on test, assuming that the op- timal length ratio for test (which we do not know) should be similar to those of dev ones. We denote˜sc denote˜ denote˜sc(y) to be the revised score of hypothesis y with the bounded length reward, i.e.,</p><formula xml:id="formula_5">˜ sc(y) ∆ = sc(y) + r · min{l, |y|}.</formula><p>We also define˜bestdefine˜ define˜best ≤i to be the revised version of best ≤i that optimizes the revised instead of the original score, i.e.,</p><formula xml:id="formula_6">˜ best ≤i ∆ = argmax y∈∪ j≤i B j ,comp(y) ˜ sc(y)</formula><p>Now with bounded length reward, we can mod- ify our beam search algorithm a little bit and still guarantee optimality. First we include in the re- vised cost a reward r for each generated word, as long as the length is less than l, the estimated op- timal length. If at step i, the highest scoring item B i,1 's revised score (i.e., including bounded length reward) plus the heuristic "future" extra length re- ward of a descendant, r · max{l − i, 0}, is worse than (or equal to) the similarly revised version of best ≤i , i.e.,</p><formula xml:id="formula_7">˜ sc(B i,1 ) + r · max{l − i, 0} ≤ ˜ sc( ˜ best ≤i ) (3)</formula><p>at which time we claim the revised optimality certificate is established, and terminate the beam search and returñ best ≤i . Actually with some trivial math we can simplify the stopping criteria to</p><formula xml:id="formula_8">sc(B i,1 ) + r · l ≤ ˜ sc( ˜ best ≤i ).<label>(4)</label></formula><p>This much simplified but still equivalent crite- ria can speed up decoding in practice, since this sents tokens vocab. w/ <ref type="table" target="#tab_1">BPE  Chinese  1M  28M  112k  18k  English  1M  23M  93k  10k   Table 1</ref>: Machine translation training set.</p><p>means we actually do not need to compute the re- vised score for every hypothesis in the beam; we only need to add the bounded length reward when one is finished (i.e., when updating˜bestupdating˜ updating˜best ≤i ), and the simplified criteria only compares it with the original score of a hypothesis plus a constant re- ward r · l.</p><p>Theorem 3 (modified optimality). Our modified beam search returns the highest-scoring com- pleted hypothesis where the score of an item is its log-probability plus a bounded length reward.</p><p>Proof. by admissibility of the heuristic.</p><p>Theorem 4 (correctness of the simplified criteria).</p><p>Eq. 4 is equivalent to Eq. 3.</p><p>Proof. trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments: Neural Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Preparation, Training, and Baselines</head><p>We conduct experiments on Chinese-to-English neural machine translation, using OpenNMT- py, 2 the PyTorch port of the Lua-based Open- NMT ( <ref type="bibr" target="#b3">Klein et al., 2017</ref>). We choose this li- brary because PyTorch's combination of Python with Torch's dynamic computation graphs made it much easier to implement various search algo- rithms on it than on Theano-based implementa- tions derived from RNNsearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) (such as the widely used GroundHog <ref type="bibr">3</ref> and Laulysta 4 codebases) as well as the original Lu- aTorch version of OpenNMT. We use 1M Chi- nese/English sentence pairs for training (see <ref type="table">Ta- ble 1</ref> for statistics); we also trained on 2M sen- tence pairs and only saw a minor improvement so below we report results from 1M training. To al- leviate the vocabulary size issue we employ byte- pair encoding (BPE) ( <ref type="bibr" target="#b7">Sennrich et al., 2015</ref>) which reduces the source and target language vocabulary sizes to 18k and 10k, respectively; we found BPE to significantly improve BLEU scores (by at least +2 BLEU) and reduce training time. Following  other papers on Chinese-English translation such as <ref type="bibr" target="#b8">Shen et al. (2016)</ref>, we use NIST 06 newswire portion (616 sentences) for development and NIST 08 newswire portion (691 sentences) for testing; we will report case-insensitive 4-reference BLEU- 4 scores (using original segmentation). Following OpenNMT-py's default settings, we train our NMT model for 20 epochs to minimize perplexity on the training set (excluding 15% sen- tences longer than 50 source tokens), with a batch size of 64, word embedding size of 500, and dropout rate of 0.3. The total number of param- eters is 29M. Training takes about an hour per epoch on Geforce 980 Ti GPU, and the model at epoch 15 reaches the lowest perplexity on the dev set (9.10) which is chosen as the model for testing.</p><note type="other">reward r 0 1 1.1 1.2 1.3 1.4 1.</note><p>On dev set, the default decoder of OpenNMT-py reaches 29.2 BLEU with beam size 1 (greedy) and 33.2 BLEU with the default beam size of 5. To put this in perspective, the most commonly used SMT toolkit Moses ( <ref type="bibr" target="#b4">Koehn et al., 2007</ref>) reaches 30.1 BLEU (with beam size 70) using the same 1M sen- tence training set (trigram language model trained on the target side). With 2.56M training sentence pairs, <ref type="bibr" target="#b8">Shen et al. (2016)</ref> reported 32.7 BLEU on the same dev set using Moses and 30.7 BLEU using the baseline RNNsearch (GroundHog) with beam size 10 (without BPE, without length nor- malization or length reward). So our OpenNMT- py baseline is extremely competitve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Beam Search &amp; Bounded Length Reward</head><p>We compare the following beam search variants:</p><p>1. OpenNMT-py's default beam search, finish- ing only when the top hypothesis in a step is completed (see Section 2);</p><p>2. The "shrinking beam" method in RNNsearch with two variants to encourage longer trans- lations:</p><p>(a) length normalization; Google NMT ( ) also adopted a similar  Notice that length reward has no effect on both methods 1 and 2(a) above. To tune the optimal length reward r we run our modified optimal- ending beam search algorithm with all combina- tions of r = 0, 0.5, 1, 1.1, 1.2, 1.3, 1.4 with beam sizes b = 1 . . . 20 on the dev set, since different beam sizes might prefer different length rewards. We found r = 1.2 to be the best among all length rewards (see <ref type="table" target="#tab_1">Table 2</ref>) which is used in <ref type="figure" target="#fig_1">Figure 2</ref> and b = 15 is the best for r = 1.2.</p><p>We can observe from <ref type="figure" target="#fig_1">Figure 2</ref> that (a) our opti- mal beam search with bounded length reward per- forms the best, and at b=15 it is +5 BLEU better than b=1; (b) pure optimal beam search degrades after b=4 due to extremely short translations; (c) both the shrinking beam method with length nor- malization and OpenNMT-py's default search al- leviate the shortening problem, but still produce very short translations (length ratio ∼0.9). (d) the shrinking beam method with length reward works well, but still 0.3 BLEU below our best method. These are confirmed by the test set (Tab. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented a beam search algorithm for neural sentence generation that always returns  <ref type="table">Table 3</ref>: Final BLEU scores on the test set (nist 08) using best settings from the dev set (nist 06).</p><p>optimal-score completed hypotheses. To counter neural generation's natural tendancy for shorter hypotheses, we introduced a bounded length re- ward mechanism which allows a modified version of our beam search algorithm to remain optimal. Experiments on top of strong baselines have con- firmed that our principled search algorithms (to- gether with our bounded length reward mecha- nism) outperform existing beam search methods in terms of BLEU scores. We will release our imple- mentations (which will hopefully be merged into OpenNMT-py) when this paper is published. 5</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU score and length ratio against beam size (on dev) of various beam search algorithms for neural machine translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Tuning length reward r (with beam size b=1..20) for optimal bounded-reward beam search.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> https://github.com/opennmt/opennmt-py 3 https://github.com/lisa-groundhog/ 4 https://github.com/laulysta/nmt/</note>

			<note place="foot" n="5"> While implementing our search algorithms we also found and fixed an obscure but serious bug in OpenNMT-py&apos;s baseline beam search code (not related to discussions in this paper), which boosts BLEU scores by about +0.7 in all cases. We will release this fix as well.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers from both EMNLP and WMT for helpful comments. This work is supported in part by NSF IIS-1656051, DARPA N66001-17-2-4030 (XAI), a Google Fac-ulty Research Award, and HP.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with smt features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">413</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
