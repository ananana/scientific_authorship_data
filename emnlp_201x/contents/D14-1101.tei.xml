<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
							<email>yutat@jp.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="938" to="950"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech (POS) tagging. We investigated several types of corpus-wide information for the words, such as word embeddings and POS tag distributions. Since these statistics are encoded as dense continuous features, it is not trivial to combine these features comparing with sparse discrete features. Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features. By using several recent advances in the activation functions for neural networks, the proposed method marks new state-of-the-art accuracies for English POS tagging tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Almost all of the approaches to NLP tasks such as part-of-speech tagging and syntactic parsing mainly use sparse discrete features to represent lo- cal information such as word surfaces in a size- limited window. The non-linearity of those dis- crete features is often used in many NLP tasks since the simple conjunction (AND) of discrete features represents the co-occurrence of the fea- tures and is intuitively understandable. In addi- tion, the thresholding of these combinatorial fea- tures by simple counts effectively suppresses the combinatorial increase of the parameters. At the same time, although global information had also been used in several reports <ref type="bibr">(Nakagawa and Matsumoto, 2006</ref>; <ref type="bibr" target="#b15">Huang and Yates, 2009;</ref><ref type="bibr" target="#b42">Turian et al., 2010;</ref><ref type="bibr" target="#b31">Schnabel and Schütze, 2014)</ref>, the non- linear interactions of these features were not well investigated since these features are often dense continuous features and the explicit non-linear ex- pansions are counterintuitive and drastically in- crease the number of the model parameters. In our work, we investigate neural networks used to rep- resent the non-linearity of global information for POS tagging in a compact way.</p><p>We focus on four kinds of corpus-wide infor- mation: (1) word embeddings, (2) POS tag dis- tributions, (3) supertag distributions, and (4) con- text word distributions. All of them are continuous dense features and we use a feed-forward neural network to exploit the non-linearity of these fea- tures. Although all of them except (3) have been used for POS tagging in previous work ( <ref type="bibr" target="#b23">Nakamura et al., 1990;</ref><ref type="bibr" target="#b30">Schmid, 1994;</ref><ref type="bibr" target="#b31">Schnabel and Schütze, 2014;</ref><ref type="bibr" target="#b15">Huang and Yates, 2009)</ref>, we propose a neu- ral network approach to capture the non-linear in- teractions of these features. By feeding these fea- tures into neural networks as an input vector, we can expect our tagger can handle not only the non- linearity of the N-grams of the same kinds of fea- tures but also the non-linear interactions among the different kind of features.</p><p>Our tagger combines a linear model using sparse high-dimensional features and a neural net- work using continuous dense features. Although <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> seeks to solve NLP tasks without depending on the feature engineering of conventional NLP methods, our architecture is more practical because it integrates the neural networks into a well-tuned conventional method. Thus, our tagger enjoys both the manually ex- plored combinations of discrete features and the automatically learned non-linearity of the contin- uous features. We also studied some of the newer activation functions: Rectified Linear Units <ref type="bibr">(Nair and Hinton, 2010)</ref>, Maxout networks ( <ref type="bibr" target="#b11">Goodfellow et al., 2013)</ref>, and L p -pooling ( <ref type="bibr" target="#b13">Gulcehre et al., 2014;</ref>.</p><p>Deep neural networks have been a hot topic in many application areas such as computer vi-sion and voice recognition. However, although neural networks show state-of-the-art results on a few semantic tasks ( <ref type="bibr" target="#b45">Zhila et al., 2013;</ref><ref type="bibr" target="#b33">Socher et al., 2013;</ref><ref type="bibr" target="#b32">Socher et al., 2011</ref>), neural net- work approaches have not performed better than the state-of-the-art systems for traditional syn- tactic tasks. Our neural tagger shows state-of- the-art results: 97.51% accuracy in the standard benchmark on the Penn Treebank ( <ref type="bibr" target="#b20">Marcus et al., 1993</ref>) and 98.02% accuracy in POS tagging on <ref type="bibr">CoNLL2009 (Hajič et al., 2009</ref>). In our experi- ments, we found that the selection of the activation functions led to large differences in the tagging ac- curacies. We also observed that the POS tags of the words are effectively clustered by the hidden activations of the intermediate layer. This obser- vation is evidence that the neural network can find good representations for POS tagging.</p><p>The remainder of this paper is organized as fol- lows. Section 2 introduces our deterministic tag- ger and its learning algorithm. Section 3 describes the continuous features that represent corpus-wide information and Section 4 is about the neural net- work we used. Section 5 presents our empiri- cal study of the effects of corpus-wide informa- tion and neural networks on English POS tagging tasks. Section 6 describes related work, and Sec- tion 7 concludes and suggests items for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transition-based tagging</head><p>Our tagging model is a deterministic tagger based on <ref type="bibr" target="#b4">Choi and Palmer (2012)</ref>, which is a one-pass, left-to-right tagging algorithm that uses well-tuned binary features.</p><p>Let x = (x 1 , x 2 , . . . , x T ) ∈ X T be an input token sequence of length T and y = (y 1 , y 2 , . . . , y T ) ∈ Y T be a corresponding POS tag sequence of x. We denote the predicted tags by a tagger asˆyasˆ asˆy and the subsequence from r to t as y t r . The prediction of the t-th tag is determinis- tically done by the classifier:</p><formula xml:id="formula_0">ˆ y t = argmax y∈Y f θ (z t , y),<label>(1)</label></formula><p>where f θ is a scoring function with arbitrary pa- rameters, θ ∈ R d , that are to be learned and z t is an arbitrary feature representation of the t-th po- sition using x andˆyandˆ andˆy t−1 1 which is the prediction history of the previous tokens.</p><p>We extend <ref type="bibr" target="#b4">Choi and Palmer (2012)</ref> in three ways: (1) an online SVM learning algorithm with L 1 and L 2 regularization, (2) continuous features for corpus-wide information, and (3) the compos- ite function of a linear model for discrete features and a non-linear model for continuous features. Since (2) and (3) are the main topics of this pa- per, they are explained in detail in Sections 3 and 4 and we describe only (1) here.</p><p>First, our learning algorithm trains a multi-class SVM with L 1 and L 2 regularization based on Fol- low the Proximally Regularized Leader (FTRL- Proximal) <ref type="bibr" target="#b22">(McMahan, 2011</ref>). In the k-th iteration, the parameter update is done by</p><formula xml:id="formula_1">θ k = argmin θ k ∑ l=1 ( g l · θ+ 1 2η l θ−θ l 2 2 ) +R(θ),</formula><p>where g k ∈ R d is a subgradient of the hinge loss function and R(θ) = λ 1 ||θ|| 1 + λ 2 2 ||θ|| 2 2 is the composite function of the L 1 and L 2 regulariza- tion terms with hyper-parameters λ 1 ≥ 0 and λ 2 ≥ 0. To incorporate an adaptive learning rate scheduling, Adagrad ( <ref type="bibr" target="#b8">Duchi et al., 2010)</ref>, we use per-coordinate learning rates for {i|1 ≤ i &lt; d}:</p><formula xml:id="formula_2">η k i = α i ( β i + √ ∑ k l=1 (g l i ) 2 ) ,</formula><p>where α ≥ 0 and β ≥ 0. Although the naive implementation may require O(k) compu- tation in the k-th iteration, FTRL-Proximal can be implemented efficiently by maintaining two <ref type="bibr" target="#b21">McMahan et al., 2013)</ref>. Second, to overcome the error propagation problem, we train the classifier with a simple vari- ant of the on-the-fly example generation algorithm from <ref type="bibr" target="#b10">Goldberg and Nivre (2012)</ref>. Since the scor- ing function refers to the prediction history, Choi and Palmer (2012) uses the gold POS tags, y t−1 1 , to generate training examples, which means they assume all of the past decisions are correct. How- ever, this causes error propagation problems, since each state depends on the history of the past deci- sions. Therefore, at the k-th iteration and the t-th position of the input sequence, we simply use the predictions of the previously learned classifiers to generate training examples, i.e.,</p><formula xml:id="formula_3">length-d vectors, m = ∑ k l g l − 1 2η l θ l and n = ∑ k l (g l i ) 2 (</formula><formula xml:id="formula_4">ˆ y t−r = argmax y∈Y f θ k−r (z t−r , y)</formula><p>for all {r|1 ≤ r &lt; t − 1}. Although it is not theoretically justified, it empirically runs as a stochastic version of DAGGER <ref type="bibr" target="#b28">(Ross et al., 2011)</ref> or SEARN <ref type="bibr" target="#b7">(Daumé III et al., 2009</ref>) with the speed benefit of online learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learning algorithm</head><formula xml:id="formula_5">function LEARN(α, β, λ 1 , λ 2 , m, n, θ k ) while ¬ stop do Select a random sentence (x, y) for t = 1 to T do u=UPDATE(α, β, λ 1 , λ 2 , m, n, θ k ) ˆ y t = argmax y∈Y f u (z t , y) ˜ y = argmax y̸ =yt f u (z t , , y) if f u (z t , y t ) − f u (z t , ˜ y) &lt; 1 then g = ∂ u ℓ(z t , y t , ˜ y) ▷ Subgradient For all i ∈ I compute σ i = (√ n i + g 2 i − √ n i ) /α i m i ← m i + g i − σ i u i n i ← n i + g 2 i end if k ← k + 1 end for end while return θ k end function function UPDATE(α, β, λ 1 , λ 2 , m, n, θ k ) for i ∈ I do θ k i =    0 if |m i | ≤ λ 1 −m i + sgn(m i )λ 1 (βiλ2+ √ n i) /α i +λ 2 otherwise u i ← θ k i if acceleration then u i ← θ k i + k k+3 ( θ k i − θ k−1 i ) end if end for for i ̸ ∈ I do u i ← θ k i ← θ k−1 i ▷ Leaving all θ for inactive i unchanged end for return u end function</formula><p>Algorithm 1 summarizes our training process where ℓ(z t , y t , ˜ y) := max(0, 1 − f θ (z t , y) + f θ (z t , ˜ y)) is the multi-class hinge loss <ref type="bibr" target="#b6">(Crammer and Singer, 2001</ref>). I in Algorithm 1 is a set of parameter indexes that correspond to the non-zero features, so the update is sparse for sparse fea- tures. In addition, for the parameter update of the neural networks, we also use an accelerated prox- imal method <ref type="bibr" target="#b25">(Parikh and Boyd, 2013)</ref>, which is considered as a variant of the momentum meth- ods ( <ref type="bibr" target="#b38">Sutskever et al., 2013)</ref>. Although u and θ are the same when the acceleration is not used, u in Algorithm 1 is an extrapolation step in the accel- erated method. Although we do not focus on the learning algorithm in this work, the algorithm con- verges quite quickly and the speed is important be- cause the neural network extension described later requires a hyper-parameter search which is com- putationally demanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus-wide Information</head><p>Since typical discrete features indicate only the occurrence in a local context and do not convey corpus-wide statistics, we studied four kinds of continuous features for POS tagging to represent the corpus-wide information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word embeddings</head><p>Word embeddings, or distributed word represen- tations, embed the words into a low-dimensional continuous space. Most of the neural network ap- plications for NLP use word embeddings <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b32">Socher et al., 2011;</ref><ref type="bibr" target="#b45">Zhila et al., 2013;</ref><ref type="bibr" target="#b33">Socher et al., 2013)</ref>, and even for linear models, <ref type="bibr" target="#b42">Turian et al. (2010)</ref> highlights the benefit of word embeddings on sequential labeling tasks.</p><p>In particular, in our experiments, we used two recently proposed algorithms, word2vec (Mikolov et al., 2013) and glove ( <ref type="bibr" target="#b26">Pennington et al., 2014</ref>), which are simple and scalable, although our method could use other word embeddings. Word2vec trains the word embeddings to pre- dict the words surrounding each word, and glove trains the word embeddings to predict the loga- rithmic count of the surrounding words of each word. Thus, these embeddings can be seen as the distributed versions of the distributional fea- tures since the word vectors compactly represent the distribution of the context in which a word ap- pears. We normalized the word embeddings to unit length and used the average vector of training vocabulary for the unknown tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">POS tag distribution</head><p>In a way similar to <ref type="bibr" target="#b30">Schmid (1994)</ref>, we use POS tag distribution over a training corpus. Each word is represented by a vector of length |Y | in which the y-th element is the conditional probabilities with which that word gets the y-th POS tag. We also use the POS tag distributions of the affixes and spelling binary features used in <ref type="bibr" target="#b4">Choi and Palmer (2012)</ref>. We cite the definitions of these features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Affix:</head><p>c :1 , c :2 , c :3 , c n: , c n−1: , c n−2: , c n−3: where c * is a character string in a word. For example c :2 is the prefix of length two of a word and c n−1: is the suffix of length two of a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Spelling:</head><p>initial uppercase, all upper- case, all lowercase, contains 1/2+ capi- tal(s) not at the beginning, contains a (pe- riod/number/hyphen).</p><p>The probabilities for a feature b is estimated with additive smoothing as</p><formula xml:id="formula_6">P (y|b) = C(b, y) + 1 C(b) + |Y | ,<label>(2)</label></formula><p>where C(b) and C(b, y) are the counts of b and co-occurrences of b and y, respectively. In addi- tion, an extra dimension for sentence boundaries is added to the vector for word-forms. In total, the POS tag distributions for each word are encoded by a vector of dimension |Y |+1+|Y |×14 (|Y | for lowercase simplified word-forms, 1 for sentence boundaries, |Y | × 7 for affixes, and |Y | × 7 for spellings).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Supertag distribution</head><p>We also use the distribution of supertags for de- pendency parsing. Supertags are lexical templates which are extracted from the syntactic dependency structures and suppertagging is often used for the pre-processing of a parsing task. Since the su- pertags encode rich syntactic information, we ex- pect the supertag distribution of a word to also provide clues for the POS tagging. We used two types of supertags: One is the dependency rela- tion label of the head of the word and the other is that of the dependents of the word. Following <ref type="bibr" target="#b24">Ouchi et al. (2014)</ref>, we added the relative posi- tion, left (L) or right (R), to the supertags. For example, a word has its dependents in the left di- rection with a label "nn" and in the right direc- tion with a label "amod", so its supertag set for dependents is {"nn/L", "amod/R"}. A special su- pertag "NO-CHILD" is used for a word that has no dependent. Note that, although the Model 2 su- pertag set of <ref type="bibr" target="#b24">Ouchi et al. (2014)</ref> is defined as the combination of head and dependent tags, we used them separately. The feature values for each word are defined in the same way as Equation 2 in Sec- tion 3.2. Since a word can have more than one dependent, the dependent supertag features are no longer multinomial distributions but we used them in that way. Note that, since the feature values are calculated using the tree annotations from training set, our tagger does not require any dependency parser at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Context word distribution</head><p>This is the simplest distributional features in which each word is represented by the distribu- tions of its left and right neighbors. Although the context word distribution is similar to word em- beddings, we believe they complement each other, as reported by <ref type="bibr" target="#b17">Levy and Goldberg (2014)</ref>. Fol- lowing <ref type="bibr" target="#b31">Schnabel and Schütze (2014)</ref>, we restricted the set of indicator words to the 500 most frequent words in the corpus, and used two special feature entries: One is the marginal probability of the non- indicator words and the other is the probabilities of neighboring sentence boundaries. The condi- tional probabilities for left and right neighbors are estimated in the same way as Equation 2 in Sec- tion 3.2, and there are a total of 1, 004 dimensions of this feature for a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Networks</head><p>The non-linearity of the discrete features has been exploited in many NLP tasks, since the simple conjunction of the discrete features is intuitive and the thresholding of these combinatorial features by their feature counts effectively suppresses the combinatorial increase of the parameters. In contrast, it is not easy to manually tune the non-linearity of the continuous features. For ex- ample, it is not intuitive to design the conjunc- tion features of two kinds of word embeddings, word2vec and glove. Although kernel methods have been used to incorporate non-linearity in prior research, they are rarely used now because their tagging speed is too slow ( <ref type="bibr" target="#b9">Giménez and M` arquez, 2003)</ref>. Our solution is to introduce feed-forward neural networks to capture the non- linearity of the corpus-wide information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hybrid model</head><p>We designed our tagger as a hybrid of a linear model and a non-linear model. <ref type="bibr" target="#b43">Wang and Manning (2013)</ref> reported that a neural network us- ing both sparse discrete features and dense (low- Formally, the scoring function (1) in Section 2 is defined as the composite function of two terms: f (z, y) := f linear (z, y) + f nn (z, y). The first f linear is the linear model and the second f nn is a neu- ral network. Since this is a linear combination of two functions, the subgradient of the loss function required for Algorithm 1 is also the linear com- bination of subgradients of two functions, which means</p><formula xml:id="formula_7">∂ θ ℓ(z t , y t , ˜ y) = ∂ θ f linear (z t , ˜ y) + ∂ θ f nn (z t , ˜ y) − ∂ θ f linear (z t , y t ) − ∂ θ f nn (z t , y t )</formula><p>if f θ (z t , y t ) − f θ (z t , ˜ y) &lt; 0. First, the linear model can be defined as</p><formula xml:id="formula_8">f linear (z, y) := θ d · ϕ d (z, y),</formula><p>where ϕ d (z, y) is a feature mapping for the dis- crete part of z and a POS tag, and θ d is the cor- responding parameter vector. Since this is a lin- ear model, the gradient of this function is simply</p><formula xml:id="formula_9">∂ θ f linear (z, y) = ϕ d (z, y).</formula><p>Second, each hidden layer of our neural net- works non-linearly transforms an input vector h ′ into an output vector h and we can say h ′ is the continuous part of z at the first layer. Let h L be a hidden activation of the top layer, which is the non-linear transformation of the continuous part of z. The output layer of the neural network is defined as</p><formula xml:id="formula_10">f nn (z, y) := θ o · ϕ o (h L , y),</formula><note type="other">where ϕ o (h, y) is a feature mapping for the hidden variables and a POS tag, and θ o is the correspond- ing parameter vector.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Activation functions</head><p>The hidden variables h are computed by the re- cursive application of a non-linear activation func- tion. Since new styles of the activation functions were recently proposed, we review several acti- vation functions here. Let v ∈ R |V | be the in- put of an activation function and each element is v j = θ nn,j · h ′ + θ bias,j , where θ nn,j is the param- eter vector for v j and θ bias,j is the bias parameter for v j . We also assume v is divided into groups of size G, and denote the j-th element of the i-th group as {v ij |1 ≤ i ≤ |V |/G ∧ 1 ≤ j ≤ G}. We studied three activation functions:</p><p>1. Rectified linear units (ReLUs) <ref type="bibr">(Nair and Hinton, 2010)</ref>:</p><formula xml:id="formula_11">h j = max(0, v j ) for all {j|1 ≤ j ≤ |V |}.</formula><p>Note that a subgradient of ReLUs is</p><formula xml:id="formula_12">∂h j ∂θ = { ∂v j ∂θ if v j &gt; 0 0</formula><p>otherwise.</p><p>2. Maxout networks (MAXOUT) ( <ref type="bibr" target="#b11">Goodfellow et al., 2013</ref>):</p><formula xml:id="formula_13">h i = max 1≤j≤G v ij for all {i|1 ≤ i ≤ |V | G }.</formula><p>Note that a subgradient of MAXOUT is</p><formula xml:id="formula_14">∂h i ∂θ = ∂v i ˆ j ∂θ , wherê j = argmax 1≤j≤G v ij 3. Normalized L p -pooling (L p ) (Gulcehre et al.,<label>2014)</label></formula><p>:</p><formula xml:id="formula_15">h i =   1 G G ∑ j=1 |v ij | p   1 p for all {i|1 ≤ i ≤ |V | G }.</formula><p>Note that a subgradient of L p is</p><formula xml:id="formula_16">∂h i ∂θ = G ∑ j=1 ∂v ij ∂θ v ij |v ij | p−2 G   1 G G ∑ j=1 |v i,j | p   1 p −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>942</head><p>The activation inputs for each predefined group, {v 1j , . . . , v Gj }, are aggregated by a non-linear function in MAXOUT or L p activation functions, while each input is transformed into a correspond- ing hidden variable in the ReLUs. When the number of parameters required for these activation functions is the same, the number of output vari- ables h for MAXOUT and L p is one-G-th smaller than that for ReLUs. <ref type="bibr" target="#b3">Boureau et al. (2010)</ref> show pooling operations theoretically reduce the vari- ance of hidden activations, and our experimental results also show MAXOUT and L p perform bet- ter than the ReLUs with the same number of pa- rameters. Note that MAXOUT is a special case of unnormalized L p pooling when p = ∞ and v j &gt; 0 for all j ( ). <ref type="figure" target="#fig_0">Figure 1</ref> summarizes the proposed architecture with a sin- gle hidden layer and a pooling activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-parameter search</head><p>Finally, the subgradients of the neural network, f nn (z, y), can be computed through standard back-propagation algorithms and we can apply them in Algorithm 1. However, many of the hyper- parameters have to be determined for the training of the neural networks, and two stages of random hyper-parameter searches <ref type="bibr" target="#b1">(Bergstra and Bengio, 2012)</ref> are used in our experiments. Note that the parameters are grouped into three sets, θ d , θ o , θ nn , and the same values for λ 1 , λ 2 , α, β are used for each parameter set.</p><p>In the first stage, we randomly select 32 combi- nations of λ 2 for f nn , λ 1 , λ 2 for f linear , the epoch to start the L1/L2 regularizations, and the on and off the acceleration in Algorithm 1. Here are the candidates of three hyper-parameters:</p><formula xml:id="formula_17">1. λ 1 :</formula><p>0 for the update of f nn and {0, 10 −8 , 10 −6 , 10 −4 , 10 −2 , 1} for the update of f linear ;</p><p>2. λ 2 : {0.1, 0.5, 1, 5, 10} for the update of f nn and {1, 5, 10, 50, 100} for the update of f linear ; and 3. Epoch to start the regularizations: {0, 1, 2}.</p><p>In the second stage with each hyper-parameter combination above, we select 8 random combina- tions of α, β for both f linear and f nn and initial pa- rameter ranges R for f nn . Here are the candidates of the three hyper-parameters:</p><p>1. α: {0.01, 0.05, 0.1, 0.5, 1, 5};</p><p>Data Set #Sent. #Tokens <ref type="table" target="#tab_2">#Unknown  Training  38,219 912,344  0  Development  5,527 131,768  4,467  Testing  5,462 129,654  3,649   Table 1</ref>: Data set splits for PTB.</p><p>2. β: {0.5, 1, 5}; The values of θ for f nn are uniformly sampled in the range of the randomly selected R. Note that, according to <ref type="bibr" target="#b12">Goodfellow et al. (2014)</ref>, the biases θ bias are initialized as 0 for MAXOUT and L p , and uniformly sampled from a range R + max(R), i.e., always initialized with non-negative values. The best combination for the development set is chosen after training that uses random 20% of the training set at the second stage, and Algorithm 1 is terminated when the all token accuracy of the development data has been declining for 5 epochs at the first stage. In other words, 32 × 8 random combinations of α, β, and θ for f nn were tested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Our experiments were mainly performed using the Wall Street Journal from Penn Treebank (PTB) ( <ref type="bibr" target="#b20">Marcus et al., 1993)</ref>. We used tagged sen- tences from the parse trees (Toutanova et al., 2003) and followed the standard approach of splitting the PTB, using sections 0-18 for training, section 19- 21 for development, and section 22-24 for testing <ref type="table">(Table 1</ref>). In addition, we used the CoNLL2009 data sets with the training, development, and test splits used in the shared task ) for better comparison with a joint model of POS tagging and dependency parsing (Bohnet and Nivre, 2012). Our baseline tagger was trained by Algorithm 1. As discrete features for our tagger, we used the same binary feature set as <ref type="bibr" target="#b4">Choi and Palmer (2012)</ref> which is composed of (a) 1, 2, 3-grams of the surface word-forms and their predicted/dominated POS tags, (b) the prefixes and suffixes of the words, and (c) the spelling types of the words. In the same way as <ref type="bibr" target="#b4">Choi and Palmer (2012)</ref>, we used lowercase simplified word-forms which appeared at least 3 times.</p><p>In addition to their binary features, we used con- tinuous features which are the concatenation of the corpus-wide features in a context window. The window of size w = 2s + 1 is the local context centered around x t : x t−s , · · · , x t , · · · , x t+s . The experimental settings of each feature described in Section 3 are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word embeddings</head><p>We used two word vectors: 300-dimensional vectors that were learned by word2vec using a part of the Google News dataset (around 100 billion tokens) 1 , and 300-dimensional vectors that were learned by glove using a part of the Common Crawl dataset (840 bil- lion tokens) 2 . For sentence boundaries, we use the vector of the special entry "&lt;/s&gt;" for the word2vec embeddings and the zero vec- tor for the glove embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS tag distribution</head><p>The counts are calculated using training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supertag distribution</head><p>In the experiments on PTB, we used the Stan- ford parser v2.0.4 3 to convert from phrase structures to dependency structures so that the dependency relation labels of the Stan- ford dependencies are used. The size of the supertag set is 85 for both heads and depen- dents in our experiments. In the experiments on CoNLL2009, the dependency structures and labels defined in CoNLL2009 are used and the size of supertag set is 99 for both heads and dependents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context word distribution</head><p>To count the neighboring words in our exper- iments, we used sections 0-18 of the Wall Street Journal and all of the Brown corpus from Penn Treebank ( <ref type="bibr" target="#b20">Marcus et al., 1993</ref>).</p><p>Since the training of the neural networks is com- putationally demanding, first, we trained the lin- ear classifiers using Algorithm 1 to select the best window sizes for each corpus-wide information of Section 3. Then the best window size setting for the development set of PTB was used for train- ing the neural networks described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Window size</head><p>Accuracy (%) # w2v glv pos stg cw All Unk <ref type="table" target="#tab_2">.  1  - - - - -97.15 86.81  2  3  - - - -97.36 88.96  3  - 3  - - -97.34 89.55  4  3  3  - - -97.40 90.44  5  3  3  3  -1</ref>   <ref type="table">Table 2</ref>: Feature and window size selection: de- velopment accuracies of all tokens (All) and un- known tokens (Unk.) of linear models trained on PTB (w2v: word2vec; glv: glove; pos: POS tag distribution; stg: supertag distribution; cw: con- text word distribution).</p><p>We fixed the group size at 8 for MAXOUT and L p , and the number of hidden variables was cho- sen from {32, 48} for MAXOUT and L p and from {32, 64, 128, 256, 384} for ReLUs according to all token accuracy on the development data of PTB. We report the POS tagging accuracy for both all of the tokens and only for the unknown tokens that do not appear in the training set. <ref type="table">Table 2</ref> shows the accuracies of the linear models on PTB with different window sizes for the con- tinuous features. The window sizes of the word embeddings (word2vec and glove) in Section 3.1, POS tag distributions in Section 3.2, supertag dis- tributions in Section 3.3, and context word distri- butions in Section 3.4 are shown in the columns of w2v, glv, pos, stg, and cw, respectively. Note that "-" denotes the corresponding feature was not used at all and the first row with all "-" denotes the results only using the original binary features from Choi and Palmer (2012). The window sizes in Ta- ble 2 are chosen mainly to investigate the effect of the word2vec embeddings, glove embeddings, and supertag distributions, since they had not pre- viously been used for POS tagging. The additions of the word embeddings improve all token accuracy by about 0.2 points accord- ing to the results shown in Nos. 1, 2, 3. Al- though both word embeddings improved the ac- curacy of the unknown tokens, the gain of the glove embeddings (No. 3) is larger than that of the   The addition of the POS tag distributions and the context word distributions improves all token accuracy <ref type="bibr">(Nos. 5, 8)</ref>. The comparison between the results with stag="-" (Nos. 5, 8) and stag = {1, 3} (Nos. 6, 7, 9) indicates the minor but consistent improvement by using the supertag distribution features in Section 3.3. Finally, the 7th window- size setting in <ref type="table">Table 2</ref> achieves the best all token accuracy among the linear models, so we chose this setting for the experiments with the neural net- works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><note type="other">Neural Network Settings Development Set Test Set # Activation functions #Hidden Group</note><p>In <ref type="table" target="#tab_2">Table 3</ref>, we compare the different settings of the neural networks with a single hidden layer 4 on the development set and test set from PTB. Since it shows the best accuracy for all tokens on the development set, we refer to L p (p = 2) with 48 hidden variables and the group size of 8 (No. 3 in <ref type="table" target="#tab_2">Table 3</ref>) as our representative tagger and denote it as L p (p = 2) in the rest of discussion. In <ref type="table" target="#tab_3">Table 4a</ref>, we compare our result with the pre- viously reported results and we see that our tagger outperforms the current state-of-the-art systems on PTB for the accuracies of all tokens and unknown tokens.</p><p>In addition, since our tagger was trained us- ing the dependency tree annotations as described in Section 3.3, we compare it with the results of Bohnet and Nivre (2012) which is also trained using both POS tag and dependency annotations. Although their focus is on the dependency pars-  ing, they report state-of-the art POS accuracies for many languages. Note that Bohnet and Nivre (2012) also used external resources. <ref type="table" target="#tab_3">Table 4b</ref> gives the results for CoNLL2009 data set 6 . Our tagger outperform Bohnet and Nivre (2012), so we believe this is the highest POS accuracy ever re- ported for a tagger trained on this data set.</p><p>Finally, to visualize the learned representations, we applied principal components analysis (PCA) to the hidden activations h L of the first 10, 000 to- kens of the development set from PTB. We also performed PCA to the raw continuous inputs of the same data set. <ref type="figure" target="#fig_4">Figure 2</ref> shows the data plots for all the combinations among the first four prin- cipal components. We plots only the verb tokens to make the plots easier to see. <ref type="figure" target="#fig_4">Figures 2a and  2b</ref> show the PCA results of the raw features and the hidden activations of L p (p = 2), respectively. Compared to <ref type="figure" target="#fig_4">Figure 2a</ref>, the tokens with the same POS tag are more clearly clustered in <ref type="figure" target="#fig_4">Figure 2b</ref>. This suggests the neural network learned the good representations for POS tagging and these hidden activations can be used as the input of the succeed- ing processes, such as parsing. <ref type="bibr">6</ref> The accuracies of our tagger on the development set of CoNLL2009 data are 97.76% for all tokens and 93.42% for unknown tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There is some old work on the POS tagging by neural networks. <ref type="bibr" target="#b23">Nakamura et al. (1990)</ref> proposed a neural tagger that predicts the POS tag using a previous POS predictions. <ref type="bibr" target="#b30">Schmid (1994)</ref> is most similar to our work. The inputs of his neural net- work are the POS tag distributions of a word and its suffix in a context window, and he reports a 2% improvement over a regular hidden Markov model. However, his tagger did not use the other kinds of corpus-wide information as we used.</p><p>Most of the recent studies on POS tagging use linear models <ref type="bibr" target="#b39">(Suzuki and Isozaki, 2008;</ref><ref type="bibr" target="#b35">Spoustová et al., 2009</ref>) or other non-linear models, such as k-nearest neighbor (kNN) <ref type="bibr">(Søgaard, 2011)</ref>. One trend in these studies is model combinations. <ref type="bibr" target="#b39">Suzuki and Isozaki (2008)</ref> combined generative and discriminative models, <ref type="bibr" target="#b35">Spoustová et al. (2009)</ref> used the combination of three taggers to gener- ate automatically annotated corpus, and <ref type="bibr">Søgaard (2011)</ref> used the outputs of a supervised tagger and an unsupervised tagger as the feature space of the kNN. Our work also follows this trend since neural networks can be considered as non-linear integra- tion of several linear classifiers.</p><p>Apart from POS tagging, some previous studies in parsing used the discretization method to handle the combination of continuous features. Bohnet and Nivre (2012) binned the difference of two con-tinuous features in discrete steps of a predefined small interval. <ref type="bibr" target="#b0">Bansal et al. (2014)</ref> used the con- junction of discretized features and studied two discretization methods: One is the binning of real values into discrete steps and the other is a hard clustering of continuous feature vectors. It is not easy to determine the optimal intervals for the bin- ning method, and the clustering method is unsu- pervised so that the clusters are not guaranteed for good representations of the target tasks.</p><p>To capture rich syntactic information for Chi- nese POS tagging, <ref type="bibr" target="#b36">Sun and Uszkoreit (2012)</ref> used the ensemble model of both a POS tagger and a constituency parser. <ref type="bibr" target="#b37">Sun et al. (2013)</ref> improved the efficiency of <ref type="bibr" target="#b36">Sun and Uszkoreit (2012)</ref> in which a single tagging model is trained using au- tomatically annotated corpus generated by the en- semble tagger. Although the supertag distribution feature in Section 3.3 is a simple way to incor- porate syntactic information, automatically parsed large corpora may make the estimate of the su- pertag distributions more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We are studying a neural network approach to han- dle the non-linear interaction among corpus-wide statistics. For POS tagging, we used word em- beddings, POS tag distributions, supertag distribu- tions, and context word distributions in a context window. These features are beneficial, even for linear classifiers, but the neural networks leverage these features for improving tagging accuracies. Our tagger with Maxout networks ( <ref type="bibr" target="#b11">Goodfellow et al., 2013)</ref> or L p -pooling ( <ref type="bibr" target="#b13">Gulcehre et al., 2014)</ref> show the state-of-the-art results on two English benchmark sets.</p><p>Our empirical results suggest further opportu- nities to investigate continuous features not only for POS tagging but also for other NLP tasks. An obvious use case for continuous features is the N-best outputs with confidence values, which were predicted by the previous process in a NLP pipeline, such as the POS tags used for syntactic parsing. Another interesting extension is the use of on-the-fly features which reflect previous network states, although the neural networks in our current work do not refer to the prediction history. Recur- rent neural networks (RNNs) may be a solution to represent the prediction history in a compact way, and <ref type="bibr">Mesnil et al. (2013)</ref> reported that RNNs out- perform conditional random fields (CRFs) on a se- quential labeling task. They also show the superi- ority of bi-directional RNNs on their task, so the bi-directional RNNs may also be effective on the POS tagging, since bi-directional inferences were also used in earlier work <ref type="bibr" target="#b41">(Tsuruoka and Tsujii, 2005)</ref>.</p><p>It has a clear benefit over kernel methods in that the test-time computational cost of neural net- works is independent from training data. How- ever, although the test-time speed of original ker- nel methods is proportional to the number of train- ing data, recent development of kernel approxima- tion techniques achieve significant speed improve- ments ( <ref type="bibr" target="#b16">Le et al., 2013;</ref><ref type="bibr" target="#b27">Pham and Pagh, 2013)</ref>. Since this work shows the non-linearity of contin- uous features should be exploited, those approxi- mated kernel methods may also improve the tag- ging accuracies without sacrifice tagging speed.</p><p>Independent from our work, <ref type="bibr" target="#b18">Ma et al. (2014)</ref> and <ref type="bibr" target="#b29">Santos and Zadrozny (2014)</ref> also recently pro- posed neural network approaches for POS tagging. <ref type="bibr" target="#b18">Ma et al. (2014)</ref>'s approach is similar to our ap- proach, with a combination of a linear model and a neural network, although a direct comparison is not easy since their focus is the Web domain adap- tation of POS tagging. Remarkably, they report n- gram embeddings are better than single word em- beddings. <ref type="bibr" target="#b29">Santos and Zadrozny (2014)</ref> proposed character-level embedding to capture the morpho- logical and shape information for POS tagging. Although the reported accuracy (97.32%) on PTB data is lower than state of the art results, their ap- proach is promising for morphologically rich lan- guages. We may study the integration of these em- beddings into our approach as future work. <ref type="bibr">Grégoire Mesnil, Xiaodong He, Li Deng, and Yoshua Bengio. 2013</ref>. Investigation of recurrent-neural- network architectures and learning methods for spo- ken language understanding. <ref type="table">In Proceedings of An- nual Conference of the International Speech Com- munication Association (INTERSPEECH),</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A hybrid architecture of a linear model and a neural network with a pooling activation function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Neural networks with the MAXOUT and L p (Nos. 3, 4, 5) significantly outperform the best lin- ear model (No. 1) 5 , but the accuracy of the Re- LUs (No. 2) was similar to that of the best lin- ear model. According to these results, we argue that the activation function selection is important, although conventional research in NLP has used only a single activation function. It took roughly 7 times as long to learn the hybrid models than the linear model (No. 1). "L p (p = 2) (w/o linear part)" (No. 6) shows the result for a L p (p = 2) model which does not include the linear model f linear for the binary features. Comparing the test results of No. 6 with that of No. 3, the proposed hybrid architecture of a linear model and a neural network enjoys the benefits of both models. Note that No. 6's accuracies of the unknown tokens are relatively competitive, and this may be because the continuous features for the neural network do not include word surfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scatter plots of verbs for all combinations between the first four principal components of the raw features and the activation of hidden variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Development and test accuracies of all tokens and unknown tokens (%) on PTB. 

Tagger 
All Unk. 
Manning (2011) 97.32 90.79 
Søgaard (2011) 97.50 
N/A 
L p (p = 2) 
97.51 91.64 

(a) Test accuracies on PTB 

Tagger 
All Unk. 
Bohnet and Nivre (2012) 97.84 
N/A 
L p (p = 2) 
98.02 92.01 

(b) Test accuracies on CoNLL2009 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test accuracies of all tokens and unknown tokens (%) comparing with the previously reported 
results 

word2vec (No. 2). The reason for this difference 
in the two embeddings may be because the train-
ing data for the glove vectors is 8 times larger than 
that for the word2vec vectors. The usage of the 
two word embeddings shows further improvement 
in the tagging accuracy over single word embed-
dings (No. 4). 
</table></figure>

			<note place="foot" n="1"> The pre-trained vectors are available at https:// code.google.com/p/word2vec 2 The pre-trained vectors are available at http://nlp. stanford.edu/projects/glove/ 3 http://nlp.stanford.edu/software/ lex-parser.shtml</note>

			<note place="foot" n="4"> We leave the investigation of deeper neural networks as future work. 5 For significance tests, we have used the Wilcoxon matched-pairs signed-rank test at the 95% confidence level dividing the data into 100 data pairs.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL). The Association for Computer Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL). The Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust part-of-speech tagging using dynamic model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="363" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Learning Theory (COLT)</title>
		<meeting>the Conference on Learning Theory (COLT)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="257" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and accurate part-of-speech tagging: The svm approach revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Recent Advances in Natural Language Processing (RANLP)</title>
		<meeting>Recent Advances in Natural Language Processing (RANLP)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics (COLING)</title>
		<meeting>International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical investigation of catastrophic forgeting in gradientbased neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)</title>
		<meeting>the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Straňák</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributional representations for handling sparsity in supervised sequence-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL/IJCNLP)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL/IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="495" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fastfood-computing Hilbert space expansions in loglinear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamás</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tagging the Web: Building a robust web tagger with neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL). The Association for Computer Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL). The Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Intelligent Text Processing and Computational Linguistics (CICLing)</title>
		<meeting>Conference on Intelligent Text Processing and Computational Linguistics (CICLing)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnar</forename><surname>Mar Hrafnkelsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Kubica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Follow-the-regularizedleader and mirror descent: Equivalence theorems and L1 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="525" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural network approach to word category prediction for English texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masami</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuteru</forename><surname>Maruyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics (COLING)</title>
		<meeting>International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="213" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving dependency parsers with supertags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="154" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="123" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GloVe: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics (COLING)</title>
		<meeting>International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="172" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FLORS: Fast and simple domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised condensed nearest neighbor for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="48" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised training for the averaged perceptron pos tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Drahomíra Johanka Spoustová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spousta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>Conference of the European Chapter of the Association for Computational Linguistics (EACL)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="763" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Capturing paradigmatic and syntagmatic lexical relations: Towards accurate Chinese part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Capturing long-distance dependencies in sequence models: A case study of Chinese part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Feature-rich partof-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bidirectional inference with the easiest-first strategy for tagging sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP)</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics, the Conference (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Effect of non-linear deep architecture in sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Combining heterogeneous models for measuring relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alisa</forename><surname>Zhila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1000" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
