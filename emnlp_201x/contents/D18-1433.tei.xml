<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Background Knowledge into Video Description Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Carolina at Chapel Hill</orgName>
								<orgName type="institution">University of North</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<email>sfchang@ee.columbia.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
							<email>clare.r.voss.civ@mail.mil</email>
							<affiliation key="aff3">
								<orgName type="laboratory">US Army Research Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Background Knowledge into Video Description Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3992" to="4001"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3992</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most previous efforts toward video caption-ing focus on generating generic descriptions, such as, &quot;A man is talking.&quot; We collect a news video dataset to generate enriched descriptions that include important background knowledge, such as named entities and related events, which allows the user to fully understand the video content. We develop an approach that uses video meta-data to retrieve topically related news documents for a video and extracts the events and named entities from these documents. Then, given the video as well as the extracted events and entities, we generate a description using a Knowledge-aware Video Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model&apos;s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video captioning is a challenging task that seeks to automatically generate a natural language descrip- tion of the content of a video. Many video cap- tioning efforts focus on learning video representa- tions that model the spatial and temporal dynam- ics of the videos ( <ref type="bibr" target="#b36">Venugopalan et al., 2016;</ref><ref type="bibr" target="#b47">Yu et al., 2017)</ref>. Although the lan- guage generation component within this task is of great importance, less work has been done to en- hance the contextual knowledge conveyed by the descriptions. The descriptions generated by pre- vious methods tend to be "generic", describing only what is evidently visible and lacking specific knowledge, like named entities and event partic- ipants, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. In many situa- tions, however, generic descriptions are uninfor- mative as they do not provide contextual knowl- edge. For example, in <ref type="figure" target="#fig_0">Figure 1b</ref>, details such as who is speaking or why they are speaking are im- perative to truly understanding the video, since contextual knowledge gives the surrounding cir- cumstances or cause of the depicted events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a)</head><p>Description ( <ref type="bibr" target="#b2">Chen and Dolan, 2011</ref>): A man is talking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b)</head><p>Human Description: Senior army officer and Zimbabwe Defence Forces' spokesperson, Major General S. B. Moyo, assures the public that President Robert Mugabe and his family are safe and denies that the military is staging a coup. To address this problem, we collect a news video dataset, where each video is accompanied by meta-data (e.g., tags and date) and a natural lan- guage description of the content in, and/or context around, the video. We create an approach to this task that is motivated by two observations. First, the video content alone is insufficient to generate the description. Named entities or spe- cific events are necessary to identify the partici- pants, location, and/or cause of the video content. Although knowledge could potentially be mined from visual evidence (e.g., recognizing the loca- tion), training such a system is exceedingly diffi-cult ( <ref type="bibr" target="#b33">Tran et al., 2016</ref>). Further, not all the knowl- edge necessary for the description may appear in the video. In <ref type="figure">Figure 2a</ref>, the video depicts much of the description content, but knowledge of the speaker ("Carles Puigdemont") is unavailable if limited to the visual evidence because the speaker never appears in the video, making it intractable to incorporate this knowledge into the description.</p><p>Second, one may use a video's meta-data to re- trieve topically related news documents that con- tain the named entities or events that appear in the video's description, but these may not be specific to the video content. For example, in <ref type="figure">Figure 2b</ref>, the video discusses the "heightened security" and does not depict the arrest directly. Topically re- lated news documents capture background knowl- edge about the attack that led to the "heightened security" as well as the arrest, but they may not describe the actual video content, which displays some of the increased security measures.</p><p>Thus, we propose to retrieve topically related news documents from which we seek to extract named entities ( <ref type="bibr" target="#b21">Pan et al., 2017</ref>) and events ( <ref type="bibr" target="#b13">Li et al., 2013</ref>) likely relevant to the video. We then propose to use this knowledge in the generation process through an entity pointer network, which learns to dynamically incorporate extracted enti- ties into the description, and through a new knowl- edge gate, which conditions the generator on the extracted event and entity types. We include the video content in the generation by learning video representations using a spatio-temporal hierarchi- cal attention that spatially attends to regions of each frame and temporally attends to different frames. We call the combination of these genera- tion components the Knowledge-aware Video De- scription (KaVD) network. The contributions of this paper are as follows:</p><p>• We create a knowledge-rich video captioning dataset, which can serve as a new benchmark for future work.</p><p>• We propose a new Knowledge-aware Video Description network that can generate de- scriptions using the video and background knowledge mined from topically related doc- uments.</p><p>• We present a knowledge reconstruction based metric, using entity and event F1 scores, to evaluate the correctness of the knowledge conveyed in the generated descriptions. Figure 2: Examples from the news video dataset (video, meta-data, and description) with some re- trieved topically related documents. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Figure 3 shows our overall approach. We first re- trieve topically related news documents using tags from the video meta-data. Next, we apply entity discovery and linking as well as event extraction methods to the documents, which yields a set of entities and events relevant to the video. We rep- resent this background knowledge in two ways: 1) we encode the entities through entity embeddings and 2) we encode the event and entity typing in- formation into a knowledge gate vector, which is a one-hot vector where each entry represents an entity or event type. Finally, with the video and these representations of the background knowl- edge, we employ our KaVD network, an encoder- decoder ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>) style model, to generate the description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Retrieval and Knowledge Extraction</head><p>We gather topically related news documents as a source of background knowledge using the video meta-data. For each video, we use the correspond- ing tags to perform a keyword search on docu- ments from a number of popular news outlet web-  sites. <ref type="bibr">3</ref> We filter these documents by the date as- sociated with video, only keeping documents that are written within d days before and after the video upload date. <ref type="bibr">4</ref> The keyword search gathers doc- uments that are at least somewhat topically rele- vant and filtering by date increases the likelihood that the documents reference the specific events and entities of the video, since the occurrences of entity and event mentions across news documents tend to be temporally correlated. We retrieve an average of 3.1 articles per video and find that on average 68.8% of the event types and 70.6% of the entities in the ground truth description also appear in corresponding news articles. In <ref type="figure" target="#fig_1">Figure 3</ref>, the retrieved background documents include the en- tity "Mugabe" and the event "detained", which are relevant to the video description. We apply a high-performing, publicly avail- able entity discovery and linking system <ref type="bibr" target="#b21">(Pan et al., 2017</ref>) to extract named entities and their types. This system is able to discover en- tities and link them to rich knowledge bases that provide fine-grained types that we can ex- ploit to better discern between entities in the news documents (e.g., "President" versus "Military Officer"). <ref type="bibr">5</ref> Additionally, we use a high-performing event extraction system ( <ref type="bibr" target="#b13">Li et al., 2013</ref>) to extract events and their arguments. For example, in <ref type="figure" target="#fig_1">Figure 3</ref>, we get entities "S. B. Moyo", "Zimbabwe", and "Mugabe" with their re- spective types, "Military Officer", "GPE", 3 BBC, CNN, and New York Times. 4 d = 3 in our experiments. <ref type="bibr">5</ref> We only use types that appear in the training data and are within 4 steps from the top of the 7,309 type hierarchy here. and "President". Likewise, we obtain events "coup" and "detained" with their respective types, "Attack" and "Arrest-Jail". The entities and events along with their types provide valuable insight into the context of the video and can bias the decoder to generate the correct event mentions and incorporate the proper entities.</p><p>We encode the entities and events into represen- tations that can be fed to the model. First, we ob- tain an entity embedding, e m , for each entity by averaging the embeddings of the words in the en- tity mention. Second, we encode the entity and event types into a one-hot knowledge gate vector, k 0 . Each element of k 0 corresponds to an event or entity type (e.g., "Arrest-Jail" event type or "President" entity type), so the j th element, k (j) , is 1 if the entity or event type is found in the related documents and 0 otherwise. k 0 serves as the initial knowledge gate vector of the decoder (Section 2.2). The entity embeddings give the model access to semantic representations of the entities, while the knowledge gate vector aids the generation process by providing the model with the event and entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">KaVD Network</head><p>Our model learns video representations using hi- erarchical, or multi-level, attention <ref type="bibr" target="#b26">Qin et al., 2017</ref>). The encoder is com- prised of a spatial attention ( <ref type="bibr">Xu et al., 2015</ref>) and bidirectional Long Short-Term Memory network (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) temporal encoder. The spatial attention allows the model to attend to different locations of each frame <ref type="figure">(Figure 4</ref>), yielding frame representations </p><formula xml:id="formula_0">p gen P v + (1 − p gen )P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Pointer Network</head><p>Figure 4: KaVD Network. At each decoder time step, the model computes p gen to determine whether to emit a vocabulary word or a named entity from the topically related documents.</p><p>that emphasize the most important regions of each frame. The temporal encoder incorporates mo- tion into the frame representations by encoding information from the preceding and subsequent frames ( ). We use a LSTM de- coder, which applies a temporal attention (Bah- danau et al., 2015) to the frame representations at each step. To generate each word, the de- coder computes its hidden state, adjusts this hid- den state with the knowledge gate output at the current time step, and determines the most proba- ble word by utilizing the entity pointer network to decide whether to generate a named entity or vo- cabulary word. Pointer networks are effective at incorporating out-of-vocabulary (OOV) words in output sequences ( <ref type="bibr" target="#b15">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b28">See et al., 2017)</ref>. In previous research, OOV words may appear in the input sequence, in which case they are copied into the output. Analogously, in our approach, named entities can be considered as OOV words that are from a separate set instead of the input sequence. In the following equations, where appropriate, we omit bias terms for brevity.</p><p>Encoder. The input to the encoder is a se- quence of video frames, {F 1 , ..., F N }. First, we extract frame-level features by applying a Con- volutional Neural Network (CNN) ( <ref type="bibr" target="#b12">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b29">Simonyan and Zisserman, 2014;</ref><ref type="bibr" target="#b9">Ioffe and Szegedy, 2015;</ref><ref type="bibr" target="#b30">Szegedy et al., , 2017</ref>) to each frame, F i , and obtaining the response of a convolutional layer, {a i,1 , ..., a i,L }, where a i,l is a D-dimensional representation of the l th location of the i th frame (e.g., the top left box of the first frame in <ref type="figure">Figure 4</ref>). We apply a spatial attention to these location representations, given by</p><formula xml:id="formula_1">α i,l = a space (a i,l ) (1) ξ i,l = softmax (α i,l )<label>(2)</label></formula><formula xml:id="formula_2">z i = L l=1 ξ i,l a i,l<label>(3)</label></formula><p>where a space is a scoring function ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Frame representations {z 1 , ..., z N } are in- put to a bi-directional LSTM, producing tempo- rally encoded frame representations {h 1 , ..., h N }.</p><p>Decoder. The decoder is an attentive LSTM cell with the addition of a knowledge gate and entity pointer network. At each decoder step t, we apply a temporal attention to the frame representations, </p><formula xml:id="formula_3">β t,i = a time (h i , s t−1 ) (4) η t,i = softmax (β t,i )<label>(5)</label></formula><formula xml:id="formula_4">v t = N i=1 η t,i h i<label>(6)</label></formula><note type="other">to s t−1 , v t , and previous word embedding, x t−1 .</note><p>The final decoder hidden state is determined after the knowledge gate computation. The motivation for the knowledge gate is that it biases the model to generate sentences that con- tain specific knowledge relevant to the video and topically related documents, acting as a kind of coverage mechanism ( <ref type="bibr" target="#b35">Tu et al., 2016</ref>). For exam- ple, given the retrieved event types in <ref type="figure" target="#fig_1">Figure 3</ref>, the knowledge gate encourages the decoder to gener- ate the event trigger "coup" due to the presence of the "Attack" event type. Inspired by the gating mechanisms from natural language gener- ation ( <ref type="bibr" target="#b40">Wen et al., 2015;</ref><ref type="bibr" target="#b34">Tran and Nguyen, 2017)</ref>, the knowledge gate, g t , is given by</p><formula xml:id="formula_5">g t = σ (W g,v [x t−1 , v t ] + W g,s ˆ s t ) (7) k t = g t k t−1<label>(8)</label></formula><p>where all W are learned parameters and</p><formula xml:id="formula_6">[x t−1 , v t ]</formula><p>is the concatenation of these two vectors. This gat- ing step determines the amount of the entity and event type features contained in k t−1 to carry to the next step. With the updated k t , we compute the decoder hidden state, s t , as</p><formula xml:id="formula_7">s t = ˆ s t + (o t tanh (W s,k k t ))<label>(9)</label></formula><p>where o t is the output gate of the LSTM and W s,k is a learned parameter. Our next step is to generate the next word. The model needs to produce named entities (e.g., "S. B. Moyo" and "Robert Mugabe") throughout the generation process. These named entities tend to occur rarely if at all in many datasets, including ours. We overcome this issue by using the entity embeddings from the topically related documents as potential entities to incorporate in the descrip- tion. We adopt a soft switch pointer network (See et al., 2017), as our entity pointer network, to per- form the selection of generating words or entities.</p><p>For our entity pointer network to predict the next word, we first predict a vocabulary distribu- tion, P v = ψ (s t , v t ), where ψ(·) is a softmax out- put layer. P v (w) is the probability of generating word w from the decoder vocabulary. Next, we compute an entity context vector, c t , using a soft attention mechanism:</p><formula xml:id="formula_8">γ t,m = a entity (e m , s t , v t ) (10) t,m = softmax (γ t,m )<label>(11)</label></formula><formula xml:id="formula_9">c t = M m=1 t,m e m<label>(12)</label></formula><p>Here, a entity is yet another scoring function. We use the scalars t,m as our entity probability distri- bution, P e , where P e (E m ) = t,m is the probabil- ity of generating entity mention E m . We compute the probability of generating a word from the vo- cabulary, p gen , as</p><formula xml:id="formula_10">p gen = σ(w c c t + w s s t + w x x t−1 + w v v t )<label>(13)</label></formula><p>where all w are learned parameters. Finally, we predict the probability of word w by</p><formula xml:id="formula_11">P (w) = p gen P v (w) + (1 − p gen )P e (w)<label>(14)</label></formula><p>and select the word of maximum probability. In Equation 14, P e (w) is 0 when w is not a named entity. Likewise, P v is 0 when w is an OOV word. For the example in <ref type="figure">Figure 4</ref>, the vocabulary distri- bution, P v , has the word "from" as the most prob- able word and the entity distribution, P e , has the entity "S. B. Moyo" as the most probable entity. However, by combining these two distribution us- ing p gen , the model switches to the entity distribu- tion and correctly generates "S. B. Moyo".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">News Video Dataset</head><p>Current datasets for video description generation focus on specific ( <ref type="bibr" target="#b27">Rohrbach et al., 2014</ref>) and gen- eral (Chen and Dolan, 2011; Xu et al., 2016) do- mains, but do not contain a large proportion of descriptions with specific knowledge like named entities as shown in <ref type="table">Table 1</ref>. In our news video dataset, the descriptions are replete with important knowledge that is both necessary and challenging to incorporate into the generated descriptions. Our news video dataset contains AFP interna- tional news videos from YouTube. 6 These videos are from October, 2015 to November, 2017 and cover a variety of topics, such as protests, at- tacks, natural disasters, trials, and political move- ments. The videos are "on-the-scene" and con- tain some depiction of the content in the descrip- tion. For each video, we take the YouTube de- scriptions given by AFP News as the ground-truth descriptions we wish to generate. We collect the tags and meta-data (e.g., upload date). We filter videos by length, with a cutoff of 2 minutes, and remove videos which are videographics or anima- tions. For preprocessing, we tokenize each sen- tence, remove punctuation characters other than </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Comparisons</head><p>We test our method against the following base- lines: Article-only. We use the summariza- tion model of <ref type="bibr" target="#b28">See et al. (2017)</ref> to generate the description by summarizing the topically re- lated documents. Video-only (VD). We train a model that does not receive any background knowledge and generates the description directly from the video. VD with knowledge gate only (VD+Knowledge Gate), VD with entity pointer network only (VD+Entity Pointer), and no- video (Entity Pointer+Knowledge Gate). These test the effects of the knowledge gate, entity pointer network, and video encoder in isolation. Each model uses a cross entropy loss. Video- based models are trained using the Adam op- timizer ( <ref type="bibr" target="#b10">Kingma and Ba, 2015</ref>) with a learn- ing rate of 0.0002 and have a hidden state size of 512 as well as an embedding size of 300. We use Google News pre-trained word embed- dings ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) to initialize our word embeddings and compute entity embed- dings. For visual features, we use the Conv3-512 layer response of VGGNet ( <ref type="bibr" target="#b29">Simonyan and Zisserman, 2014</ref>) pre-trained on ImageNet ( <ref type="bibr" target="#b4">Deng et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluations</head><p>METEOR ( <ref type="bibr" target="#b5">Denkowski and Lavie, 2014</ref>) and ROUGE-L ( <ref type="bibr" target="#b14">Lin, 2004</ref>) are adopted as metrics for evaluating the generated descriptions. We choose METEOR because we only have one reference description per video and this metric accounts for stemming and synonym matching. We also use ROUGE-L for comparison to summarization work. These capture the coherence and relevance of the generated descriptions to the ground truth.</p><p>Generating these descriptions is concerned with not only generating fluent text, but also the amount of knowledge conveyed and the accuracy of the knowledge elements (e.g., named entities or event structures). Previous work in natural language generation and summarization <ref type="bibr" target="#b18">(Nenkova and Passonneau, 2004;</ref><ref type="bibr" target="#b19">Novikova et al., 2017;</ref><ref type="bibr" target="#b41">Wiseman et al., 2017;</ref><ref type="bibr" target="#b25">Pasunuru and Bansal, 2018)</ref> scores and/or assigns weights to overlapping text, salient phrases, or information units (e.g., entity rela- tions ( <ref type="bibr" target="#b41">Wiseman et al., 2017)</ref>). However, knowl- edge elements cannot be simply represented as a set of isolated information units since they are in- herently interconnected through some structure.</p><p>Therefore, for this knowledge-centric genera- tion task, we compute F1 scores on event and en- tity extraction results from the generated descrip- tions against the extraction results on the ground truth. For entities, we measure the F1 score of the named entities in the generated description com- pared to the ground truth. For events, given a generated description, w s , and the ground truth description, w c , we extract a set of event struc- tures, Y s and Y c , for both descriptions such that Y = {(t k , r k,1 , a k,1 , ..., r k,m , a k,m )} K k=1 where there are K events extracted from the description, t k is the k th event type, r k,m is the m th argument role of t k , and a k,m is the m th argument of t k . For the description in <ref type="figure">Figure 2a</ref>, one may obtain:</p><formula xml:id="formula_12">Y = {(Demonstrate,</formula><p>Entity, "Pro-independence supporters", Place, "Barcelona")} Next, we form event type, argument role, and ar- gument triples (t s k , r s k,m , a s k,m ) and (t c j , r c j,m , a c j,m ) for each event structure in Y s and Y c , respectively. We compute the F1 score of the triples, consid- ering a triple correct if and only if it appears in the ground truth triples. <ref type="bibr">7</ref> This metric enables us to evaluate how well a generated description cap- tures the overall events, while still giving credit to partially correct event structures. We compute these F1 scores on 50 descriptions based on man- ually annotated event structures. We also perform automatic F1 score evaluation on the entire test set using the entity and event extraction systems of <ref type="bibr" target="#b21">Pan et al. (2017)</ref> and <ref type="bibr" target="#b13">Li et al. (2013)</ref>, respec- tively. The manual evaluations offer accurate com- parisons and control for correctness, while the au- tomated evaluations explore the viability of us- ing automated IE tools to measure performance, which is desirable for scaling to larger datasets for which manual evaluations are too expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>The KaVD network outperforms almost all of the baselines, as shown in <ref type="table" target="#tab_6">Table 2</ref>, achieving statisti- cally significant improvements in METEOR and ROUGE-L w.r.t. all other models besides the no- video model (p &lt; 0.05). <ref type="bibr">8</ref> The additions of the entity pointer network and knowledge gate are complementary and greatly improve the entity in- corporation performance, increasing the entity F1 scores by at least 6% in both the manual and auto- matic evaluations. In <ref type="figure" target="#fig_2">Figure 5a</ref>, the entity pointer network is able to incorporate the entity "Abdiaziz Abu Musab", who is a leader of the group respon- sible for the attack. We find that the entity and event type features from the knowledge gate help generate more precise entities. However, noise in the article retrieval process and entity extraction system limits our entity incorporation capabilities, since on average only 70.6% of the entities in the ground truth description are retrieved from the ar- ticles. Lastly, the video encoder helps generate the correct events and offers qualitative benefits, such as allowing the model to generate more concise and diverse descriptions, though it negatively af- fects the entity incorporation performance.</p><p>The video alone is insufficient to generate the correct entities <ref type="table" target="#tab_6">(Table 2)</ref>. In <ref type="figure" target="#fig_2">Figure 5a</ref>, the VD baseline generates the correct event, but generates the incorrect location "Kabul". We observe that when the visual evidence is ambiguous, this model may fail to generate the correct events and entities. For example, if a video depicts the destruction of buildings after a hurricane, then the VD baseline may mistakenly describe the video as an explosion since the visual evidence is similar.</p><p>The article-only baseline tends to mention the correct entities as shown in <ref type="figure" target="#fig_2">Figure 5a</ref>, where the description is generally on topic but provides some irrelevant information. Indeed, this model can generate descriptions unrelated to the video itself. In <ref type="figure" target="#fig_2">Figure 5b</ref>, the article-only baseline's descrip- tion contains some correct entities (e.g., "Colom- bia"), but is not focused on the announcement depicted in the video. As <ref type="bibr" target="#b28">See et al. (2017)</ref> dis- cuss, this model can be more extractive than ab- stractive, copying many sequences from the docu- ments. This can lead to irrelevant descriptions as the articles may not be specific to the video.</p><p>Our entity and event F1 score based metrics cor- relate well with the correctness of the knowledge conveyed in the generated description. The con- sistency in model rankings between the manual and automatic entity metrics shows the potential of using automated entity extraction approaches to evaluate with this metric. We observe discrep- ancies between the manual and automatic event metrics, in part, due to errors in the automated extraction and the addition of more test points. For example, in the generated sentence, "Hun- dreds of people are to take to the streets of...", the event extraction system mistakenly assigns a "Transport" event type instead of the correct "Demonstrate" event type. In contrast, such mistakes do not appear in the manual evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Most previous video captioning efforts focus on learning video representations through dif- ferent encoding techniques ( <ref type="bibr" target="#b37">Venugopalan et al., 2015a</ref>,b), using spatial or temporal attentions ( <ref type="bibr" target="#b20">Pan et al., 2016;</ref><ref type="bibr" target="#b46">Yu et al., 2016;</ref><ref type="bibr" target="#b48">Zanfir et al., 2016)</ref>, using 3D CNN features ( <ref type="bibr" target="#b32">Tran et al., 2015;</ref><ref type="bibr" target="#b20">Pan et al., 2016)</ref>, or eas- ing the learning process via multi-task learning or reinforcement rewards ( <ref type="bibr">Pasunuru and Bansal, 2017a,b</ref>). Compared to other hierarchical mod- els ( <ref type="bibr" target="#b20">Pan et al., 2016;</ref><ref type="bibr" target="#b46">Yu et al., 2016)</ref>, each level of our hierarchy encodes a different dimension of the video, leveraging global temporal features and local spatial features, which are shown to be ef- fective for different tasks ( <ref type="bibr">Xu et al., 2015;</ref><ref type="bibr" target="#b47">Yu et al., 2017)</ref>.</p><p>We move towards using datasets with captions that have specific knowledge rather than generic Model METEOR ROUGE-L Entity F1 Auto-Entity F1 Event F1 Auto-Event F1</p><p>Article-only 8.  Model Description Article-only colombia's marxist rebels against her family. and last year, when given the leg of helena gonzlez's nephew years ago is still fresh the as pope francis arrived in colombia on wednesday for a six-day the VD president donald trump says that he will be talks to be to be talks to be talks in the country's country to be talks, saying he says he would be no evidence's state and kerry says. VD+Entity Pointer President Maduro says the FARC president warns that the ceasefire to Prime Minister says that he will be ready to help President Maduro says that he is no evidence of President Bashar talks in Bogota. VD +Knowledge Gate US Secretary of State John Kerry, who will not any maintain in Syria, after a ceasefire in Syria, saying that the United Nations says, it will not to be into a speech in its interview. EntityPointer +Knowledge Gate Venezuela's President FARC envoy to Colom- bia is a definitive ceasefire in the FARC con- flict, with FARC rebels, the FARC rebels. KaVD Colombia's government, signed the peace agreement with the FARC peace accord in the FARC rebels. captions as in previous work <ref type="bibr" target="#b2">(Chen and Dolan, 2011;</ref><ref type="bibr" target="#b27">Rohrbach et al., 2014;</ref>. There are efforts in image captioning to personal- ize captions <ref type="bibr" target="#b22">(Park et al., 2017)</ref>, incorporate novel objects into captions ( <ref type="bibr" target="#b36">Venugopalan et al., 2016)</ref>, and perform open domain captioning ( <ref type="bibr" target="#b33">Tran et al., 2016)</ref>. To the best of our knowledge, our dataset is the first of its kind and offers challenges in entity and activity recognition as well as the generation low probability words. Datasets with captions rich in knowledge elements, like those in our dataset, take a necessary step towards increasing the utility of video captioning systems.</p><p>We employ similar approaches to those in automatic summarization, where pointer net- works ( <ref type="bibr" target="#b39">Vinyals et al., 2015</ref>) and copy mecha- nisms ( <ref type="bibr" target="#b6">Gu et al., 2016</ref>) are used ( <ref type="bibr" target="#b15">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b28">See et al., 2017)</ref>, and natural language gen- eration for dialogue systems <ref type="bibr" target="#b40">(Wen et al., 2015;</ref><ref type="bibr" target="#b34">Tran and Nguyen, 2017)</ref>. The KaVD network combines the copying capabilities of pointer net- works ( <ref type="bibr" target="#b28">See et al., 2017)</ref> and semantic control of gating mechanisms <ref type="bibr" target="#b40">(Wen et al., 2015;</ref><ref type="bibr" target="#b34">Tran and Nguyen, 2017</ref>) in a complementary fashion to ad- dress a new, multi-modal task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We collect a news video dataset with knowledge- rich descriptions and present a multi-modal ap- proach to this task that uses a novel Knowledge- aware Video Description network, which can uti- lize background knowledge mined from topically related documents. We offer a new metric to mea- sure a model's ability to incorporate named enti- ties and specific events into the descriptions. We show the effectiveness of our approach and set a new benchmark for this dataset. In future work, we are increasing the size of dataset and exploring other knowledge-centric metrics for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of machine (a) and human (b) generated descriptions. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall pipeline of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of generated descriptions. The KaVD network generates the correct entities and correct events, while other models may contain some wrong entities or wrong events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>s t Spatial Attention</head><label></label><figDesc></figDesc><table>e 

Entity 
Embeddings 

p gen 

c t 

v t 
s t 

S . B . M o y o 
Z im 
b a b w e 
M u g a b e 

Entity 
Distribution 

Output 
Distribution 

Entity 
Context Vector 

Vocab 
Distribution 
a s s u r e s 
s ta g in g 

S. B. Moyo 

a s s u r e s 
s ta g in g 

Knowledge 
Gate Vector 

Knowledge 
Gate 

S. B. Moyo 

Temporal 
Attention 

v t 

h 1 
h N 

General 
Major 
Senior 

fr o m 

fr o m 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>METEOR, ROUGE-L, and manual/automated entity (Entity F1/Auto-Entity F1) and event 
(Event F1/Auto-Event F1) F1 score results of the baselines and KaVD network on our news video dataset. 

b) 

Title: Santos: 'Green light' for referendum on Colombia peace deal 

a) 

Title: Deadly Shabaab attack rocks Somali capital 

Model 
Description 
Article-only somali capital mogadishu on saturday. at least 
276 people have died and the government news 
agency sonna says only 111 of them have been 
identified. a turkish military but instead wit-
nessed her burial. no group has yet said it 
was behind on instead he attended her burial. 
"anfa'a said she had spoken to her sister 20 
minutes before on 
VD 
a suicide bomber killed # people in a bus car-
rying # people killed in a bus in central kabul. 
VD+Entity 
Pointer 

A suicide bomber killed # people were killed 
in a bus near the northern city of Mogadishu, 
police said. 
VD 
+Knowledge 
Gate 

At least # people were killed and # wounded 
when a busy bus station in Kabul, killing at 
least # people dead and others who died in the 
rubble of the deadliest attack in the country. 
EntityPointer 
+Knowledge 
Gate 

At least # people were killed in a suicide car 
bomb attack on a suicide car bomb attack on a 
police vehicle in Mogadishu, police said. 
KaVD 
A suicide bombing claimed by the Abdiaziz 
Abu Musab group time killed # people in So-
malia's capital Mogadishu, killing # people, of-
ficials said. 

</table></figure>

			<note place="foot">topically related news documents for a video and extracts the events and named entities from these documents. Then, given the video as well as the extracted events and entities, we generate a description using a Knowledgeaware Video Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model&apos;s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions.</note>

			<note place="foot" n="1"> (a) https://goo.gl/2StcD8, (b) https://goo.gl/VFR5nw</note>

			<note place="foot" n="2"> (a) https://goo.gl/3cF1oU, (b) https://goo.gl/NkwHvJ</note>

			<note place="foot" n="6"> https://www.youtube.com/user/AFP</note>

			<note place="foot" n="7"> This criterion is used for computing precision and recall. 8 Found via paired bootstrap resampling (Koehn, 2004).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014 and U.S. ARL NS-CTA No. W911NF-09-2-0053. The views and conclusions contained in this document are those of the authors and should not be inter-preted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rouge: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: ACL workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Why we need new evaluation metrics for nlg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Amanda Cercas Curry, and Verena Rieser</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attend to you: Personalized image captioning with context sequence memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongchang</forename><surname>Cesc Chunseong Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multitask video captioning with video and entailment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reinforced video captioning with entailment rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multireward reinforced summarization with saliency and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A dualstage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coherent multi-sentence video description with variable level of detail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Annemarie Friedrich, Manfred Pinkal, and Bernt Schiele</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Carapcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sienkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Natural language generation for spoken dialogue system using rnn encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Minh</forename><surname>Van-Khanh Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving lstm-based video description with linguistic knowledge mined from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Pointer networks. In NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Challenges in data-to-document generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Msrvtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spatio-temporal attention models for grounded video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisabeta</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
