<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4231" to="4242"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4231</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem , varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open domain Question Answering (QA) is the task of finding answers to questions posed in nat- ural language. Historically, this required a spe- cialized pipeline consisting of multiple machine- learned and hand-crafted modules <ref type="bibr" target="#b11">(Ferrucci et al., 2010)</ref>. Recently, the paradigm has shifted towards training end-to-end deep neural network models for the task <ref type="bibr" target="#b26">Liang et al., 2017;</ref><ref type="bibr" target="#b30">Raison et al., 2018;</ref><ref type="bibr" target="#b38">Talmor and Berant, 2018;</ref><ref type="bibr" target="#b19">Iyyer et al., 2017)</ref>. Most existing models, how- ever, answer questions using a single information source, usually either text from an encyclopedia, or a single knowledge base (KB).</p><p>Intuitively, the suitability of an information source for QA depends on both its coverage and * Haitian Sun and Bhuwan Dhingra contributed equally to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q. Who voiced Meg in Family Guy? Family Guy</head><p>Meg Griffin</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVT1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVT2</head><p>Lacey Chabert</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mila Kunis</head><p>Megatron "Meg" Griffin is a character from the animated television series Family Guy. the difficulty of extracting answers from it. A large text corpus has high coverage, but the information is expressed using many different text patterns. As a result, models which operate on these patterns (e.g. BiDAF ( <ref type="bibr" target="#b36">Seo et al., 2017)</ref>) do not generalize beyond their training domains ( <ref type="bibr" target="#b46">Wiese et al., 2017;</ref><ref type="bibr" target="#b10">Dhingra et al., 2018</ref>) or to novel types of reason- ing ( <ref type="bibr" target="#b45">Welbl et al., 2018;</ref><ref type="bibr" target="#b38">Talmor and Berant, 2018)</ref>. KBs, on the other hand, suffer from low cover- age due to their inevitable incompleteness and re- stricted schema ( <ref type="bibr" target="#b29">Min et al., 2013</ref>), but are easier to extract answers from, since they are constructed precisely for the purpose of being queried. In practice, some questions are best answered using text, while others are best answered using KBs. A natural question, then, is how to effec- tively combine both types of information. Surpris- ingly little prior work has looked at this problem. In this paper we focus on a scenario in which a large-scale KB <ref type="bibr" target="#b3">(Bollacker et al., 2008;</ref><ref type="bibr" target="#b1">Auer et al., 2007</ref>) and a text corpus are available, but neither is sufficient alone for answering all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Originally voiced by</head><p>A na¨ıvena¨ıve option, in such a setting, is to take state- of-the-art QA systems developed for each source, and aggregate their predictions using some heuris- tic <ref type="bibr" target="#b11">(Ferrucci et al., 2010;</ref><ref type="bibr" target="#b2">Baudiš, 2015)</ref>. We call this approach late fusion, and show that it can be sub-optimal, as models have limited ability to aggregate evidence across the different sources <ref type="bibr">( § 5.4)</ref>. Instead, we focus on an early fusion strat- egy, where a single model is trained to extract an- swers from a question subgraph <ref type="figure" target="#fig_0">(Fig 1)</ref> containing relevant KB facts as well as text sentences. Early fusion allows more flexibility in combining infor- mation from multiple sources.</p><p>To enable early fusion, in this paper we propose a novel graph convolution based neural network, called GRAFT-Net (Graphs of Relations Among Facts and Text Networks), specifically designed to operate over heterogeneous graphs of KB facts and text sentences. We build upon recent work on graph representation learning ( <ref type="bibr" target="#b22">Kipf and Welling, 2016;</ref><ref type="bibr" target="#b35">Schlichtkrull et al., 2017</ref>), but propose two key modifications to adopt them for the task of QA. First, we propose heterogeneous update rules that handle KB nodes differently from the text nodes: for instance, LSTM-based updates are used to propagate information into and out of text nodes ( § 3.2). Second, we introduce a directed propaga- tion method, inspired by personalized Pagerank in IR <ref type="bibr" target="#b17">(Haveliwala, 2002)</ref>, which constrains the prop- agation of embeddings in the graph to follow paths starting from seed nodes linked to the question ( § 3.3). Empirically, we show that both these ex- tensions are crucial for the task of QA.</p><p>We evaluate these methods on a new suite of benchmark tasks for testing QA models when both KB and text are present. Using WikiMovies ( <ref type="bibr">Miller et al., 2016)</ref> and <ref type="bibr">WebQuestionsSP (Yih et al., 2016)</ref>, we construct datasets with a varying amount of training supervision and KB complete- ness, and with a varying degree of question com- plexity. We report baselines for future comparison, including Key Value Memory Networks ( <ref type="bibr">Miller et al., 2016;</ref><ref type="bibr" target="#b8">Das et al., 2017c)</ref>, and show that our proposed GRAFT-Nets have superior performance across a wide range of conditions ( § 5). We also show that GRAFT-Nets are competitive with the state-of-the-art methods developed specifically for text-only QA, and state-of-the art methods devel- oped for KB-only QA ( § 5.4) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Description</head><p>A knowledge base is denoted as K = (V, E, R), where V is the set of entities in the KB, and the edges E are triplets (s, r, o) which denote that re- lation r ∈ R holds between the subject s ∈ V and object o ∈ V. A text corpus D is a set of doc- uments {d 1 , . . . , d |D| } where each document is a sequence of words d i = (w 1 , . . . , w |d i | ). We fur- ther assume that an (imperfect) entity linking sys- tem has been run on the collection of documents whose output is a set L of links (v, d p ) connect- ing an entity v ∈ V with a word at position p in document d, and we denote with L d the set of all entity links in document d. For entity mentions spanning multiple words in d, we include links to all the words in the mention in L.</p><p>The task is, given a natural language question q = (w 1 , . . . , w |q| ), extract its answers {a} q from G = (K, D, L). There may be multiple correct an- swers for a question. In this paper, we assume that the answers are entities from either the documents or the KB. We are interested in a wide range of set- tings, where the KB K varies from highly incom- plete to complete for answering the questions, and we will introduce datasets for testing our models under these settings.</p><p>To solve this task we proceed in two steps. First, we extract a subgraph G q ⊂ G which contains the answer to the question with high probability. The goal for this step is to ensure high recall for an- swers while producing a graph small enough to fit into GPU memory for gradient-based learning. Next, we use our proposed model GRAFT-Net to learn node representations in G q , conditioned on q, which are used to classify each node as being an answer or not. Training data for the second step is generated using distant supervision. The entire process mimics the search-and-read paradigm for text-based QA ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Subgraph Retrieval</head><p>We retrieve the subgraph G q using two parallel pipelines -one over the KB K which returns a set of entities, and the other over the corpus D which returns a set of documents. The retrieved entities and documents are then combined with entity links to produce a fully-connected graph.</p><p>KB Retrieval. To retrieve relevant entities from the KB we first perform entity linking on the ques-tion q, producing a set of seed entities, denoted S q . Next we run the Personalized PageRank (PPR) method <ref type="bibr" target="#b17">(Haveliwala, 2002</ref>) around these seeds to identify other entities which might be an answer to the question. The edge-weights around S q are distributed equally among all edges of the same type, and they are weighted such that edges rel- evant to the question receive a higher weight than those which are not. Specifically, we average word vectors to compute a relation vector v(r) from the surface form of the relation, and a question vector v(q) from the words in the question, and use co- sine similarity between these as the edge weights. After running PPR we retain the top E entities v 1 , . . . , v E by PPR score, along with any edges be- tween them, and add them to G q .</p><p>Text Retrieval. We use Wikipedia as the corpus and retrieve text at the sentence level, i.e. docu- ments in D are defined along sentences bound- aries 2 . We perform text retrieval in two steps: first we retrieve the top 5 most relevant Wikipedia articles, using the weighted bag-of-words model from DrQA ( <ref type="bibr" target="#b26">Chen et al., 2017)</ref>; then we populate a Lucene 3 index with sentences from these arti- cles, and retrieve the top ranking ones d 1 , . . . , d D , based on the words in the question. For the sentence-retrieval step, we found it beneficial to include the title of the article as an additional field in the Lucene index. As most sentences in an arti- cle talk about the title entity, this helps in retriev- ing relevant sentences that do not explicitly men- tion the entity in the question. We add the retrieved documents, along with any entities linked to them, to the subgraph G q .</p><p>The final question subgraph is G q = (V q , E q , R + ), where the vertices V q consist of all the retrieved entities and documents, i.e.</p><formula xml:id="formula_0">V q = {v 1 , . . . , v E } ∪{d 1 , . . . , d D }.</formula><p>The edges are all relations from K among these entities, plus the entity-links between documents and entities, i.e.</p><formula xml:id="formula_1">E q ={(s, o, r) ∈ E : s, o ∈ V q , r ∈ R} ∪ {(v, d p , r L ) : (v, d p ) ∈ L d , d ∈ V q },</formula><p>where r L denotes a special "linking" relation. R + = R ∪ {r L } is the set of all edge types in the subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAFT-Nets</head><p>The question q and its answers {a} q induce a labeling of the nodes in V q : we let y v = 1 if v ∈ {a} q and y v = 0 otherwise for all v ∈ V q . The task of QA then reduces to perform- ing binary classification over the nodes of the graph G q . Several graph-propagation based mod- els have been proposed in the literature which learn node representations and then perform clas- sification of the nodes ( <ref type="bibr" target="#b22">Kipf and Welling, 2016;</ref><ref type="bibr" target="#b35">Schlichtkrull et al., 2017</ref>). Such models follow the standard gather-apply-scatter paradigm to learn the node representation with homogeneous up- dates, i.e. treating all neighbors equally.</p><p>The basic recipe for these models is as follows:</p><p>1. Initialize node representations h <ref type="formula">(0)</ref> v .</p><p>2. For l = 1, . . . , L update node representations</p><formula xml:id="formula_2">h (l) v = φ   h (l−1) v , v ∈Nr(v) h (l−1) v   ,</formula><p>where N r (v) denotes the neighbours of v along incoming edges of type r, and φ is a neural network layer.</p><p>Here L is the number of layers in the model and corresponds to the maximum length of the paths along which information should be propagated in the graph. Once the propagation is complete the final layer representations h</p><formula xml:id="formula_3">(L) v</formula><p>are used to per- form the desired task, for example link prediction in knowledge bases ( <ref type="bibr" target="#b35">Schlichtkrull et al., 2017)</ref>.</p><p>However, there are two differences in our set- ting from previously studied graph-based clas- sification tasks. The first difference is that, in our case, the graph G q consists of heterogeneous nodes. Some nodes in the graph correspond to KB entities which represent symbolic objects, whereas other nodes represent textual documents which are variable length sequences of words. The second difference is that we want to condition the repre- sentation of nodes in the graph on the natural lan- guage question q. In §3.2 we introduce heteroge- neous updates to address the first difference, and in §3.3 we introduce mechanisms for conditioning on the question (and its entities) for the second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node Initialization</head><p>Nodes corresponding to entities are initialized us- ing fixed-size vectors h</p><formula xml:id="formula_4">(0) v = x v ∈ R n , where</formula><p>x v can be pre-trained KB embeddings or random, and n is the embedding size. Document nodes in the graph describe a variable length sequence of text. Since multiple entities might link to differ- ent positions in the document, we maintain a vari- able length representation of the document in each layer. This is denoted by H (l) d ∈ R |d|×n . Given the words in the document (w 1 , . . . , w |d| ), we initial- ize its hidden representation as:</p><formula xml:id="formula_5">H (0) d = LSTM(w 1 , w 2 , . . . ),</formula><p>where LSTM refers to a long short-term memory unit. We denote the p-th row of H  <ref type="figure" target="#fig_2">Figure 2</ref> shows the update rules for entities and documents, which we describe in detail here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Heterogeneous Updates</head><p>Entities. Let M (v) = {(d, p)} be the set of po- sitions p in documents d which correspond to a mention of entity v. The update for entity nodes in- volves a single-layer feed-forward network (FFN) over the concatenation of four states:</p><formula xml:id="formula_6">h (l) v = FFN             h (l−1) v h (l−1) q r v ∈Nr(v) α v r ψ r (h (l−1) v ) (d,p)∈M (v) H (l−1) d,p             . (1)</formula><p>The first two terms correspond to the entity repre- sentation and question representation (details be- low), respectively, from the previous layer.</p><p>The third term aggregates the states from the entity neighbours of the current node, N r (v), af- ter scaling with an attention weight α v r (described in the next section), and applying relation specific transformations ψ r . Previous work on Relational- Graph Convolution Networks ( <ref type="bibr" target="#b35">Schlichtkrull et al., 2017</ref>) used a linear projection for ψ r . For a batched implementation, this results in matrices of size O(B|R q ||E q |n), where B is the batch size, which can be prohibitively large for large sub- graphs <ref type="bibr">4</ref> . Hence in this work we use relation vec- tors x r for r ∈ R q instead of matrices, and com- pute the update along an edge as:</p><formula xml:id="formula_7">ψ r (h (l−1) v ) = pr (l−1) v FFN x r , h (l−1) v .<label>(2)</label></formula><p>4 This is because we have to use adjacency matrices of size |Rq| × |Eq| × |Eq| to aggregate embeddings from neighbours of all nodes simultaneously.</p><p>Here pr</p><formula xml:id="formula_8">(l−1) v</formula><p>is a PageRank score used to control the propagation of embeddings along paths start- ing from the seed nodes, which we describe in de- tail in the next section. The memory complexity of the above is O(B(|F q | + |E q |)n), where |F q | is the number of facts in the subgraph G q .</p><p>The last term aggregates the states of all tokens that correspond to mentions of the entity v among the documents in the subgraph. Note that the up- date depends on the positions of entities in their containing document.</p><p>Documents. Let L(d, p) be the set of all entities linked to the word at position p in document d. The document update proceeds in two steps. First we aggregate over the entity states coming in at each position separately:</p><formula xml:id="formula_9">˜ H (l) d,p = FFN   H (l−1) d,p , v∈L(d,p) h (l−1) v   . (3a)</formula><p>Here h</p><formula xml:id="formula_10">(l−1) v</formula><p>are normalized by the number of out- going edges at v. Next we aggregate states within the document using an LSTM:</p><formula xml:id="formula_11">H (l) d = LSTM( ˜ H (l) d ).<label>(3b)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Conditioning on the Question</head><p>For the parts described thus far, the graph learner is largely agnostic of the question. We introduce dependence on question in two ways: by attention over relations, and by personalized propagation. To represent q, let w q 1 , . . . , w q |q| be the words in the question. The initial representation is com- puted as:</p><formula xml:id="formula_12">h (0) q = LSTM(w q 1 , . . . , w q |q| ) |q| ∈ R n ,<label>(4)</label></formula><p>where we extract the final state from the out- put of the LSTM. In subsequent layers the question representation is updated as h</p><formula xml:id="formula_13">(l) q = FFN v∈Sq h (l) v</formula><p>, where S q denotes the seed en- tities mentioned in the question.</p><p>Attention over Relations. The attention weight in the third term of Eq. <ref type="formula">(1)</ref> is computed using the question and relation embeddings:</p><formula xml:id="formula_14">α v r = softmax(x T r h (l−1) q ),</formula><p>where the softmax normalization is over all outgo- ing edges from v , and x r is the relation vector for relation r. This ensures that embeddings are prop- agated more along edges relevant to the question.  v at the nodes, we also maintain scalar "PageRank" scores pr (l) v which measure the total weight of paths from a seed entity to the current node, as follows:</p><formula xml:id="formula_15">pr (0) v = 1 |Sq| if v ∈ S q 0 o.w. , pr (l) v = (1 − λ)pr (l−1) v + λ r v ∈Nr(v) α v r pr (l−1) v .</formula><p>Notice that we reuse the attention weights α v r when propagating PageRank, to ensure that nodes along paths relevant to the question receive a high weight. The PageRank score is used as a scal- ing factor when propagating embeddings along the edges in Eq. (2). For l = 1, the PageRank score will be 0 for all entities except the seed entities, and hence propagation will only happen outward from these nodes. For l = 2, it will be non-zero for the seed entities and their 1-hop neighbors, and propagation will only happen along these edges. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Selection</head><p>The final representations h (L) v ∈ R n , are used for binary classification to select the answers: v is maintained for each node v across layers, which spreads out from the seed node. Embeddings are only propagated from nodes with pr</p><formula xml:id="formula_16">Pr (v ∈ {a} q |G q , q) = σ(w T h (L) v + b),<label>(5)</label></formula><formula xml:id="formula_17">(l) v &gt; 0.</formula><p>where σ is the sigmoid function. Training uses bi- nary cross-entropy loss over these probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Regularization via Fact Dropout</head><p>To encourage the model to learn a robust classi- fier, which exploits all available sources of infor- mation, we randomly drop edges from the graph during training with probability p 0 . We call this fact-dropout. It is usually easier to extract an- swers from the KB than from the documents, so the model tends to rely on the former, especially when the KB is complete. This method is similar to DropConnect ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The work of <ref type="bibr" target="#b8">Das et al. (2017c)</ref> attempts an early fusion strategy for QA over KB facts and text. Their approach is based on Key-Value Memory Networks (KV-MemNNs) <ref type="bibr">(Miller et al., 2016</ref>) coupled with a universal schema ( <ref type="bibr" target="#b32">Riedel et al., 2013</ref>) to populate a memory module with repre- sentations of KB triples and text snippets indepen- dently. The key limitation for this model is that it ignores the rich relational structure between the facts and text snippets. Our graph-based method, on the other hand, explicitly uses this structure for the propagation of embeddings. We compare the two approaches in our experiments ( §5), and show that GRAFT-Nets outperform KV-MemNNs over all tasks.</p><p>Non-deep learning approaches have been also attempted for QA over both text assertions and KB facts. <ref type="bibr" target="#b12">Gardner and Krishnamurthy (2017)</ref>  The key differ- ence in QA compared to KBC is that in QA the inference process on the knowledge source has to be conditioned on the question, so different ques- tions induce different representations of the KB and warrant a different inference process. Further- more, KBC operates under the fixed schema de- fined by the KB before-hand, whereas natural lan- guage questions might not adhere to this schema.</p><p>The GRAFT-Net model itself is motivated from the large body of work on graph representation learning ( <ref type="bibr" target="#b34">Scarselli et al., 2009;</ref><ref type="bibr" target="#b25">Li et al., 2016;</ref><ref type="bibr" target="#b22">Kipf and Welling, 2016;</ref><ref type="bibr" target="#b0">Atwood and Towsley, 2016;</ref><ref type="bibr" target="#b35">Schlichtkrull et al., 2017)</ref>. Like most other graph-based models, GRAFT-Nets can also be viewed as an instantiation of the Message Passing Neural Network (MPNN) framework of <ref type="bibr" target="#b13">Gilmer et al. (2017)</ref>. GRAFT-Nets are also inductive rep- resentation learners like <ref type="bibr">GraphSAGE (Hamilton et al., 2017)</ref>, but operate on a heterogeneous mix- ture of nodes and use retrieval for getting a sub- graph instead of random sampling. The recently proposed Walk-Steered Convolution model uses random walks for learning graph representations ( . Our personalization technique also borrows from such random walk literature, but uses it to localize propagation of embeddings.</p><p>Tremendous progress on QA over KB has been made with deep learning based approaches like memory networks ( <ref type="bibr" target="#b4">Bordes et al., 2015;</ref><ref type="bibr" target="#b20">Jain, 2016)</ref> and reinforcement learning ( <ref type="bibr" target="#b26">Liang et al., 2017;</ref><ref type="bibr" target="#b6">Das et al., 2017a</ref>). But extending them with text, which is our main focus, is non-trivial. In another direction, there is also work on producing parsi- monious graphical representations of textual data ( <ref type="bibr" target="#b23">Krause et al., 2016;</ref><ref type="bibr" target="#b27">Lu et al., 2017)</ref>; however in this paper we use a simple sequential representa- tion augmented with entity links to the KB which works well.</p><p>For QA over text only, a major focus has been on the task of reading comprehension ( <ref type="bibr" target="#b36">Seo et al., 2017;</ref><ref type="bibr" target="#b14">Gong and Bowman, 2017;</ref><ref type="bibr" target="#b18">Hu et al., 2017;</ref><ref type="bibr" target="#b37">Shen et al., 2017;</ref>) since the intro- duction of SQuAD ( <ref type="bibr" target="#b31">Rajpurkar et al., 2016</ref>). These systems assume that the answer-containing pas- sage is known apriori, but there has been progress when this assumption is relaxed <ref type="bibr" target="#b30">Raison et al., 2018;</ref><ref type="bibr" target="#b42">Wang et al., 2018</ref><ref type="bibr" target="#b43">Wang et al., , 2017</ref><ref type="bibr" target="#b44">Watanabe et al., 2017)</ref>. We work in the latter setting, where relevant information must be retrieved from large information sources, but we also incorporate KBs into this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments &amp; Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>WikiMovies-10K consists of 10K randomly sam- pled training questions from the WikiMovies dataset ( <ref type="bibr">Miller et al., 2016)</ref>, along with the origi- nal test and validation sets. We sample the training questions to create a more difficult setting, since the original dataset has 100K questions over only 8 different relation types, which is unrealistic in our opinion. In § 5.4 we also compare to the exist- ing state-of-the-art using the full training set.</p><p>We use the KB and text corpus constructed from Wikipedia released by <ref type="bibr">Miller et al. (2016)</ref>. For en- tity linking we use simple surface level matches, and retrieve the top 50 entities around the seeds to create the question subgraph. We further add the top 50 sentences (along with their article ti- tles) to the subgraph using Lucene search over the text corpus. The overall answer recall in our con- structed subgraphs is 99.6%. <ref type="bibr">WebQuestionsSP (Yih et al., 2016)</ref> consists of 4737 natural language questions posed over Free- base entities, split up into 3098 training and 1639 test questions. We reserve 250 training questions for model development and early stopping. We use the entity linking outputs from S-MART 5 and re- trieve 500 entities from the neighbourhood around the question seeds in Freebase to populate the question subgraphs <ref type="bibr">6</ref> . We further retrieve the top 50 sentences from Wikipedia with the two-stage pro- cess described in §2. The overall recall of answers among the subgraphs is 94.0%. <ref type="table" target="#tab_1">Table 1</ref> shows the combined statistics of all the retreived subgraphs for the questions in each dataset. These two datasets present varying levels of difficulty. While all questions in WikiMovies correspond to a single KB relation, for WebQues- tionsSP the model needs to aggregate over two KB facts for ∼30% of the questions, and also requires reasoning over constraints for ∼7% of the ques- tions ( <ref type="bibr" target="#b26">Liang et al., 2017</ref>). For maximum portabil- ity, QA systems need to be robust across several degrees of KB availability since different domains might contain different amounts of structured data; and KB completeness may also vary over time. Hence, we construct an additional 3 datasets each from the above two, with the number of KB facts downsampled to 10%, 30% and 50% of the orig- inal to simulate settings where the KB is incom- plete. We repeat the retrieval process for each sam- pled KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Compared Models</head><p>KV-KB is the Key Value Memory Networks model from <ref type="bibr">Miller et al. (2016)</ref>; <ref type="bibr" target="#b8">Das et al. (2017c)</ref> but using only KB and ignoring the text. KV-EF (early fusion) is the same model with access to both KB and text as memories. For text we use a BiLSTM over the entire sentence as keys, and entity mentions as values. This re-implementation shows better performance on the text-only and KB-only WikiMovies tasks than the results re- ported previously 7 (see <ref type="table" target="#tab_5">Table 4</ref>). GN-KB is the GRAFT-Net model ignoring the text. GN-LF is a late fusion version of the GRAFT-Net model: we train two separate models, one using text only and the other using KB only, and then ensemble the two 8 . GN-EF is our main GRAFT-Net model with early fusion. GN-EF+LF is an ensemble over the GN-EF and GN-LF models, with the same ensem- bling method as GN-LF. We report Hits@1, which <ref type="bibr">6</ref> A total of 13 questions had no detected entities. These were ignored during training and considered as incorrect dur- ing evaluation. <ref type="bibr">7</ref> For all KV models we tuned the number of layers {1, 2, 3}, batch size {10, 30, 50}, model dimension {50, 80}. We also use fact dropout regularization in the KB+Text set- ting tuned between {0, 0.2, 0.4}.</p><p>8 For ensembles we take a weighted combination of the an- swer probabilities produced by the models, with the weights tuned on the dev set. For answers only in text or only in KB, we use the probability as is.</p><p>is the accuracy of the top-predicted answer from the model, and the F1 score. To compute the F1 score we tune a threshold on the development set to select answers based on binary probabilities for each node in the subgraph. <ref type="table" target="#tab_2">Table 2</ref> presents a comparison of the above models across all datasets. GRAFT-Nets (GN) shows con- sistent improvement over KV-MemNNs on both datasets in all settings, including KB only (-KB), text only (-EF, Text Only column), and early fu- sion (-EF). Interestingly, we observe a larger rel- ative gap between the Hits and F1 scores for the KV models than we do for our GN models. We believe this is because the attention for KV is nor- malized over the memories, which are KB facts (or text sentences): hence the model is unable to assign high probabilities to multiple facts at the same time. On the other hand, in GN, we normal- ize the attention over types of relations outgoing from a node, and hence can assign high weights to all the correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>We also see a consistent improvement of early fusion over late fusion (-LF), and by ensembling them together we see the best performance across all the models. In <ref type="table" target="#tab_2">Table 2</ref> (right), we further show the improvement for KV-EF over KV-KB, and GN-LF and GN-EF over GN-KB, as the amount of KB is increased. This measures how effective these approaches are in utilizing text plus a KB. For KV-EF we see improvements when the KB is highly incomplete, but in the full KB setting, the performance of the fused approach is worse. A similar trend holds for GN-LF. On the other hand, GN-EF with text improves over the KB-only ap- proach in all settings. As we would expect, though, the benefit of adding text decreases as the KB be- comes more and more complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to Specialized Methods</head><p>In <ref type="table" target="#tab_5">Table 4</ref> we compare GRAFT-Nets to state-of- the-art models that are specifically designed and tuned for QA using either only KB or only text. For this experiment we use the full WikiMovies dataset to enable direct comparison to previously reported numbers. For DrQA , following the original paper, we restrict answer spans for WebQuestionsSP to match an entity in Freebase. In each case we also train GRAFT-Nets using only KB facts or only text sentences. In three out of the four cases, we find that GRAFT-Nets ei-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p># train / dev / test # entity nodes # edge types # document nodes # question vocab <ref type="table" target="#tab_1">WikiMovies-10K  10K / 10K / 10K  43,233  9  79,728  1759  WebQuestionsSP  2848 / 250 / 1639  528,617  513</ref> 235,567 3781  ther match or outperform the existing state-of-the- art models. We emphasize that the latter have no mechanism for dealing with the fused setting.</p><p>The one exception is the KB-only case for WebQuestionsSP where GRAFT-Net does 6.2% F1 points worse than Neural Symbolic Machines ( <ref type="bibr" target="#b26">Liang et al., 2017)</ref>. Analysis suggested three ex- planations: (1) In the KB-only setting, the recall of subgraph retrieval is only 90.2%, which lim- its overall performance. In an oracle setting where we ensure the answers are part of the subgraph, the F1 score increases by 4.8%. (2) We use the same probability threshold for all questions, even though the number of answers may vary signifi- cantly. Models which parse the query into a sym- bolic form do not suffer from this problem since answers are retrieved in a deterministic fashion. If we tune separate thresholds for each question the F1 score improves by 7.6%. (3) GRAFT-Nets per- form poorly in the few cases where there is a con- straint involved in picking out the answer (for ex- ample, "who first voiced Meg in Family Guy"). If we ignore such constraints, and consider all enti- ties with the same sequence of relations to the seed as correct, the performance improves by 3.8% F1. Heuristics such as those used by  can be used to improve these cases. <ref type="figure" target="#fig_3">Figure 3</ref> shows examples where GRAFT-Net fails to predict the correct answer set exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Model Components</head><p>Heterogeneous Updates. We tested a non- heterogeneous version of our model, where in- stead of using fine-grained entity linking informa- tion for updating the node representations (M (v) and L(d, p) in Eqs. 1, 3a), we aggregate the docu- ment states across all its positions as p H</p><p>d,p and use this combined state for all updates. Without the heterogeneous update, all entities v ∈ L(d, ·) will receive the same update from document d. There- fore, the model cannot disambiguate different en- tities mentioned in the same document. The result in <ref type="table" target="#tab_6">Table 5</ref> shows that this version is consistently worse than the heterogeneous model.</p><p>Conditioning on the Question. We performed an ablation test on the directed propagation method and attention over relations. We observe that both components lead to better performance. Such effects are observed in both complete and in- complete KB scenarios, e.g. on WebQuestionsSP dataset, as shown in <ref type="figure" target="#fig_5">Figure 4</ref> (left).</p><p>Fact Dropout. <ref type="figure" target="#fig_5">Figure 4 (</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we investigate QA using text com- bined with an incomplete KB, a task which has received limited attention in the past. We intro- duce several benchmark problems for this task by modifying existing question-answering datasets, and discuss two broad approaches to solving this problem-"late fusion" and "early fusion". We show that early fusion approaches perform better. We also introduce a novel early-fusion model, called GRAFT-Net, for classifying nodes in sub- graph consisting of both KB entities and text doc- uments. GRAFT-Net builds on recent advances in graph representation learning but includes sev- eral innovations which improve performance on this task. GRAFT-Nets are a single model which achieve performance competitive to state-of-the- art methods in both text-only and KB-only set- tings, and outperform baseline models when us- ing text combined with an incomplete KB. Cur- rent directions for future work include -(1) ex- tending GRAFT-Nets to pick spans of text as an- swers, rather than only entities and (2) improving the subgraph retrieval process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: To answer a question posed in natural language, GRAFT-Net considers a heterogeneous graph constructed from text and KB facts, and thus can leverage the rich relational structure between the two information sources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>d</head><label></label><figDesc>, correspond- ing to the embedding of p-th word in the document d at layer l, as H (l) d,p .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the heterogeneous update rules for entities (left) and text documents (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Directed propagation of embeddings in GRAFT-Net. A scalar PageRank score pr (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>use traditional feature extraction methods of open- vocabulary semantic parsing for the task. Ryu et al. (2014) use a pipelined system aggregat- ing evidence from both unstructured and semi- structured sources for open-domain QA. Another line of work has looked at learning combined representations of KBs and text for re- lation extraction and Knowledge Base Comple- tion (KBC) (Lao et al., 2012; Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2016; Das et al., 2017b; Han et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Effect of directed propagation and query-based attention over relations for the WebQuestionsSP dataset with 30% KB and 100% KB. Right: Hits@1 with different rates of factdropout on and WikiMovies and WebQuestionsSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Statistics of all the retrieved subgraphs ∪ q G q for WikiMovies-10K and WebQuestionsSP.</head><label>1</label><figDesc></figDesc><table>Model 
Text Only 
KB + Text 

10 % 
30% 
50% 
100% 

WikiMovies-10K 

KV-KB 
-
15.8 / 9.8 44.7 / 30.4 63.8 / 46.4 94.3 / 76.1 
KV-EF 
50.4 / 40.9 
53.6 / 44.0 60.6 / 48.1 75.3 / 59.1 93.8 / 81.4 
GN-KB 
-
19.7 / 17.3 48.4 / 37.1 67.7 / 58.1 97.0 / 97.6 
GN-LF 
 
 

 

73.2 / 64.0 

 
 

 

74.5 / 65.4 78.7 / 68.5 83.3 / 74.2 96.5 / 92.0 
GN-EF 
75.4 / 66.3 82.6 / 71.3 87.6 / 76.2 96.9 / 94.1 
GN-EF+LF 
79.0 / 66.7 84.6 / 74.2 88.4 / 78.6 96.8 / 97.3 

WebQuestionsSP 

KV-KB 
-
12.5 / 4.3 25.8 / 13.8 33.3 / 21.3 46.7 / 38.6 
KV-EF 
23.2 / 13.0 
24.6 / 14.4 27.0 / 17.7 32.5 / 23.6 40.5 / 30.9 
GN-KB 
-
15.5 / 6.5 34.9 / 20.4 47.7 / 34.3 66.7 / 62.4 
GN-LF 
 
 

 

25.3 / 15.3 

 
 

 

29.8 / 17.0 39.1 / 25.9 46.2 / 35.6 65.4 / 56.8 
GN-EF 
31.5 / 17.7 40.7 / 25.2 49.9 / 34.7 67.8 / 60.4 
GN-EF+LF 
33.3 / 19.3 42.5 / 26.7 52.3 / 37.4 68.7 / 62.3 

5 

0 

5 

10 

15 

Hits@1 

10.1 

3.9 
2.3 

-6.2 

10.5 

4.2 

-1.5 
-1.3 

11.2 

5.8 
2.2 
1.1 

WikiMovies-10K 
KV-EF KV-KB 
GN-LF GN-KB 
GN-EF GN-KB 

0.1 
0.3 
0.5 
1.0 
KB fraction 

0 

20 

40 

60 

Hits@1 

37.8 

15.9 
11.5 

-0.5 

54.8 

30.3 

15.6 

-0.5 

55.7 

34.2 

19.9 

-0.1 

WebQuestionsSP 
KV-EF KV-KB 
GN-LF GN-KB 
GN-EF GN-KB 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Left: Hits@1 / F1 scores of GRAFT-Nets (GN) compared to KV-MemNN (KV) in KB only 
(-KB), early fusion (-EF), and late fusion (-LF) settings. Right: Improvement of early fusion (-EF) and 
late fusion (-LF) over KB only (-KB) settings as KB completeness increases. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>right) compares the performance of the early fusion model as we vary</figDesc><table>Question 
Correct Answers 
Predicted Answers 

what language do most people speak in afghanistan 
Pashto language, 
Farsi (Eastern Language) 
Pashto language 

what college did john stockton go to 
Gonzaga University 
Gonzaga University, 
Gonzaga Preparatory School 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples from WebQuestionsSP dataset. Top: The model misses a correct answer. Bottom: The 
model predicts an extra incorrect answer. 

Method 
WikiMovies (full) 
WebQuestionsSP 
kb 
doc 
kb 
doc 

MINERVA 97.0 / -
-
-
-
R2-AsV 
-
85.8 / -
-
-
NSM 
-
-
-/ 69.0 
-
DrQA* 
-
-
-
21.5 / -
R-GCN # 
96.5 / 97.4 
-
37.2 / 30.5 
-
KV 
93.9 / -
76.2 / -
-/ -
-/ -
KV # 
95.6 / 88.0 80.3 / 72.1 
46.7 / 38.6 23.2 / 13.0 
GN 
96.8 / 97.2 86.6 / 80.8 
67.8 / 62.8 25.3 / 15.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Hits@1 / F1 scores compared to SOTA 
models using only KB or text: MINERVA 
(Das et al., 2017a), R2-AsV (Watanabe et al., 
2017), Neural Symbolic Machines (NSM) (Liang 
et al., 2017), DrQA (Chen et al., 2017), R-
GCN (Schlichtkrull et al., 2017) and KV-MemNN 
(Miller et al., 2016). *DrQA is pretrained on 
SQuAD. # Re-implemented. 

0 KB 
0.1 KB 
0.3 KB 
0.5 KB 
1.0 KB 
NH 22.7 / 13.6 28.7 / 15.8 35.6 / 23.2 47.2 / 33.3 66.5 / 59.8 
H 
25.3 / 15.3 31.5 / 17.7 40.7 / 25.2 49.9 / 34.7 67.8 / 60.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Non-Heterogeneous (NH) vs. Heteroge-
neous (H) updates on WebQuestionsSP 

the rate of fact dropout. Moderate levels of fact 
dropout improve performance on both datasets. 
The performance increases as the fact dropout rate 
increases until the model is unable to learn the in-
ference chain from KB. 

</table></figure>

			<note place="foot" n="1"> Source code and data are available at https:// github.com/OceanskySun/GraftNet</note>

			<note place="foot" n="2"> The term document will always refer to a sentence in the rest of this paper. 3 https://lucene.apache.org/</note>

			<note place="foot" n="5"> https://github.com/scottyih/STAGG</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Bhuwan Dhingra is supported by NSF under grants CCF-1414030 and IIS-1250956 and by grants from Google. Ruslan Salakhutdinov is sup-ported in part by ONR grant N000141812861, Ap-ple, and Nvidia NVAIL Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusionconvolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Yodaqa: a modular question answering system pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Baudiš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POSTER 2015-19th International Student Conference on Electrical Engineering</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05851</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>EACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Question answering on knowledge bases and text using universal schema and memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for question answering by search and reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simple and effective semi-supervised question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the deepqa project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Openvocabulary semantic parsing with both distributional statistics and formal knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3195" to="3201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Patrick F Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ruminating reader: Reasoning with gated multi-hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/1706.02216</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Joint representation learning of text and knowledge for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04125</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on World Wide Web</title>
		<meeting>the 11th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Question answering over knowledge base using factual memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL Student Research Workshop</title>
		<meeting>the NAACL Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Walk-steered convolution for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05837</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sar-graphs: A language resource connecting linguistic knowledge with semantic relations from knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Web Semantics: Science, Services and Agents on the World Wide Web</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="112" to="131" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08853</idno>
		<title level="m">Object-oriented neural programming (oonp) for document understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<imprint>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Weaver: Deep coencoding of questions and documents for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10490</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Open domain question answering using wikipedia-based knowledge model. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pum-Mo</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Gil</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun-Ki</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="683" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06103</idno>
		<title level="m">Modeling relational data with graph convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">R 3 : Reinforced reader-ranker for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05116</idno>
		<title level="m">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Question answering from unstructured text by retrieval and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08885</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Neural domain adaptation for biomedical question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="281" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improved neural relation detection for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Saidul Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
