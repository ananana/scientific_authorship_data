<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Measuring the behavioral impact of machine translation quality improvements with A/B testing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Russell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Gillespie Etsy</surname></persName>
						</author>
						<title level="a" type="main">Measuring the behavioral impact of machine translation quality improvements with A/B testing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2295" to="2299"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we discuss a process for quantifying the behavioral impact of a domain-customized machine translation system deployed on a large-scale e-commerce platform. We discuss several machine translation systems that we trained using aligned text from product listing descriptions written in multiple languages. We document the quality improvements of these systems as measured through automated quality measures and crowdsourced human quality assessments. We then measure the effect of these quality improvements on user behavior using an automated A/B testing framework. Through testing we observed an increase in key e-commerce metrics, including a significant increase in purchases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Quality evaluation is an essential task when train- ing a machine translation (MT) system. While au- tomatic evaluation methods like BLEU ( <ref type="bibr" target="#b10">Papineni et al., 2002</ref>) can be useful for estimating translation quality, a higher score is no guarantee of quality improvement <ref type="bibr" target="#b1">(Callison-Burch et al., 2006</ref>). Previ- ous studies (e.g. <ref type="bibr" target="#b5">Coughlin, 2003</ref>) have compared human evaluations of MT to metrics like BLEU and found close correspondence between the two. <ref type="bibr" target="#b7">Koehn (2004)</ref> argued that relatively small differ- ences in BLEU can indicate significant MT qual- ity differences and suggested that human evaluation, the traditional alternative to automated metrics like BLEU, is therefore unnecessarily time-consuming and costly. <ref type="bibr" target="#b2">Callison-Burch (2009)</ref> explored the use of crowdsourcing platforms for evaluating MT qual- ity, with good results. However, we are not aware of any research that investigates the effect of im- proved MT on human behavior. In a commercial application, like an e-commerce platform, it is de- sirable to have a high degree of confidence in the material effect of MT quality differences: any MT system change should positively impact user experi- ences.</p><p>Etsy is an online marketplace for handmade and vintage items, with over 40 million active listings and a community of buyers and sellers located around the world. Visitors can use MT to translate the text of product descriptions, product reviews, and private messages, making it possible for mem- bers to communicate effectively with one another, even when they speak different languages. These multilingual interactions facilitated by MT, such as reading nonnative listing descriptions or conversing with a foreign seller, are integral to the user experi- ence.</p><p>However, due to the unique nature of the products available in the marketplace, a generic third party MT system 1 often falls short when translating user- generated content. One challenging lexical item is "clutch." A generic engine, trained on commonly available parallel text, translates clutch as an "auto- motive clutch." In this marketplace, however, clutch almost always means "purse." A mistake like this is problematic: a user who sees this incorrect machine translation may lose confidence in that listing and possibly in the marketplace as a whole. To improve the translation quality for terms like clutch, we used an interface provided by a third party machine translation service 2 to train a cus- tom MT engine for English to French translations. To validate that the retrained MT systems were materially improved, we used a two step valida- tion process, first using crowd-sourced evaluations with Amazon's Mechanical Turk, and secondly us- ing A/B testing, a way of conducting randomized experiments on web sites, to measure the effect of the trained system on user behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Collection</head><p>Our online marketplace contains millions of listing descriptions posted by tens of thousands of multilin- gual sellers. We conducted an MT system training using aligned texts from these product listings. We used our third-party translation service's automated retraining framework to train multiple MT systems that were specifically tuned to the marketplace's cor- pus. To gather this data, we used a Hadoop job to parse through 130 million active and expired prod- uct listings to find listing descriptions that were writ- ten in both English and French. Once we found these listing descriptions, we tokenized the text on sentence boundary. We removed any descriptions where there was a mismatch in the number of sen- tences between the source and target descriptions.</p><p>Next, we used a language detection service to en- sure the source and target strings were the correct languages (source: English, target: French). After language detection, we removed all sentences where the ratio of alphabetic characters to total characters was below 70%. This 70% threshold was deter- mined through manual assessment of the result set, and was used to eliminate strings with low numbers of alphabetic characters, such as "25.5 in x 35.5 in".</p><p>After these preliminary filtering steps, our train- ing set consisted of 885,732 aligned sentences. To supplement the aligned text, we also collected 2,625,162 monolingual French sentences for the training. The monolingual text was parsed and cleaned in the same manner as the aligned sentences.</p><p>The commercial MT system's automatic training framework provides tools for the upload of bilingual and monolingual training data, tuning data, and test- ing data for customization of the underlying statis- tical MT system. Bilingual training data is used to modify the base translation model; monolingual data customizes the language model; the system is opti- mized for the tuning data; and the testing data is used to calculate a BLEU score. We trained over a dozen systems with a variety of datasets and selected the three systems that had the highest BLEU scores.</p><p>System 1 was trained using the aligned sentences, along with the 2.6 million monolingual sentences. The system was tuned using 2,500 sentences auto- matically separated from the training sentences by the third party's training system, and used an ad- ditional 2,500 automatically separated sentences for testing.</p><p>For System 2, we used a variation of the Gale- Church alignment algorithm (1993) to remove sen- tences predicted to be misaligned based on their length differences. The subject of sentence align- ment in parallel texts has been researched exten- sively (e.g. <ref type="bibr" target="#b0">Brown et al., 1991;</ref><ref type="bibr" target="#b6">Gale and Church, 1993)</ref>. Although more sophisticated methods ex- ist (e.g. <ref type="bibr" target="#b4">Chen, 1993;</ref><ref type="bibr" target="#b11">Wu, 1994;</ref><ref type="bibr" target="#b8">Melamed, 1996;</ref><ref type="bibr" target="#b9">Munteanu and Marcu, 2005</ref>), we used Gale-Church due to its relatively high accuracy and low im- plementation overhead. Misalignment between the same listing descriptions written in multiple lan- guages could be caused by several factors, the most common problem being that sellers do not trans- late descriptions sentence for sentence from one lan- guage to the next. We detected possible misalign- ments in 13.5% of the original 886K aligned sen- tences, leaving 776K sentences to use for training System 2. We used auto-tuning and auto-testing for this engine, as we did for System 1. System 3 was trained using the same training data as the second engine, but was tuned using 2,000 professionally-translated sentences taken from list- ing descriptions. Two hundred of these sentences were drawn semi-randomly to represent a general sample of listing description text; the remaining 1,800 contained terms, like "clutch," that were be- ing mistranslated by the generic system. This sys- tem used the same automatically-generated testing data as the other two to calculate a BLEU score. Ta- ble 1 shows the training and tuning data used for the three systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Crowdsourced Evaluation</head><p>For evaluation of the trained translation systems, we generated translations of sentences drawn randomly from our monolingual English corpus (product list- ings that sellers had not translated into languages other than English). We excluded segments that were translated the same by both the trained and </p><note type="other">BLEU Score Improvement Over Generic System System 1 48.16 +9.82 System 2 50.36 +12.02 System 3</note><p>46.85 +8.51 generic systems. (For System 1, 48 of 2,000 test sentences had the same translation as the generic system, for System 2 that number was 42, and for System 3 that number was 148.) To obtain judgments about the quality of these translations, we used Mechanical Turk to obtain hu- man evaluations of our candidate translation sys- tems <ref type="bibr" target="#b2">(Callison-Burch, 2009</ref>). To recruit Mechan- ical Turk workers with bilingual competence, we required workers to achieve at least 80% accuracy in a binary translation judgment task (workers were asked to judge whether each of 20 translations was "Good" or "Bad"; their answers were compared with those of professional translators).</p><p>Qualified workers completed a survey indicating their preference for the translation of a particular trained system compared to the generic commercial translation system. Translation pairs were presented in random order with no indication of whether a translation was produced by a human, a generic translation system, or an untrained translation sys- tem. Workers were asked to choose the better of the two translations or to indicate, "Neither is bet- ter". Workers were offered $2.00 to complete a 50- question survey. Each survey contained five hid- den questions with known answers (translation pairs judged by professional translators) for quality con- trol (we excluded responses from workers who did not answer the hidden questions with at least 80% accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BLEU Evaluation</head><p>We used the automated BLEU calculation provided by the third-party translation service to obtain scores for each of the three translation systems.</p><p>All three systems had significant BLEU improve- ments after retraining, as shown in <ref type="table" target="#tab_1">Table 2</ref>. We be-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trained</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic</head><p>Neither Ratio Sys. 1 129 (34%) 109 (29%) 138 (36%)</p><p>1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.18 Sys. 2 71 (25%) 85 (31%) 123 (44%) 0.84 Sys. 3 203 (36%) 150 (27%) 205 (37%)</head><p>1.35 <ref type="table">Table 3</ref>: Results from crowdsourced evaluations of three trans- lation systems. Columns labeled Trained, Generic, and Nei- ther include the number of responses and percentage of total responses for each response type. The Ratio column shows the number of responses that favored the trained system to the num- ber of responses that favored the generic system.</p><p>lieve System 3 has a lower BLEU score than the oth- ers because it was tuned on a different data set: the professionally-translated, in-domain sentences from product listing descriptions. This made the system's output less like the automatically-selected test set than the others, but closer, presumably, to the high- quality, low-noise tuning translations sourced from professional translators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Crowdsourced evaluation</head><p>The crowdsourced evaluation of the three systems favored System 3. <ref type="table">Table 3</ref> provides a summary of the results. Neither System 1 nor System 2 showed a significant difference between selection of transla- tions provided by the trained or untrained system: chi-squared tests did not detect a significant dif- ference between number of responses favoring the trained system and number of responses favoring the generic system (p = 0.1948 and p = 0.26, respec- tively, for the two systems). However, a chi-squared test indicated a significant preference for System 3, which was chosen 35% more often than the generic system (p = 0.0048). Based on the crowd-sourced results, we proceeded to A/B test System 3 against the generic translation system baseline. The lack of improvements for System 1 and Sys- tem 2 detected using the crowd-sourcing methods was somewhat surprising, given the large BLEU score improvements observed for all three systems. We believe this lends further support to <ref type="bibr">CallisonBurch, et al.'s (2006)</ref> critiques of BLEU as a stand- alone machine translation quality metric. In this case, it is possible that Systems 1 and 2 achieved high BLEU improvements due to over-fitting the training data from which the test set was drawn. We might speculate that this is due to the presence of low-quality translations from limited-bilingual sell- ers, or the presence of MT generated by a different online tool in some sellers' translations. By tun- ing the system using a high-quality, professionally- translated test set, we reduced overall BLEU but in- creased quality as judged by bilingual evaluators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A/B testing</head><p>A/B testing is a strategy for comparing two differ- ent versions of a website to see which one performs better. Traditionally, one of these experiences is the existing, A, control experience, and the other expe- rience is a new, B, variant experience. By randomly grouping users into one of the two experiences, and measuring the on-site behavior (e.g., clicks on a listing or items purchased) of each group, we can make data-driven decisions about whether new ex- periences are actually an improvement for our users. For our use case, the control experience is showing users content machine translated with the generic engine, and the variant experience is showing con- tent translated with the retrained engine. A/B test- ing allows us to answer the following question: will users who read a product description translated by a domain-customized translation engine be more or less likely to purchase a product?</p><p>To test the effects of the quality improvement ob- tained, we used our in-house automated A/B test- ing framework to compare the behavioral effects on users who translated text using the generic engine and those who translated using System 3. Visitors to the online marketplace were randomly "bucketed" into an experimental group or a control group. Ran- dom bucketing was achieved via a hash of a user's browser ID, which allows users who return to the site during the experimental period to be bucketed consistently across visits. For visitors who requested translations from English into French, the generic system's translations were displayed to visitors in the control group, and System 3 translations were displayed to visitors in the experimental group.</p><p>The experiment ran for 66 days for a total of 88,106 visitors (43,306 control and 44,800 experi- mental). The key metrics tracked were pages per visit (the number of pages seen in one user session), conversion rate (the percent of visits that include at least one purchase), and add-to-cart rate (the per- cent of visits in which a user adds an item to their shopping cart). We observed a significant positive Metric Trained engine Conversion rate +8.72% Visit add-to-cart rate +2.92% Pages per visit +3.37% effect of the trained system on all three metrics, as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Numerous studies have shown that automatic ma- chine translation quality estimates, such as BLEU, are correlated with human evaluations of translation quality. Our work shows that those improvements in translation quality can have a positive effect on user behavior in a commercial setting, as measured through conversion rate. These considerations sug- gest that, in domains where machine translation con- veys information upon which individuals base deci- sions, the effort needed to gather and process data to customize a machine translation system can be worthwhile. Additionally, our experiments show A/B testing can be a valuable tool to evaluate ma- chine translation quality. A/B testing goes beyond measuring the quality of translation improvements: it allows us to see the positive impact that quality im- provements are having on users' purchase behavior in a measurable way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example review translation on the website.</figDesc><graphic url="image-1.png" coords="2,72.00,57.82,481.90,151.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU score improvements for three translation sys-

tems over a baseline BLEU for the generic system of 38.34. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 : The trained translation system's (System 3) improve- ment over the generic engine on key business metrics. All dif-</head><label>4</label><figDesc></figDesc><table>ferences are statistically significant (p &lt; 0.05). Base rates are 

omitted for data privacy reasons. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>a 3.37% increase in pages per 
visit (p = 0.00153 95% CI [1.29, 5.46]), an 8.72% 
increase in purchase rate (p = 0.00513 95% CI 
[2.61, 14.82]), and a 2.92% increase in add-to-cart 
(p = 0.04689 95% CI [0.04, 5.8]). 

</table></figure>

			<note place="foot" n="1"> We use Microsoft&apos;s Bing Translator for our machine translations.</note>

			<note place="foot" n="2"> http://hub.microsofttranslator.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aligning sentences in parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 29th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Re-evaluating the role of BLEU in machine translation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast, cheap, and creative: evaluating translation quality using Amazon&apos;s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mechanical</forename><surname>Turk</surname></persName>
		</author>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aligning sentences in bilingual corpora using lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 31st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correlating automated and human assessments of machine translation quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Coughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit IX</title>
		<meeting>MT Summit IX</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A program for aligning sentences in bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="102" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A geometric approach to mapping bitext correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>I Dan Melamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the First Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving machine translation performance by exploiting non-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="504" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aligning a parallel English-Chinese corpus statistically with lexical criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
