<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Embeddings for Hypernymy Detection and Directionality</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">Anh</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Köper</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">Universität Stuttgart Pfaffenwaldring 5B</orgName>
								<address>
									<postCode>70569</postCode>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Embeddings for Hypernymy Detection and Directionality</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="233" to="243"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel neural model HyperVec to learn hierarchical em-beddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym-hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-the-art unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hypernymy represents a major semantic relation and a key organization principle of semantic mem- ory <ref type="bibr" target="#b21">(Miller and Fellbaum, 1991;</ref><ref type="bibr" target="#b23">Murphy, 2002)</ref>. It is an asymmetric relation between two terms, a hypernym (superordinate) and a hyponym (subor- diate), as in animal-bird and flower-rose, where the hyponym necessarily implies the hypernym, but not vice versa. From a computational point of view, automatic hypernymy detection is useful for NLP tasks such as taxonomy creation <ref type="bibr" target="#b38">(Snow et al., 2006;</ref><ref type="bibr" target="#b24">Navigli et al., 2011</ref>), recognizing tex- tual entailment ( <ref type="bibr" target="#b5">Dagan et al., 2013)</ref>, and text gen- eration <ref type="bibr" target="#b3">(Biran and McKeown, 2013</ref>), among many others.</p><p>Two families of approaches to identify and dis- criminate hypernyms are predominent in NLP, both of them relying on word vector representa- tions. Distributional count approaches make use of either directionally unsupervised measures or of supervised classification methods. Unsupervised measures exploit the distributional inclusion hy- pothesis <ref type="bibr" target="#b10">(Geffet and Dagan, 2005;</ref><ref type="bibr">ZhitomirskyGeffet and Dagan, 2009)</ref>, or the distributional informativeness hypothesis ( <ref type="bibr" target="#b31">Santus et al., 2014;</ref><ref type="bibr" target="#b28">Rimell, 2014</ref>). These measures assign scores to semantic relation pairs, and hypernymy scores are expected to be higher than those of other relation pairs. Typically, Average Precision (AP) <ref type="bibr" target="#b13">(Kotlerman et al., 2010</ref>) is applied to rank and distinguish between the predicted relations. Supervised clas- sification methods represent each pair of words as a single vector, by using the concatenation or the element-wise difference of their vectors <ref type="bibr" target="#b0">(Baroni et al., 2012;</ref><ref type="bibr" target="#b29">Roller et al., 2014;</ref><ref type="bibr" target="#b45">Weeds et al., 2014</ref>). The resulting vector is fed into a Sup- port Vector Machine (SVM) or into Logistic Re- gression (LR), to predict hypernymy. Across ap- proaches, <ref type="bibr" target="#b36">Shwartz et al. (2017)</ref> demonstrated that there is no single unsupervised measure which consistently deals well with discriminating hyper- nymy from other semantic relations. Furthermore, <ref type="bibr" target="#b17">Levy et al. (2015)</ref> showed that supervised meth- ods memorize prototypical hypernyms instead of learning a relation between two words.</p><p>Approaches of hypernymy-specific embed- dings utilize neural models to learn vector rep- resentations for hypernymy. <ref type="bibr" target="#b50">Yu et al. (2015)</ref> proposed a supervised method to learn term em- beddings for hypernymy identification, based on pre-extracted hypernymy pairs. Recently, <ref type="bibr" target="#b40">Tuan et al. (2016)</ref> proposed a dynamic weighting neu- ral model to learn term embeddings in which the model encodes not only the information of hy- pernyms vs. hyponyms, but also their contextual information. The performance of this family of models is typically evaluated by using an SVM to discriminate hypernymy from other relations.</p><p>In this paper, we propose a novel neural model HyperVec to learn hierarchical embeddings that (i) discriminate hypernymy from other relations (detection task), and (ii) distinguish between the hypernym and the hyponym in a given hypernymy relation pair (directionality task). Our model learns to strengthen the distributional similarity of hypernym pairs in comparison to other relation pairs, by moving hyponym and hypernym vectors close to each other. In addition, we generate a dis- tributional hierarchy between hyponyms and hy- pernyms. Relying on these two new aspects of hy- pernymy distributions, the similarity of hypernym pairs receives higher scores than the similarity of other relation pairs; and the distributional hierar- chy of hyponyms and hypernyms indicates the di- rectionality of hypernymy.</p><p>Our model is inspired by the distributional in- clusion hypothesis, that prominent context words of hyponyms are expected to appear in a subset of the hypernym contexts. We assume that each con- text word which appears with both a hyponym and its hypernym can be used as an indicator to deter- mine which of the two words is semantically more general: Common context word vectors which represent distinctive characteristics of a hyponym are expected to be closer to the hyponym vector than to its hypernym vector. For example, the con- text word flap is more characteristic for a bird than for its hypernym animal; hence, the vector of flap should be closer to the vector of bird than to the vector of animal.</p><p>We evaluate our HyperVec model on both un- supervised and supervised hypernymy detection and directionality tasks. In addition, we apply the model to the task of graded lexical entail- ment <ref type="bibr" target="#b44">(Vuli´cVuli´c et al., 2016)</ref>, and we assess the capa- bility of HyperVec on generalizing hypernymy by mapping to German and Italian. Results on bench- mark datasets of hypernymy show that the hi- erarchical embeddings outperform state-of-the-art measures and previous embedding models. Fur- thermore, the implementation of our models is made publicly available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised hypernymy measures: A vari- ety of directional measures for unsupervised hy- pernymy detection <ref type="bibr" target="#b46">(Weeds and Weir, 2003;</ref><ref type="bibr" target="#b47">Weeds et al., 2004;</ref><ref type="bibr" target="#b4">Clarke, 2009;</ref><ref type="bibr" target="#b13">Kotlerman et al., 2010;</ref><ref type="bibr"></ref> 1 www.ims.uni-stuttgart.de/data/hypervec <ref type="bibr" target="#b15">Lenci and Benotto, 2012</ref>) all rely on some varia- tion of the distributional inclusion hypothesis: If u is a semantically narrower term than v, then a significant number of salient distributional fea- tures of u is expected to be included in the fea- ture vector of v as well. In addition, <ref type="bibr" target="#b31">Santus et al. (2014)</ref> proposed the distributional informa- tiveness hypothesis, that hypernyms tend to be less informative than hyponyms, and that they oc- cur in more general contexts than their hyponyms. All of these approaches represent words as vec- tors in distributional semantic models <ref type="bibr" target="#b41">(Turney and Pantel, 2010)</ref>, relying on the distributional hy- pothesis <ref type="bibr" target="#b11">(Harris, 1954;</ref><ref type="bibr" target="#b9">Firth, 1957)</ref>. For evalua- tion, these directional models use the AP measure to assess the proportion of hypernyms at the top of a score-sorted list. In a different vein, <ref type="bibr" target="#b12">Kiela et al. (2015)</ref> introduced three unsupervised meth- ods drawn from visual properties of images to de- termine a concept's generality in hypernymy tasks.</p><p>Supervised hypernymy methods: The studies in this area are based on word embeddings which represent words as low-dimensional and real- valued vectors ( <ref type="bibr" target="#b19">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b27">Pennington et al., 2014</ref>). Each hypernymy pair is encoded by some combination of the two word vectors, such as concatenation ( <ref type="bibr" target="#b0">Baroni et al., 2012</ref>) or dif- ference ( <ref type="bibr" target="#b29">Roller et al., 2014;</ref><ref type="bibr" target="#b45">Weeds et al., 2014)</ref>. Hypernymy is distinguished from other relations by using a classification approach, such as SVM or LR. Because word embeddings are trained for similar and symmetric vectors, it is however un- clear whether the supervised methods do actually learn the asymmetry in hypernymy ( <ref type="bibr" target="#b17">Levy et al., 2015)</ref>.</p><p>Hypernymy-specific embeddings: These ap- proaches are closest to our work. <ref type="bibr" target="#b50">Yu et al. (2015)</ref> proposed a dynamic distance-margin model to learn term embeddings that capture properties of hypernymy. The neural model is trained on the taxonomic relation data which is pre-extracted. The resulting term embeddings are fed to an SVM classifier to predict hypernymy. However, this model only learns term pairs without consider- ing their contexts, leading to a lack of general- ization for term embeddings. <ref type="bibr" target="#b40">Tuan et al. (2016)</ref> introduced a dynamic weighting neural network to learn term embeddings that encode information about hypernymy and also about their contexts, considering all words between a hypernym and its hyponym in a sentence. The proposed model is trained on a set of hypernym relations extracted from WordNet <ref type="bibr" target="#b20">(Miller, 1995)</ref>. The embeddings are applied as features to detect hypernymy, us- ing an SVM classifier. <ref type="bibr" target="#b40">Tuan et al. (2016)</ref> han- dles the drawback of the approach by <ref type="bibr" target="#b50">Yu et al. (2015)</ref>, considering the contextual information be- tween two terms; however the method still is not able to determine the directionality of a hypernym pair. <ref type="bibr" target="#b42">Vendrov et al. (2016)</ref> proposed a method to encode order into learned distributed representa- tions, to explicitly model partial order structure of the visual-semantic hierarchy or the hierarchy of hypernymy in WordNet. The resulting vectors are used to predict the transitive hypernym relations in WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Embeddings</head><p>In this section, we present our model of hierar- chical embeddings HyperVec. Section 3.1 de- scribes how we learn the embeddings for hyper- nymy, and Section 3.2 introduces the unsupervised measure HyperScore that is applied to the hyper- nymy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Hierarchical Embeddings</head><p>Our approach makes use of a set of hypernyms which could be obtained from either exploiting the transitivity of the hypernymy relation <ref type="bibr" target="#b7">(Fallucchi and Zanzotto, 2011</ref>) or lexical databases, to learn hierarchical embeddings. We rely on Word- Net, a large lexical database of English <ref type="bibr">(Fellbaum, 1998)</ref>, and extract all hypernym-hyponym pairs for nouns and for verbs, including both direct and indirect hypernymy, e.g., animal-bird, bird- robin, animal-robin. Before training our model, we exclude all hypernym pairs which appear in any datasets used for evaluation.</p><p>In the following, Section 3.1.1 first describes the Skip-gram model which is integrated into our model for optimization. Section 3.1.2 then de- scribes the objective functions to train the hierar- chical embeddings for hypernymy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Skip-gram Model</head><p>The Skip-gram model is a word embed- dings method suggested by <ref type="bibr" target="#b19">Mikolov et al. (2013b)</ref>. <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> in- troduced a variant of the Skip-gram model with negative sampling (SGNS), in which the objective function is defined as follows:</p><formula xml:id="formula_0">J SGN S = w∈V W c∈V C J (w,c) (1) J (w,c) = #(w, c) log σ( w, c) + k · E c N ∼P D [log σ(− w, c N )] (2)</formula><p>where the skip-gram with negative sampling is trained on a corpus of words w ∈ V W and their contexts c ∈ V C , with V W and V C the word and context vocabularies, respectively. The collection of observed words and context pairs is denoted as D; the term #(w, c) refers to the number of times the pair (w, c) appeared in D; the term σ(x) is the sigmoid function; the term k is the number of negative samples and the term c N is the sampled context, drawn according to the empirical unigram distribution P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Hierarchical Hypernymy Model</head><p>Vector representations for detecting hypernymy are usually encoded by standard first-order dis- tributional co-occurrences. In this way, they are insufficient to differentiate hypernymy from other paradigmatic relations such as synonymy, meronymy, antonymy, etc. Incorporating direc- tional measures of hypernymy to detect hyper- nymy by exploiting the common contexts of hy- pernym and hyponym improves this relation dis- tinction, but still suffers from distinguishing be- tween hypernymy and meronymy. Our novel approach presents two solutions to deal with these challenges. First of all, the embed- dings are learned in a specific order, such that the similarity score for hypernymy is higher than the similarity score for other relations. For example, the hypernym pair animal-frog will be assigned a higher cosine score than the co-hyponymy pair eagle-frog. Secondly, the embeddings are learned to capture the distributional hierarchy between hy- ponym and hypernym, as an indicator to differ- entiate between hypernym and hyponym. For ex- ample, given a hyponym-hypernym pair (p, q), we can exploit the Euclidean norms of q and p to dif- ferentiate between the two words, such that the Euclidean norm of the hypernym q is larger than the Euclidean norm of the hyponym p. Inspired by the distributional lexical contrast model in <ref type="bibr" target="#b25">Nguyen et al. (2016)</ref> for distinguishing antonymy from synonymy, this paper proposes two objective functions to learn hierarchical embeddings for hypernymy.</p><p>Before moving to the details of the two objective functions, we first define the terms as follows: W(c) refers to the set of words co-occurring with the context c in a certain window-size; H(w) denotes the set of hypernyms for the word w; the two terms H + (w, c) and H − (w, c) are drawn from H(w), and are defined as follows:</p><formula xml:id="formula_1">H + (w, c) = {u ∈ W(c) ∩ H(w) : cos( w, c) − cos( u, c) ≥ θ} H − (w, c) = {v ∈ W(c) ∩ H(w) : cos( w, c) − cos( v, c) &lt; θ}</formula><p>where cos( x, y) stands for the cosine similarity of the two vectors x and y; θ is the margin.</p><p>The set H + (w, c) contains all hypernyms of the word w that share the context c and satisfy the constraint that the cosine similarity of pair (w, c) is higher than the cosine similarity of pair (u, c) within a max-margin framework θ. Similarly, the set H − (w, c) represents all hypernyms of the word w with respect to the common context c in which the cosine similarity difference between the pair (w, c) and the pair (v, c) is within a min-margin framework θ. The two objective functions are defined as follows:</p><formula xml:id="formula_2">L (w,c) = 1 #(w, u) u∈H + (w,c) ∂( w, u) (3) L (v,w,c) = v∈H − (w,c) ∂( v, w)<label>(4)</label></formula><p>where the term ∂( x, y) stands for the cosine derivative of ( x, y); and ∂ then is optimized by the negative sampling procedure.</p><p>The objective function in Equation 3 minimizes the distributional difference between the hyponym w and the hypernym u by exploiting the common context c. More specifically, if the common con- text c is the distinctive characteristic of the hy- ponym w (i.e. the common context c is closer to the hyponym w than to the hypernym u), the objective function L (w,c) tries to decrease the dis- tributional generality of hypernym u by moving w closer to u. For example, given a hypernym- hyponym pair animal-bird, the context flap is a distinctive characteristic of bird, because almost every bird can flap, but not every animal can flap. Therefore, the context flap is closer to the hy- ponym bird than to the hypernym animal. The model then tries to move bird closer to animal in order to enforce the similarity between bird and animal, and to decrease the distributional general- ity of animal.</p><p>In contrast to Equation 3, the objective function in Equation 4 minimizes the distributional differ- ence between the hyponym w and the hypernym v by exploiting the common context c, which is a distinctive characteristic of the hypernym v. In this case, the objective function L (v,w,c) tries to re- duce the distributional generality of hyponym w by moving v closer to w. For example, the con- text word rights, a distinctive characteristic of the hypernym animal, should be closer to animal than to bird. Hence, the model tries to move the hy- pernym animal closer to the hyponym bird. Given that hypernymy is an asymmetric and also a hier- archical relation, where each hypernym may con- tain several hyponyms, our objective functions up- dates simultaneously both the hypernym and all of its hyponyms; therefore, our objective functions are able to capture the hierarchical relations be- tween the hypernym and its hyponyms. Moreover, in our model, the margin framework θ plays a role in learning the hierarchy of hypernymy, and in pre- venting the model from minimizing the distance of synonymy or antonymy, because synonymy and antonymy share many contexts.</p><p>In the final step, the objective function which is used to learn the hierarchical embeddings for hypernymy combines Equations 1, 2, 3, and 4 by the objective function in Equations 5 and 6:</p><formula xml:id="formula_3">J (w,v,c) = J (w,c) + L (w,c) + L (v,w,c)<label>(5)</label></formula><formula xml:id="formula_4">J = w∈V W c∈V C J (w,v,c)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Hypernymy Measure</head><p>HyperVec is expected to show the two following properties: (i) the hyponym and the hypernym are close to each other, and (ii) there exists a distribu- tional hierarchy between hypernyms and their hy- ponyms. Given a hypernymy pair (u, v) in which u is the hyponym and v is the hypernym, we pro- pose a measure to detect hypernymy and to deter- mine the directionality of hypernymy by using the hierarchical embeddings as follows:</p><formula xml:id="formula_5">HyperScore(u, v) = cos( u, v) * v u<label>(7)</label></formula><p>where cos( u, v) is the cosine similarity between u and v, and · is the magnitude of the vector (or the Euclidean norm). The cosine similarity is applied to distinguish hypernymy from other re-lations, due to the first property of the hierarchi- cal embeddings, while the second property is used to decide about the directionality of hypernymy, assuming that the magnitude of the hypernym is larger than the magnitude of the hyponym. Note that the proposed hypernymy measure is unsuper- vised when the resource is only used to learn hier- archical embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe the experimental settings in our experiments (Section 4.1). We then evaluate the performance of HyperVec on three different tasks: i) unsupervised hypernymy detec- tion and directionality (Section 4.2), where we as- sess HyperVec on ranking and classifying hyper- nymy; ii) supervised hypernymy detection (Sec- tion 4.3), where we apply supervised classification to detect hypernymy; iii) graded lexical entailment (Section 4.4), where we predict the strength of hy- pernymy pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We use the ENCOW14A corpus <ref type="bibr" target="#b34">(Schäfer and Bildhauer, 2012;</ref><ref type="bibr" target="#b33">Schäfer, 2015</ref>) with approx. 14.5 billion tokens for training the hierarchi- cal embeddings and the default SGNS model. We train our model with 100 dimensions, a window size of 5, 15 negative samples, and 0.025 as the learning rate. The threshold θ is set to 0.05. The hypernymy resource for nouns comprises 105, 020 hyponyms, 24, 925 hypernyms, and 1, 878, 484 hyponym-hypernym pairs. The hypernymy resource for verbs con- sists of 11, 328 hyponyms, 4, 848 hypernyms, and 130, 350 hyponym-hypernym pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Hypernymy Detection and Directionality</head><p>In this section, we assess our model on two exper- imental setups: i) a ranking retrieval setup that ex- pects hypernymy pairs to have a higher similarity score than instances from other semantic relations; ii) a classification setup that requires both hyper- nymy detection and directionality.     presents the results of using HyperScore vs. the best baseline models, across datasets. When detecting hypernymy among all other relations (which is the most challenging task), HyperScore significantly outperforms all baseline variants on all datasets. The strongest difference is reached on the BLESS dataset, where HyperScore achieves an improvement of 40% AP score over the best baseline model. When ranking hypernymy in comparison to a single other relation, HyperScore also improves over the baseline models, except for the event relation in the BLESS dataset. We assume that this is due to the different parts-of- speech (adjective and noun) involved in the rela- tion, where HyperVec fails to establish a hierar- chy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ranking Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Classification</head><p>In this setup, we rely on three datasets of se- mantic relations, which were all used in various state-of-the-art approaches before, and brought to- gether for hypernymy evaluation by <ref type="bibr" target="#b12">Kiela et al. (2015)</ref>. (i) A subset of BLESS contains 1,337 hyponym-hypernym pairs. The task is to predict the directionality of hypernymy within a binary classification. Our approach requires no thresh- old; we only need to compare the magnitudes of the two words and to assign the hypernym la- bel to the word with the larger magnitude.  versed hypernym-hyponym pairs, plus additional holonym-meronym pairs, co-hyponyms and ran- domly matched nouns. For this classification we make use of our HyperScore measure that ranks hypernymy pairs higher than other relation pairs. A threshold decides about the splitting point be- tween the two classes: hyper vs. other. Instead of using a manually defined threshold as done by <ref type="bibr" target="#b12">Kiela et al. (2015)</ref>, we decided to run 1 000 iter- ations which randomly sampled only 2% of the available pairs for learning a threshold, using the remaining 98% for test purposes. We present av- erage accuracy results across all iterations. <ref type="figure" target="#fig_1">Fig- ure 1b</ref> compares the default cosine similarities be- tween the relation pairs (as applied by SGNS ) and HyperScore (as applied by HyperVec) on this task. Using HyperScore, the class "hyper" can clearly be distinguished from the class "other".</p><p>(iii) BIBLESS represents the most challenging dataset; the relation pairs from WBLESS are split into three classes instead of two: hypernymy pairs, reversed hypernymy pairs, and other relation pairs. In this case, we perform a three-way classification. We apply the same technique as used for the WB- LESS classification, but in cases where we clas- sify hyper we additionally classify the hypernymy direction, to decide between hyponym-hypernym pairs and reversed hypernym-hyponym pairs. <ref type="table" target="#tab_3">Table 3</ref> compares our results against related work. HyperVec outperforms all other methods on all three tasks. In addition we see again that an unmodified SGNS model cannot solve any of the three tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervised Hypernymy Detection</head><p>For supervised hypernymy detection, we make use of the two datasets: the full BLESS dataset, and ENTAILMENT ( <ref type="bibr" target="#b0">Baroni et al., 2012</ref>), contain- ing 2,770 relation pairs in total, including 1,385 hypernym pairs and 1,385 other relations pairs. We follow the same procedure as <ref type="bibr" target="#b50">Yu et al. (2015)</ref> and <ref type="bibr" target="#b40">Tuan et al. (2016)</ref> to assess HyperVec on the two datasets. Regarding BLESS, we extract pairs for four types of relations: hypernymy, meronymy, co-hyponymy (or coordination), and add the ran- dom relation for nouns. For the evaluation, we ran- domly select one concept and its relatum for test- ing, and train the supervised model on the 199 re- maining concepts and its relatum. We then report the average accuracy across all concepts. For the ENTAILMENT dataset, we randomly select one hypernym pair for testing and train on all remain- ing hypernym pairs. Again, we report the average accuracy across all hypernyms. We apply an SVM classifier to detect hyper- nymy based on HyperVec. Given a hyponym- hypernym pair (u, v), we concatenate four compo- nents to construct the vector for a pair (u, v) as fol- lows: the vector difference between hypernym and hyponym ( v− u); the cosine similarity between the hypernym and hyponym vectors (cos( u, v)); the magnitude of the hyponym ( u); and the magni- tude of the hypernym ( v). The resulting vector is fed into the SVM classifier to detect hypernymy. Similar to the two previous works, we train the SVM classifier with the RBF kernel, λ = 0.03125, and the penalty C = 8.0. <ref type="table">Table 4</ref> shows the performance of HyperVec and the two baseline models reported by <ref type="bibr" target="#b40">Tuan et al. (2016)</ref>. HyperVec slightly outperforms the method of <ref type="bibr" target="#b40">Tuan et al. (2016)</ref> on the BLESS dataset, and is equivalent to the performance of their method on the ENTAILMENT dataset. In comparison to the method of <ref type="bibr" target="#b50">Yu et al. (2015)</ref>, HyperVec achieves significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Graded Lexical Entailment</head><p>In this experiment, we apply HyperVec to the dataset of graded lexical entailment, HyperLex, as introduced by Vuli´c <ref type="bibr" target="#b44">Vuli´c et al. (2016)</ref>. The HyperLex dataset provides soft lexical entailment on a con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLESS ENTAILMENT</head><p>Yu et al. <ref type="formula" target="#formula_3">(2015)</ref> 0.90 0.87 <ref type="bibr" target="#b40">Tuan et al. (2016)</ref> 0.93 0.91 HyperVec 0.94 0.91 <ref type="table">Table 4</ref>: Classification results for BLESS and EN- TAILMENT in terms of accuracy.</p><p>tinuous scale, rather than simplifying into a bi- nary decision. HyperLex contains 2,616 word pairs across seven semantic relations and two word classes (nouns and verbs). Each word pair is rated by a score that indicates the strength of the seman- tic relation between the two words. For example, the score of the hypernym pair duck-animal is 5.9 out of 6.0, while the score of the reversed pair animal-duck is only 1.0. We compared HyperScore against the most prominent state-of-the-art hypernymy and lexical entailment models from previous work:</p><p>• Directional entailment measures (DEM) ( <ref type="bibr" target="#b46">Weeds and Weir, 2003;</ref><ref type="bibr" target="#b47">Weeds et al., 2004;</ref><ref type="bibr" target="#b4">Clarke, 2009;</ref><ref type="bibr" target="#b13">Kotlerman et al., 2010;</ref><ref type="bibr" target="#b15">Lenci and Benotto, 2012)</ref> • Generality measures (SQLS) ( <ref type="bibr" target="#b31">Santus et al., 2014)</ref> • Visual generality measures (VIS) ( <ref type="bibr" target="#b12">Kiela et al., 2015)</ref> • Consideration of concept frequency ratio (FR) <ref type="bibr" target="#b44">(Vuli´cVuli´c et al., 2016)</ref> • WordNet-based similarity measures (WN) ( <ref type="bibr" target="#b48">Wu and Palmer, 1994;</ref><ref type="bibr" target="#b26">Pedersen et al., 2004)</ref> • Order embeddings (OrderEmb) ( <ref type="bibr" target="#b42">Vendrov et al., 2016)</ref> • Skip-gram embeddings (SGNS) ( <ref type="bibr" target="#b19">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b16">Levy and Goldberg, 2014)</ref> • Embeddings fine-tuned to a paraphrase database with linguistic constraints (PARA- GRAM) <ref type="bibr" target="#b22">(Mrkši´Mrkši´c et al., 2016)</ref> • Gaussian embeddings (Word2Gauss) ( <ref type="bibr" target="#b43">Vilnis and McCallum, 2015)</ref> The performance of the models is assessed through Spearman's rank-order correlation coeffi- cient ρ ( <ref type="bibr" target="#b37">Siegel and Castellan, 1988)</ref>, comparing the ranks of the models' scores and the human judgments for the given word pairs.  <ref type="table">Table 5</ref>: Results (ρ) of HyperScore and state-of- the-art measures and word embedding models on graded lexical entailment. <ref type="table">Table 5</ref> shows that HyperScore significantly outperforms both state-of-the-art measures and word embedding models.</p><p>HyperScore out- performs even the previously best word em- bedding model PARAGRAM by .22, and the previously best measures FR by .27. The reason that HyperVec outperforms all other models is that the hierarchy between hyper- nym and hypornym within HyperVec differenti- ates hyponym-hypernym pairs from hypernym- hyponym pairs. For example, the HyperScore for the pairs duck-animal and animal-duck are 3.02 and 0.30, respectively. Thus, the magnitude proportion of the hypernym-hyponym pair duck- animal is larger than that for the pair animal-duck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Generalizing Hypernymy</head><p>Having demonstrated the general abilities of HyperVec, this final section explores its potential for generalization in two different ways, (i) by re- lying on a small seed set only, rather than using a large set of training data; and (ii) by projecting HyperVec to other languages.</p><p>Hypernymy Seed Generalization: We utilize only a small hypernym set from the hypernymy resource to train HyperVec, relying on 200 con- cepts from the BLESS dataset. The motivation behind using these concepts is threefold: i) these concepts are distinct and unambiguous noun con- cepts; ii) the concepts were equally divided be- tween living and non-living entities; iii) concepts have been grouped into 17 broader classes. Based on the seed set, we collected the hyponyms of each concept from WordNet, and then re-trained HyperVec. On the hypernymy ranking retrieval task (Section 4.2.1), HyperScore outperforms the baselines across all datasets (cf. <ref type="table" target="#tab_1">Table 1)</ref> with AP values of 0.39, 0.448, and 0.585 for EVALu- tion, LenciBenotto, and Weeds, respectively. For the graded lexical entailment task (Section 4.4), HyperScore obtains a correlation of ρ = 0.30, outperforming all models except for PARAGRAM with ρ = 0.32. Overall, the results show that HyperVec is indeed able to generalize hypernymy from small seeds of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizing Hypernymy across Languages:</head><p>We assume that hypernymy detection can be im- proved across languages by projecting representa- tions from any arbitrary language into our modi- fied English HyperVec space. We conduct experi- ments for German and Italian, where the language- specific representations are obtained using the same hyper-parameter settings as for our English SGNS model (cf. Section 4.1). As corpus re- source we relied on Wikipedia dumps 2 . Note that we do not use any additional resource, such as the German or Italian WordNet, to tune the embeddings for hypernymy detection. Based on the representations, a mapping function between a source language (German, Italian) and our En- glish HyperVec space is learned, by relying on the least-squares error method from previous work us- ing cross-lingual data ( <ref type="bibr" target="#b18">Mikolov et al., 2013a</ref>) and different modalities ( <ref type="bibr" target="#b14">Lazaridou et al., 2015)</ref>.</p><p>To learn a mapping function between two lan- guages, a one-to-one correspondence (word trans- lations) between two sets of vectors is required. We obtained these translations by using the paral- lel Europarl 3 V7 corpus for German-English and Italian-English. Word alignment counts were ex- tracted using fast align <ref type="bibr" target="#b6">(Dyer et al., 2013</ref>). We then assigned each source word to the English word with the maximum number of alignments in the parallel corpus. We could match 25,547 pairs for DE→EN and 47,475 pairs for IT→EN.</p><p>Taking the aligned subset of both spaces, we as- sume that X is the matrix obtained by concatenat- ing all source vectors, and likewise Y is the matrix obtained by concatenating all corresponding En- glish elements. Applying the 2-regularized least- squares error objective can be described using the following equation:</p><formula xml:id="formula_6">ˆ W = argmin W∈R d1×d2 XW − Y + λW<label>(8)</label></formula><p>Although we learn the mapping only on a subset of aligned words, it allows us to project every word in a source vocabulary to its English HyperVec posi- tion by using W.</p><p>Finally we compare the original representa- tions and the mapped representation on the hy- pernymy ranking retrieval task (similar to Sec- tion 4.2.1). As gold resources we relied on Ger- man and Italian nouns pairs. For German we used the 282 German pairs collected via Ama- zon Mechanical Turk by <ref type="bibr" target="#b35">Scheible and Schulte im Walde (2014)</ref>. The 1,350 Italian pairs were col- lected via Crowdflower by <ref type="bibr" target="#b39">Sucameli (2015)</ref> in the same way. Both collections contain hypernymy, antonymy and synonymy pairs. As before, we evaluate the ranking by AP, and we compare the cosine of the unmodified default representations against the HyperScore of the projected represen- tations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>German</head><p>Hyp  The results are shown in <ref type="table" target="#tab_5">Table 6</ref>. We clearly see that for both languages the default SGNS em- beddings do not provide higher similarity scores for hypernymy pairs (except for Italian Hyp/Ant), but both languages provide higher scores when we map the embeddings into the English HyperVec space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper proposed a novel neural model HyperVec to learn hierarchical embeddings for hypernymy.</p><p>HyperVec has been shown to strengthen hypernymy similarity, and to capture the distributional hierarchy of hypernymy. To- gether with a newly proposed unsupervised mea- sure HyperScore our experiments demonstrated (i) significant improvements against state-of-the- art measures, and (ii) the capability to generalize hypernymy and learn the relation instead of mem- orizing prototypical hypernyms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Hypernymy detection: hypernymy vs. other relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing SGNS and HyperVec on binary classification tasks. The y-axis shows the magnitude values of the vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig- ure 1a indicates that the magnitude values of the SGNS model cannot distinguish between a hy- ponym and a hypernym, while the hierarchical em- beddings provide a larger magnitude for the hyper- nym. (ii) Following Weeds et al. (2014), we con- duct a binary classification with a subset of 1,168 BLESS word pairs. In this dataset (WBLESS), one class is represented by hyponym-hypernym pairs, and the other class is a combination of re-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Details of the semantic relations and the number of instances in each dataset.</figDesc><table>Dataset 
Hypernymy vs. Baseline HyperScore 

EVALution 

other relations 
0.353 
0.538 
meronymy 
0.675 
0.811 
attribute 
0.651 
0.800 
antonymy 
0.55 
0.743 
synonymy 
0.657 
0.793 

BLESS 

other relations 
0.051 
0.454 
meronymy 
0.76 
0.913 
coordination 
0.537 
0.888 
attribute 
0.74 
0.918 
event 
0.779 
0.620 

Lenci&amp;Benotto 
other relations 
0.382 
0.574 
antonymy 
0.624 
0.696 
synonymy 
0.725 
0.751 
Weeds 
coordination 
0.441 
0.850 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>AP results of HyperScore in comparison 
to state-of-the-art measures. 

Kotlerman et al., 2010; Lenci and Benotto, 2012; 
Santus et al., 2016). The evaluation was performed 
on four semantic relation datasets: BLESS (Ba-
roni and Lenci, 2011), WEEDS (Weeds et al., 
2004), EVALUTION (Santus et al., 2015), and 
LENCI&amp;BENOTTO (Benotto, 2015). Table 1 de-
scribes the detail of these datasets in terms of the 
semantic relations and the number of instances. 
The Average Precision (AP) ranking measure is 
used to evaluate the performance of the measures. 

In comparison to the state-of-the-art unsuper-
vised measures compared by Shwartz et al. (2017) 
(henceforth, baseline models), we apply our un-
supervised measure HyperScore (Equation 7) to 
rank hypernymy against other relations. Table 2 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Accuracy for hypernymy directionality.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>AP results across languages, comparing 
SGNS and the projected representations. 

</table></figure>

			<note place="foot" n="2"> The Wikipedia dump for German and Italian were both downloaded in January 2017. 3 http://www.statmt.org/europarl/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research was supported by the Ministry of Education and Training of the Socialist Republic of Vietnam (Scholarship 977/QD-BGDDT; Kim-Anh Nguyen), the DFG Collaborative Research Centre SFB 732 (Kim-Anh Nguyen, Maximilian Köper, Ngoc Thang Vu), and the DFG Heisen-berg Fellowship SCHU-2580/1 (Sabine Schulte im Walde). We would like to thank three anony-mous reviewers for their comments and sugges-tions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics (GEMS)</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics (GEMS)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distributional models for semantic relations: A study on hyponymy and antonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Pisa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classifying taxonomic relations between pairs of wikipedia articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of Sixth International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>eddings of Sixth International Joint Conference on Natural Language essing (IJCNLP)<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="788" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-theoretic semantics for natural language: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Geometrical Models of Natural Language Semantics (GEMS)</title>
		<meeting>the Workshop on Geometrical Models of Natural Language Semantics (GEMS)<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing Textual Entailment: Models and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zanzotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Simple, Fast, and Effective Reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>Atlanta, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive probabilistic taxonomy learning using singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Fallucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="94" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">WordNet-An Electronic Lexical Database. Language, Speech, and Communication</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Firth</surname></persName>
		</author>
		<title level="m">Papers in Linguistics 1934-51. Longmans</title>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Michigan, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploiting image generality for lexical entailment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying hypernyms in distributional semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Benotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval)</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation (SemEval)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
	<note>*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the 27th International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>eddings of the 27th International Conference on Advances in Neural Information essing Systems (NIPS)<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 26th International Conference on Advances in Neural Information Processing Systems (NIPS)<address><addrLine>Lake Tahoe, Nevada, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic networks of english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fellbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="197" to="229" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Big Book of Concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A graph-based algorithm for inducing lexical taxonomies from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI)<address><addrLine>Barcelona, Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1872" to="1877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Kim Anh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wordnet: : Similarity-measuring the relatedness of concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Michelizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence (AAAI)</title>
		<meeting>the 19th National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence (AAAI)<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1024" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised measure of word similarity: How to outperform cooccurrence and vector cosine in vsms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tin-Shing</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Conference on Artificial Intelligence AAAI)</title>
		<meeting>the Thirtieth Conference on Artificial Intelligence AAAI)<address><addrLine>Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4260" to="4261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine Schulte Im</forename><surname>Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frances</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications</title>
		<meeting>the 4th Workshop on Linked Data in Linguistics: Resources and Applications<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Processing and querying large web corpora with the COW14 architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Schäfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora</title>
		<meeting>the 3rd Workshop on Challenges in the Management of Large Corpora<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Lancaster</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building large corpora from the web using a new efficient tool chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Bildhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="486" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Database of Paradigmatic Semantic Relation Pairs for German Nouns, Verbs, and Adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silke</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing</title>
		<meeting>Workshop on Lexical and Grammatical Resources for Language Processing<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Schlechtweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL)<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nonparametric Statistics for the Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidney</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N. John</forename><surname>Castellan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>McGraw-Hill</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 21st Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Analisi computazionale delle relazioni semantiche: Uno studio della lingua italiana</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Sucameli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Pisa</orgName>
		</respStmt>
	</monogr>
<note type="report_type">B.s. thesis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning term embeddings for taxonomic relation identification using dynamic weighting neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See</forename><forename type="middle">Kiong</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="403" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From Frequency to Meaning: Vector Space Models of Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 4th International Conference on Learning Representations (ICLR)<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)<address><addrLine>California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hyperlex: A large-scale evaluation of graded lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Characterising measures of lexical distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING)<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1015" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd</title>
		<meeting>the 32nd</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting><address><addrLine>Las Cruces, New Mexico</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning term embeddings for hypernymy identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Conference on Artificial Intelligence (IJCAI)<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1390" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bootstrapping distributional feature vector quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Geffet</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="435" to="461" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
