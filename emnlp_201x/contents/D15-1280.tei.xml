<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, it is still a challenge task to model long texts, such as sentences and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neu-ral network to model long texts. MT-LSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words have been widely used in many natural language process- ing (NLP) tasks <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b30">Turian et al., 2010;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b1">Bengio et al., 2003)</ref>. Following this success, it is rising a sub- stantial interest to learn the distributed represen- tations of the continuous words, such as phrases, sentences, paragraphs and documents <ref type="bibr" target="#b23">(Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b28">Socher et al., 2013;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b17">Le and Mikolov, 2014;</ref>). The primary role of these mod- els is to represent the variable-length sentence or document as a fixed-length vector. A good rep- resentation of the variable-length text should fully capture the semantics of natural language.</p><p>Recently, the long short-term memory neural network (LSTM) <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>) has been applied successfully in many NLP tasks, such as spoken language understanding ( <ref type="bibr" target="#b33">Yao et al., 2014</ref>), sequence labeling (Chen et al., * Corresponding author 2015) and machine translation <ref type="bibr" target="#b29">(Sutskever et al., 2014)</ref>. LSTM is an extension of the recurrent neu- ral network (RNN) <ref type="bibr" target="#b9">(Elman, 1990)</ref>, which can cap- ture the long-term and short-term dependencies and is very suitable to model the variable-length texts. Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network <ref type="bibr" target="#b28">(Socher et al., 2013)</ref>. However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time. The long- term dependencies need to be transmitted one-by- one along the sequence. Some important features could be lost in transmission process. Besides, the error signal is also back-propagated one-by- one through multiple time steps in the training phase with back-propagation through time (BPTT) <ref type="bibr" target="#b32">(Werbos, 1990)</ref> algorithm. The learning efficiency could also be decreased for the long texts. For ex- ample, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document.</p><p>In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) to capture the valuable information with different timescales. In- spired by the works of <ref type="bibr" target="#b8">(El Hihi and Bengio, 1995)</ref> and <ref type="bibr" target="#b16">(Koutnik et al., 2014</ref>), we partition the hidden states of the standard LSTM into several groups. Each group is activated and updated at different time periods. The fast-speed groups keep the short-term memories, while the slow-speed groups keep the long-term memories. We evaluate our model on four benchmark datasets of text classifi- cation. Experimental results show that our model can not only handle short texts, but can model long texts.</p><p>Our contributions can be summarized as fol- lows.</p><p>• With the multiple different timescale memo- ries, MT-LSTM easily carries the crucial in- formation over a long distance. MT-LSTM can well model both short and long texts.</p><p>• MT-LSTM has faster convergence speed than the standard LSTM since the error signal can be back-propagated through multiple timescales in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Models for Sentences and Documents</head><p>The primary role of the neural models is to repre- sent the variable-length sentence or document as a fixed-length vector. These models generally con- sist of a projection layer that maps words, sub- word units or n-grams to vector representations (often trained beforehand with unsupervised meth- ods), and then combine them with the different architectures of neural networks. Most of these models for distributed representations of sentences or documents can be classified into four cate- gories.</p><p>Bag-of-words models A simple and intuitive method is the Neural Bag-of-Words (NBOW) model, in which the representation of sentences or documents can be generated by averaging con- stituent word representations. However, the main drawback of NBOW is that the word order is lost. Although NBOW is effective for general docu- ment classification, it is not suitable for short sen- tences.</p><p>Sequence models Sequence models construct the representation of sentences or documents based on the recurrent neural network (RNN) ( <ref type="bibr" target="#b20">Mikolov et al., 2010)</ref> or the gated versions of <ref type="bibr">RNN (Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Chung et al., 2014</ref>). Sequence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at lan- guage modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. <ref type="bibr" target="#b17">Le and Mikolov (2014)</ref> proposed a Paragraph Vec- tor (PV) to learn continuous distributed vector rep- resentations for pieces of texts, which can be re- garded as a long-term memory of sentences as op- posed to the short-memory in RNN.</p><p>Topological models Topological models com- pose the sentence representation following a given topological structure over the words <ref type="bibr" target="#b25">(Socher et al., 2011a;</ref><ref type="bibr" target="#b27">Socher et al., 2012;</ref><ref type="bibr" target="#b28">Socher et al., 2013)</ref>. Recursive neural network (RecNN) adopts a more general structure to encode sentence <ref type="bibr" target="#b24">(Pollack, 1990;</ref><ref type="bibr" target="#b28">Socher et al., 2013)</ref>. At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree.</p><p>Convolutional models Convolutional neural network (CNN) is also used to model sentences <ref type="bibr" target="#b5">(Collobert et al., 2011;</ref><ref type="bibr" target="#b13">Hu et al., 2014</ref>). It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. CNN can main- tain the word order information and learn more abstract characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Long Short-Term Memory Networks</head><p>A recurrent neural network (RNN) <ref type="bibr" target="#b9">(Elman, 1990)</ref> is able to process a sequence of arbitrary length by recursively applying a transition function to its in- ternal hidden state vector h t of the input sequence. The activation of the hidden state h t at time-step t is computed as a function f of the current input symbol x t and the previous hidden state h t−1</p><formula xml:id="formula_0">h t = { 0 t = 0 f (h t−1 , x t ) otherwise (1)</formula><p>It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both x t and h t−1 .</p><p>Traditionally, a simple strategy for modeling se- quence is to map the input sequence to a fixed- sized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks ( <ref type="bibr" target="#b29">Sutskever et al., 2014;</ref>).</p><p>Unfortunately, a problem with RNNs with tran- sition functions of this form is that during training, components of the gradient vector can grow or de- cay exponentially over long sequences <ref type="bibr" target="#b0">(Bengio et al., 1994;</ref><ref type="bibr" target="#b12">Hochreiter et al., 2001;</ref><ref type="bibr" target="#b11">Hochreiter and Schmidhuber, 1997</ref>). This problem with explod- ing or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. Long short-term memory network (LSTM) was proposed by <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997)</ref> to specifically address this issue of learning long- term dependencies. The LSTM maintains a sepa- rate memory cell inside it that updates and exposes its content only when deemed necessary. A num- ber of minor modifications to the standard LSTM unit have been made. While there are numerous LSTM variants, here we describe the implementa- tion used by <ref type="bibr" target="#b10">Graves (2013)</ref>.</p><p>We define the LSTM units at each time step t to be a collection of vectors in R d : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . d is the number of the LSTM units. The entries of the gating vectors i t , f t and o t are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. The LSTM transition equations are the following:</p><formula xml:id="formula_1">i t = σ(W i x t + U i h t−1 + V i c t−1 ) (2) f t = σ(W f x t + U f h t−1 + V f c t−1 ),<label>(3)</label></formula><formula xml:id="formula_2">o t = σ(W o x t + U o h t−1 + V o c t ),<label>(4)</label></formula><formula xml:id="formula_3">˜ c t = tanh(W c x t + U c h t−1 ),<label>(5)</label></formula><formula xml:id="formula_4">c t = f i t ⊙ c t−1 + i t ⊙ ˜ c t ,<label>(6)</label></formula><formula xml:id="formula_5">h t = o t ⊙ tanh(c t ),<label>(7)</label></formula><p>where x t is the input at the current time step, σ de- notes the logistic sigmoid function and ⊙ denotes elementwise multiplication. Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. <ref type="figure" target="#fig_0">Figure 1</ref> shows the structure of a LSTM unit. In particular, these gates and the memory cell allow a LSTM unit to adaptively forget, memorize and ex- pose the memory content. If the detected feature, i.e., the memory content, is deemed important, the forget gate will be closed and carry the memory content across many time-steps, which is equiva- lent to capturing a long-term dependency. On the other hand, the unit may decide to reset the mem- ory content by opening the forget gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multi-Timescale Long Short-Term</head><p>Memory Neural Network</p><formula xml:id="formula_6">h1 h2 h3 h4 · · · hT softmax x1 x2 x3 x4 xT y (a) Unfolded LSTM g 3 1 g 3 2 g 3 3 g 3 4 · · · g 3 T g 2 1 g 2 2 g 2 3 g 2 4 · · · g 2 T softmax g 1 1 g 1 2 g 1 3 g 1 4 · · · g 1 T y x1 x2 x3 x4 xT</formula><p>(b) Unfolded MT-LSTM with Fast-to-Slow Feedback Strategy <ref type="figure">Figure 2</ref>: Illustration of the unfolded LSTM and unfolded MT-LSTM. The dotted node indicates the unit which is inactivated at current time, while the solid node indicates the unit which is activated. The dotted lines indicate the units which kept un- changed, while the solid lines indicate the units which will be updated at the next time step.</p><p>LSTM can capture the long-term and short-term dependencies in a sequence. But the long-term dependencies need to be transmitted one-by-one along the sequence. Some important informa- tion could be lost in transmission process for long texts, such as documents. Besides, the error sig- nal is back-propagated through multiple time steps when we use the back-propagation through time (BPTT) <ref type="bibr" target="#b32">(Werbos, 1990)</ref> algorithm. The training efficiency could also be low for the long texts. For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document.</p><p>Inspired by the works of (El Hihi and Bengio, 1995) and ( <ref type="bibr" target="#b16">Koutnik et al., 2014</ref>), which use de-layed connections and units operating at different timescales to improve the simple RNN, we sepa- rate the LSTM units into several groups. Different groups capture different timescales dependencies.</p><p>More formally, the LSTM units are parti- tioned into g groups</p><formula xml:id="formula_7">{G 1 , · · · , G g }. Each group G k , (1 ≤ k ≤ g</formula><p>) is activated at different time pe- riods T k . Accordingly, the gates and weight ma- trices are also partitioned to maintain the corre- sponding LSTM groups. The MT-LSTM with just one group is the same to the standard LSTM.</p><p>At each time step t, only the groups G k that sat- isfy (t MOD T k ) = 0 are executed. The choice of the set of periods T k ∈ {T 1 , · · · , T g } is arbi- trary. Here, we use the exponential series of peri- ods: group G k has the period of T k = 2 k−1 . The group G 1 is the fastest one and can be executed at every time step, which works like the standard LSTM. The group G k is the slowest one.</p><p>At time step t, the memory cell vector and hid- den state vector of group G k are calculate in two cases:</p><p>(1) When group G k is activated at time step t, the LSMT units of this group are calculated by the following equations:</p><formula xml:id="formula_8">i k t = σ(W k i xt + g ∑ j=1 U j→k i h j t−1 + g ∑ j=1 V j→k i c j t−1 ),<label>(8)</label></formula><formula xml:id="formula_9">f k t = σ(W k f xt + g ∑ j=1 U j→k f h j t−1 + g ∑ j=1 V j→k f c j t−1 ),<label>(9)</label></formula><formula xml:id="formula_10">o k t = σ(W k o xt + g ∑ j=1 U j→k o h j t−1 + g ∑ j=1 V j→k o c j t ),<label>(10)</label></formula><formula xml:id="formula_11">˜ c k t = tanh(W k c xt + g ∑ j=1 U j→k c h j t−1 ),<label>(11)</label></formula><formula xml:id="formula_12">c k t = f k t ⊙ c k t−1 + i k t ⊙ ˜ c k t ,<label>(12)</label></formula><formula xml:id="formula_13">h k t = o k t ⊙ tanh(c k t ),<label>(13)</label></formula><p>where i k t , f k t and o k t are the vectors of input gates, forget gates, and output gates of group G k at time step t respectively; c k t and h k t are the memory cell vector and hidden state vector of group G k at time step t respectively.</p><p>(2) When group G k is non-activated at time step t, its LSMT units keep unchanged. <ref type="figure" target="#fig_1">Figure 3</ref> shows the different between the stan- dard LSTM and MT-LSTM. </p><formula xml:id="formula_14">c k t = c k t−1 ,<label>(14)</label></formula><formula xml:id="formula_15">h k t = h k t−1 .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Two Feedback Strategies</head><p>The feedback mechanism of LSTM is imple- mented by the recurrent connections from time step t − 1 to t. Since the MT-LSTM groups are updated with the different frequencies, we can re- gard the different group as the human memory. The fast-speed groups are short-term memories, while the slow-speed groups are long-term mem- ories. Therefore, an important consideration is what feedback mechanism is between the short- term and long-term memories.</p><p>For the proposed MT-LSTM, we consider two feedback strategies to define the connectivity pat- terns among the different groups.</p><p>Fast-to-Slow (F2S) Strategy Intuitively, when we accumulate the short-term memory to a certain degree, we store some valuable information from the short-term memory into the long-term mem- ory. Therefore, we firstly define a fast to slow strategy, which updates the slower group using the faster group. The connections from group j to group k exist if and only if</p><formula xml:id="formula_16">T j ≤ T k . The weight matrices U j→k i , U j→k f , U j→k o , U j→k c , V j→k i , V j→k f , V j→k o are set to zero when T j &gt; T k .</formula><p>The F2S updating strategy is shown in <ref type="figure" target="#fig_1">Figure  3a</ref>.</p><p>Slow-to-Fast (S2F) Strategy Following the work of ( <ref type="bibr" target="#b16">Koutnik et al., 2014</ref>), we also investigate another update scheme from slow-speed group to fast-speed group. The motivation is that a long term memory can be "distilled" into a short-term memory. The connections from group j to group i exist only if</p><formula xml:id="formula_17">T j ≥ T i . The weight matrices U j→k i , U j→k f , U j→k o , U j→k c , V j→k i , V j→k f , V j→k o are set to zero when T j &lt; T k .</formula><p>The S2F update strategy is shown in <ref type="figure" target="#fig_1">Figure 3b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Train Size Dev. Size Test Size Class Averaged Length Vocabulary Size <ref type="table" target="#tab_2">SST-1  Sentence  8544  1101  2210  5  19  18K  SST-2  Sentence  6920  872  1821  2  18  15K  QC  Sentence  5452  - 500  6  10  9.4K  IMDB  Document  25,000  - 25,000  2  294  392K   Table 1</ref>: Statistics of the four datasets used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamic Selection of the Number of the MT-LSTM Unit Groups</head><p>Another consideration is how many groups need to be used. An intuitive way is that we need more groups for long texts than short texts. The number of the group depends the length of the texts.</p><p>Here, we use a simple dynamic strategy to choose the maximum number of groups, and then the best g is chosen as a hyperparameter according to different tasks. The upper bound of the number of groups is calculated by</p><formula xml:id="formula_18">g = log 2 L − 1,<label>(16)</label></formula><p>where L is the average length of the corpus. Thus, the slowest group is activated at least twice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>In each of the experiments, the hidden layer at the last moment has a fully connected layer fol- lowed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to min- imise the cross-entropy of the predicted and true distributions; the objective includes an L 2 regu- larization term over the parameters. The network is trained with backpropagation and the gradient- based optimization is performed using the Ada- grad update rule <ref type="bibr" target="#b7">(Duchi et al., 2011</ref>). The back propagation of the error propagation is similar to LSTM as well. The only difference is that the error propagates only from groups that were executed at time step t. The error of non- activated groups gets copied back in time (simi- larly to copying the activations of nodes not ac- tivated at the time step t during the correspond- ing forward pass), where it is added to the back- propagated error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we investigate the empirical per- formances of our proposed MT-LSTM model on four benchmark datasets for sentence and docu- ment classification and then compare it to other competitor models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We evaluate our model on four different datasets. The first three datasets are sentence-level, and the last dataset is document-level. The detailed statis- tics about the four datasets are listed in <ref type="table">Table 1</ref>. Each dataset is briefly described as follows.</p><p>• SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, some- what positive, positive) in the Stanford Senti- ment Treebank <ref type="bibr">1 (Socher et al., 2013</ref>  <ref type="bibr" target="#b18">Roth, 2002</ref>).</p><p>• IMDB The IMDB dataset <ref type="bibr">3</ref> consists of 100,000 movie reviews with binary classes <ref type="bibr" target="#b19">(Maas et al., 2011</ref>). One key aspect of this dataset is that each movie review has several sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Competitor Models</head><p>We compare our model with the following models:</p><p>• NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bi- gram features ( <ref type="bibr" target="#b31">Wang and Manning, 2012</ref>).</p><p>• NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer.</p><p>• RAE Recursive Autoencoders with pre- trained word vectors from Wikipedia ( <ref type="bibr" target="#b26">Socher et al., 2011b</ref>).</p><formula xml:id="formula_19">• MV-RNN Matrix-Vector Recursive Neural</formula><p>Network with parse trees <ref type="bibr" target="#b27">(Socher et al., 2012</ref>).</p><p>SST <ref type="table">-1 SST-2   QC  IMDB  Embedding size  100  100  100  100  hidden layer size  60  60  55  100  Initial learning rate</ref> 0.1 0.1 0.1 0.1 Regularization 10 −5 10 −5 10 −5 10 − <ref type="table" target="#tab_2">5  Number of Groups  3  3  3  5   Table 2</ref>: Hyper-parameter settings for the LSTM and MT-LSTM.</p><p>• RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees <ref type="bibr" target="#b28">(Socher et al., 2013</ref>).</p><p>• AdaSent Self-adaptive hierarchical sentence model with gated mechanism ( <ref type="bibr" target="#b34">Zhao et al., 2015</ref>).</p><p>• DCNN Dynamic Convolutional Neural Net- work with dynamic k-max pooling ).</p><p>• CNN-non-static and CNN-multichannel</p><p>Convolutional Neural Network <ref type="bibr" target="#b15">(Kim, 2014</ref>).</p><p>• PV Logistic regression on top of paragraph vectors ( <ref type="bibr" target="#b17">Le and Mikolov, 2014</ref>). Here, we use the popular open source implementation of PV in Gensim 4 .</p><p>• LSTM The standard LSTM for text classifi- cation. We use the implementation of Graves (2013). The unfolded illustration is shown in <ref type="figure">Figure 2a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hyperparameters and Training</head><p>In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words). The vocabu- lary size is about 500,000. The the word embed- dings are fine-tuned during training to improve the performance <ref type="bibr" target="#b5">(Collobert et al., 2011</ref>). The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1]. The hy- perparameters which achieve the best performance on the development set will be chosen for the fi- nal evaluation. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters for the LSTM and MT- LSTM are set as <ref type="figure">Figure 2</ref>. <ref type="table" target="#tab_2">Table 3</ref> shows the classification accuracies of the standard LSTM, MT-LSTM compared with the competitor models. Firstly, we compare two feedback strategies of MT-LSTM. The fast-to-slow feedback strat- egy (MT-LSTM (F2S)) is better than the slow-to- fast strategy (MT-LSTM (S2F)), which indicates that MT-LSTM benefits from periodically stor- ing some valuable information "purified" from the short-term memory into the long-term memory. In the following discussion, we use fast-to-slow feed- back strategy as the default setting of MT-LSTM. Compared with the standard LSTM, MT-LSTM results in significantly improvements with the same size of hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>MT-LSTM outperforms the competitor models on the SST-1, QC and IMDB datasets, and is close to the two best CNN based models on the SST-2 dataset. But MT-LSTM uses much fewer param- eters than the CNN based models. The number of parameters of LSTM range from 10K to 40K while the number of parameters is about 400K in CNN.</p><p>Moreover, MT-LSTM can not only handle short texts, but can model long texts in classification task.</p><p>Documents Modeling Most of the competitor models cannot deal with the texts of with sev- eral sentences (paragraphs, documents). For in- stance, MV-RNN and RNTN <ref type="bibr" target="#b28">(Socher et al., 2013)</ref> are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences. The convolutional models, such as CNN <ref type="bibr" target="#b15">(Kim, 2014</ref>) and AdaSent ( <ref type="bibr" target="#b34">Zhao et al., 2015)</ref>, need more hidden layers or nodes for long texts and result in a very complicated model. These models therefore are restricted to work- ing on sentences instead of paragraphs or docu- ments. <ref type="bibr" target="#b6">Denil et al. (2014)</ref> used two-level version of <ref type="bibr">DCNN (Kalchbrenner et al., 2014</ref>) to model documents. The first level uses a DCNN to trans-Model SST-1 SST-2 QC IMDB NBOW (  42.4 80.5 88.2 - RAE ( <ref type="bibr" target="#b26">Socher et al., 2011b)</ref> 43.2 82.4 - - MV-RNN ( <ref type="bibr" target="#b27">Socher et al., 2012)</ref> 44.4 82.9 - - RNTN ( <ref type="bibr" target="#b28">Socher et al., 2013)</ref> 45.7 85.4 - - DCNN (  48.5 86.8 93.0 - CNN-non-static <ref type="bibr" target="#b15">(Kim, 2014)</ref> 48.0 87.2 93.6 - CNN-multichannel <ref type="bibr" target="#b15">(Kim, 2014)</ref> 47.4 88.1 92.2 - AdaSent ( <ref type="bibr" target="#b34">Zhao et al., 2015)</ref> - - 92.4 - NBSVM ( <ref type="bibr" target="#b31">Wang and Manning, 2012)</ref> - - - 91.2 MNB ( <ref type="bibr" target="#b31">Wang and Manning, 2012)</ref> - - - 86.6 Two-level <ref type="bibr">DCNN (Denil et al., 2014)</ref> - - - 89.4 PV ( <ref type="bibr" target="#b17">Le and Mikolov, 2014)</ref> 44   <ref type="figure">Figure 5</ref>: Performances of our model with the dif- ferent numbers of memory groups g on four devel- opment datasets: SST-1,SST-2, QC, and IMDB. Y-axis represents the accuracy(%), and X-axis rep- resents different numbers of memory groups. All memory groups share a fixed-size memory layer h, and here we set h=120.</p><p>form embeddings for the words in each sentence into an embedding for the entire sentence. The second level uses another DCNN to transform sen- tence embeddings from the first level into a single embedding vector that represents the entire docu- ment. However, their result is unsatisfactory and they reported that the IMDB dataset is too small to train a CNN model. The standard LSTM has an advantage to model documents due to its simplification. However, it is also difficult to train LSTM since the error signals need to be back-propagated over a long distance with the BPTT algorithm.</p><p>Our MT-LSTM can alleviate this problem with multiple timescale memories. The experiment on IMDB dataset demonstrates this advantage. MT- LSTM achieves the accuracy of 92.1% , which are better than the other models.</p><p>Moreover, MT-LSTM converges at a faster rate than the standard LSTM. <ref type="figure" target="#fig_2">Figure 4</ref> plots the con- vergence on the IMDB dataset. In practice, MT- LSTM is approximately three times faster than the standard LSTM since the hidden states of low- speed group often keep unchanged and need not to be re-calculated at each time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the Different Number of Memory</head><p>Groups In our model, the number of memory groups is a hyperparameter. Here we plotted the accuracy curves of our model with the different numbers of memory groups in <ref type="figure">Figure 5</ref> to show its impacts on the four datasets.</p><p>When the length of text (SST-1, SST-2 and QC) is small, not all memory groups can be acti- vated if we set too many groups, which may harm the performance. When dealing with the long texts (IMBD), more groups lead to a better per- formance. The performance can be improved with the increase of the number of memory groups.</p><p>According to our dynamic strategy, the maxi- mum numbers of groups is 3, 3, 2, 7 for the four datasets. The best numbers of groups from exper- iments are 3, 3, 3, 5 respectively. Therefor, our dynamic strategy is reasonable. All the datasets except QC, the best number of groups is equal to or smaller than our calculated upper bound. MT- LSMT suffers underfitting when the number of groups is larger than the upper bound.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Case Study</head><p>To get an intuitive understanding of what is hap- pening when we use LSTM or MT-LSTM to pre- dict the class of text, we design an experiment to analyze the output of LSTM and MT-LSTM at each time step.</p><p>We sample three sentences from the SST-2 test dataset, and the dynamical changes of the pre- dicted sentiment score over time are shown in <ref type="figure" target="#fig_3">Fig- ure 6</ref>. It is intriguing to notice that our model can handle the rhetorical question well.</p><p>The first sentence "Is this progress ?" has a negative sentiment. Although the word "progress" is positive, our model can adjust the sentiment correctly after seeing the question mark "?", and finally gets a correct prediction.</p><p>The second sentence "He 'd create a movie better than this ." also has a negative sentiment. The word "better" is posi- tive. Our model finally gets a correct negative pre- diction after seeing "than this", while LSTM gets a wrong prediction.</p><p>The third sentence " It 's not exactly a gourmet meal but fare is fair , even coming from the drive ." is positive and has more complicated semantic composition. Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>There are many previous works to model the variable-length text as a fixed-length vector. Spe- cific to text classification task, most of the mod- els cannot deal with the texts of several sen- tences (paragraphs, documents), such as MV-RNN ( <ref type="bibr" target="#b27">Socher et al., 2012</ref>), <ref type="bibr">RNTN (Socher et al., 2013)</ref>, CNN <ref type="bibr" target="#b15">(Kim, 2014</ref>), AdaSent ( <ref type="bibr" target="#b34">Zhao et al., 2015)</ref>, and so on. The simple neural bag-of-words model can deal with long texts, but it loses the word order information. PV ( <ref type="bibr" target="#b17">Le and Mikolov, 2014</ref>) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task.</p><p>Our proposed MT-LSTM can handle short texts as well as long texts in classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we introduce the MT-LSTM, a gen- eralization of LSTMs to capture the information with different timescales. MT-LSTM can well model both short and long texts. With the multi- ple different timescale memories. Intuitively, MT- LSTM easily carries the crucial information over a long distance. Another advantage of MT-LSTM is that the training speed is faster than the standard LSTM (approximately three times faster in prac- tice).</p><p>In future work, we would like to investigate the other feedback mechanism between the short-term and long-term memories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A LSTM unit. The dashed line is the recurrent connection, and the solid link is the connection at the current time.</figDesc><graphic url="image-1.png" coords="3,93.82,62.44,174.80,174.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two feedback strategies of our model. The dashed line shows the feedback connection, and the solid link shows the connection at current time.</figDesc><graphic url="image-2.png" coords="4,315.46,72.72,98.26,55.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence Speeds on IMDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The dynamical changes of the predicted sentiment score over time. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The red horizontal line gives a border between the positive and negative sentiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of our MT-LSTM model against state-of-the-art neural models. All the results without 
marks are reported in the corresponding paper. 

1 
2 
3 
4 
5 

44.5 

45 

45.5 

46 

46.5 

47 

(a) SST-1 

1 
2 
3 
4 
5 
84.6 

84.8 

85 

85.2 

85.4 

85.6 

85.8 

(b) SST-2 

1 
2 
3 
4 
90 

90.5 

91 

91.5 

92 

92.5 

(c) QC 

1 
2 
3 
4 
5 
6 
8 

88.5 

89 

89.5 

90 

(d) IMDB 

</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/sentiment. 2 http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3 http://ai.stanford.edu/ ˜ amaas/data/ sentiment/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), Na-tional High Technology Research and Develop-ment Program of China (2015AA015408), Shang-hai Science and Technology Development Funds (14ZR1403200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Demiraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3830</idno>
		<title level="m">Modelling, visualising and summarising documents with a single convolutional neural network</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A clockwork rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1863" to="1871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul J Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spoken language understanding using long short-term memory neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE SLT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.05070</idno>
		<title level="m">Self-adaptive hierarchical sentence model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
