<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate Supervised and Semi-Supervised Machine Reading for Long Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
							<email>izzeddingur@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
							<email>dhewlett@google.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Element AI</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate Supervised and Semi-Supervised Machine Reading for Long Documents</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2011" to="2020"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a hierarchical architecture for machine reading capable of extracting precise information from long documents. The model divides the document into small, overlapping windows and encodes all windows in parallel with an RNN. It then attends over these window encodings, reducing them to a single encoding , which is decoded into an answer using a sequence decoder. This hierarchical approach allows the model to scale to longer documents without increasing the number of sequential steps. In a supervised setting, our model achieves state of the art accuracy of 76.8 on the WikiRead-ing dataset. We also evaluate the model in a semi-supervised setting by downsam-pling the WikiReading training set to create increasingly smaller amounts of supervision , while leaving the full unlabeled document corpus to train a sequence au-toencoder on document windows. We evaluate models that can reuse autoen-coder states and outputs without fine-tuning their weights, allowing for more efficient training and inference.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, deep neural networks (DNNs) have pro- vided promising results for a variety of reading comprehension and question answering tasks <ref type="bibr" target="#b18">(Weston et al., 2014;</ref><ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Rajpurkar et al., 2016)</ref>, which require extracting precise in- formation from documents conditioned on a query. While a basic sequence to sequence (seq2seq) model ) can perform these * Work completed while interning at Google Research. † Work completed while at Google Research.</p><p>tasks by encoding a question and document se- quence and decoding an answer sequence <ref type="bibr" target="#b6">(Hewlett et al., 2016)</ref>, it has some disadvantages. The an- swer may be encountered early in the text and need to be stored across all the further recurrent steps, leading to forgetting or corruption; Atten- tion can be added to the decoder to solve this problem ( <ref type="bibr" target="#b5">Hermann et al., 2015)</ref>. Even with at- tention, approaches based on Recurrent Neural Networks (RNNs) require a number of sequential steps proportional to the document length to en- code each document position. Hierarchical read- ing models address this problem by breaking the document into sentences ( <ref type="bibr" target="#b2">Choi et al., 2017)</ref>. In this paper, we introduce a simpler hierarchical model that achieves state-of-the-art performance on our benchmark task without this linguistic structure, and use it as framework to explore semi- supervised learning for reading comprehension.</p><p>We first develop a hierarchical reader called Sliding-Window Encoder Attentive Reader (SWEAR) that circumvents the aforementioned bottlenecks of existing readers. SWEAR, illus- trated in <ref type="figure" target="#fig_0">Figure 1</ref>, first encodes each question into a vector space representation. It then chunks each document into overlapping, fixed-length windows and, conditioned on the question representation, encodes each window in parallel. Inspired by recent attention mechanisms such as <ref type="bibr" target="#b5">Hermann et al. (2015)</ref>, SWEAR attends over the window representations and reduces them into a single vector for each document. Finally, the answer is decoded from this document vector. Our results show that SWEAR outperforms the previous state-of-the-art on the supervised WikiReading task <ref type="bibr" target="#b6">(Hewlett et al., 2016)</ref>, improving Mean F1 to 76.8 from the previous 75.6 ( <ref type="bibr" target="#b2">Choi et al., 2017)</ref>.</p><p>While WikiReading is a large dataset with mil- lions of labeled examples, many applications of machine reading have a much smaller number of labeled examples among a large set of unla- beled documents. To model this situation, we con- structed a semi-supervised version of WikiRead- ing by downsampling the labeled corpus into a variety of smaller subsets, while preserving the full unlabeled corpus (i.e., Wikipedia). To take advantage of the unlabeled data, we evaluated multiple methods of reusing unsupervised recur- rent autoencoders in semi-supervised versions of SWEAR. Importantly, in these models we are able to reuse all the autoencoder parameters without fine-tuning, meaning the supervised phase only has to learn to condition the answer on the doc- ument and query. This allows for more efficient training and online operation: Documents can be encoded in a single pass offline and these en- codings reused by all models, both during train- ing and when answering queries. Our semi- supervised learning models achieve significantly better performance than supervised SWEAR on several subsets with different characteristics. The best-performing model reaches 66.5 with 1% of the WikiReading dataset, compared to the 2016 state of the art of 71.8 (with 100% of the dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Description</head><p>Following the recent progress on end-to-end su- pervised question answering ( <ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Rajpurkar et al., 2016)</ref>, we consider the gen- eral problem of predicting an answer A given a query-document pair (Q, D). We do not make the assumption that the answer should be present ver- batim in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Version</head><p>Given a document D = {d 1 , d 2 , · · · , d N D } and a query Q = {q 1 , q 2 , · · · , q N Q } as sequences of words, our task is to generate a new sequence of words that matches the correct answer A = {a 1 , a 2 , · · · , a N A }. Because we do not assume that A is a subsequence of D, the answer may require blending information from multiple parts of the document, or may be precisely copied from a single location. Our proposed architecture sup- ports both of these use cases.</p><p>The WikiReading dataset <ref type="bibr" target="#b6">(Hewlett et al., 2016)</ref>, which includes a mix of categorization and extraction tasks, is the largest dataset match- ing this problem description. In WikiReading, documents are Wikipedia articles, while queries and answers are Wikidata properties and values, respectively. Example Wikidata property-value pairs are (place of birth, Paris), (genre, Science Fiction). The dataset contains 18.58M instances divided into training, validation, and test with an 85/10/5 split. The answer is present verbatim in the document only 47.1% of the time, severely limiting models that label document spans, such as those developed for the popular SQUAD dataset ( <ref type="bibr" target="#b12">Rajpurkar et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Supervised Version</head><p>We also consider a semi-supervised version of the task, where an additional corpus of documents without labeled (Q, A) pairs is available. Tak- ing advantage of the large size of the WikiReading dataset, we created a series of increasingly chal- lenging semi-supervised problems with the fol- lowing structure:</p><p>• Unsupervised: The entire document corpus (about 4M Wikipedia articles), with queries and answers removed.</p><p>• Supervised: Five smaller training sets cre- ated by sampling a random (1%, 0.5%, 0.1%) of the WikiReading training set, and taking (200, 100) random samples from each prop- erty in the original training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supervised Model Architecture</head><p>We now present our model, called Sliding- Window Encoder Attentive Reader (SWEAR), shown in <ref type="figure" target="#fig_0">Figure 1</ref>, and describe its operation in a fully supervised setting. Given a (Q, D) pair, the model encodes Q into a vector space represen- tation with a Recurrent Neural Network (RNN). The first layer of the model chunks the document D into overlapping, fixed-length windows and en- codes all windows in parallel with an RNN condi- tioned on the question representation. The second layer attends over the window representations, re- ducing them into a single vector representing the latent answer. Finally, the answer sequence A is decoded from this vector using an RNN sequence decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries and Notation</head><p>Each word w comes from a vocabulary V and is associated with a vector e w which constitutes the rows of an embedding matrix E. We denote by e D , e Q , and e A the vector sequences corresponding to the document, question, and answer sequences, re- spectively. More specifically, we aim at obtaining vector representations for documents and ques- tions, then generating the words of the answer se- quence.</p><p>Our model makes extensive use of RNN en- coders to transform sequences into fixed length vectors. For our purposes, an RNN encoder con- sists of GRU units ( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) defined as</p><formula xml:id="formula_0">h t = f (x t ; h t−1 ; θ)<label>(1)</label></formula><p>where h t is hidden state at time t. f is a nonlinear function operating on input vector x t and previ- ous state, h t−1 with θ being its parameter vector. Given an input sequence, the encoder runs over the sequence of words producing the hidden vectors at each step. We refer to the last hidden state of an RNN encoder as the encoding of a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sliding Window Recurrent Encoder</head><p>The core of the model is a sequence encoder that operates over sliding windows in a manner analo- gous to a traditional convolution. Before encod- ing the document, we slide a window of length l with a step size s over the document and pro- duce n = N D −l s document windows. This yields a sequence of sub-documents</p><formula xml:id="formula_1">(D 1 , D 2 , · · · , D n ),</formula><p>where each D i contains a subsequence of l words from the original document D. Intuitively, a pre- cise answer may be present verbatim in one or more windows, or many windows may contain ev- idence suggestive of a more categorical answer.</p><p>Next, the model encodes each window condi- tioned on a question encoding. We first encode the question sequence once using a RNN (Enc) as</p><formula xml:id="formula_2">h q = Enc(e Q ; θ Q )<label>(2)</label></formula><p>where h q is the last hidden state and θ Q represents the parameters of the question encoder. Initialized with this question encoding, we employ another RNN to encode each document window as</p><formula xml:id="formula_3">h w i,0 = h q h w i = Enc(e D i ; θ W )<label>(3)</label></formula><p>where h w i,0 is the initial hidden state, h w i is the last hidden state, and θ W represents the parameters of the window encoder. θ W is shared for every win- dow and is decoupled from θ Q . As the windows are significantly smaller than the documents, en- codings of windows will reflect the effect of ques- tion encodings better, mitigating any long-distance dependency problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Window Encodings</head><p>SWEAR attends over the window encoder states using the question encoding to produce a single vector h d for the document, given by</p><formula xml:id="formula_4">p i ∝ exp(u T R tanh(W R [h w i , h q ]))<label>(4)</label></formula><formula xml:id="formula_5">h d = i p i h w i (5) Model Mean F1 Placeholder seq2seq (HE16)</formula><p>71.8 SoftAttend (CH17) 71.6 Reinforce (CH17) 74.5 Placeholder seq2seq (CH17) 75.6 SWEAR (w/ zeros) 76.4 SWEAR 76.8  where <ref type="bibr">[.]</ref> is vector concatenation, and p i is the probability window i is relevant to answering the question. W R and u R are parameters of the atten- tion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Decoding</head><p>Given the document encoding h d , an RNN de- coder (Dec) generates the answer word sequence:</p><formula xml:id="formula_6">h a 0 = h d h a t = Dec(h a t−1 ; ω A ) (6) P (a * t = w j ) ∝ exp(e T j (W A h a t + b A ))<label>(7)</label></formula><formula xml:id="formula_7">a * t = argmax j (P (a * t = w j )) (8)</formula><p>where h a 0 is the initial hidden state and h a t is the hidden vector at time t.</p><formula xml:id="formula_8">A * = {a * 1 , a * 2 , · · · , a * N A</formula><p>} is the sequence of answer words generated. W A , b A , and ω A are the parameters of the answer decoder. The training objective is to minimize the average cross-entropy error between the candidate sequence A * and the correct answer sequence A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Supervised Results</head><p>Before exploring unsupervised pre-training, we present summary results for SWEAR in a fully su- pervised setting, for comparison to previous work on the WikiReading task, namely that of <ref type="bibr" target="#b6">Hewlett et al. (2016)</ref> and <ref type="bibr" target="#b2">Choi et al. (2017)</ref>, which we re- fer to as HE16 and CH17 in tables. For further ex- periments, results, and discussion see Section 5.2.  To quantify the effect of initializing the window encoder with the question state, we report results for two variants of SWEAR: In SWEAR the win- dow encoder is initialized with the question en- coding, while in SWEAR w/ zeros, the window en- coder is initialized with zeros. In both cases the question encoding is used for attention over the window encodings. For SWEAR w/ zeros it is ad- ditionally concatenated with the document encod- ing and passed through a 2-layer fully connected neural network before the decoding step. Condi- tioning on the question increases Mean F1 by 0.4. <ref type="bibr" target="#b6">Hewlett et al. (2016)</ref> grouped properties by an- swer distribution: Categorical properties have a small list of possible answers, such as countries, Relational properties have an open set of answers, such as spouses or places of birth, and Date prop- erties (a subset of relational properties) have date answers, such as date of birth. We reproduce this grouping in <ref type="table" target="#tab_1">Table 2</ref> to show that SWEAR im- proves performance for Relational and Date prop- erties, demonstrating that it is better able to extract precise information from documents.</p><p>Finally, we observe that SWEAR outperforms a baseline seq2seq model on longer documents, as shown in <ref type="table" target="#tab_3">Table 3</ref>. The baseline model is roughly equivalent to the best previously-published result, Placeholder seq2seq (CH17) in <ref type="table" target="#tab_0">Table 1</ref>, reach- ing a Mean F1 of 75.5 on the WikiReading test- set. SWEAR improves over the baseline in every length category, but the differences are larger for longer documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-Supervised Model Architecture</head><p>We now describe semi-supervised versions of the SWEAR model, to address the semi-supervised problem setting described in Section 2.2. A wide variety of approaches have been developed for semi-supervised learning with Neural Networks, with a typical scheme consisting of training an unsupervised model first, and then reusing the weights of that network as part of a supervised model. We consider each of these problems in turn, describing two types of unsupervised autoen- coder models for sequences in Section 4.1 before turning to a series of strategies for incorporating the autoencoder weights into a final supervised model in Section 4.3. All of these models reuse the autoencoder weights without modification, mean- ing a document can be encoded once by an of- fline process, and the resulting encodings can be used both during training and to answer multiple queries online in a more efficient manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recurrent Autoencoders for</head><p>Unsupervised Pre-training</p><p>Autoencoders are models that reconstruct their in- put, typically by encoding it into a latent space and then decoding it back again. Autoencoders have recently proved useful for semi-supervised learn- ing ( <ref type="bibr" target="#b3">Dai and Le, 2015;</ref><ref type="bibr" target="#b4">Fabius and van Amersfoort, 2014</ref>). We now describe two autoencoder mod- els from the recent literature that we use for un- supervised learning. The Recurrent Autoencoder (RAE) is the natural application of the seq2seq framework ( ) to autoencod- ing documents <ref type="bibr" target="#b3">(Dai and Le, 2015)</ref>: In seq2seq, an encoder RNN already produces a latent represen- tation h N , which is used to initialize a decoder RNN. In RAE, the output sequence is replaced with the input sequence, so learning minimizes the cross-entropy between the reconstructed input se- quence and the original input sequence. Encoder and decoder cells share parameters θ U .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Variational Recurrent Autoencoder</head><p>The Variational Recurrent Autoencoder (VRAE), introduced by <ref type="bibr" target="#b4">Fabius et al. (2014)</ref>, is a RAE with a variational Bayesian inference step where an un- observed latent random variable generates the se- quential data. The encoder and decoder are exactly the same as RAE, but the latent state h N is not di- rectly passed to the decoder. Instead, it is used to estimate the parameters of a Gaussian distribution with a diagonal covariance matrix: The mean is given by µ x = W µ h N + b µ and the covariance by</p><formula xml:id="formula_9">Σ x = W Σ h N + b Σ , where W µ , W Σ , b µ ,</formula><note type="other">and b Σ are new variational step parameters. The decoder is initialized with a single vector sampled from this distribution, z x ∼ N (z|µ x , Σ x ). For VRAE, the Kullback-Leibler divergence between trained</note><p>Normal distribution and standard normal distribu- tion, i.e., KL(N (µ x , Σ x )|N (0, I)), is added to the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Window Autoencoders</head><p>We take advantage of the SWEAR architecture by training autoencoders for text windows, as opposed to the standard document autoencoders. These autoencoders operate on the same sliding window subsequences as the supervised SWEAR model, autoencoding all subsequences indepen- dently and in parallel. This makes them easier to train as they only have to compress short se- quences of text into a fixed-length representation.</p><p>As the task of autoencoding is independent from our supervised problem, we refer to the generated encodings as global encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline: Initialization with Autoencoder Embeddings</head><p>Our baseline approach to reusing an unsupervised autoencoder in SWEAR is to initialize all em- beddings with the pre-trained parameters and fix them. We call this model SWEAR-SS (for semi- supervised). The embedding matrix is fixed to the autoencoder embeddings. All other parameters are initialized randomly and trained as in the fully su- pervised version. We found that initializing the encoders and decoder with autoencoder weights hurts performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reviewer Models</head><p>Unfortunately, this baseline approach to semi- supervised learning has significant disadvantages in our problem setting. Pre-trained RNN param- eters are not fully exploited since we observed catastrophic forgetting when initializing and fine- tuning SWEAR with pre-trained weights. This includes fixing window encoder parameters with autoencoders and only fine-tuning question en- coders. Second, conditioning the window en- coders on the question eliminates the possibility to train window representations offline and uti- lize them later which causes a significant overhead during testing. Inspired by recent trends in deep learning mod- els such as Progressive Neural Networks ( <ref type="bibr" target="#b14">Rusu et al., 2016)</ref> and Reviewer Models ( <ref type="bibr" target="#b19">Yang et al., 2016)</ref>, we propose multiple solutions to these problems. All of the proposed models process text input first through a fixed autoencoder layer: fixed pre-trained embeddings and fixed RNN encoder parameters, both initialized from the autoencoder weights. Above this autoencoder layer, we build layers of abstraction that learn to adapt the pre- trained models to the QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Multi-Layer Reviewer (SWEAR-MLR)</head><p>The most straightforward extension to the baseline model is to fix the pretrained autoencoder RNN as the first layer and introduce a second, train- able reviewer layer. To make this approach more suitable for question answering, reviewer layers utilize corresponding global encodings as well as hidden states of the pre-trained autoencoders as input <ref type="figure" target="#fig_1">(Figure 2</ref>). The aim is to review both pre- trained question and window encodings to com- pose a single vector representing the window con- ditioned on the question. Encoding questions: The question is first encoded by the autoencoder layer, ˜ h q = Enc(e Q ; θ U ) where both word embeddings (E and e Q ) and encoder (θ U ) are fixed and initialized with pretrained parameters. A second, learnable RNN layer then takes the output of the autoen- coder layer and corresponding input embeddings as input and produces the final question encoding,</p><formula xml:id="formula_10">h q = Enc(F C([e Q , ˜ h q , ˜ h t ]); θ Q )</formula><p>where F C is a fully connected layer with ReLU activation func- tion, and˜hand˜ and˜h t is the output of the autoencoder layer at time step t. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates a single time- step of the question encoder.</p><p>Encoding windows: Similarly, windows are encoded first by the fixed autoencoder layer and then by a reviewer layer,</p><formula xml:id="formula_11">˜ h w i = Enc(e D i ; θ U ) and h w i = Enc(F C([e D i , ˜ h w i , ˜ h w t , h q ])</formula><p>; θ W ) where˜h where˜ where˜h w t is the output of the autoencoder layer at time step t. Unlike supervised SWEAR, in SWEAR- MLR the window encoder is not initialized with the question encoder state. Instead, the question encoder state is an additional input to each unit in the reviewer layer (illustrated as the dashed line in <ref type="figure" target="#fig_2">Figure 3)</ref>. Intuitively, the reviewer layer should reuse the global window and question information and encode only information relevant to the cur- rent question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Progressive Reviewer (SWEAR-PR)</head><p>Although the reviewer layer in SWEAR-MLR has global window and question encodings as input, it requires a number of sequential steps equal to the window size, plus any additional reviewer steps. The reviewer layer also has to re-encode windows for each question, which is not ideal for online use.  To address these issues, we now present a Progres- sive Reviewer model (SWEAR-PR) that reviews the outputs of the encoders using a separate RNN that is decoupled from the window size <ref type="figure" target="#fig_3">(Figure 4)</ref>.</p><p>Encoding questions and windows: Similar to SWEAR-MLR, SWEAR-PR first encodes the questions and windows independently using au- toencoder layers,</p><formula xml:id="formula_12">˜ h q = Enc(e Q ; θ U ) and˜hand˜ and˜h w i = Enc(e D i ; θ U ).</formula><p>To decouple the question and win- dow encoders, however, SWEAR-PR does not have a second layer as a reviewer.</p><p>Reviewing questions and windows: SWEAR- PR employs two other RNNs to review the question and window encodings and to compose a single window representation conditioned on the question. Question reviewer takes the same pre-trained question encoding at each time step and attends over the hidden states and input embeddings of the pre-trained question encoder,</p><formula xml:id="formula_13">h q = AttnEnc(F C( ˜ h q ); F C([ ˜ h q t , e Q ]); θ Q )</formula><p>where AttnEnc is an RNN with an attention cell which is illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. Outputs of the fixed autoencoder layer and fixed word embed- dings, [ ˜ h q t , e Q ], are the attendable states. Window reviewer on the other hand takes the pre-trained window encoding and reviewed question encod- ing at each time step and attends over the hidden states of pre-trained window encoder,</p><formula xml:id="formula_14">h w i = AttnEnc(F C([ ˜ h w i , h q ]); F C([ ˜ h w t , e D i ]); θ W )</formula><note type="other">where outputs of the fixed autoencoder layer and fixed word embeddings, [ ˜ h w t , e D i ], are the attendable states. As length of the windows is smaller than length of the reviewers, SWEAR-PR has significantly smaller overhead compared to other supervised and semi-supervised SWEAR variants.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Shared Components</head><p>Reducing window encodings and decoding: As in the supervised case described in 4, both re- viewer models attend over the window encodings using the question encoding and reduce them into a single document encoding. Identical to answer decoding described in 6, the answer is decoded using another RNN taking the document state as the initial state. The parameters of this answer de- coder are initialized randomly.</p><p>Adapter layer: As the distribution and scale of parameters may differ significantly between the autoencoder layer and the reviewer layer, we use an adapter layer similar to the adapters in Pro- gressive Neural Networks ( <ref type="bibr" target="#b14">Rusu et al., 2016)</ref> to normalize the pre-trained parameters:</p><formula xml:id="formula_15">W out = a * tanh(b * W in )<label>(9)</label></formula><p>where a and b are scalar variables to be learnt and W in is a pre-trained input parameter. We put adapter layers after every pre-trained parameter connecting to a finetuned parameter such as on the connections from pre-trained embeddings to reviewer layer. We use dropout ( <ref type="bibr" target="#b15">Srivastava et al., 2014</ref>) regularization on both inputs and outputs of the reviewer cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>As described in Section 2, we evaluate our mod- els on the WikiReading task. In Section 3.5 we presented results for the supervised SWEAR on the full WikiReading dataset, establishing it as the highest-scoring method so far developed for WikiReading. We now compare our semi- supervised models SWEAR-MLR and SWEAR-Model 1% 0.5% 0.1% SWEAR 63.5 57.6 39.5 SWEAR-SS (RAE) 64.7 62.8 55.3 SWEAR-SS (VRAE) 65.7 64.0 60.7 PR over various subsets of the WikiReading dataset, using SWEAR as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Following Hewlett et al. <ref type="formula" target="#formula_0">(2016)</ref>, we use the Mean F1 metric for WikiReading, which assigns partial credit when there are multiple valid answers. We ran hyperparameter tuning for all models and re- port the result for the configuration with the high- est Mean F1 on the validation set. The supervised SWEAR model was trained on both the full training (results reported in Sec- tion 3.5) and on each subset of training data (re- sults reported below). Unsupervised autoencoders were trained on all documents in the WikiRead- ing training set. We selected the autoencoder with the lowest reconstruction error for use in semi-supervised experiments. After initialization with weights from the best autoencoder, learnable parameters in the semi-supervised models were trained exactly as in the supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Details</head><p>We implemented all models in a shared frame- work in TensorFlow ( <ref type="bibr">Abadi et al., 2016)</ref>. We used the Adam optimizer <ref type="bibr" target="#b7">(Kingma and Ba, 2014</ref>) for all training, periodically halving the learning rate ac- cording to a hyperparameter. Models were trained for a maximum of 4 epochs. <ref type="table" target="#tab_6">Table 7</ref> shows which hyperparameters were tuned for each type of model, and the range of values for each hyperparameter. The parameters in the second group of the table are tuned for su- pervised SWEAR and the best setting (shown in bold) was used for other models where applicable. We fixed the batch size to 8 for autoencoders and 64 for semi-supervised models. We used a trun- cated normal distribution with a standard deviation of 0.01 for VRAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>2 Initialization with Word2Vec ( <ref type="bibr" target="#b10">Mikolov et al., 2013</ref>) em- beddings on 1% subset gives 64.0 Mean F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>100 200 SWEAR 25.0 33.0 SWEAR-SS (VRAE) 39.0 45.0 <ref type="table">Table 5</ref>: Results for SWEAR and the best SWEAR-SS initial- ization (VRAE) trained on 100-and 200-per-property sub- sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Mean F1 SWEAR-PR 66.5 dropout on input only 65.4 no dropout 64.6 shared reviewer cells 63.8 SWEAR-MLR 63.0 w/o skip connections 60.0 <ref type="table">Table 6</ref>: Results for semi-supervised reviewer models trained on the 1% subset of WikiReading. <ref type="table" target="#tab_4">Table 4</ref> and 5 show the results of SWEAR and semi-supervised models with pretrained and fixed embeddings. Results show that SWEAR-SS al- ways improves over SWEAR at small data sizes, with the difference become dramatic as the dataset becomes very small. VRAE pretraining yields the best performance. As training and test- ing datasets have different distributions in per- property subsets, Mean F1 for supervised and semi-supervised models drops compared to uni- form sampling. However, initialization with pre- trained VRAE model leads to a substantial im- provement on both subsamples. We further exper- imented by initializing the decoder (vs. only the encoder) with pretrained autoencoder weights but this resulted in a lower Mean F1. <ref type="table">Table 6</ref> shows the results of semi-supervised reviewer models. When trained on 1% of the training data, SWEAR-MLR and the supervised SWEAR model perform similarly. Without us- ing skip connections between embedding and hid- den layers, the performance drops. The SWEAR- PR model further improves Mean F1 and outper- forms the strongest SWEAR-SS model, even with- out fine-tuning the weights initialized from the au- toencoder.</p><p>The success of SWEAR-PR rests on multiple design elements working together, as shown by the reduced performance caused by altering or disabling them. Using dropout only on the in- puts, or not using any dropout on reviewer cells, causes a substantial decrease in Mean F1 score (by 1.1 and 1.9, respectively). Configuring the model with many more review steps <ref type="formula" target="#formula_0">(15)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our model architecture is one of many hierarchi- cal models for documents proposed in the litera- ture. The most similar is proposed by <ref type="bibr" target="#b2">Choi et al. (2017)</ref>, which uses a coarse-to-fine approach of first encoding each sentence with a cheap BoW or Conv model, then selecting the top k sentences to form a mini-document which is then processed by a standard seq2seq model. While they also evalu- ate their approach on WikiReading, their emphasis is on efficiency rather than model accuracy, with the resulting model performing slightly worse than the full seq2seq model but taking much less time to execute. SWEAR also requires fewer sequential steps than the document length but still computes at least as many recurrent steps in parallel. Our model can also be viewed as containing a Memory Network (MemNet) built from a doc- ument ( <ref type="bibr" target="#b18">Weston et al., 2014;</ref><ref type="bibr" target="#b16">Sukhbaatar et al., 2015)</ref>, where the memories are the window encod- ings. The core MemNet operation consists of at- tention over a set of vectors (memories) based on a query encoding, and then reduction of a second set of vectors by weighted sum based on the attention weights. In particular, <ref type="bibr">Miller et al. (2016)</ref> intro- duce the Key-Value MemNet where the two sets of memories are computed from the keys and val- ues of a map, respectively: In their QA task, each memory entry consists of a potential answer (the value) and its context bag of words (the key).</p><p>Our reviewer approach is inspired by "Encode, Review, Decode" approach introduced by <ref type="bibr" target="#b19">Yang et al. (2016)</ref>, which showed the value of introducing additional computation steps between the encoder and decoder in a seq2seq model. The basic recurrent autoencoder was first intro- duced by <ref type="bibr" target="#b3">Dai et al. (2015)</ref>, a standard seq2seq model with the same input and output. <ref type="bibr" target="#b4">Fabius et al. (2014)</ref> expanded this model into the Varia- tional Recurrent Autoencoder (VRAE), which we describe in Section 4.1.1. VRAE is an applica- tion of the general idea of variational autoencod- ing, which applies variational approximation to the posterior to reconstruct the input <ref type="bibr" target="#b8">(Kingma and Welling, 2013)</ref>. While we train window autoen- coders, an alternative approach is hierarchical doc- ument autoencoders ( <ref type="bibr" target="#b9">Li et al., 2015)</ref>.</p><p>The semi-supervised approach of initializing the weights of an RNN encoder with those of a recurrent autoencoder was first studied by <ref type="bibr" target="#b3">Dai et al. (2015)</ref> in the context of document classifica- tion and further studied by <ref type="bibr" target="#b13">Ramachandran et al. (2016)</ref> for traditional sequence-to-sequence tasks such as machine translation. Our baseline semi- supervised model can be viewed as an extension of these approaches to a reading comprehension set- ting. <ref type="bibr" target="#b3">Dai et al. (2015)</ref> also explore initialization from a language model, but find that the recurrent autoencoder is superior, which is why we do not consider language models in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have demonstrated the efficacy of the SWEAR architecture, reaching state of the art performance on supervised WikiReading. The model improves the extraction of precise information from long documents over the baseline seq2seq model. In a semi-supervised setting, our method of reusing (V)RAE encodings in a reading comprehension framework is effective, with SWEAR-PR reaching an accuracy of 66.5 on 1% of the dataset against last year's state of the art of 71.8 using the full dataset. However, these methods require careful configuration and tuning to succeed, and making them more robust presents an excellent opportu- nity for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SWEAR model: Boxes are RNN cells, colors indicate parameter sharing.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,453.54,239.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-layer reviewer model (SWEAR-MLR), shown operating over a single window: Black boxes are RNN cells with fixed weights copied from the autoencoder, diamonds indicate vector concatenation with adapter and FC layers. For simplicity, only the cells in dashed boxes are fully illustrated (detailed in Figure 3), but the same structure is repeated for each cell.</figDesc><graphic url="image-2.png" coords="6,72.00,62.81,218.27,151.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Detailed illustration of the dashed box in SWEARMLR question encoder. Black boxes are fixed parameters and encodings. The window encoder is similar, except that the output of the question reviewer layer is also added to the concatenated input (dashed line).</figDesc><graphic url="image-3.png" coords="6,361.84,62.81,109.14,153.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Progressive reviewer model (SWEAR-PR), shown operating over a single window: Black boxes are RNN cells with fixed weights copied from the autoencoder, diamonds indicate vector concatenation with adapter and FC layers. Dashed boxes contain reviewer layers. Cells within reviewer layers are decoupled as indicated by different colors.</figDesc><graphic url="image-4.png" coords="7,72.00,62.81,218.27,129.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of attention cell: GRU state is used to attend over attendable states, then final state is computed by concatenating GRU state with context vector and passing through a fully connected neural network.</figDesc><graphic url="image-5.png" coords="7,99.28,277.19,163.70,126.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results for SWEAR compared to top published re-
sults on the WikiReading test set. 

HE16 Best SWEAR 
Categorical 
88.6 
88.6 
Relational 
56.5 
63.4 
Date 
73.8 
82.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean F1 for SWEAR on each type of property com-
pared with the best results for each type reported in Hewlett 
et al. (2016), which come from different models. Other pub-
lications did not report these sub-scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 shows that SWEAR outperforms the best</head><label>1</label><figDesc></figDesc><table>Doc length 
pct seq2seq SWEAR imp 
[0, 200) 44.6 
79.7 
80.7 
1.2 
[200, 400) 19.5 
76.7 
77.8 
1.5 
[400, 600) 11.0 
74.5 
76.3 
2.3 
[600, 800) 
6.6 
72.8 
74.3 
2.1 
[800, 1000) 
4.3 
71.5 
72.8 
1.8 
[1000, max) 14.0 
64.8 
65.9 
1.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of Mean F1 for SWEAR and a baseline 
seq2seq model on the WikiReading test set across different 
document length ranges. pct indicates the percentage of the 
dataset falling in the given document length range. imp is the 
percentage improvement of SWEAR over baseline. 

results for various models reported in both pub-
lications, including the hierarchical models Sof-
tAttend and Reinforce presented by Choi et al. 
(2017). 1 Interestingly, SoftAttend computes an 
attention over sentence encodings, analogous to 
SWEAR's attention over overlapping window en-
codings, but it does so on the basis of less powerful 
encoders (BoW or convolution vs RNN), suggest-
ing that the extra computation spent by the RNN 
provides a meaningful boost to performance. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Mean F1 results for SWEAR (fully supervised) 
and SWEAR-SS (semi-supervised) trained on 1%, 0.5%, and 
0.1% subsets, respectively. Variants of SWEAR-SS indicate 
different sources of fixed encoder weights. 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Hyperparameter search spaces for each model type. 
We use {. . . } to denote a set of discrete values and [. . . ] to 
denote a continuous range. Following Hewlett et al. (2016), 
we ran a random search over the possible configurations. 

a smaller hidden vector size (128) reduced Mean 
F1 to 62.5. Increasing the number of review steps 
for the question to 5 caused a decrease in Mean F1 
of 2.1. 

</table></figure>

			<note place="foot" n="1"> Document lengths differ between publications: We truncate documents to the first 600 words, while Choi et al. truncate to 1000 words or 35 sentences and Hewlett et al. truncate to 300 words.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<editor>Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Viégas</editor>
		<imprint/>
	</monogr>
	<note>and Xiaoqiang Zheng. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarse-to-fine question answering for long document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variational recurrent auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otto</forename><surname>Fabius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joost R Van Amersfoort</surname></persName>
		</author>
		<idno>arxiv</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wikireading: A novel large-scale language understanding task over wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fandrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>arxiv</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">AmirHossein Karimi, Antoine Bordes, and Jason Weston. 2016. Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<idno>arxiv</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1611.02683</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progressive Neural Networks. arxiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Encode, review, and decode: Reviewer module for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>arxiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
