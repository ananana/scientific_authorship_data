<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google AI Language</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google AI Language</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google AI Language</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google AI Language</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="305" to="315"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>305</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw, unstruc-tured text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Written language often undergoes several rounds of revision as human authors determine exactly what information they want their words to convey. On Wikipedia, this process is carried out collec- tively by a large community at a rate of nearly two revisions per second ( <ref type="bibr" target="#b26">Yang et al., 2017)</ref>. While Wikipedia's revision history contains arbitrarily complex edits, our corpus and analysis focuses on atomic insertion edits: instances in which an ed- itor has inserted a single, contiguous span of text into an existing complete sentence <ref type="table">(Table 1</ref>). This restriction allows us to make several assumptions which we believe make the data an especially pow- erful source of signal. Namely, we can assume that 1) some information was not communicated by the original sentence, 2) that information should have been communicated (according to a human editor), and 3) that information is communicated by the inserted phrase. Thus, we believe that a large data set of such edits is inherently valuable for researchers modeling inference and discourse * Both authors contributed equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adding new relevant information</head><p>She died there in 1949 after a long illness. Refining claim/Resolving ambiguity Finlay announced he'd be on the 1000th episode of "WWE Monday Night Raw", but he wasn't. Improving Discourse/Fluency It is also being evaluated as a potential bio- logical control for the invasive plant . . . and that the data can yield insights about represen- tation at both the phrase and the sentence level.</p><p>We mine Wikipedia edit history to create a cor- pus of 43 million atomic insertion and deletion ed- its covering 8 languages. We argue that the cor- pus contains distinct semantic signals not present in raw text. We thus focus our experiments on an- swering the following questions:</p><p>1. How is language that is inserted during edit- ing different from general Wikipedia text?</p><p>2. What can we learn about language by observ- ing the editing process that we cannot readily learn by observing only the final edited text?</p><p>Specifically, the contributions of this paper are:</p><p>• A new corpus (WikiAtomicEdits) of 26M atomic insertions and 17M atomic deletions covering 8 languages ( §3 and §4): http:</p><p>//goo.gl/language/wiki-atomic-edits.</p><p>• Linguistic analysis showing that inserted lan- guage differs measurably from the language observed in general Wikipedia text ( §5).</p><p>• Language modeling experiments showing that models trained on WikiAtomicEdits en- code different aspects of semantics and dis- course than models trained on raw, unstruc- tured text ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theoretical Motivation</head><p>We borrow the idea of an atomic edit from prior work in natural language inference, specif- ically natural logic <ref type="bibr" target="#b16">(Lakoff, 1970;</ref><ref type="bibr">Van Benthem, 1986</ref>). MacCartney (2009) defines an atomic edit e applied to a natural language expression s as the insertion, deletion, or substitution of a sub-expression p such that both the original ex- pression s and the resulting expression e(s) are well-formed semantic constituents. E.g. s = "She died from an illness", p = "in 1949", and e(s) = "She died in 1949 from an illness". This formulation is desirable because it exposes a rela- tionship between the surface form and the seman- tics of natural language while remaining agnos- tic about the underlying semantic representation. That is, the difference in "meaning" between s and e(s) is exactly the "meaning" of p (in context), re- gardless of how that meaning is represented. We adopt this philosophy in creating our cor- pus. We focus our analysis specifically on atomic insertion edits. We make the assumption that edi- tors on Wikipedia are attempting to communicate true information 1 and to do so effectively. Inser- tion edits are thus particularly interesting because the underlying generation process admits the fol- lowing assumptions:</p><p>1. The original sentence s does not effectively communicate some piece of information.</p><p>2. A reasonable reader of s would like/expect this information to be communicated.</p><p>3. This information is communicated by the in- serted phrase p (in the context of e(s)).</p><p>We therefore believe that the supervision provided by insertion edits can improve our understanding of semantics, discourse, and composition, and that the data released will be valuable for research in these areas. The goal of our experiments is to es- tablish that the signal provided in these edits is distinct from what one could easily obtain given currently available text corpora.  3 WikiAtomicEdits: Corpus Creation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting Edits</head><p>Wikipedia edits can be accessed through Wikipedia dumps. The edits are stored as diffs on the entire Wikipedia page, meaning some processing is required to reconstruct the changes that were made at the sentence level. We use historical snapshots of each Wikipedia document and compare against subsequent snapshots to extract sentence-level edits. We strip the HTML tags and Wikipedia markup of the page and then run a sentence splitter <ref type="bibr" target="#b9">(Gillick, 2009)</ref> to obtain a list of sentences for each snapshot. Rather than run a full, quadratic-time <ref type="bibr" target="#b22">(Myers, 1986)</ref> sequence alignment to compare the two lists of sentences, which is infeasible for long articles, we propose an efficient precision-oriented approximation.</p><p>Given n sentences in one snapshot ("base") and m sentences in a subsequent one ("edited"), we assume that most edits are local and restrict our attention to a fixed-size window. For each sen- tence s i in the base snapshot, we compute pairwise BLEU scores ( <ref type="bibr">Papineni et al., 2002</ref>) between s i and the sentences {t j } i+k j=i−k (k = 5) in the edited snapshot. We consider the sentence with the high- est BLEU score in this window as a candidate. If the sentences are not identical and the difference consists of an insertion or deletion of a single con- tiguous phrase 2 , we add this example to the cor- pus. For each article, we run this algorithm over the most recent 100,000 snapshots as of February 2018. We extract edits for 8 languages. Statistics are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Insertions vs. Deletions</head><p>We use the algorithm described above to extract both atomic insertions and atomic deletions. How- ever, we chose to omit the deletions from our lin- guistic ( §5) and language modeling ( §6) analyses for two reasons. First, our intuition is that spans which are deleted by an editor are more likely to be "bad" phrases (e.g. spam, false information, or grammatical errors introduced by a previous editor). To confirm this, we manually inspected 100 of each type of edit. We found that indeed deletions contained a higher proportion of spam text and malformed English (16/100) than did in- sertions (7/100). Second, while insertions permit a clean set of assumptions about the relationship between the original sentence and the edited sen- tence ( §2), it is more difficult to make generaliza- tions about deletions. Specifically, it is difficult to say whether the original sentence should not communicate the information in the deleted phrase (i.e. the phrase contains false, irrelevant, or oth- erwise erroneous information) or rather the origi- nal sentence/surrounding context already commu- nicates the information in the deleted phrase (i.e. the deleted phrase is redundant). As such, dele- tions are a noisier target for analysis. Nonetheless, we recognize that the deletions provide a related and likely useful signal. We thus include deletions in our corpus but leave their deeper linguistic anal- ysis for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Corpus Quality &amp; Reproducibility</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Annotation</head><p>Given the data collected as above, we now in- vestigate whether the extracted edits are suffi- ciently clean to be useful for computational lan- guage analysis and modeling. To do this, we focus our attention specifically on the English, Spanish, and German subcorpora, as these are languages for which we could find a sufficient number of native speakers to perform the necessary annotation for our analysis. Thus, the discussion and results in this section may not be representative of the other languages in the corpus.</p><p>We are interested specifically in two questions. First, we want to measure the overall corpus qual- ity: how many of the inserted phrases represent meaningful edits and how many are simply noise (e.g. from editor or preprocessing error)? Second, we want to understand, at least in part, the repro- ducibility of the corpus: could we expect a differ- en es de No Error 78% 55% 85% Possible Error 13% 30% 9% Clear Error 9% 15% 6% <ref type="table">Table 3</ref>: Corpus quality for three languages for which we were able to collect annotations. "No Error"/"Clear Error" means annotators agreed unanimously that the edit was/was not an error; "Possible Error" means annotations were mixed.</p><p>ent group of human editors to produce the same edits as those observed?</p><p>To address these questions, we collect annota- tions in a semi-generative manner. Each annotator is shown a sentence s and a phrase p to be inserted, and is asked to insert p into s in order to form a new sentence e(s). If s is not a complete and well-formed sentence, or if there is no location at which p can be inserted such that e(s) would be a complete and well-formed sentence, annotators are instructed to mark the edit as an error. We use the "error" labels in order to study corpus quality ( §4.2) and use the annotators' insertion location to estimate reproducibility ( §4.3).</p><p>We collect labels for 5,000 English edits, and 1,000 each for Spanish and German edits using a crowd-sourcing platform. We collect 5-way an- notations for English and 3-way annotations for Spanish and German. Our choices of languages and the differing levels of redundancy were due to availability of annotators. We will release these 7,000 edits and their annotations with the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Corpus Quality</head><p>To measure corpus quality, we compute the pro- portion of edits marked as errors by our annota- tors. <ref type="table">Table 3</ref> shows our results. For English, in 78% of cases our annotators agreed unanimously that p could be inserted meaningfully into s (55% for Spanish; 85% for German). These numbers re- assure us that, while there is some noise, the ma- jority of the corpus represents legitimate edits with meaningful signal. For more discussion of the er- rors refer to Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Agreement and Ambiguity</head><p>We next explore the extent to which the edits in the corpus are reproducible. In an ideal world, we would like to answer the question: given the same original sentences, would a different group of hu-man editors produce the same edits? Answering this directly would require annotators with domain expertise and is infeasible in practice. However, we can use our crowdsourced annotation to an- swer a restricted variant of this question: given a sentence s and an insertable phrase p, do humans agree on where p belongs in s? We can measure agreement in this setting straightforwardly using exact match, and can interpret human performance as that of a "perfect" language model. I.e. we can interpret disagreement as evidence that repro- ducing the particular edit is dependent on exoge- nous information not available in the language of s alone (e.g. knowledge of the underlying facts be- ing discussed, or of the author's individual style).</p><p>Based on our annotation experiment, we find that individual annotators agree with the original editor 66% of the time for English, 72% for Span- ish, and 85% for German. <ref type="bibr">3</ref> More interesting than how often humans disagree on this task, however, is why they disagree. To better understand this, we take a sample of 100 English sentences in which at least one human annotator disagreed with the original editor and no annotator marked the edit as an error. We then manually inspect the sample and record whether or not the annotators' choices of different insertion points give rise to sentences with different semantic meaning or simply to sen- tences with different discourse structure.</p><p>In particular, we consider three categories for the observed disagreements: 1) the sentences are meaning equivalent from a truth-conditional per- spective, 2) the sentences contain significant dif- ferences in meaning from a truth-conditional per- spective, or 3) the sentences contain minor dif- ferences or ambiguities in meaning (but would likely be considered equivalent from the point of view of most readers). We also include an error category, for when the disagreement stems from a single annotator making an erroneous choice. Examples of each category are given in <ref type="table" target="#tab_2">Table 4</ref>. Note that the assessment of the truth conditions of the sentence and their equivalence is based on our judgment, and many of these judgments are sub- jective. We will release our annotations for this analysis with the corpus, to enable reproducibility and refinement in future research. <ref type="table">Table 5</ref> shows our results. We found 49% to be meaning equivalent (i.e. the edit's location ef-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Corpus Linguistic Analysis</head><p>We now turn our attention to exploring the lan- guage in the corpus itself. In this section and in §6, our focus is on the questions put forth in the introduction: 1) how does the language that is in- serted during editing differ from language that is observed in general? and 2) what can we learn about language by observing the editing process that we cannot readily learn by observing only raw text? Here, we explore these questions from a cor- pus linguistics perspective. The analysis in this section is based predominantly on the 14M inser- tion edits from the English subcorpus <ref type="table" target="#tab_1">(Table 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Manual Categorization of Insertions</head><p>We first characterize the types of insertions in terms of the function they serve. Manually in- specting the edits, we identify four high-level cate- gories. Note that we do not intend these categories to be formal or exhaustive, but rather to be illus- trative of the types of semantic and discourse phe- nomena in the corpus: i.e. to give sense of the bal- ance between semantic, pragmatic, and grammati- cal edits in the corpus. The categories we identify are as follows:</p><p>1. Extension: the explicit addition of new infor- mation that the author of the original sentence did not intend to communicate. 4</p><p>2. Refinement: the addition of information that the author of the original sentence either in- tended to communicate or assumed the reader would already know. This category includes hedges, non-restrictive modifiers, and other clarifications or scoping-down of claims.</p><p>3. Fluency / Discourse: grammatical fixes, as well as the insertion of discourse connectives ("thus"), presuppositions ("also"), and edi- torializations ("very").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meaning Equivalent</head><p>Paul Wheelahan, the son of a mounted policeman, was born in Bombala, South Wales. . . Paul Wheelahan was born in Bombala, South Wales, the son of a mounted policeman,. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minor Difference / Ambiguity</head><p>She moved to Australia in 1964 and attended the University of New South Wales. . . She moved to Australia and attended the University of New South Wales in 1964. . . Significant Difference in Meaning . . . he and Bart have to share a raft with Ned Flanders and his youngest son, Todd Flanders. . . . he and his youngest son, Bart have to share a raft with Ned Flanders and Todd Flanders.  <ref type="table">Table 5</ref>: Analysis of 100 sentences for which at least one annotator disagreed with the gold label and no annotator marked as an error.</p><p>4. Referring Expressions (RE): changes in the name of an entity that do not change the un- derlying referent, such as adding a first name ("Andrew") or a title ("Dr."). RE edits could fall within our definition of "refinement", but because they are especially prevalent we an- notate them as a separate category.</p><p>We also define an Error category for spam, van- dalism, and other "mistake" edits. We manually categorize 100 randomly-sampled edits. The breakdown is shown in <ref type="table" target="#tab_3">Table 6</ref>. In our sample, the majority (43%) were extensions, and the second most frequent where refinements (24%). No single category dominates and all are well-represented, suggesting that a variety of phe- nomena can be studied using this corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparing Insertions to Raw Text</head><p>Understanding the high-level functions of edits, as above, provides some insight into the type of lin- guistic signals contained in the data. However, we are particularly interested in whether the lan- guage used for these functions is noticeably dif- ferent from general Wikipedia text. That is: it is not obvious that the language humans use to e.g. extend or refine an existing claim should necessar- ily be different, in aggregate, from the language used to formulate these claims in general. We thus explore whether this is the case.</p><p>We first compare the distribution of parts of speech observed for the inserted phrases to the distribution of parts of speech that we observe in Wikipedia overall-i.e. in the sentences appear- ing in the final, published version of Wikipedia, not only the edit history. In order to compare the relative frequencies in a straightforward way, we look only at edits in which a single word was in- serted. <ref type="bibr">5</ref>  <ref type="figure" target="#fig_0">Figure 1</ref> shows our results for English, Spanish, and German. We see, for example, that in English, adjectives and adverbs combined make up nearly 30% of all inserted words, three and a half times higher than the frequency of adjec- tives/adverbs observed in the general Wikipedia corpus, and that proper nouns are inserted at a higher rate than would be suggested given their base frequency.</p><p>Looking more carefully, we see that the nature of the edits for each part of speech are qualita- tively different as well. To explore this further, we look at which words appear at substantially higher rate as insertions than they do in the gen- eral Wikipedia corpus. We compute this as fol- lows: for a word w with part of speech pos, we compute the number of times w occurs as an in- sertion per thousand insertions of any word of type pos, and compare this to the rate of occurrence of w per thousand occurrences of any word of type pos within the general Wikipedia corpus. <ref type="table" target="#tab_5">Table 7</ref> shows our results for English (Spanish and Ger- man are given in the Supplementary Material). In particular, we see that many words which are in- serted at a significantly higher-than-baseline rate reflect "refinement"-type edits. Many of these are words which the original author may have commu-Category Freq. Example Extend 43% The population was 39,000 in 2004, measured at 29,413 at the 2011 Census. Refine 24% . . . began an investigation into Savile 's apparent history of abuse. . . RE 11% Andrew Sugerman has been involved in the production of motion pictures. . . Fluency 9% Philippine coconut jam, meanwhile, is made from coconut cream. . . Error 13% The team are well -known as a loser team in the past 5 years.The team is. . .  nicated implicitly but the editor chose to state ex- plicitly, such as whether or not a person is a "cur- rent"/"former" public figure <ref type="bibr">6</ref> or is "famous". On the other hand, words which are inserted at a sig- nificantly lower-than-baseline rate are those which would be unlikely to be omitted by the original author. For example, if an event is famously the "first" or the "only" one of its kind, it is highly unlikely for the original author describing that event not to use these words explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Language Modeling Analysis</head><p>We next explore the corpus from a language mod- eling perspective. Again, we are interested in un- derstanding how the signal captured by the editing process is distinct from that captured by the final edited text alone, and in characterizing the types of signals we can learn from modeling the insertions directly. We investigate this through two tasks: first, given a sentence s and insertable phrase p, predict the index i at which p should appear in s ( §6.1), and second, given a sentence s and an index i, generate candidate phrases that would be appro- priate to insert into s at i ( §6.2).  Words that appear as insertions at sig- nificantly higher rates (top row) and significantly lower rates (bottom row) than their rate of occu- rance in Wikipedia in general. We compute "rate" as simply the observed occurrence of the given word per thousand occurrences of any word with the given POS. Table shows each word followed by (rate as insertion):(rate in general)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Predicting Insertion Locations</head><p>Task. This task-given a phrase p and a sentence s, choose the best index i in s at which to in- sert p-is identical to the task we asked humans to perform in §4. We consider two simple models for performing this task: a basic language model and a discriminative model trained on the insertion data. We report performance as overall accuracy. We analyze whether a model which is trained to model insertions directly captures something dif- ferent than a general language model in terms of the types of errors each model makes.</p><p>Models. We evaluate two models. First, we evaluate a standard language modeling baseline (General LM), in which we simply insert the phrase p at every possible point in s and chose the index which yields the lowest perplexity. We use the LSTM language model from <ref type="bibr" target="#b14">Jozefowicz et al. (2016)</ref>, which obtained SOTA results on lan- guage modeling on the one billion words bench- mark for English ( <ref type="bibr" target="#b5">Chelba et al., 2013</ref>). We train this language model for each language on an aver- age of ∼ 500 million tokens from Wikipedia. Sec- ond, we evaluate a discriminative model specifi- cally trained on the insertion data <ref type="bibr">(Discriminative Model)</ref>. This model represents the base sentence using a sentence encoder that produces a context- dependent representation of every word index in the sentence, and then at test time, compares the learned representation of each index with the rep- resentation of the phrase p to be inserted. We use a 256-dimensional 2-layer biLSTM encoder, initial- ized with FastText 300-dimensional word vectors ( . <ref type="bibr">7</ref> We hold out 50K and 10K insertion edits for each lan- guage as development and test sets, and use the re- maining edits (insertions and deletions) as training data. This provides us with at least 1 million ex- amples for training in each language (cf. <ref type="table" target="#tab_1">Table 2</ref>). See Supplementary Material for additional details.</p><p>Results. <ref type="table">Table 8</ref> shows the accuracy of each model for each language. We see that the discri- minitve model trained on insertions directly per- forms better than the general LM by at least 1% absolute accuracy on every language, and by 3.8% absolute on average. It is worth emphasizing that this performance improvement is despite the fact that the general LM was trained with, on average, four times the number of tokens 8 and is a much larger model-the general LM has ∼ 2 billion pa- rameters (Jozefowicz et al., 2016) compared to ∼ 1 million for the discriminative model. More interesting than raw performance is the difference in the types of errors that the models <ref type="table">Table 8</ref>: Insertion accuracy on the test set.</p><note type="other">General LM Discr. Model German 68.1 72.9 English 58.7 68.4 Spanish 67.0 70.1 French 69.9 73.4 Italian 69.0 72.9 Japanese 73.0 74.2 Russian 72.9 74.3 Chinese 65.5 68.9 Average 68.0 71.8</note><p>make. For each model, we take a random sample of 50 examples on which the model made a correct prediction and 50 examples on which the model made an incorrect prediction. We annotate these 200 examples 9 according to the edit type classi- fication discussed in §5.1. <ref type="table">Table 9</ref> shows the re- sults. We find a significant difference 10 (p &lt; 0.01) between the types of edits on which the General LM makes correct predictions and the types on which it makes incorrect predictions. Specifically, the General LM appears to be especially good at predicting location for fluency/discourse edits, and especially poor at predicting the location of refine- ment edits. In contrast, we do not see any sig- nificant bias in the errors made by the discrimi- native model compared to its correct predictions (p = 0.23). We interpret this as evidence that the insertion data captures some semantic signal that is not readily gleaned from raw text corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Predicting Insertion Phrases</head><p>Task. In a final set of experiments, we explore a generative version of the language modeling task: given a sentence s and an specified index i, gener- ate a phrase p which would be appropriate to insert into s at i. We are interested in what such a model can learn about the nature of how sentences are extended: what type of information would be rele- vant from a semantic perspective, and natural from a discourse perspective to insert at a given point? We train two models for this task, one trained on the training split of the WikiAtomicEdits corpus, and one baseline trained on a comparable set of phrasal insertions not derived from human edits.</p><p>We evaluate on the same 10K held-out insertion Base General Discr. <ref type="table" target="#tab_1">Freq.  LM  Model    Extend  25  21 19 25 21  Refine  14  7 18 13 14  RE  6  7  9  4  9  Fluency  5  15 4  8  6   Table 9</ref>: Relationship between model accuracy and insertion type, based on a sample of 50 correct () and 50 incorrect () predictions from each model. Base frequency is shown for reference and is based on our analysis from §5.1. The General LM shows a bias in accuracy by insertion type. This bias is not observed for the discriminative model. edits as in §6.1, and measure performance using both a strict "exact match" as well as a softer sim- ilarity metric.</p><p>Model. We use an standard sequence-to- sequence model <ref type="bibr">(Sutskever et al., 2014</ref>), modi- fying the input with a special token denoting the insertion point. For example, given the input [" Angel " is a song recorded by &lt;ins&gt; pop music duo Eurythmics .], the model would be trained to produce the target phrase <ref type="bibr">[the British]</ref>. We use a two-layer bidirectional encoder using the same 300-dimensional FastText embeddings as in §6.1, and a sequence decoder with attention ( <ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>) using a learned wordpiece model ( <ref type="bibr">Schuster and Nakajima, 2012</ref>) with a vocabulary of 16,000.</p><p>Experimental Design. We train one version of this model on the same set of 23M English ex- amples as the discriminative insertion model from §6.1; we refer to the model trained on this data as Edits. For comparison, we train an identical model on a set of simulated insertions which we create by sampling sentences from Wikipedia and removing contiguous spans of tokens, which we then treat as the insertion phrases. To ensure that this data is reasonably comparable to the Wiki- AtomicEdits data, we parse the sampled sentences ( <ref type="bibr" target="#b1">Andor et al., 2016</ref>) and only remove a span if it represents a full subtree of the dependency parse and is not the subject of the sentence. <ref type="bibr">11</ref> We gener- ate 23M such "psuedo-edits" for training, the same size as the WikiAtomicEdits training set. We refer to the model trained on this data as General.</p><p>Results. We look at the top 10 phrases proposed by each model, as decoded by beam search. In addition to reporting standard LM perplexity, we compute two measures of performance, which are intended to provide an intuitive picture of how well each model captures the nature of the infor- mation that is introduced by the human editors. Specifically, we compute Exact Match as the pro- portion of sentences for which the model produced the gold phrase (i.e. the phrase inserted by the hu- man editor) somewhere among the top 10 phrases. We also compute Similarity@1 as the mean co- sine similarity of each top-ranked phrase and re- spective gold phrase over the test set. We use the sum of the Glove embeddings ( <ref type="bibr">Pennington et al., 2014</ref>) of each word in the phrase as a simple ap- proximation of the phrase vector. <ref type="table" target="#tab_7">Table 11</ref> shows the results. We see that, com- pared to the model trained on General Wikipedia, the model trained on WikiAtomicEdits generates edits which are more similar to the human inser- tions, according to all of our metrics. <ref type="table" target="#tab_6">Table 10</ref> provides a few qualitative examples of how the phrases generated by the Edits model differ from those generated by the General model. Specifi- cally, we see that the Edits model proposes phrases which better capture the discourse function of the human edit: e.g. providing context for/elaboration on a previously-stated fact. We note that this does not mean that training on Edits is inherently "bet- ter" than on General text, but rather that the su- pervision encoded by the WikiAtomicEdits corpus encodes aspects of language that are distinct from those easily learned from existing resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Wikipedia Edits. Wikipedia edit history has been used as a source of supervision for a vari- ety of NLP tasks, including sentence compression and simplification <ref type="bibr" target="#b25">(Yamangil and Nelken, 2008;</ref><ref type="bibr" target="#b27">Yatskar et al., 2010)</ref>, paraphrasing <ref type="bibr" target="#b19">(Max and Wisniewski, 2010)</ref>, entailment <ref type="bibr" target="#b28">(Zanzotto and Pennacchiotti, 2010;</ref><ref type="bibr" target="#b3">Cabrio et al., 2012)</ref>, and writing as- sistance <ref type="bibr" target="#b29">(Zesch, 2012;</ref><ref type="bibr" target="#b4">Cahill et al., 2013;</ref><ref type="bibr" target="#b11">Grundkiewicz and Junczys-Dowmunt, 2014</ref>). User ed- its from Wikipedia and elsewhere have also been analyzed extensively for insight into the editing process and the types of edits made <ref type="bibr">Gurevych, 2012, 2013;</ref><ref type="bibr" target="#b26">Yang et al., 2017)</ref> She is cited as the first female superstar of Hindi Cinema He is married to Aida Leanca and India 's <ref type="table" target="#tab_1">Meryl Streep  and has two children  Edits  General  Edits  General  and is the best actress of the film  in japan  and has a daughter  in january  and is the best actress of the Indian cinema in june  , and has a daughter  in june  and is the best actress of the film industry  in 2011</ref> , and has a daughter and a daughter in january 2012  Comparison of how closely each model's generated phrases match the phrase inserted by the human editor. "Edits" was trained on Wiki- AtomicEdits and "General" was trained on com- parable data not derived from human edits. We consider the top 10 phrases generated by each model. and to better understand argumentation <ref type="bibr">(Tan and Lee, 2014)</ref>. Particular attention has been given to spam edits <ref type="bibr" target="#b0">(Adler et al., 2011</ref>) and editor quality ( <ref type="bibr" target="#b17">Leskovec et al., 2010)</ref>. Our work differs in that WikiAtomicEdits is much larger than currently available corpora, both by number of languages and by size of individual languages. In addition, our focus on atomic edits should facilitate more controlled studies of semantics and discourse.</p><p>Sentence Representation and Generation. We view the WikiAtomicEdits corpus as being espe- cially valuable for ongoing work in sentence rep- resentation and generation, which requires models of what "good" sentences look like and how they are constructed. Recent work has attempted to model sentence generation by re-writing existing sentences, either using crowdsourced edit exam- ples ( <ref type="bibr" target="#b23">Narayan et al., 2017)</ref> or unsupervised heuris- tics ( <ref type="bibr" target="#b12">Guu et al., 2018)</ref>; in contrast, we provide a large corpus of natural, human-produced edits. Also related is recent work in sentence rep- resentation learning from raw text ( <ref type="bibr" target="#b15">Kiros et al., 2015;</ref><ref type="bibr">Peters et al., 2018</ref>), bitext ( <ref type="bibr" target="#b20">McCann et al., 2017)</ref>, and other supervised tasks including NLI ( <ref type="bibr" target="#b6">Conneau et al., 2017)</ref>. Especially related is work on learning representations from weakly-labelled discourse relations ( <ref type="bibr" target="#b24">Nie et al., 2017;</ref><ref type="bibr" target="#b13">Jernite et al., 2017)</ref>, as the WikiAtomicEdits corpus captures similar types of discourse signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description of Data Release</head><p>Our full corpus is available for download at http: //goo.gl/language/wiki-atomic-edits. The data contains 26M atomic insertions and 17M atomic deletions covering 8 languages. All sen- tences (both the original sentence s, and the edited sentence e(s)) have been POS-tagged and depen- dency parsed <ref type="bibr" target="#b1">(Andor et al., 2016</ref>) as well as scored using a SOTA LM ( <ref type="bibr" target="#b14">Jozefowicz et al., 2016</ref>). We also release the 5K 5-way human insertion annota- tions for English, and 1K 3-way annotations each for Spanish and German, as described in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have introduced the WikiAtomicEdits corpus, derived from Wikipedia's edit history, which con- tains 43M examples of atomic insertions and dele- tions in 8 languages. We have shown that the lan- guage in this corpus is meaningfully different from the language we observe in general, and that mod- els trained on this corpus encode different aspects of semantics and discourse than models trained on raw text. These results suggest that the corpus will be valuable to ongoing research in semantics, dis- course, and representation learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Most frequent POS tags for English, Spanish, and German single-word insertions. Dark blue bars show the relative frequency among inserted phrases and light blue bars show the relative frequency among phrases observed in Wikipedia in general.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The number of instances (in millions) of 
atomic insertions/deletions for each language. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Examples of sentences falling into three disagreement categories, defined in terms of the truth 
conditions of the edited sentence. See text for a more detailed explanation. 

Meaning Equivalent 
49 
Significant Differences in Meaning 22 
Minor Differences/Ambiguities 
13 
Annotator Error 
16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>High-level categories into which we manually characterize edits, to understand the variety of 
phenomena captured by the corpus. Frequencies are based on our annotation of a sample of 100 edits. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 10 : Predicted phrase insertions from model trained on Edits vs. General corpus.</head><label>10</label><figDesc></figDesc><table>The Edits model 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> This is true for the majority of edits, although about 13% of edits are &quot;spam&quot; ( §4.3).</note>

			<note place="foot" n="2"> We use the Python 2.7 difflib library to compute a minimal diff at the byte level.</note>

			<note place="foot" n="3"> We consider cases which the annotator marks as &quot;error&quot; to be a disagreement with the original editor. fected discourse structure only), and 22% to have significant differences in meaning (i.e. the edit&apos;s location fundamentally changed the meaning of the sentence). An additional 13% exhibited minor differences or ambiguities in meaning, and in the remaining 16% of cases, the disagreement appeared to be due to annotator error.</note>

			<note place="foot" n="4"> Whether or not the author &quot;intended&quot; to communicate something is based on our judgment. Since this annotation is intended to be exploratory, we allow a degree of informality.</note>

			<note place="foot" n="5"> In our corpus 30% of inserted phrases are a single word, and 70% are less than five words. We compared frequencies for longer POS sequences as well, but it did not yield particular insight over looking at single POS tags.</note>

			<note place="foot" n="6"> We note that the addition of &quot;former&quot; is likely tied to changes in the real world (Wijaya et al., 2015).</note>

			<note place="foot" n="7"> https://fasttext.cc/docs/en/ crawl-vectors.html 8 The number of tokens in the WikiAtomicEdits is computed as the the total number of words in the edited sentence e(s) after the insertion. Refer to Supplementary Material for more detailed statistics on the size of the dataset.</note>

			<note place="foot" n="9"> To avoid bias, the 200 examples are shuffled and the annotator does not know which group (correct/incorrect, or which model) each example belongs to. 10 We use the chi-squared test provided by scipy.stats.</note>

			<note place="foot" n="11"> Not all of the inserted phrases in WikiAtomicEdits are well-formed constituents. However, generating psuedo-edits using this heuristic provided a cleaner, more realistic comparison than using fully-random spans.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wikipedia vandalism detection: Combining natural language, metadata, and reputation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><forename type="middle">De</forename><surname>Alfaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><forename type="middle">M</forename><surname>Molavelasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CICLing</title>
		<meeting>of CICLing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Slav Petrov, and Michael Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting context-rich entailment rules from wikipedia revision history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Ivanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Workshop on the People&apos;s Web Meets NLP</title>
		<meeting>of the 3rd Workshop on the People&apos;s Web Meets NLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust systems for preposition error correction using wikipedia revisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Napolitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A corpus-based study of edit categories in featured and non-featured Wikipedia articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proce. of COLING</title>
		<meeting>e. of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically classifying edit categories in wikipedia revisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sentence boundary detection and the problem with the U.S</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The wiked error corpus: A corpus of corrective wikipedia edits and its application to grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin Junczys-Dowmunt</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Language Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8686</biblScope>
			<biblScope unit="page" from="478" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Discourse-based objectives for fast unsupervised sentence representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00557</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistics and natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="151" to="271" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Governance in social media: A case study of the Wikipedia promotion process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICWSM</title>
		<meeting>of ICWSM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining naturally-occurring corrections and paraphrases from wikipedia&apos;s revision history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélien</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An O(ND) difference algorithm and its variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eugene W Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Split and rephrase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><forename type="middle">D</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04334</idno>
		<title level="m">Sentence representation learning from explicit discourse relations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mining wikipedia revision histories for improving sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elif</forename><surname>Yamangil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rani</forename><surname>Nelken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Identifying semantic edit intentions from revisions in wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Halfaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kraut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Danescu-Niculescumizil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Expanding textual entailment corpora from wikipedia using co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on The People&apos;s Web Meets NLP</title>
		<meeting>of the 2nd Workshop on The People&apos;s Web Meets NLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring contextual fitness using error contexts extracted from the wikipedia revision history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
