<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3423" to="3432"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3423</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text might express or evoke multiple emotions with varying intensities. As such, it is crucial to predict and rank multiple relevant emotions by their intensities. Moreover, as emotions might be evoked by hidden topics, it is important to unveil and incorporate such topical information to understand how the emotions are evoked. We proposed a novel inter-pretable neural network approach for relevant emotion ranking. Specifically, motivated by transfer learning, the neural network is initialized to make the hidden layer approximate the behavior of topic models. Moreover, a novel error function is defined to optimize the whole neural network for relevant emotion ranking. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label learning methods. Moreover, the extracted emotion-associated topic words indeed represent emotion-evoking events and are in line with our common-sense knowledge.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the growth of social web, people tend to share their opinions, feelings and attitudes on the social platforms such as online news sites and blogs. Emotion detection can enhance the under- standing of users' emotional states, which is useful in many downstream applications such as human- computer interaction and personalized recommen- dation. Therefore, it is crucial to predict emotions from texts accurately <ref type="bibr" target="#b10">(Picard and Picard, 1997)</ref>.</p><p>Research on emotion detection can be rough- ly categorized into two types: generative model based and discriminative model based. Genera- tive model based approaches ( <ref type="bibr" target="#b0">Bao et al., 2012;</ref><ref type="bibr" target="#b14">Rao et al., 2014a)</ref> usually build on topic model- s and assume texts are generated from emotions * Corresponding author and hidden topics. While these models can extract emotion-associated topics, they perform less sat- isfactorily in emotion classification since they are not optimized directly to minimize the misclassifi- cation rate. Discriminative model based approach- es consider each emotion category as a class la- bel and typically cast emotion detection as a clas- sification problem. Approaches to the prediction of both multiple emotions and their intensities in- clude ( <ref type="bibr" target="#b25">Zhou et al., 2018</ref><ref type="bibr" target="#b26">Zhou et al., , 2016</ref><ref type="bibr" target="#b21">Wang and Pal, 2015)</ref>. Those approaches usually assumed word- level representations and ignored the latent topical information behind words, therefore failed to ef- fectively distinguish different emotions carried by the same word in different topical contexts.</p><p>In this paper, we focus on relevant emotion ranking (RER) by differentiating relevant emo- tions from irrelevant ones and only learning the rankings of relevant emotions while ignoring the irrelevant ones. A neural network with a novel loss function is proposed to tackle the RER prob- lem. A topic representing a real-world event, an abstract entity, or an object could indicate the sub- ject or context of the emotion. Different topics might contain or invoke different emotions <ref type="bibr" target="#b18">(Stoyanov and Cardie, 2008)</ref>. Incorporating such latent topics is essential for discovering topic-associated emotions. Motivated by transfer learning, we in- corporate hidden topics and the topic distributions generated from a topic model into a neural net- work for RER. The main contributions of the pa- per are summarized below:</p><p>• A novel Interpretable Neural Network for Relevant Emotion Ranking (INN-RER) is proposed. A novel error function is employed to optimize the whole network for parameter estimation. To the best of our knowledge, it is the first neural network based approach for RER.</p><p>• To understand how the emotions are evoked, the neural network is initialized to make it- s hidden layer approximate the behavior of topic models so that the topical information is unveiled and incorporated.</p><p>• Experimental results on three different real- world corpora show that the proposed method can effectively deal with the emotion de- tection problem and perform better than the state-of-the-art emotion detection methods and multi-label learning methods. Moreover, emotion-association topic words extracted by INN-RER indeed represent emotion-evoking events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In general, approaches for emotion detection can be divided into two categories: generative mod- el based and discriminative model based. Genera- tive model based approaches typically built on top- ic models. For example, the emotion-topic mod- el ( <ref type="bibr" target="#b0">Bao et al., 2012)</ref> was proposed by adding an extra emotion layer into traditional topic model- s to capture the generation of both emotions and topics from text at the same time. Other top- ic model based approaches such as affective top- ic model ( <ref type="bibr" target="#b14">Rao et al., 2014a</ref>), multi-label super- vised topic model and sentiment latent topic mod- el ( <ref type="bibr" target="#b15">Rao et al., 2014b</ref>) also modeled the emotions and topics simultaneously. Contextual sentiment topic model <ref type="bibr" target="#b13">(Rao, 2016)</ref> assumed each word is ei- ther drawn from a background theme, a contextual theme or a topic and explicitly distinguished be- tween context-dependent and context-independent topics. For discriminative model based methods, emo- tion detection is often casted as a classification problem by considering each emotion category as a class label. If only choosing the strongest emo- tion as the label for a given text, emotion detection is essentially a single-label classification problem. <ref type="bibr" target="#b7">Lin et al. (2008)</ref> studied the classification of news articles into different categories based on readers' emotions with various combinations of feature set- s. <ref type="bibr" target="#b20">Strapparava and Mihalcea (2008)</ref> proposed sev- eral knowledge-based and corpus-based methods for emotion classification. <ref type="bibr" target="#b12">Quan et al. (2015)</ref> pro- posed a logistic regression model with emotion de- pendency for emotion detection. Latent variables were introduced to model the latent structure of input text. <ref type="bibr" target="#b6">Li et al. (2016)</ref> combined bi-term topic model and conventional neural network to detect single social emotion from short texts. To predict multiple emotions simultaneously, emotion detec- tion can be solved using multi-label classification. <ref type="bibr" target="#b1">Bhowmick (2009)</ref> presented a method for classi- fying news sentences into multiple emotion cate- gories using an ensemble based multi-label clas- sification technique. <ref type="bibr" target="#b21">Wang and Pal (2015)</ref> out- put multiple emotions with intensities using non- negative matrix factorization with several novel constraints such as topic correlation and emotion bindings. To predict multiple emotions with dif- ferent intensities in a single sentence, <ref type="bibr" target="#b26">Zhou et al. (2016)</ref> proposed a novel approach based on emo- tion distribution learning. Following this way, a relevant label ranking framework for emotion de- tection was proposed for predict multiple relevant emotions as well as the ranking of emotions based on their intensities ( <ref type="bibr" target="#b25">Zhou et al., 2018)</ref>.</p><p>Our work is partly inspired by ( <ref type="bibr" target="#b25">Zhou et al., 2018</ref>) for relevant emotion ranking, but with the following differences: (1) our model takes into account latent topics in texts for emotion detec- tion, which was ignored in the model proposed in ( <ref type="bibr" target="#b25">Zhou et al., 2018)</ref>; (2) our model is built up- on topic models and neural networks with a nov- el objective function defined to consider the inter- play between topics and emotions, while the mod- el in ( <ref type="bibr" target="#b25">Zhou et al., 2018)</ref> was developed based on a ranking framework with a linear objective function which was not able to describe complex relations between the input texts and their emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Approach</head><p>Assuming a set of T emotions L = {e 1 , e 2 , ...e T } and a set of n text instances</p><formula xml:id="formula_0">X = {x 1 , x 2 , x 3 , ..., x n }, each instance x i ∈ R d</formula><p>is associated with a ranked list of its relevant emo- tions R i ⊆ L and also a list of irrelevant emotions</p><formula xml:id="formula_1">R i = L − R i . Relevant emotion ranking aims to learn a score function g(x i ) = [g 1 (x i ), ..., g T (x i )]</formula><p>which assigns a score g j (x i ) to each emotion e j , (j ∈ {1, ..., T }). In order to differentiate relevant emotions from irrelevant ones, we need to define a threshold Θ which could be simply set to a fixed value or learned from data ( <ref type="bibr">Fürnkranz et al., 2008)</ref>. Those emotions with scores lower than the threshold will be considered as irrelevant and hence discarded. The identification of rele- vant emotions and their ranking can be obtained simultaneously according to their scores assigned by the learned ranking function g. As mentioned before, it is unnecessary to consider the rankings of irrelevant emotions since they might introduce errors into the model during the learning process.</p><p>We propose an Interpretable Neural Network for Relevant Emotion Ranking (INN-RER) built upon a multi-layer feed-forward neural network. Instead of using the simple sum-of-squares error function, a novel loss function is designed and em- ployed. Accordingly, a new learning algorithm is proposed to minimize the new loss function. Fur- thermore, motivated by transfer learning, topical information generated from a topic model is trans- ferred into the neural network by making its hid- den layer approximate the behavior of topic mod- els.  ... The overall framework of INN-RER is shown in <ref type="figure" target="#fig_2">Figure 1</ref>. The left part is a typical topic mod- el ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>. It is designed for discover- ing the main topics that pervade a large unstruc- tured collection of documents. A document is al- lowed to contain a mixture of topics with differ- ent weights. As such, a document d can be rep- resented by its topic distribution θ d . The right part is a three-layer neural network. It has d in- put units corresponding to the d-dimensional fea- ture vector of each training sample x i , T output units corresponding to all possible emotion labels, and one hidden layer with P hidden units corre- sponding to the hidden topics. The input layer is fully connected to the hidden layer with weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RER Data Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RER Loss</head><formula xml:id="formula_2">V = [v qh ](1 ≤ q ≤ d, 1 ≤ h ≤ P )</formula><p>and the hidden layer is fully connected to the output layer with</p><formula xml:id="formula_3">weights W = [w hj ](1 ≤ h ≤ P, 1 ≤ j ≤ T ).</formula><p>The bias parameters α h (1 ≤ h ≤ P ) of the hidden u- nits are considered as weights from an extra input unit with a fixed value of 1. Similarly, the bias pa- rameters β j (1 ≤ j ≤ T ) of the output units are considered as weights from an extra hidden unit, with a fixed value of 1.</p><p>The learning process of INN-RER consists of two main steps. Firstly, the first two layers of the network are initialized based on the out- put of the topic model. The feature transfor- mation in neural network is conducted by mini- mizing the Kullback-Leibler (KL) divergence be- tween the topic distribution θ produced by the topic model and the approximated distribution [T opic 1 , T opic 2 , ..., T opic P ] learned by the first two layers of the neural network, which is denoted by the blue rectangular dash line boxes in <ref type="figure" target="#fig_2">Figure 1</ref>. Then, the whole network is learnt and fine-tuned based on the novel loss function, which is denoted as the orange rectangular dash line boxes. Each step will be described in details in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">INN-RER Initialization</head><p>As the number of hidden neurons and its seman- tic meaning is usually treated as a black box in conventional neural networks, the generated top- ics from the topic model are employed for guid- ing the construction of the hidden layer in the proposed neural network. By doing that, seman- tic topic information is incorporated to enhance the interpretability and accuracy of the proposed neural network. For a particular text sample x i in training set G, the input layer takes its term- frequency representation x q i as the input and feeds it to the hidden layer. Assuming the total num- ber of topics is fixed as P , then the hidden layer would contain P neurons. The topic mixture θ x i generated from the topic model is approximated by the weights connecting the input and the hid- den layers. Mathematically, the initialization pro- cedure learns a function f (x q i |v qh , α h ) so that the output of f (x q i |v qh , α h ) is as close to θ x i as pos- sible, where x q i , v qh , α h and θ x i denote the input, weight vector, bias of the first two layers of the network and the topic distribution of text x i gen- erated from the topic model, respectively. A soft- max function is applied to the output of the hid- den layer, i.e., f (x q i |v qh , α h ), and the Kullback- Leibler divergence <ref type="bibr" target="#b5">(Leahy, 2006</ref>) is employed as follows:</p><formula xml:id="formula_4">L(θ, f ) = θ log θ f + (1 − θ) log 1 − θ 1 − f (1)</formula><p>where θ denotes a topic distribution derived from the topic model, and f denotes the output of the hidden layer. The KL divergence is a mea- sure of the difference between two distributions. It is always non-negative and equals to zero when the two distributions are the same. As shown in Equation 1, the KL divergence can describe the difference between the topic distribution generat- ed from topic models and the approximate dis- tributions learned in the initialization procedure. Note that the topic distribution for a documen- t generated by the topic model is used as the su- pervision information for initializing INN-RER. Thus, maximizing the log-likelihood is equivalent to minimizing the KL divergence according to E- quation 1, and its gradients are as follows:</p><formula xml:id="formula_5">∂L(θ, f ) ∂v qh = −(θ x i ,h − f h (x q i |v qh , α h )) · x q i (2) ∂L(θ, f ) ∂α h = −(θ x i ,h − f h (x q i |v qh , α h ))<label>(3)</label></formula><p>According to the gradient descent method, the first two layers can be initialized iteratively by E- quation 2 and 3. The initialization procedure for INN-RER is shown in Algorithm 1. η init with the subscript init represents the learning rate during the initialization procedure and λ is the penalty term. Note that the first two layer should be learn- t from topic model as much as possible in order to incorporate topic information, thus the learning rate term η init should be larger than the learning rate during training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">INN-RER Learning</head><p>This step aims to optimize the three-layer neu- ral network to tackle the relevant emotion ranking problem. It can adjust the neural network initial- ized at previous step at the same time. An intu- itive way is to define the global error function for the network on the training set. However, some important characteristics of relevant emotion rank- ing, such as ranking, not considering irrelevant e- motions, are not considered in the classical back propagation algorithm <ref type="bibr" target="#b16">(Rumelhart et al., 1988</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Algorithm of INN-RER Initializa- tion.</head><p>Input: x q i : Term frequency of text x i ; θ x i : Topic distribution of text x i Output:∆v, ∆α: gradient approximation of ini- tialization procedure 1: Initialize ∆v, ∆α as random values 2: for each iteration do <ref type="bibr">3:</ref> for each text x i ∈ G do 4:</p><p>for q = 1, ..., d, h = 1, ..., P do 5:</p><formula xml:id="formula_6">∆v qh ← ∆v qh + η init · (θ x i ,h − f h (x q i |v qh , α h )) · x q i + λ · ∆v qh 6:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for h = 1, ..., P do 8:</p><formula xml:id="formula_7">∆α h ← ∆α h + η init · (θ x i ,h − f h (x q i |v qh , α h )) + λ · ∆α h 9:</formula><p>end for 10:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end for 11: end for</head><p>The error function defined in traditional neural network such as mean-square error only focuses on individual label discrimination, i.e. whether a predicted label is correct or not. It does not con- sider the correlations between different labels of a training instance, e.g., relevant emotions should be ranked higher than irrelevant ones and there is a ranking for relevant emotions according to their intensities. Therefore, to fulfil the requirements of relevant emotion ranking, a novel global error function is defined as follows:</p><formula xml:id="formula_8">E = n i=1 et∈R i es∈∈(et) 1 norm t,s [ exp(−(g t (x i ) − g s (x i )))+ ω ts (g t (x i ) − g s (x i )) 2 ]<label>(4)</label></formula><p>Here, emotion e t and emotion e s are two emo- tion labels and e s is less relevant than emotion e t , represented by e s ∈∈ (e t ). The normaliza- tion term norm t,s is used to balance emotion pairs (e t , e s ) to avoid dominated terms by their set sizes. The term g t (x i ) − g s (x i ) measures the difference between two emotion outputs, e t and e s , of a giv- en input text x i . We want the difference as larger as possible. Furthermore, the negation of this dif- ference is fed to the exponential function in order to severely penalize the i-th error term if emotion e t is much smaller than e s . As the relationship- s among different emotions can provide important clues for emotion detection, we further incorpo- rate the information into the loss function as con- straints. Here, ω ts is the relationship between e- motion e t and e s which is calculated by Pearson correlation coefficient <ref type="bibr" target="#b9">(Nicewander, 1988)</ref>.</p><p>The minimization of the global relevant emo- tion ranking loss function defined in Equation 4 is carried out by gradient descent combined with the back propagation <ref type="bibr" target="#b16">(Rumelhart et al., 1988</ref>). For training instance x i and its label set L i , the actu- al output of the j-th output neuron is(omitting the superscript i without loss of generality):</p><formula xml:id="formula_9">g j = f (netg j + β j )<label>(5)</label></formula><p>where β j is the bias of the j-th output neuron which is a "tanh" function: netg j is the input to the j-th output neuron:</p><formula xml:id="formula_10">netg j = P h=1 b h w hj<label>(6)</label></formula><p>where w hj is the weight which connects the h-th hidden neuron and the j-th output neuron, and P is the number of hidden neurons, i.e., the topics. b h is the output of the h-th hidden neuron:</p><formula xml:id="formula_11">b h = f (netb h + α h )<label>(7)</label></formula><p>where α h is the bias of the h-th hidden neuron, f () is also a "tanh" function. netb h is the input to the h-th hidden neuron:</p><formula xml:id="formula_12">netb h = d q=1 x q v qh<label>(8)</label></formula><p>where x q is the q-th dimension of instance x. v qh is the weight which connects the q-th input neuron and the h-th hidden neuron. "tanh" function is differentiable, the error of the j-th output neuron can be defined as:</p><formula xml:id="formula_13">d j =                      1 norm exp(−(g j − g s )) + 2ω js (g j − g s ) (1 + g j )(1 − g j ), if e j ∈ R i &amp;e s ∈∈ (e j ) − 1 norm exp(−(g t − g j )) − 2ω tj (g t − g j ) (1 + g j )(1 − g j ), if (e j ∈ R i &amp;e j ∈∈ (e t ))or (e j ∈ R i &amp;e j ∈∈ (e t ))<label>(9)</label></formula><p>Similarly, the error of the h-th hidden neuron can be defined as:</p><formula xml:id="formula_14">e h =   T j=1 g j w hj   (1 + b h )(1 − b h )<label>(10)</label></formula><p>In order to reduce the error of the neural net- work INN-RER, we can use gradient descent s- trategy:</p><formula xml:id="formula_15">∆w hj = −η ∂E i ∂w hj = −η ∂E i ∂netg j ∂netg j ∂w hj = ηd j ∂[ P h=1 b h w hj ] ∂w hj = ηd j b h<label>(11)</label></formula><formula xml:id="formula_16">∆v qh = −η ∂E i ∂v qh = −η ∂E i ∂netb h ∂netb h ∂v qh = ηe h ∂[ d q=1 x q v qh ] ∂v qh = ηe h x q<label>(12)</label></formula><p>the biases are updated as follows:</p><formula xml:id="formula_17">∆β j = ηd j ; ∆α h = ηe h<label>(13)</label></formula><p>where η is the learning rate. The training process of the neural network is p- resented in Algorithm 2. Forward compute output of INN-RER's score function g given x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Backward compute the gradient accord- ing to g and L based on the relevant e- motion ranking loss function with learn- ing rate of η learn and penalty term λ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed approach on the follow- ing three corpora:</p><p>Sina Social News (News) was collected from the Sina news Society channel where readers can choose one of the six emotions such as Amusemen- t, Touching, Anger, Sadness, Curiosity, and Shock after reading a news article. As Sina is one of the largest online news sites in China, it is sensible to carry out experiments to explore the readers' emo- tion (social emotion). News articles with less than 20 votes were discarded since few votes can not be considered as proper representation of social emotion. In total, 5,586 news articles published from January 2014 to July 2016 were kept, togeth- er with the readers' emotion votes. Ren-CECps corpus (Blogs) (Quan and Ren, 2010) contains 1,487 blogs in Chinese. Each document is annotated with eight basic emotions from writer's perspective, including anger, anxi- ety, expect, hate, joy, love, sorrow and surprise, together with their emotion scores indicating the level of emotion intensity in the range of <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Higher scores represent higher emotion intensity. <ref type="bibr">SemEval (Strapparava and Mihalcea, 2007</ref>) is an English data set containing 1,250 news headlines extracted from Google news, CNN, and many other portals. The news headlines are typically short. Each headline was manually scored in a fine-grained valence scale of 0 to 100 across 6 emotions (i.e., anger, disgust, fear, joy, sad and surprise). After pruning 4 items with the total s- cores equal to 0, 1246 headlines are got for the experiments. The statistics of the three corpora are shown in <ref type="table">Table 1</ref>. The first two corpora were preprocessed by using the python jieba segmenter 1 for word seg- mentation and filtering. The third corpus SemEval is in English and can be tokenized by white spaces. Stop words and words appeared only once or in 1 https://github.com/fxsjy/jieba less than two documents were removed to allevi- ate data sparsity. Next, TF-IDF (term frequency- inverse document frequency) was used to extract the features from text. TF-IDF is a numerical s- tatistic method that is designed to reflect how im- portant a word is to a document in a corpus. In our experiments, we set the dimension of each text representation to 2,000 according to the ranking of the TF-IDF weights with each dimension of term- frequency(TF) features. After that, the text rep- resentations are fed into the proposed INN-RER method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>News</head><p>η init , η learn , λ, the number of iterations and the number of topics are set to 0.9, 0.1, 0.001, 100 and 60 respectively. The parameters were cho- sen by 10-fold cross-validation. The topic distri- bution used in INN-RER are derived in different ways. For long text such as News and Blogs, La- tent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref> is employed for generating topic distributions. For short texts in Semeval, bi-term topic model (BT- M) ( <ref type="bibr" target="#b3">Cheng et al., 2014</ref>) was used, since short text typically contains a few words which results in s- parse word co-occurrence patterns. BTM is a vari- ant of LDA which effectively infers the latent topic distribution of short text by modeling the genera- tion of bi-terms in the whole corpus and it allevi- ates the problem of sparsity at the document level. For each method, 10-fold cross validation is con- ducted using the same feature construction method to get the final performance.</p><p>Evaluation metrics typically used in multi-label learning and label ranking are employed which are different from those of classical single-label learn- ing systems <ref type="bibr" target="#b17">(Sebastiani, 2001</ref>). The detailed ex- planation of evaluation metrics are presented in <ref type="table">Table 2</ref>. Note that metrics from PRO Loss to F 1 exam work by evaluating performance on each test example separately and returning the mean value across test set. MicroF1 and MacroF1 work by evaluating performance on each emotion cat- egory separately and returning the macro/micro- averaged value across all emotion categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Results</head><p>There are several approaches addressing multiple emotions detection from texts. Three generative model based baselines and three discriminative model based baselines are chosen.</p><p>• Emotion Distribution Learning (EDL) ( <ref type="bibr" target="#b26">Zhou et al., 2016</ref>) learns a mapping</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Category</head><p>Method Criteria</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PL(↓) HL(↓) RL(↓) OE(↓) AP(↑) Cov(↓) F1(↑) MiF1(↑) MaF1(↑)</head><p>News Generative MSTM 0.3343 0.4065 0.3097 0.2123 0.6677 3.3202 0.5666 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking Loss</head><formula xml:id="formula_18">1 n n i=1 ( (et,es)∈Ri×Ri δ[gt(xi) &lt; gs(xi)])/(|Ri| × |Ri|)</formula><p>where δ is the indicator function. <ref type="table">Table 2</ref>: Evaluation criteria for the Multi-Label Learn- ing (MLL) methods. T P t , F P t , T N t , F N t represent the number of true positive, false positive, true nega- tive, and false negative test examples with respect to emotion t respectively. F 1(T P t , F P t , T N t , F N t ) rep- resent specific binary classification metric F1 <ref type="bibr" target="#b8">(Manning et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One Error</head><formula xml:id="formula_19">1 n n i=1 δ[argmax et gt(xi) / ∈ Ri] Average Precision 1 n n i=1 1 |Ri| × ( t:et∈Ri |{es ∈ Ri|gs(xi) &gt; gt(xi)}|)/(|{es|gs(xi) &gt; gt(xi)}|) Coverage 1 n n i=1 maxt:e t∈Ri |{es|gs(xi) &gt; gt(xi)}| F 1exam 1 n n i=1 2|Ri ∩ ˆ Ri|/(|Ri| + | ˆ Ri|) MicroF1 F 1( T t=1 T Pt, T t=1 F Pt, T t=1 T Nt, T t=1 F Nt) MacroF1 1 T T t=1 F 1(T Pt, F Pt, T Nt, F Nt)</formula><p>function from texts to their emotion distri- butions based on label distribution learning.</p><p>• EmoDetect ( <ref type="bibr" target="#b21">Wang and Pal, 2015</ref>) outputs the emotion distribution based on a dimension- ality reduction method using non-negative matrix factorization which combines sever- al constraints such as emotions bindings and topic correlations.</p><p>• RER (Zhou et al., 2018) predicts multiple e- motions and their rankings from text based on relevant emotion ranking using support vec- tor machines.</p><p>• Multi-label supervised topic model (MST- M) and Sentiment latent topic model (SLT- M) (Rao et al., 2014b): As the variants of supervised topic models, MSTM and SLTM connect latent topics with evoked emotions of  <ref type="table">Amusement  Topic 1  Topic 2  Topic 1  Topic 2  Topic 1  Topic 2  Í&lt;(save)</ref> "(teacher) Žä(ruffian) ‹‚(sin) Iå(men and women) þ(network) ì(take care of)</p><formula xml:id="formula_20">"£(hard) r1(force) v¦&lt;(suspect) U,(hotel) €Ë(drunkenness) (sacrifice) áY(fall into water) h(obscenity) ä-(imprisonment) ÑÖ(service) u(procuratorate) £(cure) c"(youth) åÖ(girl) ‹&lt;(beat) ì¡(photo) Š{(illegal) )·(life) ¾oe(state of an illness) à³(murder) ð‹(hit) ´(call the police) v±(penalty) P&lt;(older) j±(persist) E¤(cause) ó/(construction site) •½(authenticate) N(investigate) a(grateful) +¯(public) Ñ¤(police station) ´(traffic police) À¢(defraud) Š5(get out of line) š(hospital) •Á(traffic accident) ŠY(commit a crime) ae-(interview) ä(internet) y7(cash) aÄ(moved) aÄ(touching) k(death) Í1(exposure) l´(divorce) ´((police officer) Sadness Curiosity Shock Topic 1 Topic 2 Topic 1 Topic 2 Topic 1 Topic 2 "l(disappear) •Á(car accident) [•(parents) i›(monitoring) s (rob) è(kitchen knife) Ø3(misfortune) Ü(thief) ¥I(China) Oå(women) ™N(corpse) Îf(neck) -(pass away) úS(public security) ´Ó(marriage) S!(spring festival) ;:(emergency) -w(sever illness) à³(murder) OE*(watch) èx(health) š(hospital) y|(scene) /c(subway¤ ‹‚(crime) •½(identify) åf(women) ~)(pregnancy) S(security) #ª(news) ;(suffer) k(apologize) c"(young) @þ(morning) £(cure) ¾,(unexpectedly) úSÛ(Public Security Bureau) -Ä(excite) (´(marry up) sÍ(rescue) )·(life) Õ1(bank) Ñ¯(have an accident) ‰{(enforce the law) I5(men) Ã(in vain) u(examine) €(compensate) xN(media) Ñ¤(police station) y7(money) U•(like) [á(family member) ž¤(consume)</formula><p>1 <ref type="figure" target="#fig_3">Figure 2</ref>: The top topic words under each emotion category from the News corpus.</p><p>texts. MSTM first generates a set of topics from words, and then samples emotions from each topic. SLTM generates topics directly from emotions.</p><p>• Affective topic model (ATM) ( <ref type="bibr" target="#b14">Rao et al., 2014a</ref>) employs the exponential distribution to generate ratings for each emotion.</p><p>We also evaluated INN-RER with random ini- tialization instead of the proposed initialization procedure, which is denoted as INN-RER(-t).</p><p>Experimental results on the three corpora are summarized in <ref type="table">Table 3</ref>. It can be observed from the table that: (1) INN-RER outperforms the base- lines on almost all evaluation metrics across all the data sets; (2) INN-RER achieves better per- formance on almost all the evaluation metrics than INN-RER(-t), which further verifies the ef- fectiveness of incorporating the topic information; (3) Both INN-RER and INN-RER(-t) perform re- markably better than RER which is based on lin- ear models. It verifies the effectiveness of using the neural networks for RER task, which are able to learn dynamic and complex functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">INN-RER Interpretation</head><p>In addition to comparing the performance of the proposed model with several baselines, we al- so present the experimental results from the per- spective of result interpretation to fully under- stand INN-RER. The topic words of each emo- tion in three corpora are extracted according to the ranking of weights learned by INN-RER, i.e., the probabilities of topics conditioned on emotions (weights between the hidden layer and the output  'm(competition) F"(hope) (baby) Ãoe(heartless) m%(happy) U,(heaven) layer) and words conditioned on topics (weight- s between the input layer and the hidden layer). Results are shown in <ref type="figure" target="#fig_3">Figure 2</ref>, 3, 4 respectively. It can be observed that the extracted topics words under each emotion category correspond to a cer- tain event, which evokes the emotion. It is in ac-   <ref type="table">Table 3.</ref> cord with what has been observed in social psy- chology ( <ref type="bibr" target="#b18">Stoyanov and Cardie, 2008)</ref>. For exam- ple, in the Sina corpus, Topic 1 under the emotion touching is about "heroic rescue"; Topic 1 under the emotion anger is about "sexual molestation of a child" and Topic 2 under the emotion sad- ness is about an "car accident". In the SemEval and the Blog corpora, we can also find that topic words listed under each emotion category are re- lated to some social events. For example, in the SemEval corpus, the Joy topic is about "home en- tertainment" and the Anger topic is about "ter- rorist attack". In the Blog corpus, the sorrow topic is about "earthquake and the lost of their loved ones". The extracted emotion-associated topic words unveil how the corresponding emotion is evoked. By incorporating topical information into neural network learning, we are able to obtain more interpretable results from INN-RER.</p><formula xml:id="formula_21">•É(enjoy) -#(again) aú(feeling) N((lonely) ¯W(happy) oeX(emotion) %¸(mood) /(earthquake) 64(wish) "(lose) ¿÷(full of) ¦·(mission) (baby) -í(temper) ©z(culture) I*l(boyfriend) m%(joyful) Û£(pain) Š¬(production) lm(leave) ‡(smile) (entirely) ´L(rich) ÃG(helpless) Anxiety Surprise Anger Expect 'f(house) çô(rainbow) lm(leave) F"(hope) ´Ó(marriage) °(Hokkaido) l´(divorce) I?(responsible) Pú(husband) ,(sudden) ÃG(helpless) å5(women) †Ø(error) PÁ(memory) {AE(law) c$¬(Olympic) %oe(mood) rÔ(gift) Õ1(bank) 34(happiness) ))(strange) Û,(miracle) (morality) 1•(action) [p(family) â`(reputedly) oea(emotion) ãå(strive) þ•(on duty) ÐÛ(curious) ú(sorrow) ±(later) ¢½(city) G!(season) gC(self) °ç(splendid)</formula><formula xml:id="formula_22">Corpus Method Criteria PL(↓) HL(↓) RL(↓) OE(↓) AP(↑) Cov(↓) F1(↑) MiF1(↑) MaF1(↑) News RANK-SVM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Multi-Label Methods</head><p>Since relevant emotion ranking can be seen as an extension of multi-label learning, the proposed INN-RER is also compared with three widely used well-established multi-label learning methods such as LIFT (Zhang, 2011), Rank-SVM (Zhang and Zhou, 2014) and BP-MLL ( <ref type="bibr" target="#b23">Zhang and Zhou, 2006</ref>). In our experiments, LIFT used linear ker- nel and Rank-SVM uses the RBF kernel with the width σ set to 1 using the threshold Θ which is initialized as 0.15 after normalization.</p><p>The results of INN-RER in comparison with M- LL baselines are presented in <ref type="table" target="#tab_3">Table 4</ref>. It can be observed that INN-RER outperforms all the base- lines across all the datasets on all evaluation mea- sures most of the time. This further verifies the effectiveness of our proposed INN-RER for multi- label emotion detection due to its consideration of rankings of the relevant emotions and the incorpo- ration of topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a novel inter- pretable neural network for relevant emotion rank- ing. Specifically, motivated by transfer learning, the neural network is initialized to make its hid- den layer approximate the behavior of a topic model. Moreover, a novel error function is de- fined to optimize the whole neural network for relevant emotion ranking. Experimental result- s on three real-world corpora show that the pro- posed approach performs remarkably better than the state-of-the-art emotion detection approach- es and multi-label learning methods. Moreover, the extracted emotion-associated topic words in- deed represent emotion-evoking events which are in line with our common-sense knowledge. In the future, we will explore the possibility of learning a topic model and an emotion ranking function si- multaneously in a unified framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of Interpretable Neural Network for Relevant Emotion Ranking (INNRER).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2</head><label>2</label><figDesc>Algorithm of INN-RER Learning. Input: x q i : Term frequency of text x i ; ∆v, ∆α: Parameters after initialization; L: emotion labels Output: A predictable neural network INN- RER. 1: Initialize INN-RER network parameters from Algorithm 1 2: for each iteration do 3: for each text x i ∈ G do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 3 :</head><label>3</label><figDesc>Experimental results of the proposed approach and the baselines. 'PL' represent Pro Loss, 'HL' represents Hamming Loss, 'RL' represents ranking loss, 'OE' represents one error, 'AP' represent average precision, 'Cov' represent coverage, 'F1' represents F 1 exam , MiF1' represents MicroF1, 'MaF1' represents MacroF1. "↓" indi- cates "the smaller the better", while "↑" indicates "the larger the better". The best performance on each evaluation measure is highlighted by boldface. Name Definition PRO Loss 1 n n i=1 et∈Ri∪{Θ} es∈∈(et) 1 normt,s lt,s lt,s is a modified 0-1 error;normt,sis the set size of label pair(et, es) Hamming Loss 1 nT n i=1 | ˆ RiRi| The predicted relevant emotions: ˆ Ri.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The top topic words under each emotion category from the Semeval corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The top topic words for each emotion category from the Blogs corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Comparison with Multi-Label Learning (MLL) Methods. The evaluation criteria are same as in</head><label>4</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work was supported by the National Key R&amp;D Program of China (No. 2017YFB1002801), the National Natural Science Foundation of Chi-na (61772132,61528302) and the Natural Sci-ence Foundation of Jiangsu Province of China (BK20161430).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining social emotions from affective text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1658" to="1670" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reader perspective emotion analysis in text through ensemble based multi-label classification framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plaban</forename><surname>Kumar Bhowmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer and Information Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Btm: Topic modeling over short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2928" to="2941" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multilabel classification via calibrated label ranking. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyke</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="133" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Searson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid neural networks for social emotion detection over short text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biyun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emotion classification of online news articles from the reader&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hsin-Yih Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="page" from="220" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schtze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="824" to="825" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Thirteen ways to look at the correlation coefficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Nicewander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Statistician</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosalind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roalind</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">252</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sentence emotion analysis and recognition based on emotion words using ren-cecps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqin</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuji</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Intelligence Paradigms</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent discriminative models for social emotion detection with emotional dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Wenyin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextual sentiment topic model for adaptive social emotion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Affective topic model for social emotion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Xiaojun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment topic models for social emotion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghui</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Annotating topics of opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources and Evaluation</title>
		<meeting><address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-05-26" />
			<biblScope unit="page" from="3213" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval2007 task 14: Affective text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to identify emotions in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM symposium on Applied computing</title>
		<meeting>the 2008 ACM symposium on Applied computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1556" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting emotions in social media: A constrained optimization approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="996" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lift: multi-label learning with label-specific features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Ling</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1609" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Relevant emotion ranking from text constrained with emotion relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the North American Chapter of the Association for Computation Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotion distribution learning from texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="638" to="647" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
