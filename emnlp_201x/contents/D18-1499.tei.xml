<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Neural Model of Adaptation in Reading</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cognitive Science</orgName>
								<orgName type="department" key="dep2">Department of Cognitive Science</orgName>
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
							<email>tal.linzen@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Cognitive Science</orgName>
								<orgName type="department" key="dep2">Department of Cognitive Science</orgName>
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Neural Model of Adaptation in Reading</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4704" to="4710"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4704</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholin-guistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading involves the integration of noisy percep- tual evidence with probabilistic expectations about the likely contents of the text. Words that are consistent with these expectations are identified more quickly <ref type="bibr" target="#b6">(Ehrlich and Rayner, 1981;</ref><ref type="bibr">Smith and Levy, 2013)</ref>. For the reader's expectations to be maximally effective, they should not only reflect the reader's past experience with the lan- guage <ref type="bibr" target="#b12">(Hale, 2001;</ref><ref type="bibr" target="#b24">MacDonald and Christiansen, 2002</ref>), but should also be adapted to the current context. Optimal adaptation would reflect prop- erties of the text being read, such as genre, topic and writer identity, as well as the general tendency for recently used words and syntactic structures to be reused with higher probability <ref type="bibr" target="#b1">(Bock, 1986;</ref><ref type="bibr" target="#b4">Church, 2000;</ref><ref type="bibr" target="#b5">Dubey et al., 2006</ref>).</p><p>Several studies have suggested that readers do in fact adapt their lexical and syntactic predictions to the current context <ref type="bibr">(Otten and Van Berkum, 2008</ref>; <ref type="bibr" target="#b8">Fine et al., 2013;</ref><ref type="bibr" target="#b7">Fine and Jaeger, 2016)</ref>. <ref type="bibr">1</ref> For example, Fine and Jaeger investigated the pro- cessing of "garden path" sentences such as (1):</p><p>(1) The experienced soldiers warned about the dangers conducted the midnight raid.</p><p>The word warned in (1) is initially ambiguous be- tween a main verb interpretation (the soldiers were doing the warning) and a reduced relative clause interpretation (the soldiers were being warned).</p><p>When the word conducted is reached, this am- biguity is resolved in favor of the reduced rela- tive parse. Reduced relatives are infrequent con- structions. This makes the disambiguating word conducted unexpected, causing it to be read more slowly than it would be in a context such as (2), in which the words who were indicate early on that only the relative clause parse is possible:</p><p>(2) The experienced soldiers who were warned about the dangers conducted the midnight raid.</p><p>Fine and Jaeger included a large proportion of re- duced relatives in their experiment. As the ex- periment progressed, the cost of disambiguation in favor of the reduced relative interpretation de- creased, suggesting that readers had come to ex- pect a construction that is normally infrequent. Human syntactic expectations have been suc- cessfully modeled with syntax-based language models <ref type="bibr" target="#b12">(Hale, 2001;</ref><ref type="bibr" target="#b22">Levy, 2008;</ref><ref type="bibr">Roark et al., 2009)</ref>. Recently, language models (LMs) based on recurrent neural networks (RNNs) have been shown to make adequate syntactic predictions ( <ref type="bibr" target="#b23">Linzen et al., 2016;</ref><ref type="bibr" target="#b11">Gulordava et al., 2018)</ref>, and to make comparable reading time predictions to syntax-based <ref type="bibr">LMs (van Schijndel and Linzen, 2018)</ref>. In this paper, we propose a simple way to continuously adapt a neural LM, and test the method's psycholinguistic plausibility. We show that LM adaptation significantly improves our ability to predict human reading times using the LM. Follow-up experiments with controlled mate- rials show that the LM adapts not only to specific vocabulary items but also to abstract syntactic con- structions, as humans do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We use a simple method to adapt our LM: at the end of each new test sentence, we update the pa- rameters of the LM based on its cross-entropy loss when predicting that sentence; the new weights are then used to predict the next test sentence. <ref type="bibr">2</ref> Our baseline LM is a long short-term memory (LSTM; <ref type="bibr" target="#b14">Hochreiter and Schmidhuber, 1997</ref>) lan- guage model trained on 90 million words of En- glish Wikipedia by <ref type="bibr" target="#b11">Gulordava et al. (2018)</ref>  We tested the model on the Natural Stories Cor- pus ( <ref type="bibr" target="#b9">Futrell et al., 2018)</ref>, which has 10 narratives with self-paced reading times from 181 native En- glish speakers. There are two narrative genres in the corpus: fairy tales (seven texts) and documen- tary accounts (three texts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Linguistic accuracy</head><p>We first measured how well the adaptive model predicted upcoming words. We report the model's perplexity, a quantity which is lower when the LM assigns higher probabilities to the words that in fact occurred. We adapted the model to the first k sentences of each text, then tested it on sentence k + 1, for all k. Adaptation dramatically improved test perplexity compared to the non-adaptive ver- sion of the model <ref type="bibr">(86.99 vs. 141.49)</ref>.</p><p>We next adapted the model to each genre sep- arately. If the model adapts to stylistic or syn- tactic patterns, we might expect adaptation to be more helpful in the fairy tale than the documen- tary genre: the Wikipedia corpus that the LM was originally trained on is likely to be more simi- lar in style to the documentary genre. Consistent with this hypothesis, the documentary texts bene- fited less from adaptation (99.33 to 73.20) than the fairy tales (160.05 to 86.47), though the fact that both saw improvement from adaptation suggests that text-specific adaptation is beneficial even if the genre is similar to the training genre.  Each genre consists of multiple texts. Does adaptation to a particular text lead to catastrophic forgetting <ref type="bibr" target="#b25">(McCloskey and Cohen, 1989)</ref>, such that the LM overfits to the text and forgets its more general knowledge acquired from the Wikipedia training corpus? This was not the case; in fact, adapting to the entirety of each genre without re- verting to the baseline model after each text led to a very slightly better perplexity (fairytales: 86.47, documentaries: 73.20) compared with a setting in which the LM was reverted after each text (fairy- tales: 86.61, documentaries: 73.63).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling human expectations</head><p>We next tested whether our adaptive LM matches human expectations better than a non-adaptive model. Since each reader saw the texts in a dif- ferent order, we adapted the LM to each text sep- arately: after each story, we reverted to the initial Wikipedia-trained LM and restarted adaptation on the next text. If anything, this likely resulted in a conservative estimate of the benefit of adapta- tion compared to a model that adapts continuously across multiple stories from the same genre, as hu- mans might do. <ref type="bibr">3</ref> We used surprisal as a linking function between the LM's predictions and human reading times ( <ref type="bibr" target="#b12">Hale, 2001;</ref><ref type="bibr">Smith and Levy, 2013)</ref>. Surprisal quantifies how unpredictable each word (w i ) is given the preceding words:</p><formula xml:id="formula_0">surprisal(w i ) = −log P(w i | w 1 ...w i−1 ) (1)</formula><p>We fit the self-paced reading times in the Natu- ral Stories Corpus with linear mixed effects mod- els (LMEMs), a generalization of linear regression (see Supplementary Materials for details).</p><p>In line with previous work, non-adaptive sur- prisal was a significant predictor of reading times (p &lt; 0.001) when the model only included other baseline factors <ref type="table">(Table 1</ref>, Top). Adaptive sur- prisal was a significant predictor of reading times (p &lt; 0.001) over non-adaptive surprisal and all baseline factors <ref type="table">(Table 1</ref>, Bottom). Crucially, non- adaptive surprisal was no longer a significant pre- dictor of reading times once adaptive surprisal was included. This indicates that the predictions of the adaptive model subsume the predictions of the non-adaptive one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Does the model adapt to syntax?</head><p>We have shown that LM adaptation improves our ability to model human expectations as reflected in a self-paced reading time corpus. How much of this improvement is due to adaptation of the model's syntactic representations ( <ref type="bibr" target="#b0">Bacchiani et al., 2006;</ref><ref type="bibr" target="#b5">Dubey et al., 2006</ref>) and how much is simply due to the model assigning a higher probability to words that have recently occurred (Kuhn and de <ref type="bibr" target="#b21">Mori, 1990;</ref><ref type="bibr" target="#b4">Church, 2000</ref>)? We address this ques- Item order (#RCs seen) <ref type="bibr">Order</ref>  tion using two syntactic phenomena: reduced rel- ative clauses and the dative alternation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reduced relative clauses</head><p>We adapted the model independently to random orderings of the critical and filler stimuli used in Experiment 3 of Fine and Jaeger (2016); 4 this experiment (described in the Introduction) con- tained a much higher proportion of reduced rela- tive clauses than their general distribution in En- glish. We used surprisal as our proxy for read- ing times. Following Fine and Jaeger, we took the mean surprisal over three words in each am- biguous sentence: the disambiguating word and the following two words (e.g., conducted the mid- night in example (1)). To estimate the magni- tude of the syntactic disambiguation penalty while also controlling for lexical content, we subtracted this quantity from the mean surprisal over the ex- act same words in the paired unambiguous sen- tence (2). Linear regression showed that the dis- ambiguation penalty decreased as the model was exposed to more critical items (item order coeffi- cient: ˆ β = −0.0804, p &lt; 0.001), indicating that the LM was adapting to reduced relatives, a syn- tactic construction without any lexical content.</p><p>In order to compare our findings more directly with the results given by Fine and Jaeger (2016) (shown in <ref type="figure" target="#fig_1">Figure 1</ref>), we mimicked their method of plotting reading times. First, we fit a linear model of the mean surprisal of each disambiguating re- gion with the number of trials the model had seen in the experiment thus far to account for a gen- eral trend of subjects speeding up over the course of the experiment. Then, we plotted the mean residual model surprisal that was left in the disam- biguating region in both the ambiguous and unam- biguous conditions as the experiment progressed. The shape of our model's adaptation to the re- duced relative construction (upper curve in <ref type="figure" target="#fig_3">Fig- ure 2)</ref> matched the human results reported by Fine and Jaeger. Like humans, the model showed an initially large adaptation effect, followed by more gradual adaptation thereafter. Both humans and our model continued to adapt over all the items rather than just at the beginning of the experiment. Also like humans, the model's response to unam- biguous items did not change significantly over the course of the experiment (p = 0.91).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The dative alternation</head><p>Dative events can be expressed using two roughly equivalent English constructions:</p><formula xml:id="formula_1">(3) a. Prepositional object (PO):</formula><p>The boy threw the ball to the dog. b. Double object (DO):</p><p>The boy threw the dog the ball.</p><p>Work in psycholinguistics has shown that recent experience with one of these variants increases the probability of producing that variant <ref type="bibr" target="#b1">(Bock, 1986;</ref><ref type="bibr" target="#b17">Kaschak et al., 2006</ref>) as well as the likelihood of predicting it in reading <ref type="bibr">(Tooley and Bock, 2014</ref>).</p><p>To test whether our adaptation method can repro- duce this behavior, we generated 200 pairs of da- tive sentences similar to (3). We shuffled 100 DO sentences into 1000 filler sentences sampled from the Wikitext-2 training corpus ( <ref type="bibr">Merity et al., 2016)</ref> and adapted the model to these 1100 sentences.</p><p>We then froze the weights of the adapted model and tested its predictions for two types of sen- tences: the PO counterparts of the DO sentences in the adaptation set, which shared the vocabulary of the adaptation set but differed in syntax; and 100 new DO sentences, which shared syntax but no content words with the adaptation set. <ref type="bibr">5</ref> An additional goal of this experiment was to examine the effect of learning rate on adaptation. During adaptation the model performs a single pa- rameter update after each sentence and does not train until convergence with gradual reduction of the learning rate as would normally be the case during LM training. Consequently, the learning <ref type="bibr">5</ref> For additional details as well as the reverse setting (adap- tation to PO), see Supplementary Materials.  <ref type="figure">Figure 3</ref>: Learning rate influence over syntactic and lexical adaptation. The initial non-adaptive model performance is equivalent to the perfor- mance when using a learning rate of 0; the learning rate of 200 resulted in perplexity in the billions.</p><p>rate parameter crucially determines the amount of adaptation the model can undertake after each sen- tence. If the learning rate is very low, adaptation will not have any effect; if it is too high, either the model will overfit after each update and will not generalize well, or the model will forget its trained representation as it overshoots the targeted minima. The optimal rate may differ between lexi- cal and syntactic adaptation. Our experiments thus far all used the same learning rate as our original model <ref type="bibr">(20)</ref>; here, we varied the learning rate on a logarithmic scale between 0.002 and 200.</p><p>The results of this experiment are shown in <ref type="figure">Fig- ure 3</ref>. The model successfully adapted to the DO construction as well as to the vocabulary of the adaptation sentences. This was the case for all of the learning rates except for 200, which resulted in enormous perplexity on both sentence types. Both lexical and syntactic adaptation were most suc- cessful when the learning rate was around 2, with perplexity reductions of 94% for lexical adaptation and 84% for syntactic adaptation.</p><p>Syntactic adaption was penalized at higher learning rates more than lexical adaptation (com- pare learning rates of 2 and 20). This fragility of syntactic adaptation likely stems from the fact that the model can directly observe the relevant vocab- ulary but syntax is latent and must be inferred from multiple similar sentences, a generalization which is impeded by overfitting at higher learning rates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Testing for catastrophic forgetting</head><p>Our analysis of the Natural Stories corpus did not indicate that the model suffered from catas- trophic forgetting. Yet the Natural Stories cor- pus contained only two genres; to address the is- sue of catastrophic forgetting more systematically, we used the premise sentences from the MultiNLI corpus ( <ref type="bibr">Williams et al., 2018</ref>) -a total of 2000 sentences for each of 10 genres.</p><p>For each genre pair G 1 and G 2 (omitting cases where G 1 = G 2 ), we first adapted the baseline Wikipedia model to 1000 sentences of G 1 using a learning rate of 2 (shown to be optimal in Sec- tion 5.2). We then adapted the model to 1000 sentences of G 2 . Finally, we froze the model's weights and tested its perplexity on the 1000 held- out sentences from G 1 .</p><p>The results averaged across all pairs of genres are plotted in <ref type="figure" target="#fig_4">Figure 4</ref>. Unsurprisingly, the model performed best on G 1 immediately after adapting to it (middle bar). Crucially, even after adapting to 1000 sentences of G 2 after its last exposure to G 1 (right bar), it still modeled G 1 much better than the non-adapted model (left bar). These results sug- gest that catastrophic forgetting is not a concern even with a relatively large amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Adaptation greatly improved an RNN LM's word prediction accuracy, in line with other work on LM adaptation <ref type="bibr" target="#b19">(Kneser and Steinbiss, 1993)</ref>. We showed that the adapted model was psycholin- guistically plausible, in two senses. First, it im- proved the correlation between surprisal derived from the model and human reading times, sug- gesting that the model generated more human-like expectations. Second, using materials that teased apart lexical content from syntax, we showed that the model adapted both its lexical and its syntac- tic predictions, in line with findings from human experiments. Finally, as in other neural-network based models in psychology ( <ref type="bibr" target="#b3">Chang et al., 2006</ref>), our gradient-based updates naturally incorporate the error-driven nature of syntactic adaptation; while we did not demonstrate this in the current paper, we hypothesize that our model will repro- duce the finding that more surprising words lead to greater adaptation <ref type="bibr" target="#b16">(Jaeger and Snider, 2013</ref>).</p><p>The simplicity of our adaptation method makes it attractive for use in modeling human expecta- tions. Since adaptive surprisal is strictly supe- rior to non-adaptive surprisal in modeling reading times, it would be a stronger baseline in analyses that aim to demonstrate the contribution of factors other than predictability.</p><p>We used a simple neural adaptation approach, where we performed continuous gradient updates based on the prediction error on the adaptation sentences (see also <ref type="bibr" target="#b20">Krause et al., 2017</ref>). An al- ternative approach to neural LM adaptation uses recent RNN states in conjunction with the current state to make word predictions ( <ref type="bibr" target="#b10">Grave et al., 2017;</ref><ref type="bibr">Merity et al., 2017)</ref>; a comparison of the two meth- ods using our paradigms may provide insight into their relative strengths and weaknesses.</p><p>Finally, we reverted to the base model after the end of each text in our experiments, forgetting any text-specific adaptation. This mimics the ef- fect of a participant leaving an experiment that had an unusual distribution of syntactic construc- tions and reverting to their standard expectations. In practice, however, humans are able to general- ize from prior experience when they begin adapt- ing to a new speaker or text if it is similar in some way to their previous experiences. For ex- ample, the model of <ref type="bibr" target="#b15">Jaech and Ostendorf (2018)</ref> adapts to environmental factors, so it could po- tentially draw on independent experiences with fe- male speakers and with lawyer speech in order to initialize a model of adaptation to a new female lawyer (see also <ref type="bibr">Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b18">Kleinschmidt, 2018</ref>). The psycholinguistic plausibility of these models can be tested in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>WITHOUT</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Mean length-and order-corrected reading times over the disambiguating region of the critical items in Fine and Jaeger (2016). Figure adopted from that paper.</figDesc><graphic url="image-1.png" coords="3,73.42,62.81,215.44,160.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mean order-corrected model surprisal over the disambiguating region of the critical items in Fine and Jaeger (2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Perplexity on the held-out set of G 1 (a) before adaptation, (b) after adaptation to G 1 , (c) after adapting to G 1 then adapting to G 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>−corrected surprisal (bits) condition • ambiguous unambiguous</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Recently, Harrington Stack et al. (2018) questioned the robustness of the results of Fine et al. (2013).</note>

			<note place="foot" n="2"> Our code is publicly available at: https://github. com/vansky/neural-complexity.gitˆβˆσ</note>

			<note place="foot" n="3"> We do not distinguish between priming and adaptation in this paper. While it may be tempting to think of the LSTM memory cell as a model of priming and of the weight updates as a model of adaptation, Bock and Griffin (2000) provide evidence that priming cannot simply be a function of residual activation and that priming can be driven by longer-term learning (see Tooley and Traxler (2010) for more discussion on priming vs. adaptation).</note>

			<note place="foot" n="4"> See details in the Supplementary Materials.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MAP adaptation of stochastic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="68" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Syntactic persistence in language production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="387" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The persistence of structural priming: transient activation or implicit learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenzi</forename><forename type="middle">M</forename><surname>Griffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="192" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Becoming syntactic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franklin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="272" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical estimates of adaptation: the chance of two noriegas is closer to p/2 than p 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Linguistics</title>
		<meeting>the 18th Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating syntactic priming into an incremental probabilistic parser, with an application to psycholinguistic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sturt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contextual effects on word perception and eye movements during reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Ehrlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rayner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Verbal Learning and Verbal Behavior</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="655" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The role of verb repetition in cumulative structural priming in comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">B</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Florian</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rapid expectation adaptation during syntactic comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">B</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Florian</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Anastasia Vishnevetsky, Steve Piantadosi, and Evelina Fedorenko</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Tily</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="76" to="82" />
		</imprint>
	</monogr>
	<note>The natural stories corpus</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving neural language models with a continuous cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Colorless green recurrent networks dream hierarchically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1195" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic Earley parser as a psycholinguistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies</title>
		<meeting>the second meeting of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A failure to replicate rapid syntactic adaptation in comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Harrington</forename><surname>Caoimhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><forename type="middle">N</forename><surname>Stack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duane</forename><forename type="middle">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Personalized language model for query auto-completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="705" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alignment as a consequence of expectation adaptation: Syntactic priming is affected by the prime&apos;s prediction error given both prior and recent experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">E</forename><surname>Snider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recent experience affects the strength of structural priming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renrick</forename><forename type="middle">A</forename><surname>Kaschak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">L</forename><surname>Loney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borreggine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structure in talker variability: How much is there and how much can it help? Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><forename type="middle">F</forename><surname>Kleinschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition and Neuroscience</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the dynamic adaptation of stochastic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Steinbiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP-93</title>
		<meeting>ICASSP-93</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dynamic evaluation of neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07432</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A cachebased natural language model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>De Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="570" to="583" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Expectation-based syntactic comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1126" to="1177" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reassessing working memory: Comment on Just and Carpenter (1992) and Waters and Caplan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maryellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><forename type="middle">H</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christiansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="54" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Catas</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
