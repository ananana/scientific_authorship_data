<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Keyboard Logs as Natural Annotations for Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumihiko</forename><surname>Takahashi</surname></persName>
							<email>ftakahas@yahoo-corp.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename><surname>Honmachi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakyo-Ku</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation Midtown Tower</orgName>
								<address>
									<addrLine>9-7-1 Akasaka, Minato-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Kyoto University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Keyboard Logs as Natural Annotations for Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmen-tation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The first step of almost all natural language processing (NLP) for languages with ambiguous word boundaries (such as Japanese and Chinese) is solving the problem of word identification am- biguity. This task is called word segmentation (WS) and the accuracy of state-of-the-art methods based on machine learning techniques is more than 98% for Japanese and 95% for Chinese <ref type="bibr" target="#b28">Yang and Vozila, 2014</ref>). Compared to languages like English with clear word bound- aries, this ambiguity poses an additional problem for NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degrada- tion in WS performance. Examples include ma-chine translation of patents, text mining of med- ical texts, and marketing on the micro-blog site Twitter <ref type="bibr">1</ref> . Some papers have reported low accuracy on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these do- mains <ref type="bibr" target="#b14">(Mori and Neubig, 2014;</ref><ref type="bibr" target="#b5">Kaji and Kitsuregawa, 2014;</ref><ref type="bibr" target="#b9">Liu et al., 2014)</ref> To cope with this problem, we propose a way to collect information from people as they type Japanese or Chinese on computers. These lan- guages use far more characters than the number of keys on a keyboard, so users use software called an input method (IM) to type text in these languages. Unlike written texts in these languages, which lack word boundary information, text entered with an IM can provide word boundary information that can used by NLP systems. As we show in this pa- per, logs collected from IMs are a valuable source of word boundary information.</p><p>An IM consists of a client (front-end) and a server (back-end). The client receives a key se- quence typed by the user and sends a phoneme sequence (kana in Japanese or pinyin in Chinese) or some predefined commands to the server. The server converts the phoneme sequence into normal written text as a word sequence or proposes word candidates for the phoneme sequence in the region specified by the user. We noticed that the actions performed by people using the IM (such as typ- ing and selecting word candidates) provide infor- mation about word boundaries, including context information.</p><p>In this paper, we first describe an IM for Japanese which allows us to collect this informa- tion. We then propose an automatic word seg- menter that uses IM logs as a language resource to improve its performance. Finally, we report exper- imental results showing that our method increases the accuracy of a word segmenter on Twitter texts by using logs collected from a browser add-on ver-sion of our IM.</p><p>The three main contributions of this paper are:</p><p>• an IM server that proposes word candidates which are not included in the vocabulary (Section 3),</p><p>• a publicly usable IM that logs user behavior (such as typing and selection of word candi- dates) (Section 4),</p><p>• a method for improving word segmentation by using these logs (Section 5).</p><p>To the best of our knowledge, this is the first paper proposing a method for using IM logs to success- fully improve WS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early <ref type="bibr">90's (Nagata, 1994)</ref>. <ref type="bibr" target="#b12">Then (Mori and Kurata, 2005</ref>) extended it by lexicalizing the states like many researches in that era, grouping the word- POS pairs into clusters inspired by the class-based n-gram model ( <ref type="bibr" target="#b0">Brown et al., 1992)</ref>, and making the history length variable like a POS tagger in English ( <ref type="bibr" target="#b23">Ron et al., 1996)</ref>. In parallel, there were attempts at solving Chinese WS in a similar way <ref type="bibr">(Sproat and Chang, 1996)</ref>. WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) ( <ref type="bibr" target="#b22">Peng et al., 2004;</ref><ref type="bibr" target="#b8">Lafferty et al., 2001</ref>) have been applied to this task and showed bet- ter performance than POS-based Markov models ( <ref type="bibr" target="#b7">Kudo et al., 2004</ref>). The training time of sequence- based methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based meth- ods and fast enough to make active learning prac- tically possible ). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments.</p><p>Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources <ref type="bibr" target="#b20">(Neubig and Mori, 2010;</ref><ref type="bibr" target="#b14">Mori and Neubig, 2014)</ref>. <ref type="bibr">So Mori and Oda (2009)</ref> proposed to incorpolate dictio- naries for human into a WS system with a differ- ent word definition. CRFs were also extended to enable training from partially annotated sentences ( <ref type="bibr" target="#b26">Tsuboi et al., 2008)</ref>. When using partially anno- tated sentences for WS training data, word bound- ary information exists only between some charac- ter pairs and is absent for others. This extension was adopted in Chinese WS to make use of so- called natural annotations <ref type="bibr" target="#b28">(Yang and Vozila, 2014;</ref><ref type="bibr" target="#b4">Jiang et al., 2013)</ref>. In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but con- tain much more noise. In addition, we need an IM that is specifically designed to collect logs as nat- ural annotations.</p><p>Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical lan- guage modeling <ref type="bibr" target="#b17">(Mori et al., 1999;</ref><ref type="bibr" target="#b1">Chen and Lee, 2000;</ref><ref type="bibr" target="#b11">Maeta and Mori, 2012)</ref>. Their param- eters are trained from manually segmented sen- tences whose words are annotated with phoneme sequences, and from sentences automatically an- notated with NLP tools which are also based on machine learning models trained on the annotated sentences. Thus normal IM servers are not capa- ble of presenting out-of-vocabulary (OOV) words (which provide large amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on <ref type="bibr" target="#b18">(Mori et al., 2006</ref>), and ensure that it is compu- tationally efficient enough for practical use by the public.</p><p>The target domain in our experiments is Twit- ter, a site where users post short messages called tweets. Since tweets are an immediate and power- ful reflection of public attitudes and social trends, there have been numerous attempts at extracting information from them. Examples include infor- mation analysis of disasters ( <ref type="bibr" target="#b24">Sakai et al., 2010)</ref>, estimation of depressive tendencies <ref type="bibr" target="#b27">(Tsugawa et al., 2013)</ref>, speech diarization (Higashinaka et al., 2011), and many others. These works require pre- processing of tweets with NLP tools, and WS is the first step. So it is clear that there is strong de- mand for improving WS accuracy. Another reason why we have chosen Twitter for the test domain is that the tweets typed using our server are open and we can avoid privacy problems. Our method does not utilize any other characteristics of tweets. So it also works in other domains such as blogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Input Method Suggesting OOV Words</head><p>In this section we propose a practical statistical IM server that suggests OOV word candidates in ad- dition to words in its vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Statistical Input Method</head><p>An input method (IM) is software which converts a phoneme sequence into a word sequence. This is useful for languages which contain far more char- acters than keys on a keyboard. Since there are some ambiguities in conversion, a conversion en- gine based on a word n-gram model has been pro- posed <ref type="bibr" target="#b1">(Chen and Lee, 2000</ref>). Today, almost all IM engines are based on statistical methods.</p><p>For the LM unit, instead of words we propose to adopt word-pronunciation pairs u = y, w. Thus given a phoneme sequence y l 1 = y 1 y 2 · · · y l as the input, the goal of our IM engine is to output a word sequencê w m 1 that maximizes the probabil- ity P (w, y l 1 ) as follows:</p><formula xml:id="formula_0">ˆ w m 1 = argmax w P (w, y l 1 ), P (w, y l 1 ) = m+1 ∏ i=1 P (u i |u i−1 i−n+1 ),</formula><p>where the concatenation of y i in each u i is equal to the input: y l 1 = y 1 y 2 · · · y m . In addition u j (j ≤ 0) are special symbols introduced to simplify the notation and u m+1 is a special symbol indicating a sentence boundary.</p><p>As in existing statistical IM engines, parame- ters are estimated from a corpus whose sentences are segmented into words annotated with their pro- nunciations as follows:</p><formula xml:id="formula_1">P (u i |u i−1 i−n+1 ) = F (u i i−n+1 ) F (u i−1 i−n+1 ) ,<label>(1)</label></formula><p>where F (·) denotes the frequency of a pair se- quence in the corpus. In contrast to IM engines based on a word n-gram model, ours does not re- quire an additional model describing relationships between words and pronunciations, and thus it is much simpler and more practical. Existing statistical IM engines only need an ac- curate automatic word segmenter to estimate the parameters of the word n-gram model. As the equation above shows, our pair-based engine also needs an accurate way of automatically estimat- ing pronunciation (phoneme sequences). How- ever, recently an automatic pronunciation estima- tor ( ) that delivers as accu- rate as state-of-the-art word segmenters has been proposed. As we explain in Section 6, in our ex- periments both our IM engine and existing ones delivered accuracy of 91%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Enumerating Substrings as Candidate Words</head><p>Essentially, the IM engine which we have ex- plained above does not have the ability to enumer- ate words which are unknown to the word seg- menter and the pronunciation estimator used to build the training data. The aim of our research is to gather language information from user behav- ior as they use an IM. So we extend the basic IM engine to enumerate all the substrings in a corpus with all possible pronunciations. For that purpose, we adopt the notion of a stochastically segmented corpus (SSC) ( <ref type="bibr" target="#b16">Mori and Takuma, 2004</ref>) and ex- tend it to the pronunciation annotation to words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Stochastically Segmented Corpora</head><p>An SSC is defined as a combination of a raw cor- pus C r (hereafter referred to as the character se- quence x nr 1 ) and word boundary probabilities of the form P i , which is the probability that a word boundary exists between two characters x i and x i+1 . These probabilities are estimated by a model based on logistic regression (LR) <ref type="bibr" target="#b2">(Fan et al., 2008</ref>) trained on a manually segmented corpus referring to the same features as those used in ). Since there are word boundaries be- fore the first character and after the last character of the corpus, P 0 = P nr = 1. Then word n-gram frequencies on an SSC are calculated as follows:</p><p>Word 0-gram frequency: This is defined as the expected number of words in the SSC:</p><formula xml:id="formula_2">f (·) = 1 + nr−1 ∑ i=1 P i .</formula><p>Word n-gram frequency (n ≥ 1): Consider the situation in which a word sequence w n 1 occurs in the SSC as a subsequence beginning at the (i + 1)-th character and ending at the k-th char- acter and each word w m in the word sequence is equal to the character sequence beginning at the</p><formula xml:id="formula_3">xi x b 1 xe 1 | {z } w 1 x b 2 xe 2 | {z } w 2 x bn x b n+1 xe n x k+1 | {z } wn fr(w n 1 ) = Pi(1 − P b 1 )Pe 1 (1 − P b 2 )Pe 2 · · · (1 − P bn )(1 − P b n+1 )Pe n</formula><p>Figure 1: Word n-gram frequency in a stochastically segmented corpus.</p><p>b m -th character and ending at the e m -th charac- ter (x em bm = w m , 1 ≤ ∀m ≤ n; e m + 1 = b m+1 , 1 ≤ ∀m ≤ n − 1; b 1 = i + 1; e n = k) (See <ref type="figure">Figure 1</ref> for an example). The word n- gram frequency of a word sequence f r (w n 1 ) in the SSC is defined by the summation of the stochastic frequency at each occurrence of the character sequence of the word sequence w n 1 over all of the occurrences in the SSC:</p><formula xml:id="formula_4">f r (w n 1 ) = ∑ (i,e n 1 )∈On P i   n ∏ m=1    em−1 ∏ j=bm (1 − P j )    P em   ,</formula><p>where e n</p><formula xml:id="formula_5">1 = (e 1 , e 2 , · · · , e n ) and O n = {(i, e n 1 )|x em bm = w m , 1 ≤ m ≤ n}.</formula><p>We calculate word n-gram probabilities by divid- ing word n-gram frequencies by word (n − 1)- gram frequencies. For a detailed explanation and a mathematical proof of this method, please refer to ( <ref type="bibr" target="#b16">Mori and Takuma, 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Pseudo-Stochastically Segmented</head><p>Corpora The computational costs (in terms of both time and space) for calculating an n-gram model from an SSC are very high 2 , so it is not a practical tech- nique for implementing an IM engine. In order to reduce the computational costs we approximate an SSC using a deterministically tagged corpus, which is called a pseudo-stochastically segmented corpus (pSSC) ( <ref type="bibr" target="#b6">Kameko et al., 2015</ref>). The follow- ing is the method for producing a pSSC from an SSC.</p><p>• For i = 1 to n r − 1 1. output a character x i , 2. generate a random number 0 ≤ p &lt; 1, 3. output a word boundary if p &lt; P i or output nothing otherwise.</p><p>Now we have a corpus in the same format as a standard segmented corpus with variable (non- constant) segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Pseudo-Stochastically Tagged Corpora</head><p>We can annotate a word with its all possi- ble pronunciations and their probabilities, as is done in an SSC. We call a corpus containing sequences of words (w 1 w 2 · · · w i · · · ) annotated with a sequence of pairs of a pronunciation and its probability (y i,1 , p i,1 , y i,2 , p i,2 , · · · , where ∑ j p i,j = 1, for ∀i) a stochastically tagged cor- pus (STC) <ref type="bibr">3</ref> . We can estimate these probabilities using an LR model built from sentences annotated with pronunciations ( ).</p><p>Similar to pSSC we then produce a pseudo- stochastically tagged sentence (pSTC) from an STC as follows:</p><p>• For each w i in the sentence 1. generate a random number 0 ≤ p &lt; 1, 2. annotate w i with its j-th phoneme se- quence y i,j , where</p><formula xml:id="formula_6">∑ j−1 1 p i,j ≤ p &lt; ∑ j 1 p i,j</formula><p>Now we have a corpus in the same format as a standard corpus annotated with variable pronunci- ation. By estimating the parameters in Equation (1) from a pSTC derived from a pSSC, our IM en- gine can also suggest OOV word candidates with various possible segmentation and pronunciations without incurring high computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Suggestion of OOV Words</head><p>Here we give an intuitive explanation why our IM engine can suggest OOV words for a certain phoneme sequence. Let us take an OOV word example: "/yo-ko-a-ri," an abbreviation of "" (Yokohama city arena). A WS system tends to segment it into "" (side) and " " (ant) because they are frequent nouns. In a pSSC, however, some occurrences of the string "" are remain concatenated as the correct word. For pronunciation, the first character has two possible pronunciations "yo-ko" and "o-u." So deterministic pronunciation estimation of this new word has the risk of outputting the erroneous result "o-u-a-ri." This prevents our engine from presenting "" as a conversion candidate for the input "yo-ko-a-ri." The pSTC, however, con- tains two possible pronunciations for this word and allows our engine to present the OOV word "" for the input "yo-ko-a-ri."</p><p>Thus when the user of our IM engine types "yo- ko-a-ri-ni-i-ku" and selects " (to) (go)," the engine can learn an OOV word " /yo-ko-a-ri" with context "/ni /i-ku".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Input Method Logs</head><p>In this section we first propose an IM which al- lows us to collect user logs. We then examine the characteristics of these logs and some difficulties in using them as language resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Collecting Logs from an Input Method</head><p>As <ref type="figure" target="#fig_0">Figure 2</ref> shows, the client of our IM, running on the user's PC, is used to input characters and to modify conversion results. The server logs both input from the client and the results of conversions performed in response to requests from the client.</p><p>Our IM has two phases: phoneme typing and conversion result editing. In each phase, the client sends the typed keys to the server with a timestamp and its IP address.</p><p>Phoneme typing: First the user inputs ASCII characters for a phoneme sequence. If the phoneme sequence itself is what the user wants to write, the user may not go to the next phase. The server records the keys typed to enter the phoneme sequence, cursor move- ments, and the phoneme sequence if the user selects it as-is.</p><p>Conversion result editing: Then the user presses a space key to make the IM engine con- vert the phoneme sequence to the most likely word sequence based on Equation (1). Some- times the user changes some word bound- aries, makes the IM engine enumerate can- didate words covering the region, and selects the intended one from the list of candidates. The server records a space key and the final word sequence. <ref type="table">Table 1</ref> shows an example of interesting log mes- sages from the same IP address 4 . In many cases, users type sentence fragments but not a complete sentence. So in the example there are six frag- ments within a short period indicated by the times- tamps. If the user selects the phoneme sequence as-is without going to the conversion result editing phase, we can expect that there are word bound- aries on both sides of the phoneme sequence. In- <ref type="table">Table 1</ref>: Input method logs of a tweet '' (It is cheap compared with Yoko- hama arena). Timestamp Phoneme sequence Edit result Note 18:37:11.21 /yo-ko-a-ri-ni /yo-ko-a-ri /ni (with Yokohama arena) 18:37:12.60 /ku-ra-b-be-ru /ku-ra-b /be-ru Mistyping 18:37:14.94 /ku-ra-be-ru /ku-ra-be /ru Revised input (compare) 18:37:15.32 /to N/A (inflectional ending) 18:37:19.82 /mo-no-no N/A Discarded in the twitter 18:37:22.42 /ya-su-me-ka-to /ya-su-me /ka /to (cheap) side the phoneme sequence, however, there is no information. If the user goes to the conversion result editing phase, we can expect that the final word sequence has correct word boundary infor- mation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Characteristics of Input Method Logs</head><p>There are two main problems that make it dif- ficult to directly use IM logs as a training cor- pus for word segmentation. The first problem is fragmentation. IM users send the phoneme se- quences for sentence fragments to the engine to avoid editing long conversion results that require many cursor movements. Thus the phoneme se- quence and the final word sequence tend to be sentence fragments (as we noted above) and as a result they lose context information. The second problem is noise. Word boundary information is unreliable even when it is present because of mis- takenly selected conversions or words entered sep- arately. From these observations, the IM logs are treated as partially segmented sentence fragments that include noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Word Segmentation Using Input Method Logs</head><p>In this section we first explain various ways to generate language resources for a word segmenter from IM logs. We then describe an automatic word segmenter which utilizes these resources. In the examples below we use the three-valued notation <ref type="bibr" target="#b15">(Mori and Oda, 2009</ref>) to denote partial segmenta- tion as follows: | : there is a word boundary, -: there is not a word boundary, : there is no information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Input Method Logs as Language Resources</head><p>The phoneme sequences and edit results in the fi- nal selection themselves are considered to be par- tially segmented sentences. We call the corpus generated directly from the logs "Log-as-is." Ex- amples in <ref type="table">Table 1</ref> are converted as following.</p><p>Example of Log-as-is (12 annotations)</p><formula xml:id="formula_7">--| --|- -| -||</formula><p>Here the number of annotations is the sum of "-" and "|". In this example, one entry corresponds to one entry of the training data for the word seg- menter. As you can easily imagine, Log-as-is may contain mistaken results (noise) and short entries (fragmentation). Both are harmful for a word seg- menter.</p><p>To cope with the fragmentation problem, we propose to connect some logs based on their times- tamps. If the difference between the timestamps of two sequential logs is short, both logs are proba- bly from the same sentence. So we connect two sequential logs if the time difference between the last key of the first log and the first key of the sec- ond log is smaller than a certain threshold s. In the experiment we set s = 500[ms] based on observa- tions of our behavior <ref type="bibr">5</ref> . This method is referred to as "Log-chunk." Using this method, we obtain the following from the examples in <ref type="table">Table 1</ref>.</p><p>Example of Log-chunk (15 annotations)</p><formula xml:id="formula_8">--||--|- -||| -||</formula><p>We see that Log-chunk contains more context in- formation than Log-as-is. For preventing the noise problem, we propose to filter out logs with a small number of conver- sions. We expect that an edited sentence will have many OOV words and not much noise. Therefore we use logs which were converted more than n c times. In the experiment we set n c = 2 based on  <ref type="table" target="#tab_0">Training  BCCWJ  56,753 1,324,951 1,911,660  Newspaper  8,164  240,097  361,843  Conversation 11,700  147,809  197,941  Test  BCCWJ-test  6,025  148,929  212,261  TWI-test  2,976  37,010  58,316</ref> observations of our behavior <ref type="bibr">6</ref> . This method is re- ferred to as "Log-mconv." Using this method, the examples in <ref type="table">Table 1</ref> becomes the following. Example of Log-mconv (3 annotations)</p><formula xml:id="formula_9">--|</formula><p>As this example shows, Log-mconv contains short entries (fragmentation) like Log-as-is. However, we expect that the annotated tweets do not include mistaken boundaries or conversions that were dis- carded.</p><p>Obviously we can combine Log-chunk and Log-mconv to avoid both the fragmentation and noise problems. This combination is referred to as "Log-chunk-mconv."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training a Word Segmenter on Logs</head><p>The IM logs give us partially segmented sentence fragments, so we need a word segmenter capa- ble of learning from them. We can use a word segmenter based on a sequence classifier ( <ref type="bibr" target="#b26">Tsuboi et al., 2008;</ref><ref type="bibr" target="#b28">Yang and Vozila, 2014;</ref><ref type="bibr" target="#b4">Jiang et al., 2013)</ref> or one based on a pointwise classifier ( ). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy.</p><p>Here is a brief explanation of the word seg- menter based on the pointwise method. For more detail the reader may refer to ). The input is an unsegmented character sequence x = x 1 x 2 · · · x k . The word segmenter decides if there is a word boundary t i = 1 or not t i = 0 by using support vector machines (SVMs) <ref type="bibr" target="#b2">(Fan et al., 2008)</ref>  <ref type="bibr">7</ref> . The features are character n-grams <ref type="bibr">6</ref> The results were stable for nc in the preliminary experi- ments. <ref type="bibr">7</ref> The reason why we use SVM for word segmentation is that the accuracy is generally higher than that based on LR. It was so in the experiments of this paper. The F-measure of LR on TWI-test was 91.30 (Recall = 89.50, Precision = 93.17), <ref type="table" target="#tab_0">Table 3: Language resources derived from logs.  #sentence  fragments #annotations  Log-as-is  32,119  39,708  Log-chunk  8,685  63,144  Log-mconv  4,610  10,852  Log-chunk-mconv  1,218  14,242</ref> and character type n-grams (n = 1, 2, 3) around the decision points in a window with a width of 6 characters. Additional features are triggered if character n-grams in the window match with char- acter sequences in the dictionary. This approach is called pointwise because the word boundary deci- sion is made without referring to the other deci- sions on the points j = i. As you can see from the explanation given above, we can also use partially segmented sentences from IM logs for training in the standard way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>As an evaluation of our methods, we measured the accuracy of WS without using logs (the base- line) and using logs converted by several methods. There are two test corpora: one is the general do- main corpus from which we built the baseline WS, and the other is the same domain that the IM logs were collected from, Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Corpora</head><p>The annotated corpus we used to build the base- line word segmenter is the manually annotated part (core data) of the Balanced Corpus of Con- temporary Written Japanese (BCCWJ) <ref type="bibr" target="#b10">(Maekawa, 2008)</ref>, plus newspaper articles and daily conver- sation sentences. We also used a 234,652-word dictionary (UniDic) provided with the BCCWJ. A small portion of the BCCWJ core data is reserved for testing. In addition, we manually segmented sentences randomly obtained from Twitter 8 during the same period as the log collection for the test corpus. <ref type="table" target="#tab_0">Table 2</ref> shows the details of these corpora.</p><p>which is lower than that of SVM (see <ref type="table" target="#tab_1">Table 4</ref>). To make an SSC, however, we use an LR model because we need word boundary probabilities. <ref type="bibr">8</ref> We extracted body text from 1,592 tweets excluding mentions, hash tags, URLs, and ticker symbols. Then we divided the body text into sentences by separating on newline characters, resulting in 2,976 sentences.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Models using Input Method Logs</head><p>To make the training data for our IM server, we first chose randomly selected tweets (786,331 sen- tences) in addition to the unannotated part of the BCCWJ (358,078 sentences). We then trained LR models which estimate word boundary probabili- ties and pronunciation probabilities for words (and word candidates) from the training data shown in <ref type="table" target="#tab_0">Table 2</ref> and UniDic. We made a pSTC for our IM engine from 1,207,182 sentences randomly ob- tained from Twitter by following the procedure which we explained in Subsection 3.2.3 9 . We launched our IM as a browser add-on for Twitter and collected 19,770 IM logs from 7 users between April 24 and December 31, 2014. Fol- lowing the procedures in Section 5.1, we obtained the language resources shown in <ref type="table">Table 3</ref>. We com- bined them with the training corpus and dictionar- ies to build four WSs, which we compared with the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results and Discussion</head><p>Following the standard in WS experiments, the evaluation criteria are recall, precision, and F- measure (their harmonic mean). Recall is the number of correctly segmented words divided by the number of words in the test corpus. Preci- sion is the number of correctly segmented words divided by the number of words in the system out- put. <ref type="table" target="#tab_1">Table 4</ref> and 5 show WS accuracy on TWI-test and BCCWJ-test, respectively. The difference in <ref type="bibr">9</ref> There is no overlap with the test data.</p><p>accuracy of the baseline method on BCCWJ-test and TWI-test shows that WS of tweets is very dif- ficult. The fact that the precision on TWI-test is much higher than the recall indicates that the base- line model suffers from over-segmentation. This over-segmentation problem is mainly caused by OOV words being divided into known words. For example, "" (Yokohama arena) is divided into the two know words "" (side) and "" (ant).</p><p>When we compare the F-measures on TWI-test, all the models referring to the IM logs outperform the baseline model trained only from the BCCWJ. The highest is the Log-chunk-mconv model and the improvement over the baseline is statistically significant (significance level: 1%). In addition the accuracies of the five methods on the BCCWJ <ref type="table" target="#tab_2">(Table 5</ref>) are almost the same and there is no statis- tical significance (significance level: 1%) between any two of them.</p><p>We analyzed the words misrecognized by the WSs, which we call error words. <ref type="table" target="#tab_3">Table 6</ref> shows the number of error words, the number of OOV words, and the ratio of OOV words to error words. Here the vocabulary is the set of the words appear- ing in the training data or in UniDic (see <ref type="table" target="#tab_0">Table 2</ref>). Although the result of the WS trained on Log-as- is contains more error words than the baseline, the OOV ratio is less than the baseline. This means that the IM logs have a potential to reduce errors caused by OOV words. <ref type="table" target="#tab_3">Table 6</ref> also indicates that the best method Log- chunk-mconv had the greatest success in reducing  errors caused by OOV words. However, the ma- jority of error words are in-vocabulary words. It can be said that our log chunking method (Log- chunk or Log-chunk-mconv) enabled the WSs to eliminate many known word errors by using con- text information.</p><p>To investigate the impact of the log size, we measured WS accuracy on TWI-test when vary- ing the log size during training. <ref type="figure" target="#fig_1">Figure 3</ref> shows the results. <ref type="table" target="#tab_1">Table 4</ref> says that Log-chunk-mconv and Log-chunk increase the accuracy nicely. The graph, however, clarifies that Log-chunk-mconv achieves high accuracy with fewer training data converted from logs. In other words, the method Log-chunk-mconv is good at distilling the in- formative parts and filtering out the noisy parts. These characteristics are very important properties to have as we consider deploying our IM to a wider audience. An IM is needed to type Japanese and the number of Japanese speakers is more than 100 million. If we can use input logs of even 1% of them for the same or longer period 10 , the idea we propose in this paper can improve WS accuracy on various domains efficiently and automatically.</p><p>As a final remark, this paper describes a suc- <ref type="bibr">10</ref> The number of users using our system in this paper is 7 for 8 months.</p><p>cessful example of how to build a useful tool for the NLP community. This process has three steps: 1) design a useful NLP application that can collect user logs, 2) deploy it for public use, and 3) devise a method for mining data from the logs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper described the design of a publicly us- able IM which collects natural annotations for use as training data for another system. Specifically, we (1) described how to construct an IM server that suggests OOV word candidates, (2) designed a publicly usable IM that collects logs of user behavior, and (3) proposed a method for using this data to improve word segmenters. Tweets from Twitter are a promising source of data with great potential for NLP, which is one reason why we used them as the target domain for our ex- periments. The experimental results showed that our methods improve accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain, so it is worth testing on other domains and with much longer pe- riods of data collection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Input method for collecting logs.</figDesc><graphic url="image-1.png" coords="5,80.85,62.93,435.89,240.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relationship between WS accuracy on the tweets and log size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Corpus specifications. 
#sent. 
#words 
#char. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>WS accuracy on the tweets. 
Recall [%] Precision [%] F-measure 
Baseline 
90.31 
94.05 
92.14 
+ Log-as-is 
90.33 
93.77 
92.02 
+ Log-chunk 
91.04 
94.29 
92.64 
+ Log-mconv 
90.62 
94.09 
92.32 
+ Log-chunk-mconv 
91.40 
94.45 
92.90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>WS accuracy on BCCWJ. 
Recall [%] Precision [%] F-measure 
Baseline 
99.01 
98.97 
98.99 
+ Log-as-is 
99.02 
98.87 
98.94 
+ Log-chunk 
99.05 
98.88 
98.96 
+ Log-mconv 
98.98 
98.91 
98.95 
+ Log-chunk-mconv 
98.98 
98.92 
98.95 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 6 : Ratio of OOV words in error words.</head><label>6</label><figDesc></figDesc><table>#Error words #OOV words (ratio[%]) 
Baseline 
446 
103 
(23.09) 
+ Log-as-is 
467 
89 
(19.06) 
+ Log-chunk 
428 
81 
(18.93) 
+ Log-mconv 
443 
88 
(19.86) 
+ Log-chunk-mconv 
413 
74 
(17.79) 

92.1 

92.3 

92.5 

92.7 

92.9 

0 
20000 
40000 60000 
80000 100000 

F-measure 

#characters 

Log-chunk 

Log-chunk-mconv 

</table></figure>

			<note place="foot">* This work was done when the first author was at Kyoto University.</note>

			<note place="foot" n="1"> https://twitter.com/ (accessed in 2015 May).</note>

			<note place="foot" n="2"> This is because an SSC has many words and word fragments. Additionally, word n-gram frequencies must be calculated using floating point numbers instead of integers.</note>

			<note place="foot" n="3"> Because the existence or non-existence of a word boundary information can also be expressed as a tag, a stochastically tagged corpus includes stochastic segmentation.</note>

			<note place="foot" n="4"> In reality, logs from different IPs are stored in the order that they were received.</note>

			<note place="foot" n="5"> The results were stable for s in preliminary experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by JSPS Grants-in-Aid for Scientific Research Grant Number <ref type="bibr">26280084</ref> and Microsoft CORE project. We thank Dr. Hisami Suzuki, Dr. Koichiro Yoshino, and Mr. Daniel Flannery for their valuable comments and suggestions on the manuscript. We are also grate-ful to the anonymous users of our input method.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new statistical approach to Chinese pinyin input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Fu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="241" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Building a conversational model from two-tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Kawamae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kugatsu</forename><surname>Sadamitsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyomi</forename><surname>Meguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohji</forename><surname>Dohsaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirohito</forename><surname>Inagaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on ASRU</title>
		<imprint>
			<biblScope unit="page" from="330" to="335" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative learning with natural annotations: Word segmentation as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate word segmentation and POS tagging for Japanese microblogs: Corpus annotation and joint modeling with lexical normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can symbol grounding improve lowlevel NLP? Word segmentation as a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirotaka</forename><surname>Kameko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Applying conditional random fields to Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth ICML</title>
		<meeting>the Eighteenth ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for CRF-based Chinese word segmentation using free annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="864" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Balanced corpus of contemporary written Japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Asian Language Resources</title>
		<meeting>the 6th Workshop on Asian Language Resources</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="101" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical input method based on a phrase class n-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokuni</forename><surname>Maeta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Advances in Text Input Methods</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Class-based variable memory length markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the InterSpeech2005</title>
		<meeting>the InterSpeech2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A pointwise approach to pronunciation estimation for a TTS front-end</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the InterSpeech2011</title>
		<meeting>the InterSpeech2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2181" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language resource addition: Dictionary or corpus?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineth International Conference on Language Resources and Evaluation</title>
		<meeting>the Nineth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1631" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic word segmentation using three types of dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Oda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference Pacific Association for Computational Linguistics</title>
		<meeting>the Eighth International Conference Pacific Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word n-gram probability estimation from a Japanese raw corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Takuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Speech and Language Processing</title>
		<meeting>the Eighth International Conference on Speech and Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1037" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Kana-kanji conversion by a stochastic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuchiya</forename><surname>Masatoshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yamaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Nagao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Transactions of Information Processing Society of Japan</publisher>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2946" to="2953" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Phoneme-to-text transcription system with an infinite vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Takuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A stochastic Japanese morphological analyzer using a forward-DP backwardA * n-best search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Computational Linguistics</title>
		<meeting>the 15th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wordbased partial annotation for efficient corpus construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2723" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointwise prediction for robust, adaptable Japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="562" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The power of amnesia: Learning probabilistic automata with variable memory length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="117" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Earthquake shakes Twitter users: Real-time event detection by social sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW &apos;10</title>
		<meeting>the 19th International Conference on World Wide Web, WWW &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A stochastic finite-state wordsegmentation algorithm for Chinese</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Richard Sproat and Chilin Shih William Gale Nancy Chang</editor>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="404" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Training conditional random fields using incomplete annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On estimating depressive tendencies of Twitter users utilizing their tweet data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sho</forename><surname>Tsugawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukiko</forename><surname>Mogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumio</forename><surname>Kishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Ohsaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VR&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vozila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="98" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
