<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Semi-Supervised Learning for Deep Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanket</forename><surname>Vaibhav</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehta</forename><forename type="middle">⇤</forename><surname>Jay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Semi-Supervised Learning for Deep Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4958" to="4963"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4958</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). However, the neural models require an immense amount of semantic-role corpora and are thus not well suited for low-resource languages or domains. The paper proposes a semi-supervised semantic role labeling method that outperforms the state-of-the-art in limited SRL training corpora. The method is based on explicitly enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component and uses SRL-unlabeled instances to train a joint-objective LSTM. On CoNLL-2012 English section, the proposed semi-supervised training with 1%, 10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively , over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL), a.k.a shallow se- mantic parsing, identifies the arguments corre- sponding to each clause or proposition, i.e. its se- mantic roles, based on lexical and positional in- formation. SRL labels non-overlapping text spans corresponding to typical semantic roles such as Agent, Patient, Instrument, Beneficiary, etc. This task finds its use in many downstream applica- tions such as question-answering <ref type="bibr" target="#b13">(Shen and Lapata, 2007</ref>), information extraction ( <ref type="bibr" target="#b0">Bastianelli et al., 2013)</ref>, machine translation, etc.</p><p>Several SRL systems relying on large annotated corpora have been proposed <ref type="bibr" target="#b9">(Peters et al., 2018;</ref><ref type="bibr"></ref> ⇤ Equal contribution, name order decided by coin flip. <ref type="bibr" target="#b4">He et al., 2017)</ref>, and perform relatively well. A more challenging task is to design an SRL method for low resource scenarios (e.g. rare languages or domains) where we have limited annotated data but where we may leverage annotated data from related tasks. Therefore, in this paper, we focus on building effective systems for low resource sce- narios and illustrate our system's performance by simulating low resource scenarios for English.</p><p>SRL systems for English are built using large annotated corpora of verb predicates and their ar- guments provided as part of the PropBank and OntoNotes v5.0 projects <ref type="bibr" target="#b5">(Kingsbury and Palmer, 2002;</ref><ref type="bibr" target="#b11">Pradhan et al., 2013</ref>). These corpora are built by adding semantic role annotations to the constituents of previously-annotated syntactic parse trees in the Penn Treebank ( <ref type="bibr" target="#b8">Marcus et al., 1993)</ref>. Traditionally, SRL relies heavily on using syntactic parse trees either from shallow syntac- tic parsers (chunkers) or full syntactic parsers and Punyakanok et al. shows significant improvements by using syntactic parse trees.</p><p>Recent breakthroughs motivated by end-to-end deep learning techniques ( <ref type="bibr" target="#b14">Zhou and Xu, 2015;</ref><ref type="bibr" target="#b4">He et al., 2017</ref>) achieve state-of-the-art perfor- mance without leveraging any syntactic signals, relying instead on ample role-label annotations. We hypothesize that by leveraging syntactic struc- ture while training neural SRL models, we may achieve robust performance, especially for low resource scenarios. Specifically, we propose to leverage syntactic parse trees as hard constraints for the SRL task i.e., we explicitly enforce that the predicted argument spans of the SRL network must agree with the spans implied by the syn- tactic parse of the sentence via scoring function in the training objective. Moreover, we present a semi-supervised learning (SSL) based formula- tion, wherein we leverage syntactic parse trees for SRL-unlabeled data to build effective SRL for low resource scenarios.</p><p>We build upon the state-of-the-art SRL system by ( <ref type="bibr" target="#b9">Peters et al., 2018;</ref><ref type="bibr" target="#b4">He et al., 2017)</ref>, where we formulate SRL as a BIO tagging problem and use multi-layer highway bi-directional LSTMs. How- ever, we differ in terms of our training objective. In addition to the log-likelihood objective, we also include syntactic inconsistency loss (defined in Section 2.3) which quantifies the hard constraint (spans implied by syntactic parse) violations in our training objective. In other words, while training our model, we enforce the outputs of our system to agree with the spans implied by the syntactic parse of the sentence as much as possible. In summary, our contributions to low-resource SRL are:</p><p>1. A novel formulation which leverages syntac- tic parse trees for SRL by introducing them as hard constraints while training the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Experiments with varying amounts of SRL- unlabeled data that point towards semi- supervised learning for low-resource SRL by leveraging the fact that syntactic inconsis- tency loss does not require labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approach</head><p>We build upon an existing deep-learning approach to SRL <ref type="bibr" target="#b4">(He et al., 2017</ref>). First we revisit defini- tions introduced by <ref type="bibr" target="#b4">(He et al., 2017)</ref> and then dis- cuss about our formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task definition</head><p>Given a sentence-predicate pair (x, v), SRL task is defined as predicting a sequence of tags y, where each y i belongs to a set of BIO tags (⌦). So, for an argument span with semantic role ARG i , B-ARG i tag indicates that the corresponding to- ken marks the beginning of the argument span and I-ARG i tag indicates that the corresponding token is inside of the argument span and O tag indicates that token is outside of all argument spans. Let n = |x| = |y| be the length of the sentence. Fur- ther, let srl-spans(y) denote the set of all argument spans in the SRL tagging y. Similarly, parse- spans(x) denotes the set of all unlabeled parse constituents for the given sentence x. Lastly, SRL- labeled/unlabeled data refers to sentence-predicate pairs with/without gold SRL tags.</p><p>2.2 State-of-the-Art (SOTA) Model</p><p>He et al. proposed a deep bi-directional LSTM to learn a locally decomposed scoring func- tion conditioned on the entire input sentence- P n i=1 log p(y i |x). To learn the parameters of a network, the conditional negative log-likelihood L(w) of a sample of training data</p><formula xml:id="formula_0">T = {x (t) , y (t) } 8t is minimized, where L(w) is L(w) = X (x,y)2T |y| X i=1</formula><p>log p(y i |x; w). <ref type="formula">(1)</ref> Since Eq. <ref type="formula">(1)</ref> does not model any dependencies between the output tags, the predicted output tags tend to be structurally inconsistent. To alleviate this problem, ( <ref type="bibr" target="#b4">He et al., 2017</ref>) searches for the bestˆy bestˆ bestˆy over the space of all possibilities (⌦ n ) using the scoring function f (x, y), which incorporates log probability and structural penalty terms. The de- tails of scoring function is on Appendix Eq. <ref type="formula">(7)</ref>.</p><formula xml:id="formula_1">ˆ y = arg max y 0 2⌦ n f (x, y 0 ) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Structural Constraints</head><p>There are different types of structural constraints: BIO, SRL and syntactic constraints. BIO con- straints define valid BIO transitions for sequence tagging. For example, B-ARG0 cannot be fol- lowed by I-ARG1. SRL constraints define rules on the role level and has three particular con- straints: unique core roles (U), continuation roles (C) and reference roles (R) ( <ref type="bibr" target="#b12">Punyakanok et al., 2008)</ref>. Lastly, syntactic constraints state that srl- spans(y) have to be subset of parse-spans(x).</p><p>( <ref type="bibr" target="#b4">He et al., 2017</ref>) use BIO and syntactic con- straints at decoding time by solving Eq.(2) where f (x, y) incorporates those constraints and report that SRL constraints do not show significant im- provements over the ensemble model. In partic- ular, by using syntactic constraints, ( <ref type="bibr" target="#b4">He et al., 2017</ref>) achieves up to +2 F1 score on CoNLL-2005 dataset via A* decoding. Improvements of SRL system via use of syntac- tic constraints is consistent with other observations <ref type="bibr" target="#b12">(Punyakanok et al., 2008)</ref>. However, all previous works enforce syntactic consistency only during decoding step. We propose that enforcing syntac- tic consistency during training time would also be beneficial and show the efficacy experimentally on Section 3.3.</p><p>Enforcing Syntactic Consistency To quantify syntactic inconsistency, we define disagreeing- spans(x, y) = {span i 2 srl-spans(y) | span i / 2 parse-spans(x)}. Further, we define disagreement rate, d(x, y) 2 <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and syntactic inconsistency score, s(x, y) 2 [1, 1], as follows:</p><formula xml:id="formula_2">d(x, y) = |disagreeing-spans(x, y)| |srl-spans(y)| (3) s(x, y) = 2 ⇥ d(x, y) 1 (4)</formula><p>Syntactic Inconsistency Loss (SI-Loss) For a given (x, v), let us considerˆyconsiderˆ considerˆy (t) to be the best possible tag sequence (according to Eq.(2) during epoch t of model training. Ideally, if our model is syntax-aware, we would have d(x, ˆ y (t) ) = 0 or r(x, ˆ y (t) ) = 1. We define a loss component due to syntactic inconsistency (SI-Loss) as follows:</p><formula xml:id="formula_3">SI-Loss = s(x, ˆ y (t) ) |ˆy|ˆy (t) | X i=1 log p(ˆ y (t) i |x; w (t) ) (5)</formula><p>During training, we want to minimize SI-Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training with Joint Objective</head><p>Based on Eq.</p><p>(1), a supervised loss, and Eq. <ref type="formula">(5)</ref>, the SI-Loss, we propose a joint training objective. For a given sentence-predicate pair (x, v) and SRL tags y, our joint training objective (at epoch t) is defined as:</p><formula xml:id="formula_4">Joint loss = ↵ 1 |y| X i=1 log p(y i |x; w) +↵ 2 r(x, ˆ y (t) ) |ˆy|ˆy (t) | X i=1 log p(ˆ y (t) i |x; w (t) )<label>(6)</label></formula><p>Here, ↵ 1 and ↵ 2 are weights (hyper-parameters) for different loss components and are tuned using a development set. During training, we minimize joint loss -i.e., negative log-likelihood (or cross- entropy loss) and syntactic inconsistency loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Semi-supervised learning formulation</head><p>In low resource scenarios, we have limited labeled data and larger amounts of unlabeled data. The obvious question is how to leverage large amounts of unlabeled data for training accurate models. In context of SRL, we propose to leverage SRL- unlabeled data in terms of parse trees.</p><p>Observing Eq.(5), one can notice that our formulation of SI-Loss is only dependent upon model's predicted tag sequencê y (t) at a particular time point t during training and the given sentence and it does not depend upon gold SRL tags. We leverage this fact in our SSL formulation to com- pute SI-loss from SRL-unlabeled sentences.  Let sup-s be a batch of SRL-labeled sentences and usup-s be a batch SRL-unlabeled sentences only with parse information. In SSL setup, we propose to train our model with joint objective where sup-s only contributes to supervised loss Eq.(1) and unsup-s contributes in terms of syn- tactic inconsistency objective Eq.(5) and combine them according to Eq.(6) to train them with joint loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We evaluate our model's performance on span- based SRL dataset from CoNLL-2012 shared task ( <ref type="bibr" target="#b11">Pradhan et al., 2013)</ref>. This dataset con- tains gold predicates as part of the input sen- tence and also gold parse information correspond- ing to each sentence which we use for defin- ing hard constraints for SRL task. We use standard train/development/test split containing 278K/38.3K/25.6K sentences. Further, there is ap- prox. 10% disagreement between gold SRL-spans and gold parse-spans (we term these as noisy syn- tactic constraints). During training, we do not preprocess data to handle these noisy constraints but for the analysis related to enforcing syntactic constraints during inference, we study both cases: with and without noisy constraints. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model configurations</head><p>For the SOTA system proposed in ( <ref type="bibr" target="#b9">Peters et al., 2018)</ref>, we use code from Allen AI 2 to implement our approach. We follow their initialization and training configurations.</p><p>Let BX, JX denote model trained with X% of the SRL-labeled data with cross-entropy and joint training objective, re-  spectively. BX-SIU x and BX-JU x denote model trained with SI-loss and Joint loss, respectively, on the pre-trained BX model where X ⇥U amount of SRL-unlabeled data were used for further training.</p><p>To satisfy BIO constraints, we run Viterbi decod- ing by default for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We are interested in answering following ques- tions. (Q1) how well does the baseline model produce syntactically consistent outputs, (Q2) does our approach actually enforce syntactic con- straints, (Q3) does our approach enforce syntactic constraints without compromising the quality of the system, (Q4) how well does our SSL formu- lation perform, especially in low-resource scenar- ios, and lastly (Q5) what is the difference in using the syntactic constraints in training time compared to using it at decoding time. To answer (Q1-2) fa- vorably we report average disagreement rate com- puted over test split. To answer (Q3-5), we report overall F1-scores on CoNLL-2012 test set (using standard evaluation script). For experiments us- ing SRL-unlabeled data, we report average results after running multiple experiments with different random samples of it.</p><p>Does training with joint objective help? We trained 3 models with random 1%, 10% and whole 100% of the training set with joint objec- tive (↵ 1 = ↵ 2 = 0.5). For comparison, we trained 3 SOTA models with the same training sets. All models were trained for max 150 epochs and with a patience of 20 epochs.   by training models jointly to satisfy syntactic con- straints helps in better generalization when trained with limited SRL corpora.</p><p>Does SSL based training work for low-resource scenarios? To enforce syntactic constraints via SI-loss on SRL-unlabeled data, we further train pre-trained model with two objectives in SSL set up: (a) SI-loss <ref type="table" target="#tab_3">(Table 2)</ref> and (b) joint objective <ref type="table" target="#tab_5">(Table 3)</ref> For experiment (a), we use square loss, kW W pre-train k 2 regularizer to keep the model W close to the pre-trained model W pre-train to avoid catas- trophic forgetting ( set to 0.005). We optimize with SGD with learning rate of 0.01, ↵ 2 = 1.0, patience of 10 epochs. We see that with SI-loss improvements are significant in terms of average disagreement rate as compared to F1.</p><p>For experiment (b), we train B1 and B10 with joint objective in SSL set-up (as discussed in Sec- tion 2.5). We use SGD with learning rate of 0.05, ↵ 1 = ↵ 2 = 1.0 and patience of 10 epochs. We report +1.58 F1 and +0.78 F1 improvement over B1 and B10, trained with 5% and 100% SRL- unlabeled data, respectively. Note that we can- not achieve these improvements with simply fine- tunning BX with supervised loss, as seen with BX-further on <ref type="table" target="#tab_5">Table 3</ref>. This provides evidence to answer (Q4) favorably. In general, the per- formance gains increase as the size of the SRL- unlabeled data increases.</p><p>Is it better to enforce syntactic consistency on decoding or on training time? To answer (Q5), we conducted three experiments: using syntactic constraints on (a) inference only, i.e. structured prediction, (b) training only, and (c) both training and inference steps. For the structured prediction, we consider A* decoding, as used in ( <ref type="bibr" target="#b4">He et al., 2017)</ref> and gradient-based inference ( , which optimizes loss function similar to SI- loss on Eq. <ref type="formula">(5)</ref> per example basis. If neither A* decoding nor gradient-based inference is used, we use Viterbi algorithm to enforce BIO constraints. The performance is the best (bold on <ref type="table" target="#tab_6">Table 4</ref>) when syntactic consistency is enforced both on training and inference steps, +3.67, +2.1 F1 score improvement over B1 and B10 respectively, and we conclude that the effort of enforcing syntactic consistency on inference time is complementary to the same effort on training time. However, note that the overall performance increases as the bene- fit from enforcing syntactic consistency with SSL is far greater compared to marginal decrement on structured prediction. While syntactic constraints help both train and in- ference, injecting constraints on train time is far more robust compared to enforcing them on de- coding time. The performance of the structured prediction drops rapidly when the noise in the parse information is introduced (x column of Ta- ble 4). On the other hand, SSL was trained on CoNLL2012 data where about 10% of the gold SRL-spans do not match with gold parse-spans and even when we increase noise level to 20% the performance drop was only around 0.1 F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The traditional approaches for SRL ( <ref type="bibr" target="#b10">Pradhan et al., 2005;</ref><ref type="bibr" target="#b6">Koomen et al., 2005</ref>) constituted of cascaded system with four subtasks: pruning, ar- gument identification, argument labeling, and in- ference.</p><p>Recent approaches ( <ref type="bibr" target="#b14">Zhou and Xu, 2015;</ref><ref type="bibr" target="#b4">He et al., 2017)</ref> proposed end-to-end system for SRL using deep recurrent or bi-LSTM-based architecture with no syntactic inputs and have achieved SOTA results on English SRL. Lastly, ( <ref type="bibr" target="#b9">Peters et al., 2018)</ref> proposed ELMo, a deep con- textualized word representations, and improved the SOTA English SRL by 3.2 F1-points.</p><p>Even on the end-to-end learning, inference still remains as a separate subtask and would be for- malized as a constrained optimization problem. To solve this problem ILP ( <ref type="bibr" target="#b12">Punyakanok et al., 2008)</ref>, A* algorithm <ref type="bibr" target="#b4">(He et al., 2017</ref>) and gradient-based inference (  were employed. Fur- ther, all of these works leveraged syntactic parse during inference and was never used during train- ing unless used as a cascaded system.</p><p>To the best of our knowledge, this work is the first attempt towards SSL span-based SRL model. Nonetheless, there were few efforts in SSL in dependency-based SRL systems <ref type="bibr" target="#b3">(Fürstenau and Lapata, 2009;</ref><ref type="bibr" target="#b2">Deschacht and Moens, 2009;</ref><ref type="bibr" target="#b1">Croce et al., 2010)</ref>. <ref type="bibr" target="#b3">(Fürstenau and Lapata, 2009)</ref> pro- posed to augment the dataset by finding similar unlabeled sentences to already labeled set and an- notate accordingly. While interesting, the similar augmentation technique is harder to apply to span- based SRL as one requires to annotate the whole span. <ref type="bibr" target="#b2">(Deschacht and Moens, 2009;</ref><ref type="bibr" target="#b1">Croce et al., 2010)</ref> proposed to leverage the relation between words by learning latent word distribution over the context, i.e. language model. Our paper also in- corporates this idea by using ELMo as it is trained via language model objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We presented a SI-loss to enforce SRL systems to produce syntactically consistent outputs. Fur- ther, leveraging the fact that SI-loss does not re- quire labeled data, we proposed a SSL formulation with joint objective constituting of SI-loss and su- pervised loss together. We show the efficacy of the proposed approach on low resource settings, +1.58, +0.78 F1 on 1%, 10% SRL-labeled data respectively, via further training on top of pre- trained SOTA model. We further show the struc- tured prediction can be used as a complimentary tool and show performance gain of +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively. Semi-supervised training from the scratch and examination of semi-supervised setting on large dataset remains as part of the fu- ture work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Comparison of baseline models (B) with the 
models trained with joint objective (J). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training with SI-loss for varying sizes of 
SRL-unlabeled data on top of the pre-trained baseline 
models (B1, B10 on Table 1). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 reports</head><label>1</label><figDesc></figDesc><table>the results 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Training with joint objective (J) on top of the 
baseline models (B1, B10 from Table 1), with the same 
SRL-labeled data used to train the baseline models and 
with varying sizes of SRL-unlabeled data. 

Decoding 
B10 B10-J10x 
B1 B1-J5x 
Viterbi 
78.56 
79.34 
67.28 
68.86 
Noisy syntactic constraints 

A* 
72.95 
73.57 
63.77 
64.73 
(-5.61) 
(-5.77) 
(-3.51) 
(-4.13) 
Gradient-
79.7 
80.21 
69.85 
70.95 
based 
(+1.41) 
(+0.87) (+2.57) 
(+2.1) 
Noise-free syntactic constraints 

A* 
78.87 
79.51 
67.97 
68.97 
(+0.31) 
(+0.17) (+0.69) (+0.11) 
Gradient-
80.18 
80.66 
69.97 
70.94 
based 
(+1.62) 
(+1.32) (+2.69) (+2.08) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of different decoding techniques: 
Viterbi, A* (He et al., 2017) and gradient based infer-
ence (Lee et al., 2017) with noisy and noise-free syn-
tactic constraints 1 . Note that the (+/-) F1 are reported 
w.r.t Viterbi decoding on the same column. 

</table></figure>

			<note place="foot" n="1"> We preprocessed data for inference by simply deleting syntactic parse trees for the sentences where we have disagreement and perform standard Viterbi decoding for those sentences and note that this preprocessing scheme was never used during training. 2 https://github.com/allenai/allennlp</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Textual inference and meaning representation in human robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bastianelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</title>
		<meeting>the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards open-domain semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Giannone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Annesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="237" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic role labeling using the latent words language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Deschacht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;09</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semisupervised semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="220" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="473" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From treebank to propbank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Language Resources and Evaluation (LREC&apos;02). European Language Resources Association (ELRA)</title>
		<meeting>the Third International Conference on Language Resources and Evaluation (LREC&apos;02). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized inference with multiple semantic role labeling systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Koomen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jay Yoon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Sanket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Tristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carbonell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08608v2</idno>
		<title level="m">Gradient-based inference for networks with output constraints</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic role chunking combining complementary syntactic views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kadri</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2007 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
