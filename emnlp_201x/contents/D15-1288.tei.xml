<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compact, Efficient and Unlimited Capacity: Language Modeling with Compressed Suffix Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Petri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution" key="instit1">Monash University Computing and Information Systems</orgName>
								<orgName type="institution" key="instit2">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compact, Efficient and Unlimited Capacity: Language Modeling with Compressed Suffix Trees</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Efficient methods for storing and querying language models are critical for scaling to large corpora and high Markov orders. In this paper we propose methods for mod-eling extremely large corpora without imposing a Markov condition. At its core, our approach uses a succinct index-a compressed suffix tree-which provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through ∞-order modeling over the full Wikipedia collection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models (LMs) are critical components in many modern NLP systems, including machine translation <ref type="bibr" target="#b13">(Koehn, 2010)</ref> and automatic speech recognition <ref type="bibr" target="#b19">(Rabiner and Juang, 1993)</ref>. The most widely used LMs are mgram models <ref type="bibr" target="#b3">(Chen and Goodman, 1996)</ref>, based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact.</p><p>Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams ( <ref type="bibr" target="#b0">Brants et al., 2007)</ref>, raising challenges of efficient stor- age and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs ( <ref type="bibr" target="#b2">Chazelle et al., 2004;</ref><ref type="bibr" target="#b26">Talbot and Osborne, 2007;</ref><ref type="bibr" target="#b8">Guthrie and Hepple, 2010)</ref>, or loss-less LMs backed by tries <ref type="bibr" target="#b24">(Stolcke et al., 2011</ref>), or re- lated compressed structures ( <ref type="bibr" target="#b6">Germann et al., 2009;</ref><ref type="bibr" target="#b9">Heafield, 2011;</ref><ref type="bibr" target="#b18">Pauls and Klein, 2011;</ref><ref type="bibr" target="#b23">Sorensen and Allauzen, 2011;</ref><ref type="bibr" target="#b28">Watanabe et al., 2009)</ref>. How- ever, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An impor- tant exception is <ref type="bibr" target="#b11">Kennington et al. (2012)</ref>, who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus).</p><p>In contrast, we 1 make use of recent advances in compressed suffix trees (CSTs) <ref type="bibr" target="#b21">(Sadakane, 2007)</ref> to build compact indices with much more mod- est memory requirements (≈ the size of the cor- pus). We present methods for extracting frequency and unique context count statistics for mgram queries from CSTs, and two algorithms for com- puting Kneser-Ney LM probabilities on the fly us- ing these statistics. The first method uses two CSTs (over the corpus and the reversed corpus), which allow for efficient computation of the num- ber of unique contexts to the left and right of an mgram, but is inefficient in several ways, most notably when computing the number of unique contexts to both sides. Our second method ad- dresses this problem using a single CST backed by a wavelet tree based FM-index <ref type="bibr" target="#b4">(Ferragina et al., 2007)</ref>, which results in better time complexity and considerably faster runtime performance.</p><p>Our experiments show that our method is prac- tical for large-scale language modelling, although querying is substantially slower than a SRILM benchmark. However our technique scales much more gracefully with Markov order m, allowing unbounded 'non-Markov' application, and enables training on large corpora as we demonstrate on the complete Wikipedia dump. Overall this paper il- lustrates the vast potential succinct indexes have for language modelling and other 'big data' prob- lems in language processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Suffix Arrays and Suffix Trees Let T be a string of size n drawn from an alphabet Σ of size σ. Let T [i..n − 1] be a suffix of T . The suffix tree <ref type="bibr" target="#b29">(Weiner, 1973)</ref> of T is the compact labeled tree of n + 1 leaves where the root to leaf paths correspond to all suffixes of T $, where $ is a ter- minating symbol not in Σ. The path-label of each node v corresponds to the concatenation of edge labels from the root node to v. The node depth of v corresponds to the number of ancestors in the tree, whereas the string depth corresponds to the length of the path-label. Searching for a pattern α of size m in T translates to finding the locus node v closest to the root such that α is a prefix of the path-label of v in O(m) time. We refer to this ap- proach as forward search. <ref type="figure">Figure 1a</ref> shows a suffix tree over a sample text. A suffix tree requires O(n) space and can be constructed in O(n) time <ref type="bibr" target="#b27">(Ukkonen, 1995)</ref>. The children of each node in the suffix tree are lexicographically ordered by their edge la- bels. The i-th smallest suffix in T corresponds to the path-label of the i-th leaf. The starting position of the suffix can be associated its corresponding leaf in the tree as shown in <ref type="figure">Figure 1a</ref>. All occur- rences of α in T can be retrieved by visiting all leaves in the subtree of the locus of α. For exam- ple, pattern "the night" occurs at positions 12 and 19 in the sample text. We further refer the number of children of a node v as its degree and the num- ber of leaves in the subtree rooted at v as the size of v.</p><p>The suffix array <ref type="bibr" target="#b15">(Manber and Myers, 1993)</ref> of T is an array SA[0 . . . n − 1] such that SA <ref type="bibr">[i]</ref> corre- sponds to the starting position of the i-th smallest suffix in T or the i-th leaf in the suffix tree of T . The suffix array requires n log n bits of space and can also be constructed in O(n) time <ref type="bibr">(Kärkkäinen et al., 2006</ref>). Using only the suffix array and the text, pattern search can be performed using bi- nary search in O(m log n) time. For example, the pattern "the night" is found by performing binary search using SA and T to determine SA <ref type="bibr">[18,</ref><ref type="bibr">19]</ref>, the interval in SA corresponding the the suffixes in T prefixed by the pattern. In practice, suffix arrays use 4 − 8n bytes of space whereas the most ef- ficient suffix tree implementations require at least 20n bytes of space <ref type="bibr" target="#b14">(Kurtz, 1999</ref>) which are both much larger than T and prohibit the use of these structures for all but small data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compressed Suffix Structures</head><p>Reducing the space usage of suffix based index structure has recently become an active area of research. The space usage of a suffix array can be reduced sig- nificantly by utilizing the compressibility of text combined with succinct data structures. A suc- cinct data structure provides the same function- ality as an equivalent uncompressed data struc- ture, but requires only space equivalent to the information-theoretic lower bound of the underly- ing data. For simplicity, we focus on the FM-Index which emulates the functionality of a suffix array over T using nH k (T ) + o(n log σ) bits of space where H k refers to the k-th order entropy of the text <ref type="bibr" target="#b4">(Ferragina et al., 2007)</ref>. In practice, the FM- Index of T uses roughly space equivalent to the compressed representation of T using a standard compressor such as bzip2. For a more compre- hensive overview on succinct text indexes, see the excellent survey of <ref type="bibr" target="#b5">Ferragina et al. (2008)</ref>.</p><p>The FM-Index relies on the duality between the suffix array and the BWT ( <ref type="bibr" target="#b1">Burrows and Wheeler, 1994)</ref>, a permutation of the text such that <ref type="figure">Figure 1</ref> , we additionally store the starting positions C s of all suffixes for each symbol s in Σ at a negligi- ble cost of σ log n bits. Thus, the new interval is computed as l i−1 = C c +RANK(T bwt , l i , c) and</p><formula xml:id="formula_0">T bwt [i] = T [SA[i] − 1] (see</formula><formula xml:id="formula_1">r i−1 = C c +RANK(T bwt , r i + 1, c).</formula><p>The time and space complexity of the FM- index thus depends on the cost of storing and pre-  </p><formula xml:id="formula_2"># n T $ p p t t n n r r t t o t s s i # # i t T [SA[i]] SA T bwt</formula><p>Figure 1: Data structures for the sample text T ="#the old night keeper keeps the keep in the town# the night keeper keeps the keep in the night#$" with alphabet Σ={the, old, night, keeper, keeps, keep, in, town, #} and code words $=0000, #=0001, i=in=001, p=keep=010, r=keeper=011, s=keeps=1000, o=old=101, t=the=110, n=night=1001 and T=town=111.</p><p>processing T bwt to answer RANK efficiently. A wavelet tree can be used to answer RANK over T bwt in O(log σ) time. The wavelet tree re- duces RANK over an alphabet Σ into multiple RANK operations over a binary alphabet which can be answered in O(1) time and o(n) bits extra space by periodically storing absolute and relative RANK counts <ref type="bibr" target="#b16">(Munro, 1996)</ref>. The al- phabet is reduced by recursively splitting sym- bols based on their code words into subgroups to form a binary tree as shown in <ref type="figure">Figure 1b</ref> for T bwt . To answer RANK(T bwt , i, c), the tree is traversed based on the code word of c, per- forming binary RANK at each level. For exam- ple, RANK(T bwt , 17, 't') translates to performing RANK(W T root , 17, 1) = 12 on the top level of the wavelet tree, as t=the=110. We recurse to the right subtree of the root node and com- pute RANK(W T 1 , 12, 1) as there were 12 ones in the root node and the next bit in the code- word of 'the' is also one. This process contin- ues until the correct leaf node is reached to answer RANK(T bwt , 17, 't') = 5 in O(log σ) time. The space usage of a regular wavelet tree is n log σ + o(n log σ) bits which roughly matches the size of the text. <ref type="bibr">2</ref> If locations of matches are required, ad-bits. Using little extra space and advanced bit- operations, the BP-sequence can be used to per- form operations such as string-depth(v), parent(v) or accessing the i-th leaf can be answered in con- stant time. To support more advanced operations such as accessing path-labels, the underlying CSA or a compressed version of the LCP array are re- quired which can be more expensive. <ref type="bibr">3</ref> In practice, a CST requires roughly 4 − 6n bits in addition to the cost of storing the CSA. For a more extensive overview of CSTs see <ref type="bibr" target="#b20">Russo et al. (2011)</ref>.</p><p>Kneser Ney Language Modelling Recall our problem of efficient mgram language modeling backed by a corpus encoded in a succinct index. Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM ( <ref type="bibr" target="#b12">Kneser and Ney, 1995)</ref>, specifically the inter- polated variant described in <ref type="bibr" target="#b3">Chen and Goodman (1996)</ref>, which has been shown to outperform other ngram LMs and has become the de-facto standard. Interpolated Kneser-Ney describes the condi- tional probability of a word w i conditioned on the context of m − 1 preceding words, w i−1 i−m+1 , as</p><formula xml:id="formula_3">P (w i |w i−1 i−m+1 ) = max c(w i i−m+1 ) − D m , 0 c(w i−1 i−m+1 ) + D m N 1+ (w i−1 i−m−1 · ) c(w i−1 i−m+1 ) ¯ P (w i |w i−1 i−m+2 ),<label>(1)</label></formula><p>where lower-order smoothed probabilities are de- fined recursively (for 1 &lt; k &lt; m) as</p><formula xml:id="formula_4">¯ P (w i |w i−1 i−k+1 ) = max N 1+ ( · w i i−k+1 ) − D k , 0 N 1+ ( · w i−1 i−k+1 · ) + D k N 1+ (w i−1 i−k+1 · ) N 1+ ( · w i−1 i−k+1 · ) ¯ P (w i |w i−1 i−k+2 ) . (2)</formula><p>In the above formula, D k is the kgram-specific discount parameter, and the occurrence count </p><formula xml:id="formula_5">N 1+ (α · ) = |{w : c(αw) &gt; 0}| is</formula><formula xml:id="formula_6">w i ) = N 1+ ( · w i )/N 1+ ( ·· ). 4</formula><p>3 See Supplementary Materials <ref type="table">Table 1</ref> for an overview of the complexities of the functionality of the CST that is used in our experiments. <ref type="bibr">4</ref> Modified Kneser-Ney, proposed by Chen and Good- man (1996), typically outperforms interpolated Kneser-Ney through its use of context-specific discount parameters. The</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Using CSTs for KN Computation</head><p>The key requirements for computing probability under a Kneser-Ney language model are two types of counts: raw frequencies of mgrams and occur- rence counts, quantifying how many different con- texts the mgram has occurred in. <ref type="figure" target="#fig_3">Figure 2</ref> (right) illustrates the requisite counts for calculating the probability of an example 4-gram. In electing to store the corpus directly in a suffix tree, we need to provide mechanisms for computing these counts based on queries into the suffix tree.</p><p>The raw frequency counts are the simplest to compute. First we identify the locus node v in the suffix tree for the query mgram; the frequency corresponds to the node's size, an O(1) operation which returns the number of leaves below v. To il- lustrate, consider searching for c(the night) in <ref type="figure">Fig- ure 1a</ref>, which matches a node with two leaves (la- belled 19 and 12), and thus c = 2.</p><p>More problematic are the occurrence counts, which come in several flavours: right contexts,</p><formula xml:id="formula_7">N 1+ (α · ), left contexts, N 1+ ( · α)</formula><p>, and contexts to both sides of the pattern, N 1+ ( · α · ). The first of these can be handled easily, as</p><formula xml:id="formula_8">N 1+ (α · ) = degree(v), if α = label(v) 1, otherwise</formula><p>where v is the node matching α, and label(v) de- notes the path-label of v. <ref type="bibr">5</ref> For example, keep in has two child nodes in <ref type="figure">Figure 1a</ref>, and thus there are two unique contexts in which it can oc- cur, N 1+ (keep in · ) = 2, while the keep par- tially matches an edge in the forward suffix tree in <ref type="figure">Figure 1a</ref> as it can only be followed by in, N 1+ (the keep · ) = 1. A similar line of reason- ing applies to computing N 1+ ( · α). Assuming we also have a second suffix tree representing the reversed corpus, we first identify the reversed pat- tern (e.g., in keep R ) and then use above method to compute the occurrence count (denoted hereafter N1P(t, v, α) 6 , where t is the CST.).</p><p>implementation of this with our data structures is straight- forward in principle, but brings a few added complexities in terms of dynamic computing other types of occurrence counts, which we leave for future work. <ref type="bibr">5</ref> See the Supplementary Materials for the explicit algo- rithm, but note there are some corner cases involving sen- tinels # and $, which must be excluded when computing oc- currence counts. Such tests have been omitted from the pre- sentation for clarity. <ref type="bibr">6</ref> In the presented algorithms, we overload the pattern ar- gument in function calls for readability, and use · to denote the query context. </p><formula xml:id="formula_9">meration, N 1+ ( · α · ) = s∈F (α) N 1+ ( · αs),</formula><p>where F (α) is the set of symbols that can follow α. Algorithm 1 shows how this is computed, with lines 7 and 8 enumerating s ∈ F (α) using the edge labels of the children of v. For each symbol, line 9 searches for an extended pattern incorporat- ing the new symbol s in the reverse CSA (part of the reverse CST), by refining the existing match v R using a single backward search operation af- ter which we can compute N 1+ ( · αs). 7 Line 5 deals with the special case where the pattern does not match a complete edge, in which case there is only only unique right context and therefore</p><formula xml:id="formula_10">N 1+ ( · α · ) = N 1+ ( · α).</formula><p>N1P and N1PFRONTBACK can compute the requisite occurence counts for mgram language modelling, however at considerable cost in terms of space and time. The need for twin reverse and forward CSTs incurs a significant storage over- head, as well as the search time to match the pat- tern in both CSTs. We show in Section 5 how we can avoid the need for the reversed suffix tree, giving rise to lower memory requirements and faster runtime. Beyond the need for twin suf- fix trees, the highest time complexity calls are string-depth, edge and backward-search. Calling string-depth is constant time for internal nodes, but O(SAS log σ) for leaf nodes; fortunately we <ref type="bibr">7</ref> Backward search in the reverse tree corresponds to searching for the reversed pattern appended with one symbol.</p><formula xml:id="formula_11">Algorithm 1 Two-sided occ., N 1+ ( · α · ) Precondition: v F in forward CST t F matches α Precondition: v R in reverse CST t R matches α 1: function N1PFRONTBACK(t F , v F , t R , v R , α) 2: o ← 0 3: d ← string-depth(v F ) 4: if d &gt; |α| then 5: o ← N1P(t R , v R , · α) 6: else 7: for u F ← children(v F ) do 8:</formula><p>s ← edge(u F , d + 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>u R ← back-search(v R , s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>o ← o + N1P(t R , u R , · αs)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>return o can avoid this call for leaves, which by definition extend to the end of the corpus and consequently extend further than our pattern. <ref type="bibr">8</ref> The costly calls to edge and backward-search however cannot be avoided. This leads to an overall time complex- ity of O(1) for N1P and O(F (α) × SAS × log σ) for N1PFRONTBACK, where F (α) is the number of following symbols and SAS is the suffix array value sample rate described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dual CST Algorithm</head><p>The methods above for computing the frequency and occurrence counts provide the ingredients necessary for computing mgram language model probabilities. This leaves the algorithmic problem</p><formula xml:id="formula_12">Algorithm 2 KN probability P w k |w k−1 k−(m−1)</formula><p>1: function PROBKNESERNEY(tF, tR, w, m) 2: vF ← root(tF) match for suffix of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5: p ← 1 6:</head><p>for i ← 1 to m do 7:</p><p>v all R ← forw-search(v all R , w k−i+1 ) 8:</p><p>if i &gt; 1 then 9:</p><p>vF ← back-search(vF, w k−i+1 ) 10: if i &lt; m then 11:</p><p>vR ← forw-search(vR, w k−i+1 ) 12:</p><p>Di ← lookup discount for igram 13:</p><formula xml:id="formula_13">if i = m then 14: c ← size(v all R ) 15: d ← size(vF) 16: else 17: c ← N1P(tR, v all R , · w k k−i+1 ) 18: d ← N1PFRONTBACK(tF, vF, tR, vR, · w k−1 k−i+1 · ) 19: if i &gt; 1 then 20:</formula><p>if vF is valid then 21:</p><formula xml:id="formula_14">q ← N1P(tF, vF, w k−1 k−i+1 · ) 22: p ← 1 d (max(c − Di, 0) + Diqp) 23:</formula><p>else if i = 1 then 24:</p><formula xml:id="formula_15">p ← c/N 1+ ( ·· ) 25:</formula><p>return p of efficiently ordering the search operations in for- ward and reverse CST structures. This paper considers an interpolated LM for- mulation, in which probabilities from higher or- der contexts are interpolated with lower order es- timates. This iterative process is apparent in <ref type="figure" target="#fig_3">Fig- ure 2 (right)</ref> which shows the quantities required for probability scoring for an example mgram. Equivalently, the iteration can be considered in re- verse, starting from unigram estimates and suc- cessively growing to large mgrams, in each stage adding a single new symbol to left of the pattern. This suits incremental search in a CST in which search bounds are iteratively refined, which has a substantially lower time complexity compared to searching over the full index in each step.</p><p>Algorithm 2 presents an outline of the approach. This uses a forward CST, t F , and a reverse CST, t R , with three CST nodes (lines 2-4) tracking the match progress for the full igram (v all R ) and the (i − 1)gram context (v F , v R ), i = 1 . . . m. The need to maintain three concurrent searches arises from the calls to size, N 1+ ( · α), N 1+ (α · ) and N 1+ ( · α · ) (lines 14, 15; 17; 21; and 18, respec- tively). These calls impose conditions on the di- rection of the suffix tree, e.g., such that the edge labels and node degree can be used to compute</p><formula xml:id="formula_16">Algorithm 3 Precompute KN discounts 1: function PRECOMPUTEDISCOUNTS(tR, m) 2: c k,f ← 0 ∀k ∈ [1, m], f ∈ [1, 2] 3: N 1 k,g ← 0 ∀k ∈ [1, m], g ∈ [1, 2] 4: N 1+ ( ·· ) ← 0 5:</formula><p>for vR ← descendents(root(tR)) do depth-first 6:</p><p>dP ← string-depth(parent(vR)) 7:</p><p>d ← string-depth(vR) 8:</p><p>for k ← dP + 1 to min (d, dP + m) do 9:</p><p>s ← edge(vR, k) 10:</p><p>if s is the end of sentence sentinel then 11:</p><p>skip all children of vR 12:</p><p>else 13:</p><p>if k = 2 then 14:</p><formula xml:id="formula_17">N 1+ ( ·· ) ← N 1+ ( ·· ) + 1 15: f ← size(vR) 16: if 1 ≤ f ≤ 2 then 17: c k,f ← c k,f + 1 18:</formula><p>if k &lt; d then 19:</p><p>g ← 1 20: else 21:</p><p>g ← degree(vR) 22:</p><p>if 1 ≤ g ≤ 2 then 23:</p><formula xml:id="formula_18">N 1 k,g ← N 1 k,g + 1 24: return c, N 1 , N 1+ ( ·· )</formula><p>the number of left or right contexts in which a pattern appears. The matching process is illus- trated in <ref type="figure" target="#fig_3">Figure 2</ref>  . Because of the mix of forward and reverse CSTs, coupled with search patterns that are revealed right-to-left, incremen- tal search in each of the CSTs needs to be han- dled differently (lines 7-11). In the forward CST, we perform backward search to extend the search pattern to the left, which can be computed very ef- ficiently from the BWT in the CSA. <ref type="bibr">9</ref> Conversely in the reverse CST, we must use forward search as we are effectively extending the reversed pattern to the right; this operation is considerably more costly.</p><p>The discounts D on line 12 of Algorithm 2 and N 1+ ( ·· ) (a special case of line 18) are precom- puted directly from the CSTs thus avoiding several costly computations at runtime. The precomputa-tion algorithm is provided in Algorithm 3 which operates by traversing the nodes of the reverse CST and at each stage computing the number of mgrams that occur 1-2 times (used for computing</p><formula xml:id="formula_19">D m in eq. 1), or with N 1+ ( · α) ∈ [1 − 2] (used for computing D k in eq. 2)</formula><p>, for various lengths of mgrams. These quantities are used to compute the discount parameters, which are then stored for later use in inference. <ref type="bibr">10</ref> Note that the PRECOM- PUTEDISCOUNTS algorithm can be slow, although it is significantly faster if we remove the edge calls and simply include in our counts all mgrams fin- ishing a sentence or spanning more than one sen- tence. This has a negligible (often beneficial) ef- fect on perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Improved Single CST Approach</head><p>The above dual CST algorithm provides an el- egant means of computing LM probabilities of arbitrary order and with a limited space com- plexity (O(n), or roughly n in practice). How- ever the time complexity is problematic, stem- ming from the expensive method for computing N1PFRONTBACK and repeated searches over the CST, particularly forward-search. Now we out- line a method for speeding up the algorithm by doing away with the reverse CST. Instead the crit- ical counts, N 1+ ( · α) and N 1+ ( · α · ) are com- puted directly from a single forward CST. This confers the benefit of using only backward search and avoiding redundant searches for the same pat- tern (cf. lines 9 and 11 in Algorithm 2). The full algorithm for computing LM prob- abilities is given in Algorithm 4, however for space reasons we will not describe this in de- tail. Instead we will focus on the method's most critical component, the algorithm for computing N 1+ ( · α · ) from the forward CST, presented in Algorithm 5. The key difference from Algorithm 1 is the loop from lines 6-9, which uses the interval- symbols ( <ref type="bibr" target="#b22">Schnattinger et al., 2010)</ref> method. This method assumes a wavelet tree representation of the SA component of the CST, an efficient encod- ing of the BWT as describes in section 2. The interval-symbols method uses RANK operations to efficiently identify for a given pattern the set of preceding symbols P (α) and the ranges SA[l s , r s ] corresponding to the patterns sα for all s ∈ P (α) <ref type="bibr">10</ref> Discounts are computed up to a limit on mgram size, here set to 10. The highest order values are used for comput- ing the discount of mgrams above the limit at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 KN probability</head><formula xml:id="formula_20">P w k |w k−1 k−(m−1) using a single CST 1: function PROBKNESERNEY1(tF, w, m) 2: vF ← root(tF) match for context w k−1 k−i 3: v all F ← root(tF) match for w k k−i 4: p ← 1 5: for i ← 1 to m do 6: v all F ← back-search([lb(v all F ), rb(v all F )], w k−i+1 ) 7: if i &gt; 1 then 8: vF ← back-search([lb(vF), rb(vF)], w k−i+1 ) 9:</formula><p>Di ← discount parameter for igram 10: if i = m then 11:</p><p>c ← size(v all F ) 12:</p><p>d ← size(vF) 13:</p><formula xml:id="formula_21">else 14: c ← N1PBACK1(tF, v all F , · w k−1 k−i+1 ) 15: d ← N1PFRONTBACK1(tF, vF, · w k−1 k−i+1 · ) 16: if i &gt; 1 then 17:</formula><p>if vF is valid then 18:</p><p>q ← N1P(tF, vF, w k−1</p><formula xml:id="formula_22">k−i+1 · ) 19: p ← 1 d (max(c − Di, 0) + Diqp) 20:</formula><p>else 21:</p><formula xml:id="formula_23">p ← c/N 1+ ( ·· ) 22: return p Algorithm 5 N 1+ ( · α · ), using forward CST Precondition: vF in forward CST tF matches α 1: function N1PFRONTBACK1(tF, vF, α) 2: o ← 0 3:</formula><p>if string-depth(vF) &gt; |α| then 4: From T bwt we can see that this is pre- ceeded by s ="old" (1 st occurrence in T bwt ) and s ="the" (3 rd and 4 th ); from which we can com- pute the suffix tree nodes, namely <ref type="bibr">[15,</ref><ref type="bibr">15]</ref> and [16 + (3 − 1), 16 + (4 − 1)] = <ref type="bibr">[18,</ref><ref type="bibr">19]</ref> for "old" and "the" respectively. 11 N1PBACK1 is computed in a similar way, us- ing the interval-symbols method to compute the number of unique preceeding symbols (see Sup- plementary Materials, Algorithm 7). Overall the time complexity of inference for both N1PBACK1 and N1PFRONTBACK1 is O(P (α) log σ) where P (α) is the number of preceeding symbols of α, a considerable improvement over N1PFRONTBACK using the forward and reverse CSTs. Overall this leads to considerably faster computation of mgram probabilities compared to the two CST ap- proach, and although still slower than highly opti- mised LM toolkits like SRILM, it is fast enough to support large scale experiments, and has consider- ably better scaling performance with the Markov order m (even allowing unlimited order), as we will now demonstrate.</p><formula xml:id="formula_24">o ← N1PBACK1(tF, vF, · α)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We used Europarl dataset and the data was num- berized after tokenizing, splitting, and excluding XML markup. The first 10k sentences were used as the test data, and the last 80% as the train- ing data, giving rise to training corpora of be- tween 8M and 50M tokens and uncompressed size of up to 200 MiB (see <ref type="table">Table 1</ref> for detailed cor- pus statistics). We also processed the full 52 GiB uncompressed "20150205" English Wikipedia ar- ticles dump to create a character level language model consisting of 72M sentences. We excluded 10k random sentences from the collection as test data. We use the SDSL library ( <ref type="bibr" target="#b7">Gog et al., 2014</ref>) to implement all our structures and compare our in- dexes to SRILM <ref type="bibr" target="#b25">(Stolcke, 2002</ref>). We refer to our dual-CST approach as D-CST, and the single-CST as S-CST. We evaluated the perplexity across different lan- guages and using mgrams of varying order from m = 2 to ∞ (unbounded), as shown on <ref type="figure" target="#fig_7">Figure 3</ref>  SRILM (for smaller values of m in which SRILM training was feasible, m ≤ 10). Note that perplex- ity drops dramatically from m = 2 . . . 5 however the gains thereafter are modest for most languages. Despite this, several large mgram matches were found ranging in size up to a 34-gram match. We speculate that the perplexity plateau is due to the simplistic Kneser-Ney discounting formula which is not designed for higher order mgram LMs and appear to discount large mgrams too aggressively. We leave further exploration of richer discounting techniques such as Modified Kneser-Ney <ref type="bibr" target="#b3">(Chen and Goodman, 1996)</ref> or the Sequence Memoizer <ref type="bibr" target="#b30">(Wood et al., 2011</ref>) to our future work. <ref type="figure">Figure 4</ref> compares space and time of our in- dexes with SRILM on the German part of Eu- roparl. The construction cost of our indexes in terms of both space and time is comparable to that of a 3/4-gram SRILM index. The space us- age of D-CST index is comparable to a compact 3-gram SRILM index. Our S-CST index uses only 177 MiB RAM at query time, which is compara- ble to the size of the collection (172 MiB). How- ever, query processing is significantly slower for both our structures. For 2-grams, D-CST is 3 times slower than a 2-gram SRILM index as the expen- sive N 1+ ( · α · ) is not computed. However, for large mgrams, our indexes are much slower than SRILM. For m &gt; 2, the D-CST index is roughly six times slower than S-CST. Our fastest index, is 10 times slower than the slowest SRILM 10-gram index. However, our run-time is independent of m. Thus, as m increases, our index will become more competitive to SRILM while using a constant amount of space. Next we analyze the performance of our in- dex on the large Wikipedia dataset. The S-CST, character level index for the data set requires 22 GiB RAM at query time whereas the D-CST re- quires 43 GiB. <ref type="figure" target="#fig_8">Figure 5</ref> shows the run-time per- formance of both indexes for different mgrams, broken down by the different components of the computation. As discussed above, 2-gram per- formance is much faster. For both indexes, most time is spent computing N1PFRONTBACK (i.e., N 1+ ( · α · )) for all m &gt; 2. However, the wavelet tree traversal used in S-CST roughly reduces the running time by a factor of three. The complex- ity of N1PFRONTBACK depends on the number of contexts, which is likely small for larger mgrams, but can be large for small mgrams, which sug- gest partial precomputation could significantly in- crease the query performance of our indexes. Ex- ploring the myraid of different CST and CSA con- figurations available could also lead to significant improvements in runtime and space usage also re- mains future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper has demonstrated the massive poten- tial that succinct indexes have for language mod- elling, by developing efficient algorithms for on- the-fly computing of mgram counts and language model probabilities. Although we only consid- ered a Kneser-Ney LM, our approach is portable to the many other LM smoothing method formulated around similar count statistics. Our complexity analysis and experimental results show favourable scaling properties with corpus size and Markov or- der, albeit running between 1-2 orders of magni- tude slower than a leading count-based LM. Our ongoing work seeks to close this gap: preliminary experiments suggest that with careful tuning of the succinct index parameters and caching expensive computations, query time can be competitive with state-of-the-art toolkits, while using less memory and allowing the use of unlimited context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>). Search- ing for a pattern using the FM-Index is performed in reverse order by performing RANK(T bwt , i, c) operations O(m) times. Here, RANK(T bwt , i, c) counts the number of times symbol c occurs in T bwt [0 . . . i − 1]. This process is usually referred to as backward search. Let SA[l i , r i ] be the in- terval corresponding to the suffixes in T match- ing α[i . . . m − 1]. By definition of the BWT, T bwt [l i , r i ] corresponds to the symbols in T pre- ceding α[i . . . m − 1] in T . Due to the lexico- graphical ordering of all suffixes in SA, the interval SA[l i−1 , r i−1 ] corresponding to all occurrences of α[i − 1 . . . m − 1] can be determined by comput- ing the rank of all occurrences of c = α[i − 1] in T bwt [l i , r i ]. Thus, we compute RANK(T bwt , l i , c), the number of times symbol c occurs before l i and RANK(T bwt , r i + 1, c), the number of occurrences of c in T bwt [0, r i ]. To determine SA[l i−1 , r i−1 ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the number of observed word types following the pattern α; the occurrence counts N 1+ ( · α) and N 1+ ( · α · ) are defined accordingly. The recursion stops at uni- gram level where the unigram probabilities are de- fined as ¯ P (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>cFigure 2 :</head><label>2</label><figDesc>Figure 2: Counts required for computing P (town|keep in the) (right) and the suffix tree nodes required for computing each value (left). The two left-most columns correspond to v all R and v R and are updated using forward-search in the reverse CST, while the righter-most column correspond to v F and is updated using backward-search in the forward CST. See Algorithm 2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where the three search nodes are shown on the left, considered bottom to top, and their corresponding count operations are shown to the right. The N 1+ ( · α) calls require a match in the reverse CST (left-most column, v all R ), while the N 1+ (α · ) require a match in the forward CST (right-most column, v F , matching the (i − 1)gram context). The N 1+ ( · α · ) computation reuses the forward match while also requiring a match for the (i−1)gram context in the reversed CST, as tracked by the middle column (v R )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>o</head><label></label><figDesc>← o + N1P(tF, node(l , r ), sα · ) 10: return o by visiting all leaves of the wavelet tree of sym- bols occurring in T bwt [l, r] (corresponding to α) in O(|P (α)| log σ) time (lines 6-8). These ranges SA[l , r ] can be used to find the corresponding suf- fix tree node for each sα in O(1) time. To illus- trate, consider the pattern α = "night" in Fig- ure 1a.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Perplexity results on several Europarl languages for different mgram sizes, m = 2. .. 10, 15, 20, ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Time versus space tradeoffs measured on Europarl German (de) dataset, showing memory and time requirements.</figDesc></figure>

			<note place="foot" n="1"> For the implementation see: https://github.com/eehsan/ lm-sdsl.</note>

			<note place="foot" n="2"> However, if code-words for each symbol are chosen based on their Huffman-codes the size of the wavelet tree ditional space is needed to access SA[i] or the inverse suffix array SA −1 [SA[i]] = i. In the simplest scheme, both values are periodically sampled using a given sample rate SAS (e.g. 32) such that SA[i] mod SAS = 0. Then, for any SA[i] or SA −1 [i], at most O(SAS) RANK operations on T bwt are required to access the value. Different sample rates, bitvector implementations and wavelet tree types result in a wide variety of timespace tradeoffs which can be explored in practice (Gog et al., 2014). In the same way the FM-index emulates the functionality of the suffix array in little space, compressed suffix trees (CST) provide the functionality of suffix trees while requiring significantly less space than their uncompressed counterparts (Sadakane, 2007). A CST uses a compressed suffix array (CSA) such as the FM-Index but stores additional information to represent the shape of the suffix tree as well as information about path-labels. Again a variety of different storage schemes exist, however for simplicity we focus on the CST of Ohlebusch et al. (2010) which we use in our experiments. Here, the shape of the tree is stored using a balanced-parenthesis (BP) sequence which for a tree of p nodes requires ≈ 2p reduces to nH0(T )(1 + o(1)) bits which can be further be reduced to to nH k (T ) + o(n log σ) bits by using entropy compressed bitvectors.</note>

			<note place="foot" n="8"> We assume search patterns do not extend beyond a single sentence, and thus will always be shorter than the edge labels.</note>

			<note place="foot" n="9"> See Supplementary Materials Table 1 for the time complexities of this and other CSA and CST methods.</note>

			<note place="foot" n="11"> Using the offsets into the SA for each symbol, Cold = 15 and Cthe = 16, while −1 adjusts for counting from 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Ehsan Shareghi and Gholamreza Haffari are grate-ful to National ICT Australia (NICTA) for gen-erous funding, as part of collaborative machine learning research projects. Matthias Petri is the recipient of an Australian Research Councils Dis-covery Project scheme (project DP140103256). Trevor Cohn is the recipient of an Australian Re-search Council Future Fellowship (project number FT130101105).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A block sorting lossless data compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wheeler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report 124</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The bloomier filter: An efficient data structure for static support lookup tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronitt</forename><surname>Rubinfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SODA</title>
		<meeting>SODA</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compressed representations of sequences and full-text indexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mäkinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Algorithms</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compressed text indexes: From theory to practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rossano</forename><surname>Venturini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM J. of Exp. Algorithmics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tightly packed tries: How to fit large models into memory, and make them load fast, too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Joanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing</title>
		<meeting>of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From theory to practice: Plug and play with succinct data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Beller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SEA</title>
		<meeting>SEA</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="326" to="337" />
		</imprint>
	</monogr>
	<note>Alistair Moffat, and Matthias Petri</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Storing the web in memory: Space efficient language models with constant time retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hepple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">KenLM: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Linear work suffix array construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Kärkkäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="918" to="936" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Suffix trees as language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Redd Kennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annemarie</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="446" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reducing the space requirement of suffix trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kurtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Softw., Pract. Exper</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1149" to="1171" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Suffix arrays: A new method for on-line string searches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udi</forename><surname>Manber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">W</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="935" to="948" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Munro</surname></persName>
		</author>
		<title level="m">Proc. FSTTCS</title>
		<meeting>FSTTCS</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enno</forename><surname>Ohlebusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Gog</surname></persName>
		</author>
		<title level="m">Proc. SPIRE</title>
		<meeting>SPIRE</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="322" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster and smaller n-gram language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fundamentals of speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fullycompressed suffix trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Algorithms</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compressed suffix trees with full functionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Sadakane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="607" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional search in a string with wavelet trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schnattinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enno</forename><surname>Ohlebusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Gog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CPM</title>
		<meeting>CPM</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="40" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unary data structures for language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1425" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Srilm at sixteen: Update and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Abrash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Randomised language modelling for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On-line construction of suffix trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esko Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A succinct n-gram language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Short Papers</title>
		<meeting>ACL Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="341" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linear pattern matching algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SWAT</title>
		<meeting>SWAT</meeting>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The sequence memoizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lancelot</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="98" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
