<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Sentiment Classification with User and Product Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Imaging Technology</orgName>
								<orgName type="institution">Capital Normal University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunchao</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">National Lab for Information Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab on Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Sentiment Classification with User and Product Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1650" to="1659"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document-level sentiment classification aims to predict user&apos;s overall sentiment in a document about a product. However, most of existing methods only focus on local text information and ignore the global user preference and product characteristics. Even though some works take such information into account, they usually suffer from high model complexity and only consider word-level preference rather than semantic levels. To address this issue, we propose a hierarchical neural network to incorporate global user and product information into sentiment classification. Our model first builds a hierarchical LSTM model to generate sentence and document representations. Afterwards, user and product information is considered via attentions over different semantic levels due to its ability of capturing crucial semantic components. The experimental results show that our model achieves significant and consistent improvements compared to all state-of-the-art methods. The source code of this paper can be obtained from https://github. com/thunlp/NSC.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis aims to analyze people's sen- timents or opinions according to their generated texts and plays a critical role in the area of data mining and natural language processing. Recently, sentiment analysis draws increasing attention of re- searchers with the rapid growth of online review In this work, we focus on the task of document- level sentiment classification, which is a fundamen- tal problem of sentiment analysis. Document-level sentiment classification assumes that each docu- ment expresses a sentiment on a single product and targets to determine the overall sentiment about the product.</p><p>Most existing methods take sentiment classifica- tion as a special case of text classification problem. Such methods treat annotated sentiment polarities or ratings as categories and apply machine learning algorithms to train classifiers with text features, e.g., bag-of-words vectors ( <ref type="bibr" target="#b14">Pang et al., 2002</ref>). Since the performance of text classifiers heavily depends on the extracted features, such studies usually attend to design effective features from text or additional sen- timent lexicons <ref type="bibr" target="#b4">(Ding et al., 2008;</ref><ref type="bibr" target="#b18">Taboada et al., 2011</ref>).</p><p>Motivated by the successful utilization of deep neural networks in computer vision ( <ref type="bibr" target="#b2">Ciresan et al., 2012)</ref>, speech recognition ( <ref type="bibr" target="#b3">Dahl et al., 2012</ref>) and natural language processing ( <ref type="bibr" target="#b0">Bengio et al., 2006</ref>), some neural network based sentiment analysis mod- els are proposed to learn low-dimensional text fea- tures without any feature engineering <ref type="bibr" target="#b7">(Glorot et al., 2011;</ref><ref type="bibr" target="#b15">Socher et al., 2011;</ref><ref type="bibr" target="#b16">Socher et al., 2012;</ref><ref type="bibr" target="#b17">Socher et al., 2013;</ref><ref type="bibr" target="#b9">Kim, 2014</ref>). Most proposed neural network models take the text information in a sentence or a document as input and generate the semantic representations using well-designed neu- ral networks. However, these methods only focus on the text content and ignore the crucial character- istics of users and products. It is a common sense that the user's preference and product's characteris- tics make significant influence on the ratings.</p><p>To incorporate user and product information into sentiment classification, ( <ref type="bibr" target="#b22">Tang et al., 2015b</ref>) bring in a text preference matrix and a representation vec- tor for each user and product into CNN sentiment classifier. It modifies the word meaning in the in- put layer with the preference matrix and concate- nates the user/product representation vectors with generated document representation before softmax layer. The proposed model achieves some im- provements but suffers the following two problems: (1) The introduction of preference matrix for each user/product is insufficient and difficult to be well trained with limited reviews. For example, most users in IMDB and Yelp only have several tens of reviews, which is not enough to obtain a well-tuned preference matrix. <ref type="formula" target="#formula_2">(2)</ref> The characteristics of user and product should be reflected on the semantic level besides the word level. For example, a two star review in Yelp said "great place to grab a steak and I am a huge fan of the hawaiian pizza · · · but I don't like to have to spend 100 bucks for a diner and drinks for two". It's obvious that the poor rating result mainly relies on the last sentence compared with others.</p><p>To address these issues, we propose a novel hier- archical LSTM model to introduce user and prod- uct information into sentiment classification. As illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, our model mainly consists of two parts. Firstly, we build a hierarchical LSTM model to generate sentence-level representation and document-level representation jointly. Afterwards, we introduce user and product information as atten- tions over different semantic levels of a document.</p><p>With the consideration of user and product informa- tion, our model can significantly improve the per- formance of sentiment classification in several real- world datasets.</p><p>To summarize, our effort provide the following three contributions:</p><p>(1) We propose an effective Neural Sentiment Classification model by taking global user and prod- uct information into consideration. Comparing with ( <ref type="bibr" target="#b22">Tang et al., 2015b</ref>), our model contains much less parameters and is more efficient for training.</p><p>(2) We introduce user and product information based attentions over different semantic levels of a document. Traditional attention-based neural net- work models only take the local text information into consideration. In contrast, our model puts for- ward the idea of user-product attention by utilizing the global user preference and product characteris- tics.</p><p>(3) We conduct experiments on several real- world datasets to verify the effectiveness of our model. The experimental results demonstrate that our model significantly and consistently outper- forms other state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>With the trends of deep learning in computer vi- sion, speech recognition and natural language pro- cessing, neural models are introduced into senti- ment classification field due to its ability of text representation learning.  <ref type="bibr" target="#b9">(Kim, 2014)</ref> and <ref type="bibr" target="#b8">(Johnson and Zhang, 2014</ref>) adopt convolution neural network (CNN) to learn sentence representa- tions and achieve outstanding performance in senti- ment classification.</p><p>Recurrent neural network also benefits sentiment classification because it is capable of capturing the sequential information. ( <ref type="bibr" target="#b11">Li et al., 2015)</ref>, <ref type="bibr" target="#b19">(Tai et al., 2015</ref>) investigate tree-structured long-short term memory (LSTM) networks on text or sentiment classification. There are also some hierarchical models proposed to deal with document-level senti- ment classification ( <ref type="bibr" target="#b21">Tang et al., 2015a;</ref><ref type="bibr" target="#b1">Bhatia et al., 2015)</ref>, which generate different levels (e.g., phrase, sentence or document) of semantic representations within a document. Moreover, attention mecha- nism is also introduced into sentiment classification, which aims to select important words from a sen- tence or sentences from a document ( <ref type="bibr" target="#b23">Yang et al., 2016)</ref>.</p><p>Most existing sentiment classification models ig- nore the global user preference and product charac- teristics, which have crucial effects on the sentiment polarities. To address this issue, ( <ref type="bibr" target="#b22">Tang et al., 2015b</ref>) propose to add user/product preference matrices and representation vectors into CNN models. Neverthe- less, it suffers from high model complexity and only considers word-level preference rather than seman- tic levels. In contrast, we propose an efficient neural sentiment classification model with users and prod- ucts to serve as attentions in both word and semantic levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we will introduce our User Prod- uct Attention (UPA) based Neural Sentiment Clas- sification (NSC) model in detail. First, we give the formalizations of document-level sentiment classi- fication. Afterwards, we discuss how to obtain doc- ument semantic representation via the Hierarchical Long Short-term Memory (HLSTM) network . At last, we present our attention mechanisms which in- corporates the global information of users and prod- ucts to enhance document representations. The en- hanced document representation is used as features for sentiment classification. An overall illustration of UPA based NSC model is shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formalizations</head><p>Suppose a user u ∈ U has a review about a prod- uct p ∈ P . We represent the review as a document d with n sentences {S 1 , S 2 , · · · , S n }. Here, l i is the length of i-th sentence. The i-th sentence S i con- sists of l i words as</p><formula xml:id="formula_0">{w i 1 , w i 2 , · · · , w i l i</formula><p>}. Document- level sentiment classification aims to predict the sentiment distributions or ratings of these reviews according to their text information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Sentiment Classification Model</head><p>According to the principle of compositionality (Frege, 1892), we model the semantic of a docu- ment through a hierarchical structure composed of word-level, sentence-level and document-level. To model the semantic representations of sentences, we adopt Long Short-Term Memory (LSTM) network because of its excellent performance on sentiment classification, especially for long documents. Sim- ilarly, we also use LSTM to learn document repre- sentations.</p><p>In word level, we embed each word in a sentence into a low dimensional semantic space. That means, each word w i j is mapped to its embedding w i j ∈ R d . At each step, given an input word w i j , the current cell state c i j and hidden state h i j can be updated with the previous cell state c i j−1 and hidden state h i j−1 as</p><formula xml:id="formula_1">follows:   i i j f i j o i j   =   σ σ σ   (W · h i j−1 , w i j + b),<label>(1)</label></formula><formula xml:id="formula_2">ˆ c i j = tanh(W · h i j−1 , w i j + b),<label>(2)</label></formula><formula xml:id="formula_3">c i j = f i j c i j−1 + i i j ˆ c i j ,<label>(3)</label></formula><formula xml:id="formula_4">h i j = o i j tanh(c i j ),<label>(4)</label></formula><p>where i, f , o are gate activations, stands for element-wise multiplication, σ is sigmoid function, W, b are the parameters we need to train. We then feed hidden states</p><formula xml:id="formula_5">[h i 1 , h i 2 , · · · , h i l i</formula><p>] to an average pooling layer to obtain the sentence representation s i .</p><p>In sentence level, we also feed the sentence em- beddings [s 1 , s 2 , · · · , s n ] into LSTM and then ob- tain the document representation d through an aver- age pooling layer in a similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">User Product Attention</head><p>We bring in User Product Attention to capture the crucial components over different semantic levels for sentiment classification. Specifically, we em- ploy word-level UPA to generate sentence represen- tations and sentence-level UPA to obtain document representation. We give the detailed implementa- tions in the following parts.</p><p>It is obvious that not all words contribute equally to the sentence meaning for different users and products. Hence, in word level, instead of feed- ing hidden states to an average pooling layer, we adopt a user product attention mechanism to extract user/product specific words that are important to the meaning of sentence. Finally, we aggregate the representations of those informative words to form the sentence representation. Formally, the enhanced sentence representation is a weighted sum of hidden states as:</p><formula xml:id="formula_6">s i = l i j=1 α i j h i j ,<label>(5)</label></formula><p>where α i j measures the importance of the j-th word for current user and product. Here, we embed each user u and each product p as continuous and real- valued vectors u ∈ R du and p ∈ R dp , where d u and d p are the dimensions of user embeddings and product embeddings respectively. Thus, the atten- tion weight α i j for each hidden state can be defined as:</p><formula xml:id="formula_7">α i j = exp(e(h i j , u, p)) l i k=1 exp(e(h i k , u, p)) ,<label>(6)</label></formula><p>where e is a score function which scores the impor- tance of words for composing sentence representa- tion. The score function e is defined as:</p><formula xml:id="formula_8">e(h i j , u, p) = v T tanh(W H h ij + W U u + W P p + b),<label>(7)</label></formula><p>where W H , W U and W P are weight matrices, v is weight vector and v T denotes its transpose.</p><p>The sentences that are clues to the meaning of the document vary in different users and products. Therefore, in sentence level, we also use a attention mechanism with user vector u and product vector p in word level to select informative sentences to compose the document representation. The docu- ment representation d is obtained via:</p><formula xml:id="formula_9">d = n i=1 β i h i ,<label>(8)</label></formula><p>where β i is the weight of hidden state h i in sentence level which can be calculated similar to the word attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentiment Classification</head><p>Since document representation d is hierarchically extracted from the words and sentences in the doc- uments, it is a high level representation of the docu- ment. Hence, we regard it as features for document sentiment classification. We use a non-linear layer to project document representation d into the target space of C classes:</p><formula xml:id="formula_10">ˆ d = tanh(W c d + b c ).<label>(9)</label></formula><p>Afterwards, we use a softmax layer to obtain the document sentiment distribution:</p><formula xml:id="formula_11">p c = exp( ˆ d c ) C k=1 exp( ˆ d k ) ,<label>(10)</label></formula><p>where C is the number of sentiment classes, p c is the predicted probability of sentiment class c. In our model, cross-entropy error between gold senti- ment distribution and our model's sentiment distri- bution is defined as loss function for optimization when training:</p><note type="other">Datasets #classes #docs #users #products #docs/user #docs/product #sens/doc #words/sen IMDB 10 84,919 1,310 1,</note><formula xml:id="formula_12">L = − d∈D C c=1 p g c (d) · log(p c (d)),<label>(11)</label></formula><p>where p g c is the gold probability of sentiment class c with ground truth being 1 and others being 0, D represents the training documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we introduce the experimental set- tings and empirical results on the task of document- level sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We evaluate the effectiveness of our NSC model on three sentiment classification datasets with user and product information: IMDB, Yelp 2013 and Yelp 2014, which are built by <ref type="bibr" target="#b22">(Tang et al., 2015b</ref>). The statistics of the datasets are summarized in <ref type="table">Table 1</ref>. We split the datasets into training, de- velopment and testing sets in the proportion of 8:1:1, with tokenization and sentence splitting by Stanford CoreNLP ( <ref type="bibr" target="#b12">Manning et al., 2014</ref>). We use two metrics including Accuracy which mea- sures the overall sentiment classification perfor- mance and RM SE which measures the divergences between predicted sentiment classes and ground truth classes. The Accuracy and RM SE metrics are defined as:</p><formula xml:id="formula_13">Accuracy = T N (12) RM SE = N i=1 (gd i − pr i ) 2 N ,<label>(13)</label></formula><p>where T is the numbers of predicted sentiment rat- ings that are identical with gold sentiment ratings, N is the numbers of documents and gd i , pr i repre- sent the gold sentiment rating and predicted senti- ment rating respectively. Word embeddings could be randomly initialized or pre-trained. We pre-train the 200-dimensional word embeddings on each dataset in ( <ref type="bibr" target="#b21">Tang et al., 2015a</ref>) with SkipGram ( <ref type="bibr" target="#b13">Mikolov et al., 2013)</ref>. We set the user embedding dimension and product em- bedding dimension to be 200, initialized to zero. The dimensions of hidden states and cell states in our LSTM cells are set to 200. We tune the hy- per parameters on the development sets and use adadelta <ref type="bibr" target="#b24">(Zeiler, 2012)</ref> to update parameters when training. We select the best configuration based on performance on the development set, and evaluate the configuration on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our NSC model with several base- line methods for document sentiment classification:</p><p>Majority regards the majority sentiment cate- gory in training set as the sentiment category of each document in test set.</p><p>Trigram trains a SVM classifier with unigrams, bigrams and trigrams as features.</p><p>TextFeature extracts text features including word and character n-grams, sentiment lexicon fea- tures, etc, and then train a SVM classifier.</p><p>UPF extracts use-leniency features ( <ref type="bibr" target="#b6">Gao et al., 2013</ref>) and corresponding product features from training data, which is further concatenated with the features in Trigram an TextFeature.</p><p>AvgWordvec averages word embeddings in a document to obtain document representation which is fed into a SVM classifier as features.</p><p>SSWE generates features with sentiment-specific word embeddings (SSWE) ( <ref type="bibr" target="#b20">Tang et al., 2014</ref>   <ref type="bibr" target="#b10">and Mikolov, 2014</ref>) for document sentiment clas- sification.</p><p>JMARS considers the information of users and aspects with collaborative filtering and topic model- ing for document sentiment classification.</p><p>UPNN brings in a text preference matrix and a representation vector for each user and product into CNN sentiment classifier <ref type="bibr" target="#b9">(Kim, 2014)</ref>. It modifies the word meaning in the input layer with the prefer- ence matrix and concatenates the user/product rep- resentation vectors with generated document repre- sentation before softmax layer.</p><p>For all baseline methods above, we report the re- sults in ( <ref type="bibr" target="#b22">Tang et al., 2015b</ref>) since we use the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Comparisons</head><p>We list the experimental results in <ref type="table" target="#tab_2">Table 2</ref>. As shown in this table, we manually divide the results into two parts, the first one of which only considers the local text information and the other one incorpo- rates both local text information and the global user product information.</p><p>From the first part in <ref type="table" target="#tab_2">Table 2</ref>, we observe that NSC, the basic implementation of our model, sig- nificantly outperforms all the other baseline meth- ods which only considers the local text informa- tion. To be specific, NSC achieves more than 4% improvements over all datasets compared to typical well-designed neural network models. It demon- strates that NSC is effective to capture the sequen- tial information, which can be a crucial factor to sentiment classification. Moreover, we employ the idea of local semantic attention (LA) in <ref type="bibr" target="#b23">(Yang et al., 2016</ref>) and implement it in NSC model (denoted as NSC+LA). The results shows that the attention based NSC obtains a considerable improvements than the original one. It proves the importance of selecting more meaningful words and sentences in sentiment classification, which is also a main reason of introducing global user and product information in an attention form.</p><p>In the second part of <ref type="table" target="#tab_2">Table 2</ref>, we show the per- formance of models with user product information. From this part, we have the following observations:</p><p>(1) The global user and product information is    <ref type="table">Table 4</ref>: Effect of user and product attention mechanisms. UA represents the user attention mechanism, and PA indicates the product attention mechanism.</p><p>helpful to neural network based models for senti- ment classification. With the consideration of such information in IMDB, UPNN achieves 3% improve- ment and our proposed NSC+UPA obtains 9% im- provement in accuracy. The significant improve- ments state the necessity of considering these global information in sentiment classification.</p><p>(2) Our proposed NSC model with user produc- tion attention (NSC+UPA) significantly and consis- tently outperforms all the other baseline methods. It indicates the flexibility of our model on various real- world datasets. Note that, we also implement ( <ref type="bibr" target="#b22">Tang et al., 2015b</ref>)'s method to deal with user and prod- uct information on NSC (denoted as UPNN (NSC)). Though the employment of NSC improves the per- formance of UPNN, it is still not comparable to our model. More specifically, UPNN exceed the mem- ory of our GPU (12G) when dealing with Yelp2014 dataset due to the high complexity of its parame- ters. Compared to UPNN which utilizes the user product information with matrices and vectors si- multaneously, our model only embeds each user and product as a vector, which makes it suitable to large- scale datasets. It demonstrates that our NSC model is more effective and efficient to handle additional user and product information.</p><p>Observations above demonstrate that NSC with user product attention (NSC+UPA) is capable of capturing meanings of multiple semantic layers within a document. Comparing with other user product based models, our model incorporates global user product information in an effective and efficient way. Furthermore, the model is also robust and achieves consistent improvements than state-of- the-art methods on various real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis: Effect of Attention</head><p>Mechanisms in Word and Sentence Level <ref type="table" target="#tab_4">Table 3</ref> shows the effect of attention mechanisms in word or sentence level respectively. From the table, we can observe that: (1) Both the atten- tion mechanisms applied in word level and sentence level improve the performance for document senti- ment classification compared with utilizing average pooling in word and sentence level; (2) The atten- tion mechanism in word level improves more for our model as compared to sentence level. The reason is that the word attention mechanism can capture the informative words in all documents, while the sen- tence attention mechanism may only work in long documents with various topics. (3) The model con- sidering both word level attention and sentence level attention outperforms the ones considering only one semantic level attention. It proves that the charac- teristics of users and products are reflected on mul- tiple semantic levels, which is also a critical mo- tivation of introducing User Product Attention into sentiment classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis: Effect of User Product</head><p>Attention Mechanisms <ref type="table">Table 4</ref> shows the performance of attention mech- anisms with the information of users or products. From the table, we can observe that:</p><p>(1) The information of both users and products contributes to our model as compared to a semantic attention. It demonstrates that our attention mech- anism can catch the specific characteristic of a user or a product.</p><p>(2) The information of users is more effective than the products to enhance document representa- tions. Hence, the discrimination of user preference is more obvious than product characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Analysis: Performance over Sentence Numbers and Lengths</head><p>To investigate the performance of our model over documents with various lengths, we compare the performance of different implementations of NSC under different document lengths and sentence number settings. <ref type="figure" target="#fig_4">Fig. 2</ref> shows the accuracy of sen- timent classification generated by NSC, NSC+ATT, UPNN(NSC) and NSC+UPA on the IMDB test set with respect to input document lengths and input sentence numbers in a document. <ref type="figure" target="#fig_4">From Fig. 2</ref>, we observe that our model NSC with attention mecha- nism of user and product information consistently outperforms other baseline methods for all input document lengths and sentence numbers. It indi- cates the robustness and flexibility of NSC on dif- ferent datasets.  To demonstrate the effectiveness of our global at- tention, we provide a review instance in Yelp2013 dataset for example. The content of this review is "Great wine, great ambiance, amazing music!". We visualize the attention weights in word-level for two distinct users and the local semantic attention (LA) in <ref type="figure" target="#fig_6">Fig 3.</ref> Here, the local semantic attention rep- resents the implementation in ( <ref type="bibr" target="#b23">Yang et al., 2016)</ref>, which calculates the attention without considering the global information of users and products. Note that, darker color means lower weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Case Study</head><p>According to our statistics, the first user often mentions "wine" in his/her review sentences. On the contrary, the second user never talks about "wine" in his/her review sentences. Hence, we in- fer that the first user may has special preference to wine while the second one has no concern about wine. From the figure, we observe an interesting phenomenon which confirms to our inference. For the word "wine", the first user has the highest atten-tion weight and the second user has the lowest atten- tion weight. It indicates that our model can capture the global user preference via our user attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a hierarchical neural network which incorporates user and product in- formation via word and sentence level attentions. With the user and product attention, our model can take account of the global user preference and prod- uct characteristics in both word level and semantic level. In experiments, we evaluate our model on sentiment analysis task. The experimental results show that our model achieves significant and consis- tent improvements compared to other state-of-the- art models.</p><p>We will explore more in future as follows:</p><p>(1) In this paper, we only consider the global user preference and product characteristics according to their personal behaviors. In fact, most users and products usually have some text information such as user and product profiles. We will take advan- tages of those information in sentiment analysis in future.</p><p>(2) Aspect level sentiment classification is also a fundamental task in the field of sentiment analy- sis. The user preference and product characteristics may also implicitly influence the sentiment polarity of the aspect. We will explore the effectiveness of our model on aspect level sentiment classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(Glorot et al., 2011) use Stacked Denoising Autoencoder in sentiment clas- sification for the first time. Socher conducts a se- ries of recursive neural network models to learn representations based on the recursive tree struc- ture of sentences, including Recursive Autoen- coder (RAE) (Socher et al., 2011), Matrix-Vector Recursive Neural Network (MV-RNN) (Socher et al., 2012) and Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). Besides,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of User Product Attention based Neural Sentiment Classification model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Basic</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy over various input document lengths on IMDB test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of attentions over words</figDesc><graphic url="image-1.png" coords="8,349.10,325.83,177.95,59.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Document-level sentiment classification results. Acc.(Accuracy) and RMSE are the evaluation metrics. The best perfor-

mances are in bold in both groups. 

the Recurrent Neural Network (RNN). Afterwards, 
the hidden vectors of RNN are averaged to obtain 
document representation for sentiment classifica-
tion. 
Paragraph Vector implements the PVDM (Le 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Model</head><label></label><figDesc></figDesc><table>Level 
IMDB 
Yelp2013 
Yelp2014 
Word Sentence 
Acc RMSE 
Acc RMSE 
Acc RMSE 

NSC 

AVG 
AVG 
0.443 1.465 
0.627 0.701 
0.637 0.686 
AVG 
ATT 
0.498 1.336 
0.632 0.701 
0.653 0.672 
ATT 
AVG 
0.513 1.330 
0.640 0.686 
0.662 0.657 
ATT 
ATT 
0.533 1.281 
0.650 0.692 
0.667 0.654 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Effect of attention mechanisms in word and sentence level. AVG means an average pooling layer, and ATT represents 

the attention mechanism in word or sentence level. 

Basic Model Attention Type 
IMDB 
Yelp2013 
Yelp2014 
Acc RMSE 
Acc RMSE 
Acc RMSE 

NSC 

ATT 
0.487 1.381 
0.631 0.706 
0.630 0.715 
PA 
0.485 1.456 
0.630 0.704 
0.644 0.676 
UA 
0.525 1.276 
0.645 0.699 
0.644 0.680 
UPA 
0.533 1.281 
0.650 0.692 
0.667 0.654 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work is supported by the National So-cial Science Foundation of China (13&amp;ZD190) and the National Natural Science Foundation of China (NSFC No. 61331013). We sincerely thank Shiqi Shen and Lei Xu for their insightful discussions, and thank Ayana, Yu Zhao, Ruobing Xie, Jiacheng Zhang and Meng Zhang in Tsinghua University Natural Language Processing group for their con-structive comments. We also thank all anonymous reviewers for their insightful suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Better document-level sentiment analysis from rst discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gottlob Frege. 1892. On sense and reference</title>
		<editor>Ludlow</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling user leniency and product popularity for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1107" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1058</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eudard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00185</idno>
		<title level="m">When are tree structures necessary for deep learning of representations? arXiv preprint</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semisupervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lexicon-based methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CL</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning semantic representations of users and products for document level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
