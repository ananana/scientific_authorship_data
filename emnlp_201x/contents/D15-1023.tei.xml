<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On a Strictly Convex IBM Model 1</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Stein</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University New York</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
								<address>
									<addrLine>Computer Science New York</addrLine>
									<postCode>10027, 10027</postCode>
									<region>NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IEOR Department New York</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On a Strictly Convex IBM Model 1</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>IBM Model 1 is a classical alignment model. Of the first generation word-based SMT models, it was the only such model with a concave objective function. For concave optimization problems like IBM Model 1, we have guarantees on the convergence of optimization algorithms such as Expectation Maximization (EM). However , as was pointed out recently, the objective of IBM Model 1 is not strictly concave and there is quite a bit of alignment quality variance within the optimal solution set. In this work we detail a strictly concave version of IBM Model 1 whose EM algorithm is a simple modification of the original EM algorithm of Model 1 and does not require the tuning of a learning rate or the insertion of an l 2 penalty. Moreover , by addressing Model 1&apos;s shortcomings , we achieve AER and F-Measure improvements over the classical Model 1 by over 30%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The IBM translation models were introduced in <ref type="bibr" target="#b1">(Brown et al., 1993)</ref> and were the first-generation Statistical Machine Translation (SMT) systems. In the current pipeline, these word-based models are the seeds for more sophisticated models which need alignment tableaus to start their optimization procedure. Among the original IBM Models, only IBM Model 1 can be formulated as a concave opti- mization problem. Recently, there has been some research on IBM Model 2 which addresses either the model's non-concavity ( <ref type="bibr" target="#b11">Simion et al., 2015</ref>) * Currently on leave at Google Inc. <ref type="bibr">New York.</ref> or over parametrization ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>). We make the following contributions in this paper:</p><p>• We utilize and expand the mechanism intro- duced in <ref type="bibr" target="#b11">(Simion et al., 2015</ref>) to construct strictly concave versions of IBM Model 1 1 . As was shown in ( <ref type="bibr" target="#b12">Toutanova and Galley, 2011)</ref>, IBM Model 1 is not a strictly con- cave optimization problem. What this means in practice is that although we can initialize the model with random parameters and get to the same objective cost via the EM algorithm, there is quite a bit of alignment quality vari- ance within the model's optimal solution set and ambiguity persists on which optimal so- lution truly is the best. Typically, the easiest way to make a concave model strictly con- cave is to append an l 2 regularizer. However, this method does not allow for seamless EM training: we have to either use a learning-rate dependent gradient based algorithm directly or use a gradient method within the M step of EM training. In this paper we show how to get via a simple technique an infinite supply of models that still allows a straightforward application of the EM algorithm.</p><p>• As a concrete application of the above, we detail a very simple strictly concave version of IBM Model 1 and study the performance of different members within this class. Our strictly concave models combine some of the elements of word association and positional dependance as in IBM Model 2 to yield a sig- nificant model improvement. Furthermore, we now have guarantees that the solution we find is unique.</p><p>• We detail an EM algorithm for a subclass of strictly concave IBM Model 1 variants. The EM algorithm is a small change to the orig- inal EM algorithm introduced in <ref type="bibr" target="#b1">(Brown et al., 1993)</ref>.</p><p>Notation. Throughout this paper, for any posi- tive integer N , we use [N ] to denote {1 . . . N } and [N ] 0 to denote {0 . . . N }. We denote by R n + the set of nonnegative n dimensional vectors. We denote by <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> n the n−dimensional unit cube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IBM Model 1</head><p>We begin by reviewing IBM Model 1 and in- troducing the necessary notation. To this end, throughout this section and the remainder of the paper we assume that our set of training exam- ples is (e (k) , f (k) ) for k = 1 . . . n, where e (k) is the k'th English sentence and f (k) is the k'th French sentence. Following standard convention, we assume the task is to translate from French (the "source" language) into English (the "target" lan- guage). We use E to denote the English vocabu- lary (set of possible English words), and F to de- note the French vocabulary. The k'th English sen- tence is a sequence of words e</p><formula xml:id="formula_0">(k) 1 . . . e (k) l k</formula><p>where l k is the length of the k'th English sentence, and each e (k) i ∈ E; similarly the k'th French sentence is a sequence f</p><formula xml:id="formula_1">(k) 1 . . . f (k) m k where each f (k) j ∈ F . We define e (k)</formula><p>0 for k = 1 . . . n to be a special NULL word (note that E contains the NULL word).</p><p>For each English word e ∈ E, we will assume that D(e) is a dictionary specifying the set of pos- sible French words that can be translations of e. The set D(e) is a subset of F . In practice, D(e) can be derived in various ways; in our experiments we simply define D(e) to include all French words f such that e and f are seen in a translation pair.</p><p>Given these definitions, the IBM Model 1 opti- mization problem is given in <ref type="figure" target="#fig_0">Fig. 1</ref> and, for exam- ple, <ref type="bibr" target="#b5">(Koehn, 2008)</ref>. The parameters in this prob- lem are t(f |e). The t(f |e) parameters are transla- tion parameters specifying the probability of En- glish word e being translated as French word f . The objective function is then the log-likelihood of the training data (see Eq. 3):</p><formula xml:id="formula_2">1 n n � k=1 m k � j=1 log p(f (k) j |e (k) ) , where log p(f (k) j |e (k) ) is log l k � i=0 t(f (k) j |e (k) i ) 1 + l k = C + log l k � i=0 t(f (k) j |e (k) i ) ,</formula><p>and C is a constant that can be ignored.</p><formula xml:id="formula_3">Input: Define E, F , L, M , (e (k) , f (k) , l k , m k ) for k = 1 . . . n, D(e)</formula><p>for e ∈ E as in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters:</head><p>• A parameter t(f |e) for each e ∈ E, f ∈ D(e).</p><p>Constraints:</p><formula xml:id="formula_4">∀e ∈ E, f ∈ D(e), t(f |e) ≥ 0 (1) ∀e ∈ E, � f ∈D(e) t(f |e) = 1 (2)</formula><p>Objective: Maximize</p><formula xml:id="formula_5">1 n n � k=1 m k � j=1 log l k � i=0 t(f (k) j |e (k) i )<label>(3)</label></formula><p>with respect to the t(f |e) parameters. While IBM Model 1 is concave optimization problem, it is not strictly concave ( <ref type="bibr" target="#b12">Toutanova and Galley, 2011</ref>). Therefore, optimization methods for IBM Model 1 (specifically, the EM algorithm) are typically only guaranteed to reach a global maximum of the objective function (see the Ap- pendix for a simple example contrasting convex and strictly convex functions). In particular, al- though the objective cost is the same for any op- timal solution, the translation quality of the so- lutions is not fixed and will still depend on the initialization of the model (Toutanova and Galley, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Strictly Concave IBM Model 1</head><p>We now detail a very simple method to make IBM Model 1 strictly concave with a unique optimal so- lution without the need for appending an l 2 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. Consider IBM Model 1 and modify its objective to be</head><formula xml:id="formula_6">1 n n � k=1 m k � j=1 log l k � i=0 h i,j,k (t(f (k) j |e (k) i )) (4)</formula><p>where h i,j,k : R + → R + is strictly concave. With the new objective and the same constraints as IBM Model 1, this new optimization problem is strictly concave.</p><p>Proof. To prove concavity, we now show that the new likelihood function</p><formula xml:id="formula_7">L(t) = 1 n n � k=1 m k � j=1 log l k � i=0 h i,j,k (t(f (k) j |e (k) i )) ,</formula><p>is strictly concave (concavity follows in the same way trivially). Suppose by way of contradiction that there is (t) � = (t � ) and θ ∈ (0, 1) such that equality hold for Jensen's inequality. Since h i,j,k is strictly concave and (t) � = (t � ) we must have that there must be some (k, j, i) such that t(f</p><formula xml:id="formula_8">(k) j |e (k) i ) � = t � (f (k) j |e (k)</formula><p>i ) so that Jensen's in- equality is strict for h i,j,k and we have</p><formula xml:id="formula_9">h i,j,k (θt(f (k) j |e (k) i ) + (1 − θ)t � (f (k) j |e (k) i )) &gt; θh i,j,k (t(f (k) j |e (k) i )) + (1 − θ)h i,j,k (t � (f (k) j |e (k) i ))</formula><p>Using Jensen's inequality, the monotonicity of the log, and the above strict inequality we have</p><formula xml:id="formula_10">L(θt + (1 − θ)t � ) = n � k=1 m k � j=1 log l k � i=0 h i,j,k (θt(f (k) j |e (k) i ) + (1 − θ)t � (f (k) j |e (k) i )) &gt; n � k=1 m k � j=1 log l k � i=0 θh i,j,k (t(f (k) j |e (k) i )) + (1 − θ)h i,j,k (t � (f (k) j |e (k) i )) ≥ θ n � k=1 m k � j=1 log l k � i=0 h i,j,k (t(f (k) j |e (k) i )) + (1 − θ) n � k=1 m k � j=1 log l k � i=0 h i,j,k (t � (f (k) j |e (k) i )) = θL(t) + (1 − θ)L(t � )</formula><p>The IBM Model 1 strictly concave optimiza- tion problem is presented in <ref type="figure" target="#fig_1">Fig. 2</ref>. In (7) it is crucial that each h i,j,k be strictly concave within</p><formula xml:id="formula_11">� l k i=0 h i,j,k (t(f (k) j |e (k) i )).</formula><p>For example, we have that √ x 1 + x 2 is concave but not strictly concave and the proof of Theorem 1 would break down. To see this, we can consider (x 1 , x 2 ) � = (x 1 , x 3 ) and note that equality holds in Jensen's inequality. We should be clear: the main reason why Theorem 1 works is that we have h i,j,k are strictly concave (on R + ) and all the lexical probabilities that are argu- ments to L are present within the log-likelihood.</p><formula xml:id="formula_12">Input: Define E, F , L, M , (e (k) , f (k) , l k , m k ) for k = 1 . .</formula><p>. n, D(e) for e ∈ E as in Section 2. A set of strictly concave functions h i,j,k : R + → R + .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters:</head><p>• A parameter t(f |e) for each e ∈ E, f ∈ D(e).</p><p>Constraints:</p><formula xml:id="formula_13">∀e ∈ E, f ∈ D(e), t(f |e) ≥ 0 (5) ∀e ∈ E, � f ∈D(e)</formula><p>t(f |e) = 1 (6)</p><formula xml:id="formula_14">Objective: Maximize 1 n n � k=1 m k � j=1 log l k � i=0 h i,j,k (t(f (k) j |e (k) i ))<label>(7)</label></formula><p>with respect to the t(f |e) parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parameter Estimation via EM</head><p>For the IBM Model 1 strictly concave optimization problem, we can derive a clean EM Algorithm if we base our relaxation of</p><formula xml:id="formula_15">h i,j,k (t(f (k) j |e (k) i )) = α(e (k) i , f (k) j )(t(f (k) j |e (k) i )) β(e (k) i ,f (k) j ) with β(e (k) i , f (k) j ) &lt; 1.</formula><p>To justify this, we first need the following: Lemma 1. Consider h : R + → R + given by h(x) = x β where β ∈ (0, 1). Then h is strictly concave.</p><p>Proof. The proof of this lemma is elementary and follows since the second derivative given by h �� (x) = β(β − 1)x β−2 is strictly negative.</p><p>For our concrete experiments, we picked a model based on Lemma 1 and used h(x) = αx β with α, β ∈ (0, 1) so that</p><formula xml:id="formula_16">h i,j,k (t(f (k) j |e (k) i )) = α(f (k) j , e (k) i )(t(f (k) j |e (k) i )) β(f (k) j ,e (k) i ) .</formula><p>Using this setup, parameter estimation for the new model can be accomplished via a slight modifica- tion of the EM algorithm for IBM Model 1. In particular, we have that the posterior probabilities of this model factor just as those of the standard Model 1 and we have an M step that requires opti- mizing �</p><formula xml:id="formula_17">a (k) q(a (k) |e (k) , f (k) ) log p(f (k) , a (k) |e (k) ) 1: Input: Define E, F , L, M , (e (k) , f (k) , l k , m k ) for k = 1 . . . n,</formula><p>D(e) for e ∈ E as in Section 2. An integer T specifying the number of passes over the data. A set of weighting parameter α(e, f ), β(e, f ) ∈ (0, 1) for each e ∈ E, f ∈ D(e). A tuning parameter λ &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2: Parameters:</head><p>• A parameter t(f |e) for each e ∈ E, f ∈ D(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: Initialization:</head><p>• ∀e ∈ E, f ∈ D(e), set t(f |e) = 1/|D(e)|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4: EM Algorithm:</head><p>5: for all t = 1 . . . T do 6:</p><p>∀e ∈ E, f ∈ D(e), count(f, e) = 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>∀e ∈ E, count(e) = 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>EM Algorithm: Expectation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>for all k = 1 . . . n do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>for all j = 1 . . . m k do 11:</p><formula xml:id="formula_18">δ1[i] = 0 ∀i ∈ [l k ]0</formula><p>12:</p><formula xml:id="formula_19">Δ1 = 0</formula><p>13:</p><formula xml:id="formula_20">for all i = 0 . . . l k do 14: δ1[i] = α(f (k) j , e (k) i )(t(f (k) j |e (k) i )) β(f (k) j ,e (k) i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>Δ1 += δ1 <ref type="bibr">[i]</ref> 16:</p><formula xml:id="formula_21">for all i = 0 . . . l k do 17: δ1[i] = δ 1 [i] Δ 1</formula><p>18:</p><formula xml:id="formula_22">count(f (k) j , e (k) i ) += β(f (k) j , e (k) i )δ1[i]</formula><p>19:</p><formula xml:id="formula_23">count(e (k) i ) += β(f (k) j , e (k) i )δ1[i]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>20:</head><p>EM Algorithm: Maximization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>for all e ∈ E do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>22:</head><p>for all f ∈ D(e) do 23:</p><formula xml:id="formula_24">t(f |e) = count(e,f ) count(e)</formula><p>24: Output: t parameters where</p><formula xml:id="formula_25">q(a (k) |e (k) , f (k) ) ∝ m k � j=1 h a (k) j ,j,k (t(f (k) j |e (k) a (k) j ))</formula><p>are constants gotten in the E step. This optimiza- tion step is very similar to the regular Model 1 M step since the β drops down using log t β = β log t; the exact same count-based method can be ap- plied. The details of this algorithm are in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Choosing α and β</head><p>The performance of our new model will rely heav- ily on the choice of α(e</p><formula xml:id="formula_26">(k) i , f (k) j ), β(e (k) i , f (k)</formula><p>j ) ∈ (0, 1) we use. In particular, we could make β de- pend on the association between the words, or the words' positions, or both. One classical measure of word association is the dice coefficient <ref type="bibr" target="#b9">(Och and Ney, 2003)</ref> given by</p><formula xml:id="formula_27">dice(e, f ) = 2c(e, f ) c(e) + c(f ) .</formula><p>In the above, the count terms c are the number of training sentences that have either a particular word or a pair of of words (e, f ). As with the other choices we explore, the dice coefficient is a frac- tion between 0 and 1, with 0 and 1 implying less and more association, respectively. Additionally, we make use of positional constants like those of the IBM Model 2 distortions given by</p><formula xml:id="formula_28">d(i|j, l, m) =    1 (l+1)Z(j,l,m) : i = 0 le −λ| i l − j m | (l+1)Z(j,l,m) : i � = 0</formula><p>In the above, Z(j, l, m) is the partition func- tion discussed in ( <ref type="bibr" target="#b3">Dyer et al., 2013</ref>). The previ- ous measures all lead to potential candidates for β(e, f ), we have t(f |e) ∈ (0, 1), and we want to enlarge competing values when decoding (we use αt β instead of t when getting the Viterbi align- ment). The above then implies that we will have the word association measures inversely propor- tional to β, and so we set β(e, f</p><formula xml:id="formula_29">) = 1 − dice(e, f ) or β(e, f ) = 1 − d(i|j, l, m).</formula><p>In our experiments we picked α(f</p><formula xml:id="formula_30">(k) j , e (k) i ) = d(i|j, l k , m k ) or 1;</formula><p>we hold λ to a constant of either 16 or 0 and do not estimate this variable (λ = 16 can be chosen by cross validation on a small trial data set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Sets</head><p>For our alignment experiments, we used a subset of the Canadian Hansards bilingual corpus with 247,878 English-French sentence pairs as training data, 37 sentences of development data, and 447 sentences of test data <ref type="bibr" target="#b6">(Michalcea and Pederson, 2003)</ref>. As a second validation corpus, we con- sidered a training set of 48,706 Romanian-English sentence-pairs, a development set of 17 sentence pairs, and a test set of 248 sentence pairs (Michal- cea and Pederson, 2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Methodology</head><p>Below we report results in both AER (lower is better) and F-Measure (higher is better) <ref type="bibr" target="#b9">(Och and Ney, 2003)</ref> for the English → French translation direction. To declare a better model we have to settle on an alignment measure. Although the relationship between AER/F-Measure and trans- lation quality varies ( <ref type="bibr" target="#b3">Dyer et al., 2013)</ref>, there are some positive experiments ( <ref type="bibr">Fraser and Marcu, 2004)</ref> showing that F-Measure may be more use- ful, so perhaps a comparison based on F-Measure is ideal. <ref type="table">Table 1</ref> contains our results for the Hansards data. For the smaller Romanian data, we obtained similar behavior, but we leave out these results due <ref type="bibr">(α, β)</ref>   <ref type="formula">(1,1</ref>). The not necessarily strictly concave model with (d,1) setting gives the best AER, while the strictly concave model given by the (1, 1−d) setting has the highest F-Measure.</p><formula xml:id="formula_31">(1, 1) (d, 1) (1, 1 − dice) (1, 1 − d) (d, 1 − d) Iteration AER 0 0.8716</formula><p>to space limitations. Our experiments show that using</p><formula xml:id="formula_32">h i,j,k (t(f (k) j |e (k) i )) = (t(f (k) j |e (k) i )) 1−d(i|j,l k ,m k )</formula><p>yields the best F-Measure performance and is not far off in AER from the "fake" 2 IBM Model 2 (gotten by setting (α, β) = (d, 1)) whose results are in column 2 (the reason why we use this model at all is since it should be better than IBM 1: we want to know how far off we are from this obvi- ous improvement). Moreover, we note that dice does not lead to quality β exponents and that, un- fortunately, combining methods as in column 5</p><formula xml:id="formula_33">((α, β) = (d, 1 − d))</formula><p>does not necessarily lead to additive gains in AER and F-Measure perfor- mance. <ref type="bibr">2</ref> Generally speaking, when using</p><formula xml:id="formula_34">h i,j,k (t(f (k) j |e (k) i )) = d(i|j, l k , m k )t(f (k) j |e (k) i )</formula><p>with d constant we cannot use Theorem 3 since h is linear. Most likely, the strict concavity of the model will hold be- cause of the asymmetry introduced by the d term; however, there will be a necessary dependency on the data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Comparison with Previous Work</head><p>In this section we take a moment to also compare our work with the classical IBM 1 work of <ref type="bibr" target="#b7">(Moore, 2004</ref>). Summarizing <ref type="bibr" target="#b7">(Moore, 2004)</ref>, we note that this work improves substancially upon the classi- cal IBM Model 1 by introducing a set of heuris- tics, among which are to (1) modify the lexical parameter dictionaries (2) introduce an initializa- tion heuristic (3) modify the standard IBM 1 EM algorithm by introducing smoothing (4) tune ad- ditional parameters. However, we stress that the main concern of this work is not just heuristic- based empirical improvement, but also structured learning. In particular, although using an regular- izer l 2 and the methods of <ref type="bibr" target="#b7">(Moore, 2004</ref>) would yield a strictly concave version of IBM 1 as well (with improvements), it is not at all obvious how to choose the learning rate or set the penalty on the lexical parameters. The goal of our work was to offer a new, alternate form of regularization. Moreover, since we are changing the original log- likelihood, our method can be thought of as way of bringing the l 2 regularizer inside the log like- lihood. Like <ref type="bibr" target="#b7">(Moore, 2004</ref>), we also achieve ap- preciable gains but have just one tuning parame- ter (when β = 1 − d we just have the centering λ parameter) and do not break the probabilistic in- terpretation any more than appending a regularizer would (our method modifies the log-likelihood but the simplex constrains remain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper we showed how IBM Model 1 can be made into a strictly convex optimization prob- lem via functional composition. We looked at a specific member within the studied optimization family that allows for an easy EM algorithm. Fi- nally, we conducted experiments showing how the model performs on some standard data sets and empirically showed 30% important over the stan- dard IBM Model 1 algorithm. For further re- search, we note that picking the optimal h i,j,k is an open question, so provably finding and justify- ing the choice is one topic of interest.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The IBM Model 1 Optimization Problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The IBM Model 1 strictly concave optimization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pseudocode for T iterations of the EM Algorithm for the IBM Model 1 strictly concave optimization problem.</figDesc></figure>

			<note place="foot" n="1"> Please refer as needed to the Appendix for examples and definitions of convexity/concavity and strict convexity/concavity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Andrei Simion was supported by a Google re-search award. Cliff Stein is supported in part by NSF grants CCF-1349602 and CCF-1421161.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<title level="m">The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum Likelihood From Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society, series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Simple, Fast, and Effective Reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring Word Alignment Quality for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="303" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Evaluation Exercise in Word Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Michalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2003: Workshop in building and using Parallel Texts: Data Driven Machine Translation and Beyond</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving IBM WordAlignment Model 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HMM-Based Word Alignment in Statistical Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational-Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="52" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Convex Alternative to IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Family of Latent Variable Convex Relaxations for IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Simion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the L0norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
