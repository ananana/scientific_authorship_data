<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Methods for Incorporating Knowledge into Topic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yiyang@u.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">Northwestern University Evanston</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<region>IL, CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>ddowney@eecs.northwetsern.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">Northwestern University Evanston</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<region>IL, CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">Northwestern University Evanston</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<region>IL, CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">Boyd</forename><surname>Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science</orgName>
								<orgName type="institution" key="instit1">Northwestern University Evanston</orgName>
								<orgName type="institution" key="instit2">University of Colorado Boulder</orgName>
								<address>
									<region>IL, CO</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Methods for Incorporating Knowledge into Topic Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Latent Dirichlet allocation (LDA) is a popular topic modeling technique for exploring hidden topics in text corpora. Increasingly , topic modeling needs to scale to larger topic spaces and use richer forms of prior knowledge, such as word correlations or document labels. However, inference is cumbersome for LDA models with prior knowledge. As a result, LDA models that use prior knowledge only work in small-scale scenarios. In this work, we propose a factor graph framework, Sparse Constrained LDA (SC-LDA), for efficiently incorporating prior knowledge into LDA. We evaluate SC-LDA&apos;s ability to incorporate word correlation knowledge and document label knowledge on three benchmark datasets. Compared to several baseline methods, SC-LDA achieves comparable performance but is significantly faster.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Challenge: Leveraging Prior Knowledge in Large-scale Topic Models</head><p>Topic models, such as Latent Dirichlet Alloca- tion ( <ref type="bibr">Blei et al., 2003, LDA)</ref>, have been success- fully used for discovering hidden topics in text col- lections. LDA is an unsupervised model-it re- quires no annotation-and discovers, without any supervision, the thematic trends in a text collec- tion. However, LDA's lack of supervision can lead to disappointing results. Often, the hidden topics learned by LDA fail to make sense to end users. Part of the problem is that the objective function of topic models does not always corre- late with human judgments of topic quality ). Therefore, it's often necessary to incorporate prior knowledge into topic models to improve the model's performance. Recent work has also shown that by interactive human feedback can improve the quality and stability of topics (Hu and Boyd-Graber, 2012; . In- formation about documents ( <ref type="bibr" target="#b33">Ramage et al., 2009)</ref> or words (Boyd- <ref type="bibr" target="#b5">Graber et al., 2007</ref>) can improve LDA's topics.</p><p>In addition to its occasional inscrutability, scal- ability can also hamper LDA's adoption. Conven- tional Gibbs sampling-the most widely used in- ference for LDA-scales linearly with the num- ber of topics. Moreover, accurate training usu- ally takes many sampling passes over the dataset. Therefore, for large datasets with millions or even billions of tokens, conventional Gibbs sampling takes too long to finish. For standard LDA, re- cently introduced fast sampling methods ( <ref type="bibr" target="#b40">Yao et al., 2009;</ref><ref type="bibr" target="#b18">Li et al., 2014;</ref><ref type="bibr" target="#b41">Yuan et al., 2015</ref>) en- able industrial applications of topic modeling to search engines and online advertising, where cap- turing the "long tail" of infrequently used topics requires large topic spaces. For example, while typical LDA models in academic papers have up to 10 3 topics, industrial applications with 10 5 -10 6 topics are common ( <ref type="bibr" target="#b37">Wang et al., 2014</ref>). Moreover, scaling topic models to many topics can also re- veal the hierarchical structure of topics ( <ref type="bibr" target="#b11">Downey et al., 2015)</ref>.</p><p>Thus, there is a need for topic models that can both benefit from rich prior information and that can scale to large datasets. However, existing methods for improving scalability focus on topic models without prior information. To rectify this, we propose a factor graph model that encodes a potential function over the hidden topic variables, encouraging topics consistent with prior knowl- edge. The factor model representation admits an efficient sampling algorithm that takes advantage of the model's sparsity. We show that our method achieves comparable performance but runs signifi- cantly faster than baseline methods, enabling mod-els to discover models with many topics enriched by prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Efficient Algorithm for Incorporating Knowledge into LDA</head><p>In this section, we introduce the factor model for incorporating prior knowledge and show how to efficiently use Gibbs sampling for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: LDA and SparseLDA</head><p>A statistical topic model represents words in doc- uments in a collection D as mixtures of T top- ics, which are multinomials over a vocabulary of size V . In LDA, each document d is associated with a multinomial distribution over topics, θ d . The probability of a word type w given topic z is φ w|z . The multinomial distributions θ d and φ z are drawn from Dirichlet distributions: α and β are the hyperparameters for θ and φ. We represent the document collection D as a sequence of words w, and topic assignments as z. We use symmetric priors α and β in the model and experiment, but asymmetric priors are easily encoded in the mod- els ( <ref type="bibr" target="#b36">Wallach et al., 2009)</ref>. Discovering the latent topic assignments z from observed words w requires inferring the the pos- terior distribution P (z|w). <ref type="bibr" target="#b13">Griffiths and Steyvers (2004)</ref> propose using collapsed Gibbs sampling. The probability of a topic assignment z = t in document d given an observed word type w and the other topic assignments z − is</p><formula xml:id="formula_0">P (z = t|z − , w) ∝ (n d,t + α) n w,t + β n t + V β<label>(1)</label></formula><p>where z − are the topic assignments of all other tokens. This conditional probability is based on cumulative counts of topic assignments: n d,t is the number of times topic t is used in document d, n w,t is the number of times word type w is used in topic t, and n t is the marginal count of the number of tokens assigned to topic t. Unfortunately, explicitly computing the condi- tional probability is quite for models with many topics. The time complexity of drawing a sample by Equation 1 is linear to the number of topics. <ref type="bibr" target="#b40">Yao et al. (2009)</ref> propose a clever factorization of Equation 1 so that the complexity is typically sub- linear by breaking the conditional probability into three "buckets":</p><formula xml:id="formula_1">t P (z = t|z − , w) = t αβ n t + V β s (2) + t,n d,t &gt;0 n d,t β n t + V β r + t,nw,t&gt;0 (n d,t + α)n w,t n t + V β q .</formula><p>The first term s is the "smoothing only" bucket-constant for all documents. The second term r is the "document only" bucket that is shared by a document's tokens. Both s and r have simple constant time updates. The last term q has to be computed specifically for each token, only for the few types with non-zero counts in a topic, due to the sparsity of word-topic count. Since q often has the largest mass and few non-zero terms, we start the sampling from bucket q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Factor Model for Incorporating Prior Knowledge</head><p>With SparseLDA, inferring LDA models over large topic spaces becomes tractable. However, existing methods for incorporating prior knowl- edge use conventional Gibbs sampling, which hin- ders inference. We address this limitation in this section by adding a factor graph to encode prior knowledge. LDA assumes that the hidden topic assignment of a word is independent from other hidden top- ics, given the document's topic distribution θ. While this assumption facilitates computational efficiency, it loses the rich correlation between words. In many scenarios, users have external knowledge regarding word correlation, document labels, or document relations, which can reshape topic models and improve coherence.</p><p>Prior knowledge can constrain what models dis- cover. A correlation between two words v and w indicates that they have a similar topic distribu- tion, i.e., p(z|v) ≈ p(z|w). 1 Therefore, the poste- rior topic assignments v and w will be correlated. In contrast, if v and w are uncorrelated, nothing- other than the Dirichlet's rich get richer effect- prevents the topics from diverging. Similarly, if two documents share a label, then it is reasonable to assume that they are more likely than two ran- dom documents to share topics.</p><p>We denote the set of prior knowledge as M . Each prior knowledge m ∈ M defines a potential function f m (z, w, d) of the hidden topic z of word type w in document d with which m is associated. Therefore, the complete prior knowledge M de- fines a score on the current topic assignments z:</p><formula xml:id="formula_2">ψ(z, M ) = z∈z exp f m (z, w, d)<label>(3)</label></formula><p>If m is knowledge about word type w, then f m (z, w, d) applies to all hidden topics of word w. If m is knowledge about document d, then f m (z, w, d) applies to all topics that are in docu- ment d. The potential function assigns large values to the topics that accord with prior knowledge but penalizes the topic assignments that disagree with the prior knowledge. In an extreme case, if a prior knowledge m says word type w in document d is Topic 3, then the potential function f m (z, w, d) is zero for all topics but Topic 3.</p><p>Since the potential function ψ is a function of z, and it is only a real-value score of current topic assignments, the potential can be factored out of the marginalized joint:</p><formula xml:id="formula_3">P (w, z|α, β, M ) = P (w|z, β)P (z|α)ψ(z, M ) (4) = θ φ p(w|z, φ)p(φ|β)p(z|θ)p(θ|α)ψ(z, M )dθdφ = ψ(z, M ) θ φ p(w|z, φ)p(φ|β)p(z|θ)p(θ|α)dθdφ.</formula><p>Given the joint likelihood and observed data, the goal is evaluate the posterior P (z|w). Com- puting P (z|w) involves evaluating a probabil- ity distribution on a large discrete state space: P (z|w) = P (z, w)/ z P (z, w). <ref type="bibr" target="#b13">Griffiths and Steyvers (2004)</ref>-mirroring the original inspira- tions for Gibbs sampling <ref type="bibr" target="#b12">(Geman and Geman, 1990)</ref>-draw an analogy to statistical physics, viewing standard LDA as a system that favors con- figurations z that compromise between having few topics per document and having few words per topic, with the terms of this compromise being set by the hyperparameters α and β. Our factor model representation of prior knowledge adds a further constraint that asks the model to also consider en- sembles of topic assignments z that are compatible with a standard LDA model and the given prior knowledge.</p><p>The collapsed Gibbs Sampling for inferring topic assignment z of word w in document d is:</p><formula xml:id="formula_4">P (z = t|w, z − , M ) (5) = P (w, z − , z = t|α, β, M ) P (w, z − |α, β, M ) = P (w, z − , z = t) P (w, z − ) ψ(z − , z = t, M ) ψ(z − , M ) ∝ (n d,t + α) n w,t + β n t + W β ψ(z − , z = t, M ) ψ(z − , M ) ∝ (n d,t + α) n w,t + β n t + W β exp f m (z = t, w, d).</formula><p>The first term is identical to standard LDA, and admits efficient computation using SparseLDA. However, if the second term, exp f m (z, w, d), is dense, we still need to compute it explicitly T times (once for each topic) because we need the summation of P (z = t) for sampling. There- fore, the critical part of speeding up the sampler is finding a sparse representation of the second term.</p><p>In the following sections, we show that natural, sparse prior knowledge representations are possi- ble. We first present an efficient sparse representa- tion of word correlation prior knowledge and then one for document-label knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Correlation Prior Knowledge</head><p>We now illustrate how we can encode word cor- relation knowledge as a set of sparse constraints f m (z, w, d) in our model. In previous work <ref type="bibr" target="#b0">(Andrzejewski et al., 2009;</ref><ref type="bibr" target="#b16">Hu et al., 2011;</ref><ref type="bibr" target="#b38">Xie et al., 2015)</ref>, word correlation prior knowledge is repre- sented as word must-link constraints and cannot- link constraints. A must-link relation between two words indicates that the two words tend to be re- lated to the same topics, i.e. their topic probabil- ities are correlated. In contrast, a cannot-link re- lation between two words indicates that these two words are not topically similar, and they should not both be prominent within the same topic. For example, "quarterback" and "fumble" are both re- lated to American football, so they can share a must-link relation. But "fumble" and "bank" im- ply two different topics, so they share a cannot- link. Let us say word w is associated with a set of prior knowledge correlations M w . Each prior knowledge m ∈ M w is a word pair (w, w ), and it has "topic preference" of w given its correla- tion word w . The must-link set of w is M m w , and the cannot-link set of w is M c w , i.e.,</p><formula xml:id="formula_5">M w = M c w M m</formula><p>w . In the example above, M m f umble = {quarterback}, and M c f umble = {bank}, so M f umble = {quarterback, bank}. The topic as- signment of word "fumble" has higher conditional probability for the same topics as "quarterback" but lower probability for topics containing "bank".</p><p>The potential score of sampling topic t for word type w-if M w is not empty-is</p><formula xml:id="formula_6">f m (z, w, d) = u∈M m w log max(λ, n u,z )+ v∈M c w log 1 max(λ, n v,z ) .<label>(6)</label></formula><p>where λ is a hyperparameter, which we call the correlation strength. The intuitive explanation of Equation 6 is that the prior knowledge about the word type w will make an impact on the condi- tional probability of sampling the hidden topic z.</p><p>Unlike standard LDA where every word's hidden topic is independent of other words given θ, Equa- tion 6 instead increases the probability that a word w will be drawn from the same topics as those of w's must-link word set, and decreases its probabil- ity of being drawn from the same topics as those of w's cannot-link word set. The hyperparameter λ controls the strength of each piece of prior knowledge. The smaller λ is, the stronger this correlation is. For large λ, the constraint is inactive for topics except those with the large counts. As λ decreases, the constraint becomes active for topics with lesser counts. We can adjust the value of λ for each piece of prior knowledge based on our confidence. In our exper- iments, for simplicity, we use the same value λ for all knowledge and set λ = 1.</p><p>From Equation 6 and Equation 5, the condi- tional probability of a topic z in document d given an observed word type w is:</p><formula xml:id="formula_7">P (z = t|w, z − , M ) ∝ αβ n t + V β + n d,t β n t + V β + (n d,t + α)n w,t n t + V β u∈M m w max(λ, n u,t ) v∈M c w 1 max(λ, n v,t )<label>(7)</label></formula><p>As explained above, λ controls the "strength" of the prior knowledge term. If λ is large, the prior knowledge has little impact on the conditional probability of topic assignments.</p><p>Let's return to the question whether Equation 6 is sparse, allowing efficient computation of Equa- tion 7. Fortunately, n u,t and n v,t , which are the <ref type="figure">Figure 1</ref>: Histogram of nonzero topic counts for word types in NYT-News dataset after inference. 81.9% word types have fewer than 50 topics with nonzero counts. This sparsity allows our sparse constraints to speed inference. topic counts for must-link word u and cannot- link word v, are often sparse. For example, in a 100-topic model trained on the NIPS dataset, 87.2% of word types have fewer than ten top- ics with nonzero counts. In a 500-topic model trained on a larger dataset like the New York Times News <ref type="bibr" target="#b35">(Sandhaus, 2008)</ref>, 81.9% of word types have fewer than 50 topics with nonzero counts. Moreover, the model becomes increasingly sparse with additional Gibbs iterations. <ref type="figure">Figure 1</ref> shows the word frequency histogram of nonzero topic counts of NYT-News dataset.</p><p>Therefore, the computational cost of Equation 7 can be reduced. SparseLDA efficiently computes the s, r, q bins as in Equation 3. Then for words that are associated with prior knowledge, we up- date s, r, q with an additional potential term. We only need to compute the potential term for the topics whose counts are greater than λ. The col- lapsed Gibbs sampling procedure is summarized in Algorithm 1.</p><p>Algorithm 1 Gibbs Sampling for word type w in document d, given w's correlation set M w 1: compute s t , r t , q t with SparseLDA, (see Eq.</p><p>3) 2: for t ← 0 to T do 3: update s t , r t , q t . ∀u ∈ M w if n u,t &gt; λ 4: end for 5: p(t) = s t + r t + q t 6: sample new topic assignment for w from p(t)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">311</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Other Types of Prior Knowledge</head><p>The factor model framework can also handle other types of prior knowledge, such as document la- bels, sentence labels, and document link relations. We briefly describe document labels here. <ref type="bibr" target="#b33">Ramage et al. (2009)</ref> propose Labeled-LDA, which improves LDA with document labels. It as- sumes that there is a one-to-one mapping between topics and labels, and it restricts each document's topics to be sampled only from those allowed by the documents label set. Therefore, Labeled-LDA can be expressed in our model. We define</p><formula xml:id="formula_8">f m (z, w, d) = 1, if z ∈ m d −∞, else<label>(8)</label></formula><p>where m d specifies document d's label set con- verted to corresponding topic labels. Since f m (z, w, d) is sparse, we can speed up the train- ing as well. Sentence-level prior knowledge (e.g., for sentiment or aspect models (Paul and Girju, 2010)) can be defined in a similar way.</p><p>Documents can be associated with other useful metadata. For example, a scientific paper and the prior work it cites might have similar topics <ref type="bibr" target="#b10">(Dietz et al., 2007</ref>) or friends in a social network might talk about the same topics . To model link relations, we can use Equa- tion 6 and replace the word-topic counts n v,z with document-topic counts n d,z . By doing so, we en- courage related documents to have similar topic structures. Moreover, the document-topic count is also sparse, which fits into the efficient learning framework.</p><p>Therefore, for different types of prior knowl- edge, as long as we can define ψ(z, M ) appropri- ately so that f (z, w, d) are sparse, we are able to speed up learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we demonstrate the effectiveness of our SC-LDA by comparing it with several baseline methods on three benchmark datasets. We first evaluate the convergence rate of each method and then evaluate the learned model parameter φ-the topic-word distribution-in terms of topic coher- ence. We show that SC-LDA can achieve results comparable to the baseline models but is signifi- cantly faster. We set up all experiments on a 8- Core 2.8GHz CPU, 16GB RAM machine. 2 <ref type="bibr">2</ref> Our implementation of SC-LDA is avail- able at https://github.com/yya518 <ref type="table" target="#tab_0">/   DATASET   DOCS  TYPE  TOKEN(APPROX)   NIPS  1,500  12,419  1,900,000  NYT-NEWS 3,000,000 102,660  100,000,000  20NG</ref> 18,828 21,514 1,946,000 <ref type="table">Table 1</ref>: Characteristics of benchmark datasets. We use NIPS and NYT for word correlation exper- iments and 20NG for document label experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We use the NIPS and NYT-News datasets from the UCI bag of words data collections. 3 These two datasets have no document labels, and we use them for word correlation experiments. We also use the 20Newsgroup (20NG) dataset, 4 which has document labels, for document label experiments. <ref type="table">Table 1</ref> shows the characteristics of each dataset.</p><p>Since NIPS and NYT-News have already been pre- processed, to ensure repeatability, we use the data "as they are" from the sources. For 20NG, we perform tokenization and stopword removal using Mallet <ref type="bibr" target="#b20">(McCallum, 2002</ref>) and remove words that appear fewer than 10 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prior Knowledge Generation</head><p>Word Correlation Prior Knowledge Previous work proposes two methods to automatically gen- erate prior word correlation knowledge from ex- ternal sources. Hu and Boyd-Graber (2012) use WordNet 3.0 to obtain synsets for word types, and then if a synset is also in the vocabulary, they add a must-link correlation between the word type and the synset. <ref type="bibr" target="#b38">Xie et al. (2015)</ref> use a different method that takes advantage of an existing pre- trained word embedding. Each word embedding is a real-valued vector capturing the word's semantic meaning based on distributional similarity. If the similarity between the embeddings of two word types in the vocabulary exceeds a threshold, they generate a must-link between the two words.</p><p>In our experiments, we adopt a hybrid method that combines the above two methods. For a noun word type, we first obtain its synsets from Word- Net 3.0. We also obtain the embeddings of each word from word2vec ( <ref type="bibr" target="#b21">Mikolov et al., 2013)</ref>. If the synset is also in the vocabulary, and the similar- ity between the synset and the word is higher than a threshold, which in our experiment is 0.2, we generate a must-link between thee words. Empir-</p><formula xml:id="formula_9">sparse-constrained-lda.</formula><p>ically, this hybrid method is able to obtain high quality correlated words. For example, for the NIPS dataset, the must-links we obtain for ran- domness are {noise, entropy, stochasticity}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Label Prior Knowledge</head><p>Since doc- uments in the 20NG dataset are associated with labels, we use the labels directly as prior knowl- edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>The baseline methods for incorporating word cor- relation prior knowledge in our experiments are as follows: DF-LDA: incorporates word must-links and cannot-links using a Dirichlet Forest prior in LDA ( <ref type="bibr" target="#b0">Andrzejewski et al., 2009</ref>). Here we use Hu and Boyd-Graber (2012)'s efficient implementa- tion FAST-RB-SDW for DF-LDA. Logic-LDA: encodes general domain knowledge as first-order logic and incorporates it in LDA <ref type="bibr" target="#b1">(Andrzejewski et al., 2011</ref>). Logic-LDA has been used for word correlations and document label knowledge. MRF-LDA: encodes word correlations in LDA as a Markov random field ( <ref type="bibr" target="#b38">Xie et al., 2015</ref>).</p><p>We also use Mallet's SparseLDA implementa- tion for vanilla LDA in the topic coherence exper- iment. We use a symmetric Dirichlet prior for all models. We set α = 1.0, β = 0.01. For DF-LDA, η = 100. For Logic-LDA, we use the default pa- rameter setting in the package: a sample rate of 1.0 and step rate of 10.0. For MRF-LDA, we use the default setting with γ = 1.0. (Parameter se- mantics can be found in the original papers.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Convergence</head><p>The main advantage of our method over other ex- isting methods is efficiency. In this experiment, we show the change of our model's log likelihood over time. In topic models, the log likelihood change is a good indicator of whether a model has converged or not. <ref type="figure">Figure 2</ref> shows the log like- lihood change over time for SC-LDA and three baseline methods on NIPS and NYT-News dataset. SC-LDA converges faster than all the other meth- ods.</p><p>We also conduct experiments on SC-LDA with varying numbers of word correlations. <ref type="table" target="#tab_0">Table 2</ref> shows the Gibbs sampling iteration time on the 1st, 50th, 100th and the 200th iteration. We also incorporate different numbers of word correlations <ref type="figure">Figure 2</ref>: Models' log likelihood convergence on NIPS dataset (above) and NYT-News dataset (be- low). For NIPS, a 100-topic model with 100 must-links is trained. For NYT-News, a 500- topic model with 100 must-links is trained. SC- LDA reaches likelihood convergence much more rapidly than the other methods.  in SC-LDA. SC-LDA runs faster as sampling pro- ceeds as the sparsity increases, but additional cor- relations slow the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Topic Coherence</head><p>Topic models are often evaluated using perplex- ity on held-out test data, but this evaluation is of- 6 313 ten at odds with human evaluations ( . Following <ref type="bibr" target="#b23">Mimno et al. (2011)</ref>, we em- ploy Topic Coherence-a metric that is consis- tent with human judgment-to measure a topic model's quality. Topic t's coherence is defined as C(t :</p><formula xml:id="formula_10">V (t) ) = M m=2 m−1 l=1 log F (v (t) m ,v (t) l )+ F (v (t) l )</formula><p>, where F (v) is the document frequency of word type v, F (v, v ) is the co-document frequency of word type v and v , and</p><formula xml:id="formula_11">V (t) = (v (t) 1 , ..., v (t)</formula><p>M ) is a list of the M most probable words in topic t.</p><p>In our experiments, we choose the ten words with highest probability in the topic to compute topic coherence, i.e., M = 10.  <ref type="formula">)</ref> improves coherence stability, so we set = 10 −12 . Larger topic coherence scores imply more coherent topics.</p><p>We train a 500-topic model on the NIPS dataset with different methods and compare the average topic coherence score and the average of the top twenty topic coherence scores. Since the topics learned by topic model often contain "bad" top- ics (Mimno et al., 2011) which do not make sense to end users, evaluating the top twenty topics re- flects the model's performance. We let each model train for one hour. <ref type="figure" target="#fig_2">Figure 3</ref> shows the topic co- herence of each method. SC-LDA has about the same average topic coherence with LDA but has higher coherence score (-36.6) for the top 20 top- ics than LDA (-39.1). This is because incorporat- ing word correlation knowledge encourages cor- related words to have high probability under the same topic, thus improving the coherence score. For the other methods, however, because they can- not converge within an hour, their topic coherence scores are much worse than SC-LDA and LDA. This again demonstrates the efficiency of SC-LDA over other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Document Label Prior Knowledge</head><p>SC-LDA can also handle other types of prior knowledge. We compare it with Labeled-LDA ( <ref type="bibr" target="#b33">Ramage et al., 2009</ref>). Labeled-LDA also uses Gibbs sampling for inference, allowing direct computation time comparisons. <ref type="table" target="#tab_2">Table 3</ref> shows the average running time per it- eration for Labeled-LDA and SC-LDA. Because document labels apply sparsity to the document- topic counts, the average running time per itera- tion decreases as the number of labeled document increases. SC-LDA exhibits greater speedup with   more topics; when T = 500, 5 SC-LDA runs more than ten times faster than Labeled-LDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Topics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>This works brings together two lines of research: incorporating rich knowledge into probabilistic models and efficient inference of probabilistic models on large datasets. Both are common ar- eas of interest across many machine learning for- malisms: probabilistic logic ( <ref type="bibr" target="#b2">Bach et al., 2015)</ref>, graph algorithms ( <ref type="bibr" target="#b19">Low et al., 2012)</ref>, and proba- bilistic grammars <ref type="bibr" target="#b8">(Cohen et al., 2008)</ref>. However, our focus in this paper is the intersection of these lines of research with topic models.</p><p>Adding knowledge and metadata to topic mod- els makes the models richer, more understandable, and more domain-specific. A common distinc- tion is upstream (conditioning on metadata) vs. downstream models (conditioning on variables al- ready present in a topic model to predict meta- data) <ref type="bibr" target="#b22">(Mimno et al., 2008</ref>). Downstream models are typically better at prediction tasks such as pre- dicting sentiment <ref type="bibr" target="#b3">(Blei and McAuliffe, 2007)</ref>, ide- ology ( <ref type="bibr" target="#b28">Nguyen et al., 2014a)</ref>, or links in a social network . In contrast, our approach-an upstream model-is often easier to implement and leads to more interpretable topics. Upstream models at the document level have been used to understand the labels in large document collections <ref type="bibr" target="#b33">(Ramage et al., 2009;</ref><ref type="bibr" target="#b29">Nguyen et al., 2014b</ref>) and capture relationships in document net- works using Markov random fields <ref type="bibr" target="#b9">(Daumé III, 2009)</ref>. At the word level, <ref type="bibr" target="#b38">Xie et al. (2015)</ref> in- corporate word correlation to LDA by building a Markov Random Field regularization, similar to <ref type="bibr" target="#b27">Newman et al. (2011)</ref>, who use regularization to improve topic coherence. However, despite these exciting applications, the experiments in the above work are typically on small datasets.</p><p>In contrast, there is a huge interest in improving the scalability of topic models to large numbers of documents, numbers of topics, and vocabular- ies. Attempts to scale inference for topic mod- els have started from both variational inference and Gibbs sampling-two popular learning infer- ence techniques for topic modeling. Gibbs sam- pling is a popular technique because of its sim- plicitly and low latency. However, for large num- bers of topics, Gibbs sampling can become un- wieldy. <ref type="bibr" target="#b32">Porteous et al. (2008)</ref> address this issue by creating an upper bound approximation that pro- duces accurate results, while SparseLDA ( <ref type="bibr" target="#b40">Yao et al., 2009</ref>) present an effective factorization that speeds inference without sacrificing accuracy. Just as our model builds on SparseLDA's insights, SparseLDA has been incorporated into commer- cial deployments ( <ref type="bibr" target="#b37">Wang et al., 2014</ref>) and im- proved using alias tables ( <ref type="bibr" target="#b18">Li et al., 2014</ref>). <ref type="bibr" target="#b41">Yuan et al. (2015)</ref> also presents an efficient constant time sampling algorithm for building big topic models. Variational inference can easily be paral- lelized ( <ref type="bibr" target="#b25">Nallapati et al., 2007;</ref><ref type="bibr" target="#b42">Zhai et al., 2012</ref>), but has high latency, which has been addressed by performing online updates <ref type="bibr" target="#b14">(Hoffman et al., 2010)</ref> and taking stochastic gradients estimated by MCMC inference <ref type="bibr" target="#b24">(Mimno et al., 2012)</ref>. In this paper, we only focus on single-processor learning, but existing parallelization techniques <ref type="bibr" target="#b26">(Newman et al., 2009)</ref> are applicable to our model. At the intersection lies models that improve the scalability of upstream topic model inference. In addition to our SC-LDA, Hu and Boyd-Graber (2012) speed Gibbs sampling in tree-based topic models using SparseLDA's factorization strategy, and <ref type="bibr" target="#b17">Hu et al. (2014)</ref> extend this approach by paral- lelizing global parameter updates using variational inference. Our work is more general (also encom- passing document-based constraints) and is faster. In contrast to these upstream models, <ref type="bibr" target="#b43">Zhu et al. (2013)</ref> and <ref type="bibr" target="#b30">Nguyen et al. (2015)</ref> improve inference of downstream models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a factor graph framework for incorpo- rating prior knowledge into topic models. By ex- pressing the prior knowledge as sparse constraints on the hidden topic variables, we are able to take advantage of the sparsity to speed up training. We demonstrate in experiments that our model runs significantly faster than the other alternative mod- els and achieves comparable performance in terms of topic coherence. Efficient algorithms for incor- porating prior knowledge with large topic models will benefit several downstream applications. For example, interactive topic modeling becomes fea- sible because fast model updates reduce the user's waiting time and thus improve the user experience. Personalized topic modeling is also an interesting future direction in which the model will generate a personalized topic structure based on the user's preferences or interests. For all these applications, an efficient learning algorithm is a crucial prereq- uisite.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Mimno et al. (2011) use = 1, but Röder et al. (2015) show smaller (such as 10 −12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average topic coherence and average top 20 topic coherence. The models are trained on NIPS dataset with 500-topic and 100 word correlations. SC-LDA achieves higher topic coherence than other methods.</figDesc><graphic url="image-4.png" coords="7,307.28,62.80,218.27,161.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>SC-LDA runtime (in seconds) in the 
1st, 50th, 100th, and 200th iteration with different 
numbers of correlations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The average running time per iteration 
over 100 iterations, averaged over 5 seeds, on 
20NG dataset. Experiments begin with 100 top-
ics, 1000 labeled documents, and then vary one 
dimension: number of topics (top), or number of 
labeled documents (bottom). 

</table></figure>

			<note place="foot" n="1"> In (Andrzejewski et al., 2009) two correlated words are taken to indicate that p(v|z) ≈ p(w|z). However, for word types that have very different frequencies, these two quantities would never be close, and thus p(z|v) ≈ p(z|w) is a more intuitive constraint. 2</note>

			<note place="foot" n="3"> https://archive.ics.uci.edu/ml/datasets/Bag+of+Words 4 http://qwone.com/ jason/20Newsgroups/ 5</note>

			<note place="foot" n="5"> For 20NG dataset, it may overfit the data with 500 topics, but here we use it to demonstrate the scalability. 7</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviews for their helpful comments. This research was supported in part by NSF grant IIS-1351029 and DARPA contract D11AP00268. Boyd-Graber is supported by NSF Grants CCF-1409287, IIS-1320538, and NCSE-1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via Dirichlet forest priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for incorporating general domain knowledge into latent Dirichlet allocation using first-order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Paired-dual learning for fast training of latent variable hinge-loss mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A topic model for word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relational topic models for document networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics</title>
		<meeting>Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Logistic normal priors for unsupervised probabilistic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Markov random topic fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics</title>
		<meeting>Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised prediction of citation influences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient methods for inferring large sparse topic hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="452" to="472" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient tree-based topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Polylingual tree-based topic models for translation domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing the sampling complexity of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed graphlab: A framework for machine learning and data mining in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.cs.umass.edu/mccallum/mallet" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gibbs sampling for logistic normal topic models with graph-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2008 Workshop on Analyzing Graphs: Theory and Applications</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparse stochastic inference for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parallelized variational EM for latent Dirichlet allocation: An experimental evaluation of speed and scalability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed Algorithms for Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1801" to="1828" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving topic coherence with regularized topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling topic control to detect influence in conversations using nonparametric topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanxin</forename><surname>Midberry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="381" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a concept hierarchy from multi-labeled documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Viet-An Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Is your anchor going up or down? Fast and accurate supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A twodimensional topic-aspect model for discovering multi-faceted topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast collapsed gibbs sampling for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Labeled LDA: A supervised topic model for credit attribution in multilabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Web Search and Data Mining</title>
		<meeting>ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The New York Times annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards topic modeling for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenlong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><surname>Law</surname></persName>
		</author>
		<idno>abs/1405.4402</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Incorporating word correlation knowledge into topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">User-directed non-disruptive topic model update for effective exploration of dynamic content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercan</forename><surname>Topkara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Intelligent User Interfaces</title>
		<meeting>the 20th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lightlda: Big topic models on modest computer clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Po</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><surname>Alkhouja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Wide Web Conference</title>
		<meeting>the World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gibbs max-margin topic models with fast sampling algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
