<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Updating of Word Representations for Part-of-Speech Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<email>wenpeng@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LMU Munich</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LMU Munich</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LMU Munich</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">LMU Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LMU Munich</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
								<orgName type="institution" key="instit3">LMU Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Online Updating of Word Representations for Part-of-Speech Tagging</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose online unsupervised domain adaptation (DA), which is performed in-crementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised domain adaptation is a scenario that practitioners often face when having to build ro- bust NLP systems. They have labeled data in the source domain, but wish to improve performance in the target domain by making use of unlabeled data alone. Most work on unsupervised domain adaptation in NLP uses batch learning: It assumes that a large corpus of unlabeled data of the tar- get domain is available before testing. However, batch learning is not possible in many real-world scenarios where incoming data from a new target domain must be processed immediately. More im- portantly, in many real-world scenarios the data does not come with neat domain labels and it may not be immediately obvious that an input stream is suddenly delivering data from a new domain.</p><p>Consider an NLP system that analyzes emails at an enterprise. There is a constant stream of in- coming emails and it changes over time -without any clear indication that the models in use should be adapted to the new data distribution. Because the system needs to work in real-time, it is also de- sirable to do any adaptation of the system online, without the need of stopping the system, changing it and restarting it as is done in batch mode.</p><p>In this paper, we propose online unsupervised domain adaptation as an extension to traditional unsupervised DA. In online unsupervised DA, do- main adaptation is performed incrementally as data comes in. Specifically, we adopt a form of representation learning. In our experiments, the incremental updating will be performed for repre- sentations of words. Each time a word is encoun- tered in the stream of data at test time, its repre- sentation is updated.</p><p>To the best of our knowledge, the work re- ported here is the first study of online unsuper- vised DA. More specifically, we evaluate online unsupervised DA for the task of POS tagging. We compare POS tagging results for three distinct ap- proaches: static (the baseline), batch learning and online unsupervised DA. Our results show that online unsupervised DA is comparable in perfor- mance to batch learning while requiring no retrain- ing or prior data in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental setup</head><p>Tagger. We reimplemented the FLORS tagger ( <ref type="bibr" target="#b12">Schnabel and Schütze, 2014</ref>), a fast and simple tagger that performs well in DA. It treats POS tag- ging as a window-based (as opposed to sequence classification), multilabel classification problem. FLORS is ideally suited for online unsupervised DA because its representation of words includes distributional vectors -these vectors can be easily updated in both batch learning and online unsu- pervised DA. More specifically, a word's represen- tation in FLORS consists of four feature vectors: one each for its suffix, its shape and its left and right distributional neighbors. Suffix and shape features are standard features used in the litera- ture; our use of them is exactly as described by <ref type="bibr" target="#b12">Schnabel and Schütze (2014)</ref>.</p><p>Distributional features. The i th entry x i of the left distributional vector of w is the weighted num- ber of times the indicator word c i occurs immedi- ately to the left of w:</p><p>x i = tf (freq (bigram(c i , w))) where c i is the word with frequency rank i in the corpus, freq (bigram(c i , w)) is the number of oc- currences of the bigram "c i w" and we weight non-  zero frequencies logarithmically: tf(x) = 1 + log(x). The right distributional vector is defined analogously. We restrict the set of indicator words to the n = 500 most frequent words. To avoid zero vectors, we add an entry x n+1 to each vector that counts omitted contexts:</p><formula xml:id="formula_0">x 501 = tf( j:j&gt;n freq (bigram(c j , w))) Let f (w)</formula><p>be the concatentation of the two dis- tributional and suffix and shape vectors of word w. Then FLORS represents token v i as follows:</p><formula xml:id="formula_1">f (v i−2 )⊕f (v i−1 )⊕f (v i )⊕f (v i+1 )⊕f (v i+2 )</formula><p>where ⊕ is vector concatenation. FLORS then tags token v i based on this representation.</p><p>FLORS assumes that the association between distributional features and labels does not change fundamentally when going from source to target. This is in contrast to other work, notably <ref type="bibr" target="#b0">Blitzer et al. (2006)</ref>, that carefully selects "stable" dis- tributional features and discards "unstable" dis- tributional features. The hypothesis underlying FLORS is that basic distributional POS properties are relatively stable across domains -in contrast to semantic and other more complex tasks. The high performance of FLORS ( <ref type="bibr" target="#b12">Schnabel and Schütze, 2014)</ref> suggests this hypothesis is true.</p><p>Data. Test set. We evaluate on the development sets of six different TDs: five SANCL ( <ref type="bibr" target="#b10">Petrov and McDonald, 2012</ref>) domains -newsgroups, we- blogs, reviews, answers, emails -and sections 22- 23 of WSJ for in-domain testing.</p><p>We use two training sets of different sizes. In condition l:big (big labeled data set), we train FLORS on sections 2-21 of Wall Street Journal (WSJ). Condition l:small uses 10% of l:big.</p><p>Data for word representations. We also vary the size of the datasets that are used to compute the word representations before the FLORS model is trained on the training set. In condition u:big, we compute distributional vectors on the joint corpus of all labeled and unlabeled text of source and tar- get domains (except for the test sets). We also include 100,000 WSJ sentences from 1988 and 500,000 sentences from Gigaword <ref type="bibr" target="#b9">(Parker, 2009)</ref>. In condition u:0, only labeled training data is used.</p><p>Methods. We implemented the following mod- ification compared to the setup in ( <ref type="bibr" target="#b12">Schnabel and Schütze, 2014</ref>): distributional vectors are kept in memory as count vectors. This allows us to in- crease the counts during online tagging.</p><p>We run experiments with three versions of FLORS: STATIC, BATCH and ONLINE. All three methods compute word representations on "data for word representations" (described above) be- fore the model is trained on one of the two "train- ing sets" (described above).</p><p>STATIC. Word representations are not changed during testing.</p><p>BATCH. Before testing, we update count vectors by freq (bigram(c i , w)) += freq * (bigram(c i , w)), where freq * (·) denotes the number of occurrences of the bigram "c i w" in the entire test set.</p><p>ONLINE. Before tagging a test sentence, both left and right distributional vectors are updated via freq (bigram(c i , w)) += 1 for each appearance of bigram "c i w" in the sentence. Then the sentence is tagged using the updated word representations. As tagging progresses, the distributional represen- tations become increasingly specific to the target domain (TD), converging to the representations that BATCH uses at the end of the tagging process.</p><p>In all three modes, suffix and shape features are always fully specified, for both known and un- known words.   <ref type="table" target="#tab_2">Table 2</ref> investigates the effect of sizes of labeled and unlabeled data on performance of ONLINE and BATCH. We report accuracy for all (ALL) to- kens, for tokens occurring in both l:big and l:small (KN), tokens occurring in neither l:big nor l:small (OOV) and tokens ocurring in l:big, but not in l:small (SHFT). 1 Except for some minor variations in a few cases, both using more labeled data and using more unlabeled data improves tagging accu- racy for both ONLINE and BATCH. ONLINE and BATCH are generally better or as good as STATIC (in bold), always on ALL and OOV, and with a few exceptions also on KN and SHFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head><p>ONLINE performance is comparable to BATCH performance: it is slightly worse than BATCH on u:0 (largest ALL difference is .29) and at most .02 different from BATCH for ALL on u:big. We ex-  <ref type="table">Table 3</ref>: Error rates (err) and standard deviations (std) for tagging. † (resp. * ): significantly different from ONLINE error rate above&amp;below (resp. from "u:0" error rate to the left).</p><p>plain below why ONLINE is sometimes (slightly) better than BATCH, e.g., for ALL and condition l:small/u:big.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Time course of tagging accuracy</head><p>The ONLINE model introduced in this paper has a property that is unique compared to most other work in statistical NLP: its predictions change as it tags text because its representations change.</p><p>To study this time course of changes, we need a large application domain because subtle changes will be too variable in the small test sets of the SANCL TDs. The only labeled domain that is big enough is the WSJ corpus. We therefore reverse the standard setup and train the model on the dev sets of the five SANCL domains (l:big) or on the first 5000 labeled words of reviews (l:small). In this reversed setup, u:big uses the five unlabeled SANCL data sets and Gigaword as before. Since variance of performance is important, we run on 100 randomly selected 50% samples of WSJ and report average and standard deviation of tagging error over these 100 trials.</p><p>The results in <ref type="table" target="#tab_2">Table 3 2</ref> show that error rates are only slightly worse for ONLINE than for BATCH or the same. In fact, l:small/u:0 known error rate (.1186) is lower for ONLINE than for BATCH (sim- ilar to what we observed in <ref type="table" target="#tab_2">Table 2</ref>). This will be discussed at the end of this section. <ref type="table">Table 3</ref> includes results for "unseens" as well as unknowns because <ref type="bibr" target="#b12">Schnabel and Schütze (2014)</ref> show that unseens cause at least as many errors as unknowns. We define unseens as words with a tag that did not occur in training; we compute unseen error rates on all occurrences of unseens, i.e., occurrences with both seen and unseen tags are included. As <ref type="table">Table 3</ref> shows, the error rate for unknowns is greater than for unseens which is in turn greater than the error rate on known words. <ref type="bibr">2</ref> Significance test: test of equal proportion, p &lt; .05</p><p>Examining the single conditions, we can see that ONLINE fares better than STATIC in 10 out of 12 cases and only slightly worse for l:small/u:big (unseens, known words: .1086 vs .1084, .0802 vs .0801). In four conditions it is significantly better with improvements ranging from .005 (.1404 vs .1451: l:big/u:0, unknown words) to &gt;.06 (.3094 vs .3670: l:small/u:0, unknown words).</p><p>The differences between ONLINE and STATIC in the other eight conditions are negligible. For the six u:big conditions, this is not surprising: the Gigaword corpus consists of news, so the large un- labeled data set is in reality the same domain as WSJ. Thus, if large unlabeled data sets are avail- able that are similar to the TD, then one might as well use STATIC tagging since the extra work re- quired for ONLINE/BATCH is unlikely to pay off.</p><p>Using more labeled data (comparing l:small and l:big) always considerably decreases error rates. We did not test for significance here because the differences are so large. By the same token, us- ing more unlabeled data (comparing u:0 and u:big) also consistently decreases error rates. The differ- ences are large and significant for ONLINE tagging in all six cases (indicated by * in the table).</p><p>There is no large difference in variability ON- LINE vs. BATCH (see columns "std"). Thus, given that it has equal variability and higher perfor- mance, ONLINE is preferable to BATCH since it assumes no dataset available prior to the start of tagging. <ref type="figure">Figure 1</ref> shows the time course of tagging ac- curacy. 3 BATCH and STATIC have constant error rates since they do not change representations dur- ing tagging. ONLINE error decreases for unknown words -approaching the error rate of BATCH -as Interestingly the error of ONL NE increases for unseens and known words (middle&amp;bottom pan- els) (even though it is always below the error rate of BATCH) The reason is that the BATCH update swamps the original training data for l:small/u:0 because the WSJ test set is bigger by a large fac- tor than the training set ONL NE fares better here because in the beginning of tagging the updates of the distributional representations consist of small increments We noticed this in <ref type="table" target="#tab_2">Table 2</ref> too: there ONL NE outperformed BATCH in some cases on KN for l:small/u:big In future work we plan to investigate how to weight distributional counts from the target data relative to that from the (la- beled und unlabeled) source data</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Online learning usually refers to supervised learn- ing algorithms that update the model each time after processing a few training examples Many supervised learning algorithms are online or have online versions Active learning ( <ref type="bibr" target="#b8">Lewis and Gale 1994;</ref><ref type="bibr" target="#b14">Tong and Koller 2001;</ref><ref type="bibr" target="#b7">Laws et al 2011)</ref> is another supervised learning framework that pro- cesses training examples -usually obtained inter- actively -in small batches ( <ref type="bibr" target="#b1">Bordes et al 2005)</ref> All of this work on supervised online learning is not directly relevant to this paper since we ad- dress the problem of unsupervised DA Unlike on- line supervised learners we keep the statistical model unchanged during DA and adopt a repre- sentation learning approach: each unlabeled con- text of a word is used to update its representation There is much work on unsupervised DA for POS tagging including work using constraint- based methods <ref type="figure">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced online updating of word represen- tations a new domain adaptation method for cases where target domain data are read from a stream and BATCH processing is not possible We showed that online unsupervised DA performs as well as batch learning It also significantly lowers error rates compared to STAT C (i e no domain adapta- tion) Our implementation of FLORS is available at cistern.cis.lmu.de/flors</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>newsgroups</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F</head><label></label><figDesc>gu e 1 E o a es o unknown wo ds wo ds w h unseen ags and known wo ds o sma u 0 The x ax s ep esen s he numbe o okens o he espec ve ype e g numbe o okens o unknown wo ds more and more is learned with each additional oc- currence of an unknown word (top)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Subramanya et al 2010; Rush et al 2012) instance weighting (Choi and Palmer 2012) self-training (Huang et al 2009; Huang and Yates 2010) and co-training (Kübler and Baucom 2011) All of this work uses batch learn- ing For space reasons we do not discuss super- vised DA (e g Daumé III and Marcu (2006))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 compares</head><label>1</label><figDesc></figDesc><table>performance on SANCL for a 
number of baselines and four versions of FLORS: 
S&amp;S, Schnabel and Schütze (2014)'s version of 
FLORS, "S&amp;S (reimpl.)", our reimplementation 
of that version, and BATCH and ONLINE, the two 
versions of FLORS we use in this paper. Compar-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>ONLINE / BATCH accuracies are generally better than STATIC (see bold numbers) and improve with both more training 
data and more unlabeled data. 

ing lines "S&amp;S" and "S&amp;S (reimpl.)" in the ta-
ble, we see that our reimplementation of FLORS 
is comparable to S&amp;S's. For the rest of this pa-
per, our setup for BATCH and ONLINE differs from 
S&amp;S's in three respects. (i) We use Gigaword as 
additional unlabeled data. (ii) When we train a 
FLORS model, then the corpora that the word rep-
resentations are derived from do not include the 
test set. The set of corpora used by S&amp;S for this 
purpose includes the test set. We make this change 
because application data may not be available at 
training time in DA. (iii) The word representations 
used when the FLORS model is trained are derived 
from all six SANCL domains. This simplifies the 
experimental setup as we only need to train a sin-
gle model, not one per domain. Table 1 shows that 
our setup with these three changes (lines BATCH 
and ONLINE) has state-of-the-art performance on 
SANCL for domain adaptation (bold numbers). 

</table></figure>

			<note place="foot" n="1"> We cannot give the standard, single OOV evaluation number here since OOVs are different in different conditions, hence the breakdown into three measures.</note>

			<note place="foot" n="3"> In response to a reviewer question, the initial (leftmost) errors of ONLINE and STATIC are not identical; e.g., ONLINE has a better chance of correctly tagging the very first occurrence of an unknown word because that very first occurrence has a meaningful (as opposed to random) distributed representation.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast kernel classifiers with online and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyda</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1579" to="1619" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and robust part-of-speech tagging using dynamic model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Short Papers</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="363" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring representation-learning approaches to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DANLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving a simple bigram HMM part-of-speech tagger by latent annotation and selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT: Short Papers</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast domain adaptation for part of speech tagging for dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Baucom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active learning with Amazon Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Laws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">English gigaword fourth edition. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overview of the 2012 shared task on parsing the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved parsing and POS tagging using inter-sentence consistency constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1434" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FLORS: Fast and simple domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient graph-based semisupervised learning of structured tagging models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
