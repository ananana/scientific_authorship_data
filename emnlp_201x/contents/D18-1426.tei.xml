<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Natural Language Generation with Denoising Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">Roy</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
						</author>
						<title level="a" type="main">Unsupervised Natural Language Generation with Denoising Autoencoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3922" to="3929"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3922</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denois-ing auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Generation (NLG) is the task of generating text from structured data. Recent success in Deep Learning motivated researchers to use neural networks instead of human designed rules and templates to generate meaningful sen- tences from structured information. However, these supervised models work well only when pro- vided either massive amounts of labeled data or when restricted to a limited domain. Unfortu- nately, labeled examples are costly to obtain and are non-existent for many domains. Conversely, large amount of unlabeled data are often freely available in many languages.</p><p>A labeled example is given in <ref type="table" target="#tab_0">Table 1</ref>. One la- beled example consists of a set of slot pairs and at least one golden target sequence. Each slot pair has a slot name (e.g. "name") and a slot value (e.g. "Loch Fyne"). In this work, we present an unsupervised NLG approach that learns its pa- rameters without the slot pairs on target sequences only. We use the approach of a denoising auto- encoder (DAE) <ref type="bibr" target="#b20">(Vincent et al., 2008</ref>) to train our name type food family friendly Loch Fyne restaurant Indian yes model. During training, we use corrupt versions of each target sequence as input and learn to re- construct the correct sequence using a sequence- to-sequence network <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2015)</ref>. We show how to introduce noise into the training data in such a way that the resulting DAE is capable of generating sentences out of a set of slot pairs. Taking advantage of using unlabeled data only, we also incorporate out-of-domain data into the training process to improve the quality of the generated text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network</head><p>In all our experiments, we use our in-house attention-based sequence-to-sequence (seq2seq) implementation which is similar to <ref type="bibr">Bahdanau et al. (2015)</ref>. The approach is based on an encoder- decoder network. The encoder employs a bi- directional RNN to encode the input words x = (x 1 , ..., x l ) into a sequence of hidden states h = (h 1 , ..., h l ), where l is the length of the input se- quence. Each h i is a concatenation of a left-to- right − → h i and a right-to-left ← − h i RNN:</p><formula xml:id="formula_0">h i = ← − h i − → h i = ← − f (x i , ← − h i+1 ) − → f (x i , − → h i−1 )</formula><p>where ← − f and − → f are two gated recurrent units (GRU) proposed by <ref type="bibr" target="#b1">Cho et al. (2014)</ref>. Given the encoded h, the decoder predicts the target sentence by maximizing the conditional log- probability of the correct output y * = (y * 1 , ...y * m ), where m is the length of the target. At each time t, the probability of each word y t from a target vocabulary V y is:</p><formula xml:id="formula_1">p(y t |h, y * t−1 ..y * 1 ) = g(s t , y * t−1 , H t ),<label>(1)</label></formula><p>where g is a two layer feed-forward neural net- work over the embedding of the previous target word y * t−1 , the hidden state s t , and the weighted sum of h (H t ).</p><p>Before we compute s t and H t , we first covert s t−1 and the embedding of y * t−1 into an intermedi- ate state s t with a GRU u as:</p><formula xml:id="formula_2">s t = u(s t−1 , y * t−1 ).<label>(2)</label></formula><p>Then we have s t as:</p><formula xml:id="formula_3">s t = q(s t , H t )<label>(3)</label></formula><p>where q is a GRU, and the H t is computed as:</p><formula xml:id="formula_4">H t = l i=1 (α t,i · ← − h i ) l i=1 (α t,i · − → h i ) ,<label>(4)</label></formula><p>The attention weights, α in H t , are computed with a two layer feed-forward neural network r:</p><formula xml:id="formula_5">α t,i = exp{r(s t , h i )} l j=1 exp{r(s t , h j )}<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Approach</head><p>Our unsupervised model is based on the same training idea as a denoising auto-encoder (DAE) similar to <ref type="bibr" target="#b20">Vincent et al. (2008)</ref>. The original DAEs were feedforward nets applied to (image) data.</p><p>In our experiments, the model architecture is a seq2seq model similar to <ref type="bibr">Bahdanau et al. (2015)</ref>. The idea of a DAE is to train a model that is able to reconstruct each training example from a par- tially destroyed input. This is done by first cor- rupting each training sequence x i to get a partially destroyed versioñ x i . In our unsupervised experiments, we generate the training data with the following corrupting process, parameterized by the desired percentage p of deletion: for each target sequence x i , a fixed percentage p of words are removed at random, while the others are left untouched. We sample a new corrupt versioñ x i in each training epoch.</p><p>Instead of always removing a fixed percentage of words, we sample p for each sequence separately from a Gaussian distribution with mean p = 0.6 and variance 0.1. We chose p = 0.6 based on the average length ratio between the slot values and the target sequences in our labeled training data.</p><p>This corruption approach is motivated by the fact that many NLG problems are facing a simi- lar task to the one the DAE is solving. Given some structured information, the task is to generate a tar- get sequence that includes all the information. If we map the structured information to phrases that should be in the desired output, then the structured data problem resembles the DAE problem. For in- stance, if we have the following structured exam- ple: name: Aromi -family friendly: yes → Aromi has a family friendly atmosphere. , we convert it into the input Aromi family friendly that we can feed to the DAE. To preprocess the structured data, we convert the boolean feature family friendly into a meaningful phrase ("family friendly") by using the slot name. For all non boolean slot pairs, we just use the slot values as meaningful phrases. Please keep in mind this transformation is only needed during inference as the training data has no slot pairs and only consists of pairs of corrupt and correct target sequences.</p><p>Nevertheless, there are two major differences between the training procedure of a DAE and an inference instance in NLG: First, we do not need to predict any content information in NLG as all of the content information is already provided by the structured data. On the other hand, a DAE training instance can also remove content words from the sentence. To align the two much closer, we restrict the words which the DAE is allowed to remove and apply the following heuristic to the corruption process of the DAE: Given the absolute counts N (v i ) for each word v i in our vocabulary, we only allow v i to be removed when its count N (v i ) is larger than a threshold. This heuristic is motivated by the fact that the corpus frequency of content words like a restaurant name is most likely low and the corpus frequency of non-content words like "the" is most likely high. The corpus frequencies can be either calculated on the train- ing data itself or on a different corpora. The latter one has the advantage that domain specific content words that are frequent in the training data will have a low frequency in an out-of-domain corpora.</p><p>The second difference is that in a DAE training original Loch Fyne is a family friendly restaurant providing Indian food . (a) remove random 60%</p><p>Fyne is restaurant food .</p><p>(b) remove only words w i Loch Fyne family friendly Indian with N (w i ) &gt; 100 (c) shuffle words family friendly Indian Loch Fyne <ref type="table">Table 2</ref>: Training data generation heuristics. (a): random 60% of the words are removed. (b): 60% of the words are removed, but only words that occur more than 100 times in the training data. Our assumption is that these are the non-content words. (c): On top of (b), all words are shuffled while keeping all word pairs (e.g. Loch Fyne) together that also occur in the original sentence.</p><p>instance, the words in a corrupt input occur in the same order as in the desired target. For an NLG inference instance, the order of the structured in- put does not need to match the order of the words in the output. To overcome this issue, we shuf- fle the words within the corrupt sentence while not splitting bigrams that also exist in the original sen- tence. An example of all three heuristics is given In <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supervised Approach</head><p>For comparison, we train a supervised baseline based on the vanilla seq2seq model as described in Section 2. To make better use of the structured data, we found that the input word embeddings (wemb) of the seq2seq network should be repre- sented together by the slot name and value. We split the word embedding vector into two parts and use the upper half for a word embedding of the slot name and the lower half for the word embedding of the slot value. If a slot value has multiple words, we build separate word embeddings for each word, but all having the same upper part (slot name). An example for the slot pairs of <ref type="table" target="#tab_0">Table 1</ref> is given in <ref type="figure">Figure 1</ref>.</p><formula xml:id="formula_6">Wemb (name) Wemb (Loch) Wemb (name) Wemb (Fyne) Wemb (type) Wemb (rest.) Wemb (food) Wemb (Indian) Wemb (fam_ friend)</formula><p>Wemb (yes) <ref type="figure">Figure 1</ref>: Example input word embeddings for our su- pervised baseline (Section 4) from the training Exam- ple of <ref type="table" target="#tab_0">Table 1</ref>. The upper half of the word embedding is used for the slot names; the lower half for the slot values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Sets</head><p>The  <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The news-commentary data set is a paral- lel corpus of news provided by the WMT con- ference ( <ref type="bibr" target="#b0">Bojar et al., 2017</ref>) for training ma- chine translation (MT) systems. For our unsu- pervised experiments, we use the English news- commentary part of the corpora only which con- tains 256,715 sentences.</p><p>All corpora are tokenized and we remove sen- tences that are longer than 60 tokens. In addition to tokenization, we also apply byte-pair encoding <ref type="bibr" target="#b17">(Sennrich et al., 2016</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Parameters</head><p>For all of our experiments we utilize the seq2seq implementation as described in Section 2. We run inference with a beam size of 5. We use a hid- den layer size of 1024 and a word embedding size of 620 and use SGD with an initial learning rate of 0.5. We halve the learning rate every other epoch starting from the 5th epoch. We evaluate the generated text with BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>), ROUGE-L <ref type="bibr" target="#b11">(Lin, 2004)</ref>, and NIST <ref type="bibr" target="#b2">(Doddington, 2002</ref>) and use the evaluation tool provided by the E2E organizers to calculate the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Automatic Scores</head><p>Our experimental results are summarized in <ref type="table" target="#tab_5">Ta- ble 4</ref>. We list two supervised baselines: The first one is from the organizers of the E2E challenge, the second one is from our supervised setup (Sec- tion 4). Our baseline yields better performance on BLEU and ROUGE-L while reaching similar per- formance in NIST. Our third (unsupervised) base- line copy input just runs the evaluation metrics on the input (slot values of the structured data). This system performs much worse, but serves as a lower bound for our unsupervised experiments.</p><p>We report results on different unsupervised se- tups as described in Section 3. The system ran- domly drop just randomly drops 60% of the words, but still yields 57.3 BLEU, 65.9 ROUGE-L and 7.3 NIST points. You can easily detect a lot of extra in- formation in the output that can not be explained by the structured input. Further, the output sounds very machine generated as the output depends on the order of the structured data. The heuristics + only words w/ count &gt;100 (on ind data) and + only words w/ count &gt;100 (on ood data) forbid removing words in the corruption phase that ap- pear less than 100 times in the in-domain data or out-of-domain (ood) data, respectively. The lat- ter setup uses the out-of-domain data for generat- ing the word counts only and yields an improve- ment of 5.2 BLEU, 1.3 ROUGE-L and 0.2 NIST points compared to just randomly dropping words. The output still sounds very machine generated, but stops hallucinating additional information. We further improve the performance by 3.5 BLEU, 3.9 ROUGE-L, and 0.2 NIST points when shuffling the words in the corrupted input and using the out-of- domain data also as training examples. We use the English side of the 256,715 sentences from the news-commentary dataset as out-of-domain data only. We did not see any further improvements by adding more out-of-domain training data.</p><p>Finally, we build a semi supervised system that in addition to the unlabeled data includes the la- beled information for some of the training exam- ples. For these, we remove the slot names from the structured data and use a concatenation of all slot values as input to learn the correct output. By jointly using both unlabeled and labeled data, we yield an additional improvement of 1.0 BLEU points compared to our best fully unsupervised system. In our semi supervised setup, we only use the slot values as input even for the labeled examples. This explains the drop in performance when comparing to the supervised setups. All su- pervised setups also include the slot names in their input representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Human Evaluation</head><p>In addition to automatic scores, we ran human as- sessment of the generated text as none of the au- tomatic metrics correlates well with human judg- ment ( <ref type="bibr">Belz and Reiter, 2006</ref>).</p><p>To collect human rankings, we presented 3 out- puts generated by 3 different systems side-by-side to crowd-workers, who were asked to score each sentence on a 6-point scale for:</p><p>• fluency: How do you judge the overall natural- ness of the utterance in terms of its grammatical correctness and fluency?</p><p>For the next questions, we presented in addi- tion to the 3 different system outputs, the struc- tured representations of each example. We asked the crowd-worker to score the following two ques- tions on a 5-point scale:</p><p>• all information: How much of the given infor- mation is mentioned in the text?</p><p>• bad/ false information: How much false or ex- tra information is mentioned in the text?</p><p>Each task has been given to three different raters. Consequently, each output has a separate score for each question that is the average of 3 dif- ferent ratings. The human evaluation results are summarized in <ref type="table">Table 5</ref>. We included the two su- pervised baselines and our best unsupervised setup in the human evaluation. The unsupervised setup outperforms the supervised setups in fluency. One explanation is that our unsupervised system in- cludes additional unlabeled data that can not be included in a supervised setup. Due to our unsu- pervised learning approach that all words in the structured data need to be included in the final out- put, the unsupervised system did not miss any in- formation. Further, all three outputs included little false or wrong information that was not included in the structured data. All in all the output of the setup model BLEU ROUGE-L NIST supervised baseline E2E challenge <ref type="bibr" target="#b3">(Dušek and Jurcıcek, 2016)</ref>    <ref type="table">Table 5</ref>: Human evaluation results: We generated 279 output sequences for each of the 3 listed systems. Each sequence has been evaluated by 3 different raters and the score is the average of 837 ratings per system. For each task and sequence, the raters where asked to give a score between 0 and 5. A score of 5 for fluency means that the text is fluent and grammatical correct. A score of 5 for all information means that all information from the structured data is mentioned. A score of 0 for extra/ false information means that no information besides the structured data is mentioned in the sequence. Scores labeled with † are significant better than all other systems (p &lt; 0.0001).</p><p>unsupervised system is better than the two super- vised systems.</p><p>We used approximate randomization (AR) as our significance test, as recommended by <ref type="bibr" target="#b16">(Riezler and Maxwell, 2005</ref>). Pairwise tests between results in <ref type="table">Table 5</ref> showed that our novel unsu- pervised approach is significantly better than both baselines regarding fluency and mentioning all in- formation with the likelihood of incorrectly reject- ing the null hypothesis of p &lt; 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Our unsupervised approach has two limitations and is therefore not easily applicable to all NLG problems or datasets. First, we can only run our approach for datasets where the input meaning representation either overlaps with target texts or we need to generate rules that map the structured data to target words. Unfortunately, the needed pattern can be very complicated and the effort of writing rules can be similar to the one of building a template based system. Second, to be able to generate text from struc- tured data during inference, the original structured input is converted to an unstructured one by dis- carding the slot names. This can be problematic in scenarios where the slot name itself contributes to the meaning representation, but the slot name should not be in the target text. For instances the structured data of a WEBNLG ( <ref type="bibr" target="#b4">Gardent et al., 2017</ref>) training example consists of several subject- predicate-object tuple features. Many of the fea- tures for one example have the same subject, but different predicates and objects. But yet in the fi- nal output, we prefer to have the subject only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Neural Language Generation</head><p>Due to the recent success in Deep Learning, re- searchers started to use end-to-end systems to jointly model the traditional separated tasks of content selection, sentence planning and surface realization in one system. Recently, <ref type="bibr">RNNs (Wen et al., 2015b</ref>), attention-based methods ( <ref type="bibr" target="#b13">Mei et al., 2016</ref><ref type="bibr">) or LSTMs (Wen et al., 2015a</ref>) were success- fully applied for the task of NLG. <ref type="bibr" target="#b12">Liu et al. (2018)</ref> introduced a modified LSTM that adds a field gate into the LSTM to incorporate the structured data. Further, they used a dual attention mechanism that combines attention of both the slot names and the actual slot content. <ref type="bibr" target="#b18">Sha et al. (2017)</ref> extended this approach and integrated a linked matrix in their model that learns the desired order of the slots in the target text. <ref type="bibr">Further, Dušek and Jurcıcek (2016)</ref> reranked the n-best output from a seq2seq model to penalize sentences that miss required informa- tion or add irrelevant ones. Instead of RNNs, <ref type="bibr" target="#b9">Lebret et al. (2016)</ref> introduced a neural feed-forward language model conditioned on both the full struc- tured data and the structured information of the previous generated words. In addition, the au- thors introduced a copy mechanism for boosting the words given by the structured data.</p><p>In contrast to the above mentioned related work, we train our model in a fully unsupervised fash- ion. Although, all our experiments have been con- ducted with the seq2seq model, our unsupervised approach can be applied on top of all of the dif- ferent network architectures that are introduced by the above mentioned papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">DAE and Unsupervised Learning</head><p>Denoising auto-encoders and unsupervised train- ing have been applied to various other NLP tasks. <ref type="bibr" target="#b20">Vincent et al. (2008)</ref> introduced denoising one- layer auto-encoders that are optimized to recon- struct input data from random corruption. The outputs of the intermediate layers of these de- noisers are then used as input features for subse- quent learning tasks such as supervised classifica- tion ( <ref type="bibr" target="#b10">Lee et al., 2009;</ref><ref type="bibr" target="#b5">Glorot et al., 2011</ref>). They showed that transforming data into DAE repre- sentations (as a pre-training or initialization step) gives more robust (supervised) classification per- formance. <ref type="bibr" target="#b8">Lample et al. (2018)</ref> used a denoising auto-encoder to build an unsupervised Machine Translation model. <ref type="bibr" target="#b6">Hill et al. (2016)</ref> trained a de- noising auto-encoder on a seq2seq network archi- tecture for training sentence and paragraph repre- sentations from the output of the intermediate lay- ers. They showed that using noise in the encoder step is helpful to learn a better sentence represen- tation.</p><p>In contrast to the above mentioned related work, we train a DAE directly on a task and do not take the intermediate hidden states of a DAE as sentence representation to help learning a differ- ent task. Further, none of the related work ap- plied DAEs on the task of generating sentences out of structured data. In addition, we modify the original DAE corruption process by introduc- ing heuristics that remove non-content words only to match the input representation of a supervised NLG training instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We showed how to train a denoising auto-encoder that is able to generate correct English sentences from structured data. By applying several heuris- tics to the corruption phase of the auto-encoder, we reach better performance compared to two fully supervised systems. As no labeled data is needed in our approach, we further success- fully improve the quality by incorporating out-of- domain data into the training phase. We run a hu- man evaluation for the two supervised baselines and our best unsupervised setup. We see that the output of our unsupervised setup not only includes 100% of the structured information, but also out- performs both supervised baselines in terms of flu- ency and grammatical correctness.</p><p>The unsupervised training scheme gives us the option to incorporate any unlabeled data. One pos- sible addition to our approach would be to incor- porate text in different languages into our system, so that we can generate the output in any language from the same structured data.</p><p>Our approach is appropriate only for NLG prob- lems where the goal is to include all the informa- tion from the structured data in the output. In fu- ture work, we will focus on the semi-supervised approach to make the DAE also suitable for prob- lems where instead of all, only a subset of the structured information should be included in the output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Three possible correct target sequences for the 
structured data above: (a) There is an Indian restaurant 
that is kids friendly. It is Loch Fyne. (b) Loch Fyne is a 
well-received restaurant with a wide range of delicious 
Indian food. It also delivers a fantastic service to young 
children. (c) Loch Fyne is a family friendly restaurant 
providing Indian food. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Training data statistics. Each training instance 
in NLG contains structured information and one refer-
ence sequence. Each training instance in MT contains 
one sentence written in both German and English. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the E2E dataset. The terms ood and ind are abbreviations for out-of-domain and in-domain 
respectively. The baseline systems as well as the unsupervised systems that are not labeled with + ood data are 
trained on the E2E in-domain training data only. 

system 
fluency 
all 
extra/ false 
information information 
baseline E2E challenge (Dušek and Jurcıcek, 2016) 
4.01 
4.89 
0.05 
baseline vanilla seq2seq (Section 4) 
4.46 
4.91 
0.08 
unsupervised (random drop + words w/ count &gt;100 (ood data) 4.70  † 
5.00  † 
0.05 
+ shuffle pos + ood data) 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<title level="m">Findings of the 2017 Conference on Machine Translation (WMT17). In Proceedings of the Second Conference on Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="169" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the Properties of Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Encoder-Decoder Approaches. CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research (HLT)</title>
		<meeting>the second international conference on Human Language Technology Research (HLT)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequence-toSequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurcıcek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Creating Training Corpora for Micro-Planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Sentences from Unlabelled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised Machine Translation Using Monolingual Corpora Only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Text Generation from Structured Data with Application to the Biography Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Table-to-text Generation by Structure-aware Seq2seq Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="720" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The E2E Dataset: New Challenges For Endto-End Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On some pitfalls in automatic evaluation and significance testing for mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John T</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Order-Planning Neural Text Generation From Structured Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning (ICML)</title>
		<meeting>the 25th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Gašic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkšic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">275</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
