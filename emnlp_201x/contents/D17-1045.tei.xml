<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Communication for Distributed Gradient Descent</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri</surname></persName>
							<email>a.fikri@ed.ac.uk, kheafiel@inf.ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aji</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<addrLine>10 Crichton Street Edinburgh</addrLine>
									<postCode>EH8 9AB</postCode>
									<settlement>Scotland</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">European Union</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Communication for Distributed Gradient Descent</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="440" to="445"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We make distributed stochastic gradient descent faster by exchanging sparse updates instead of dense updates. Gradient updates are positively skewed as most updates are near zero, so we map the 99% smallest updates (by absolute value) to zero then exchange sparse matrices. This method can be combined with quan-tization to further improve the compression. We explore different configurations and apply them to neural machine translation and MNIST image classification tasks. Most configurations work on MNIST, whereas different configurations reduce convergence rate on the more complex translation task. Our experiments show that we can achieve up to 49% speed up on MNIST and 22% on NMT without damaging the final accuracy or BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed computing is essential to train large neural networks on large data sets <ref type="bibr" target="#b11">(Raina et al., 2009)</ref>. We focus on data parallelism: nodes jointly optimize the same model on different parts of the training data, exchanging gradients and param- eters over the network. This network commu- nication is costly, so prior work developed two ways to approximately compress network traffic: 1-bit quantization ( <ref type="bibr" target="#b14">Seide et al., 2014</ref>) and sending sparse matrices by dropping small updates <ref type="bibr" target="#b17">(Strom, 2015;</ref><ref type="bibr" target="#b5">Dryden et al., 2016)</ref>. These methods were developed and tested on speech recognition and toy MNIST systems. In porting these approxima- tions to neural machine translation (NMT) <ref type="bibr" target="#b9">( ˜ Neco and Forcada, 1996;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>), we find that translation is less tolerant to quantization.</p><p>Throughout this paper, we compare neural ma- chine translation behavior with a toy MNIST sys- tem, chosen because prior work used a similar system ( <ref type="bibr" target="#b5">Dryden et al., 2016)</ref>. NMT parameters are dominated by three large embedding matrices: source language input, target language input, and target language output. These matrices deal with vocabulary words, only a small fraction of which are seen in a mini-batch, so we expect skewed gra- dients. In contrast, MNIST systems exercise ev- ery parameter in every mini-batch. Additionally, NMT systems consist of multiple parameters with different scales and sizes, compared to MNIST's 3-layers network with uniform size. More for- mally, gradient updates have positive skewness co- efficient <ref type="bibr" target="#b20">(Zwillinger and Kokoska, 1999</ref>): most are close to zero but a few are large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>An orthogonal line of work optimizes the SGD algorithm and communication pattern. <ref type="bibr" target="#b19">Zinkevich et al. (2010)</ref> proposed an asynchronous ar- chitecture where each node can push and pull the model independently to avoid waiting for the slower node. <ref type="bibr" target="#b3">Chilimbi et al. (2014) and</ref><ref type="bibr" target="#b12">Recht et al. (2011)</ref> suggest updating the model without a lock, allowing race conditions. Additionally, <ref type="bibr" target="#b4">Dean et al. (2012)</ref> run multiple minibatches be- fore exchanging updates, reducing the communi- cation cost. Our work is a more continuous ver- sion, in which the most important updates are sent between minibatches. <ref type="bibr" target="#b18">Zhang et al. (2015)</ref> down- weight gradients based on stale parameters.</p><p>Approximate gradient compression is not a new idea. 1-Bit SGD ( <ref type="bibr" target="#b14">Seide et al., 2014)</ref>, and later Quantization SGD ( <ref type="bibr" target="#b1">Alistarh et al., 2016)</ref>, work by converting the gradient update into a 1-bit ma- trix, thus reducing data communication signifi- cantly. <ref type="bibr" target="#b17">Strom (2015)</ref> proposed threshold quantiza-tion, which only sends gradient updates that larger than a predefined constant threshold. However, the optimal threshold is not easy to choose and, more- over, it can change over time during optimization. <ref type="bibr" target="#b5">Dryden et al. (2016)</ref> set the threshold so as to keep a constant number of gradients each iteration. We used distributed SGD with parameter shard- ing <ref type="bibr" target="#b4">(Dean et al., 2012</ref>), shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Each of the N workers is both a client and a server. Servers are responsible for 1/N th of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Distributed SGD</head><p>Clients have a copy of all parameters, which they use to compute gradients. These gradients are split into N pieces and pushed to the appro- priate servers. Similarly, each client pulls param- eters from all servers. Each node converses with all N nodes regarding 1/N th of the parameters, so bandwidth per node is constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Sparse Gradient Exchange</head><p>We sparsify gradient updates by removing the R% smallest gradients by absolute value, dubbing this Gradient Dropping. This approach is slightly dif- ferent from <ref type="bibr" target="#b5">Dryden et al. (2016)</ref> as we used a sin- gle threshold based on absolute value, instead of dropping the positive and negative gradients sepa- rately. This is simpler to execute and works just as well.</p><p>Small gradients can accumulate over time and we find that zeroing them damages convergence. Following <ref type="bibr" target="#b14">Seide et al. (2014)</ref>, we remember resid- uals (in our case dropped values) locally and add them to the next gradient, before dropping again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Gradient dropping algorithm given gradient and dropping rate R.</head><p>function GRADDROP(, R)</p><formula xml:id="formula_0">+ = residuals Select threshold: R% of ||| is smaller dropped ← 0 dropped[i] ← [i]∀i : ||[i]| &gt; threshold residuals ← − dropped return sparse(dropped) end function</formula><p>Gradient Dropping is shown in Algorithm 1. This function is applied to all data transmissions, including parameter pulls encoded as deltas from the last version pulled by the client. To compute these deltas, we store the last pulled copy server- side. Synchronous SGD has one copy. Asyn- chronous SGD has a copy per client, but the server is responsible for 1/N th of the parameters for N clients so memory is constant.</p><p>Selection to obtain the threshold is expensive ( <ref type="bibr" target="#b0">Alabi et al., 2012</ref>). However, this can be approxi- mated. We sample 0.1% of the gradient and obtain the threshold by running selection on the samples.</p><p>Parameters and their gradients may not be on comparable scales across different parts of the neural network. We can select a threshold locally to each matrix of parameters or globally for all pa- rameters. In the experiments, we find that layer normalization <ref type="bibr" target="#b8">(Lei Ba et al., 2016</ref>) makes a global threshold work, so by default we use layer normal- ization with one global threshold. Without layer normalization, a global threshold degrades conver- gence for NMT. Prior work used global thresholds and sometimes column-wise quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We experiment with an image classification task based on MNIST dataset ( <ref type="bibr" target="#b7">LeCun et al., 1998)</ref> and Romanian→English neural machine transla- tion system.</p><p>For our image classification experiment, we build a fully connected neural network with three 4069-neuron hidden layers. We use AdaGrad with an initial learning rate of 0.005. The mini-batch size of 40 is used. This setup is identical to <ref type="bibr" target="#b5">Dryden et al. (2016)</ref>.</p><p>Our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Drop Ratio</head><p>To find an appropriate dropping ratio R%, we tried 90%, 99%, and 99.9% then measured perfor- mance in terms of loss and classification accuracy or translation quality approximated by BLEU ( <ref type="bibr" target="#b10">Papineni et al., 2002</ref>) for image classification and NMT task respectively. <ref type="figure" target="#fig_1">Figure 3</ref> shows that the model still learns af- ter dropping 99.9% of the gradients, albeit with a worse BLEU score. However, dropping 99% of the gradient has little impact on convergence or BLEU, despite exchanging 50x less data with offset-value encoding. The x-axis in both plots is batches, showing that we are not relying on speed improvement to compensate for convergence. <ref type="bibr" target="#b5">Dryden et al. (2016)</ref> used a fixed dropping ratio of 98.4% without testing other options. Switching to 99% corresponds to more than a 1.5x reduction in network bandwidth.</p><p>For MNIST, gradient dropping oddly improves accuracy in early batches. The same is not seen for NMT, so we caution against interpreting slight gains on MNIST as regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Local vs Global Threshold</head><p>Parameters may not be on a comparable scale so, as discussed in Section 4, we experiment with lo- cal thresholds for each matrix or a global threshold for all gradients. We also investigate the effect of layer normalization. We use a drop ratio of 99% as suggested previously. <ref type="bibr">Based</ref>   due to the complicated interaction with sharding, we did not implement locally thresholded pulling, so only locally thresholded pushing is shown.</p><p>The results show that layer normalization has no visible impact on MNIST. On the other side, our NMT system performed poorly as, without layer normalization, parameters are on various scales and global thresholding underperforms. Further- more, our NMT system has more parameter cate- gories compared to MNIST's 3-layer network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Convergence Rate</head><p>While dropping gradients greatly reduces the com- munication cost, it is shown in <ref type="table">Table 1</ref> that overall speed improvement is not significant for our NMT experiment. For our NMT experiment with 4 Ti- tan Xs, communication time is only around 13% of the total training time. Dropping 99% of the gradient leads to 11% speed improvement. Addi- tionally, we added an extra experiment of NMT with batch-size of 32 to give more communication cost ratio. In this scenario, communication is 17% of the total training time and we see a 22% aver- age speed improvement. For MNIST, communi- cation is 41% of the total training time and we see a 49% average speed improvement. Computation got faster by reducing multitasking. We investigate the convergence rate: the combi- nation of loss and speed. For MNIST, we train the model for 20 epochs as mentioned in <ref type="bibr" target="#b5">Dryden et al. (2016)</ref>. For NMT, we tested this with batch sizes of 80 and 32 and trained for 13.5 hours. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, our baseline MNIST experiment reached 99.28% final accuracy, and reached 99.42% final accuracy with a 99% drop rate. It also shown that it has better convergence rate in general with gradient dropping. Our NMT experiment result is shown in <ref type="table">Table  2</ref>. Final BLEU scores are essentially unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final</head><p>Time to reach %BLEU 33% BLEU batch-size 80 + baseline 34.51 2.6 hours + 99% grad-drop 34.40 2.7 hours batch-size 32 + baseline 34.16 4.2 hours + 99% grad-drop 34.08 3.2 hours <ref type="table">Table 2</ref>: Summary of BLEU score obtained.</p><p>Our algorithm converges 23% faster than the base- line when the batch size is 32, and nearly the same with a batch size of 80. This in a setting with fast communication: 15.75 GB/s theoretical over PCI express 3.0 x16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">1-Bit Quantization</head><p>We can obtain further compression by applying 1-bit quantization after gradient dropping. Strom (2015) quantized simply by mapping all surviving values to the dropping threshold, effectively the minimum surviving absolute value. <ref type="bibr" target="#b5">Dryden et al. (2016)</ref> took the averages of values being quan- tized, as is more standard. They also quantized at the column level, rather than choosing centers globally. We tested 1-bit quantization with 3 dif- ferent configurations: threshold, column-wise av- erage, and global average. The quantization is ap- plied after gradient dropping with a 99% drop rate, layer normalization, and a global threshold. <ref type="figure" target="#fig_6">Figure 8</ref> shows that 1-bit quantization slows down the convergence rate for NMT. This differs from prior work ( <ref type="bibr" target="#b14">Seide et al., 2014;</ref><ref type="bibr" target="#b5">Dryden et al., 2016</ref>) which reported no impact from 1-bit quan- tization. Yet, we agree with their experiments: all tested types of quantization work on MNIST. This emphasizes the need for task variety in ex- periments.</p><p>NMT has more skew in its top 1% gradients, so it makes sense that 1-bit quantization causes more loss. 2-bit quantization is sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Gradient updates are positively skewed: most are close to zero. This can be exploited by keeping 99% of gradient updates locally, reducing com- munication size to 50x smaller with a coordinate- value encoding.</p><p>Prior work suggested that 1-bit quantization can be applied to further compress the communication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.6">2000 4000 6000 8000 10000</head><p>Training loss (MNIST) 1-bit min global 1-bit avg. global 2-bit min. global 2-bit avg. global 1-bit avg. col. However, we found out that this is not true for NMT. We attribute this to skew in the embedding layers. However, 2-bit quantization is likely to be sufficient, separating large movers from small changes. Additionally, our NMT system consists of many parameters with different scales, thus layer normalization or using local threshold per- parameter is necessary. On the hand side, MNIST seems to work with any configurations we tried. Our experiment with 4 Titan Xs shows that on average only 17% of the time is spent communi- cating (with batch size 32) and we achieve 22% speed up. Our future work is to test this approach on systems with expensive communication cost, such as multi-node environments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributed SGD architecture with parameter sharding.</figDesc><graphic url="image-1.png" coords="2,72.00,190.73,181.42,156.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NMT: Training loss and validation BLEU for different dropping ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: MNIST: Comparison of local and global thresholds with and without layer normalization. 20 40 60 80 100 120 140 160 180 0 25000 50000 75000 100000 Training loss Layer+Globally drop 99% Layer+Locally drop 99% Layer+Baseline</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: MNIST classification accuracy over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: NMT validation BLEU and loss over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Training loss for different quantization methods.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Alham Fikri Aji is funded by the Indonesia En-dowment Fund for Education scholarship scheme. Marcin Junczys-Dowmunt wrote the baseline dis-tributed code and consulted on this implementa-tion. We thank School of Informatics computing and finance staff for working around NVIDIA's limit of two Pascal Titan Xs per customer. Com-puting was funded by the Amazon Academic Re-search Awards program and by Microsoft's dona-tion of Azure time to the Alan Turing Institute. This work was supported by The Alan Turing In-444 stitute under the EPSRC grant EP/N510129/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast k-selection algorithms for graphics processing units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolu</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Algorithmics (JEA)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="4" to="6" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">QSGD: randomized quantization for communication-optimal stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Vojnovic</surname></persName>
		</author>
		<idno>CoRR abs/1610.02132</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<ptr target="http://arxiv.org/abs/1409.0473" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Project adam: Building an efficient and scalable deep learning training system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Trishul M Chilimbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Communication quantization for data-parallel training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">Ade</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Van Essen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Learning in High Performance Computing Environments</title>
		<meeting>the Workshop on Machine Learning in High Performance Computing Environments</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is neural machine translation ready for deployment? A case study on 30 translation directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Program of the 13th International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>IWSLT 2016</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond mealy machines: Learning translators with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P ˜</forename><surname>Ramón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Neco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forcada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 International Neural Network Society Annual Meeting</title>
		<meeting>the 1996 International Neural Network Society Annual Meeting<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evalution of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale deep unsupervised learning using graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf" />
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">1-bit stochastic gradient descent and application to data-parallel distributed training of speech DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/publication/1-bit-stochastic-gradient-descent-and-application-to-data-parallel-distributed-training-of-speech-dnns/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">First Conference on Machine Translation (WMT16)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scalable distributed dnn training using commodity gpu cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikko</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Staleness-aware async-sgd for distributed deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR abs/1511.05950</idno>
		<ptr target="http://arxiv.org/abs/1511.05950" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2595" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zwillinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Kokoska</surname></persName>
		</author>
		<title level="m">CRC standard probability and statistics tables and formulae</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
