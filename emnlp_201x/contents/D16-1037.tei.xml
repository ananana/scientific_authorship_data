<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Neural Discourse Relation Recognizer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Neural Discourse Relation Recognizer</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="382" to="391"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VarNDRR. VarNDRR establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce neural discourse relation models to approximate the prior and posterior distributions of the latent variable, and employ these approximated distributions to optimize a repa-rameterized variational lower bound. This allows VarNDRR to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VarNDRR can achieve comparable results against state-of-the-art baselines without using any manual features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discourse relation characterizes the internal struc- ture and logical relation of a coherent text. Automat- ically identifying these relations not only plays an important role in discourse comprehension and gen- eration, but also obtains wide applications in many * Corresponding author other relevant natural language processing tasks, such as text summarization ( <ref type="bibr" target="#b27">Yoshida et al., 2014</ref>), conversation ( <ref type="bibr" target="#b7">Higashinaka et al., 2014)</ref>, question an- swering ( <ref type="bibr" target="#b25">Verberne et al., 2007)</ref> and information ex- traction ( <ref type="bibr" target="#b3">Cimiano et al., 2005</ref>). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the follow- ing example:</p><p>The company was disappointed by the rul- ing. because The obligation is totally un- warranted. (adapted from wsj 0294)</p><p>With the discourse connective because, these two sentences display an explicit discourse relation CONTINGENCY which can be inferred easily. Once this discourse connective is removed, however, the discourse relation becomes implicit and difficult to be recognized. This is because almost no surface in- formation in these two sentences can signal this re- lation. For successful recognition of this relation, in the contrary, we need to understand the deep seman- tic correlation between disappointed and obligation in the two sentences above. Although explicit dis- course relation recognition (DRR) has made great progress ( <ref type="bibr" target="#b16">Miltsakaki et al., 2005;</ref><ref type="bibr" target="#b19">Pitler et al., 2008)</ref>, implicit DRR still remains a serious challenge due to the difficulty in semantic analysis.</p><p>Conventional approaches to implicit DRR often treat the relation recognition as a classification prob- lem, where discourse arguments and relations are re- garded as the inputs and outputs respectively. Gen- erally, these methods first generate a representation for a discourse, denoted as x 1 (e.g., manual fea-  tures in SVM-based recognition <ref type="bibr" target="#b20">(Pitler et al., 2009;</ref><ref type="bibr" target="#b14">Lin et al., 2009</ref>) or sentence embeddings in neu- ral networks-based recognition <ref type="bibr" target="#b8">(Ji and Eisenstein, 2015;</ref><ref type="bibr" target="#b28">Zhang et al., 2015)</ref>), and then directly model the conditional probability of the corresponding dis- course relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representa- tion x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data.</p><p>Instead, we assume that there is a latent continu- ous variable z from an underlying semantic space. It is this latent variable that generates both dis- course arguments and the corresponding relation, i.e. p(x, y|z). The latent variable enables us to jointly model discourse arguments and their rela- tions, rather than conditionally model y on x. How- ever, the incorporation of the latent variable makes the modeling difficult due to the intractable compu- tation with respect to the posterior distribution.</p><p>Inspired by  as well as  who introduce a variational neural inference model to the intractable posterior via optimizing a reparameterized variational lower bound, we propose a variational neural discourse re- lation recognizer (VarNDRR) with a latent contin- uous variable for implicit DRR in this paper. The key idea behind VarNDRR is that although the pos- terior distribution is intractable, we can approximate it via a deep neural network. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the treat them as univariate variables in most cases. Additionally, we use bold symbols to denote variables, and plain symbols to denote values. graph structure of VarNDRR. Specifically, there are two essential components:</p><p>• neural discourse recognizer As a discourse x and its corresponding relation y are indepen- dent with each other given the latent variable z (as shown by the solid lines), we can formulate the generation of x and y from z in the equa- tion p θ (x, y|z) = p θ (x|z)p θ (y|z). These two conditional probabilities on the right hand side are modeled via deep neural networks (see sec- tion 3.1).</p><p>• neural latent approximator VarNDRR assumes that the latent variable can be inferred from dis- course arguments x and relations y (as shown by the dash lines). In order to infer the la- tent variable, we employ a deep neural net- work to approximate the posterior q φ (z|x, y) as well as the prior q φ (z|x) (see section 3.2), which makes the inference procedure efficient. We further employ a reparameterization tech- nique to sample z from q φ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the stan- dard stochastic gradient ascent techniques for optimization (see section 3.3).</p><p>The main contributions of our work lie in two as- pects. 1) We exploit a generative graphic model for implicit DRR. To the best of our knowledge, this has never been investigated before. 2) We develop a neural recognizer and two neural approximators specifically for implicit DRR, which enables both the recognition and inference to be efficient.</p><p>We conduct a series of experiments for English implicit DRR on the PDTB-style corpus to evaluate the effectiveness of our proposed VarNDRR model. Experiment results show that our variational model achieves comparable results against several strong baselines in term of F1 score. Extensive analysis on the variational lower bound further reveals that our model can indeed fit the data set with respect to discourse arguments and relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Variational Autoencoder</head><p>The variational autoencoder (VAE) , which forms the basis of our model, is a generative model that can be regarded as a regularized version of the standard autoencoder. With a latent random variable z, VAE significantly changes the autoencoder architecture to be able to capture the variations in the observed vari- able x. The joint distribution of (x, z) is formulated as follows:</p><formula xml:id="formula_0">p θ (x, z) = p θ (x|z)p θ (z)<label>(1)</label></formula><p>where p θ (z) is the prior over the latent variable, usu- ally equipped with a simple Gaussian distribution. p θ (x|z) is the conditional distribution that models the probability of x given the latent variable z. Typi- cally, VAE parameterizes p θ (x|z) with a highly non- linear but flexible function approximator such as a neural network. The objective of VAE is to maximize a variational lower bound as follows:</p><formula xml:id="formula_1">L V AE (θ, φ; x) = −KL(q φ (z|x)||p θ (z)) +E q φ (z|x) [log p θ (x|z)] ≤ log p θ (x)<label>(2)</label></formula><p>where KL(Q||P ) is Kullback-Leibler divergence be- tween two distributions Q and P . q φ (z|x) is an approximation of the posterior p(z|x) and usually follows a diagonal Gaussian N (µ, diag(σ 2 )) whose mean µ and variance σ 2 are parameterized by again, neural networks, conditioned on x.</p><p>To optimize Eq. (2) stochastically with respect to both θ and φ, VAE introduces a reparameterization trick that parameterizes the latent variable z with the Gaussian parameters µ and σ in q φ (z|x):</p><formula xml:id="formula_2">˜ z = µ + σ (3)</formula><p>where is a standard Gaussian variable, and de- notes an element-wise product. Intuitively, VAE learns the representation of the latent variable not as single points, but as soft ellipsoidal regions in la- tent space, forcing the representation to fill the space rather than memorizing the training data as isolated representations. With this trick, the VAE model can be trained through standard backpropagation tech- nique with stochastic gradient ascent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The VarNDRR Model</head><p>This section introduces our proposed VarNDRR model. Formally, in VarNDRR, there are two ob- served variables, x for a discourse and y for the cor- responding relation, and one latent variable z. As</p><formula xml:id="formula_3">z y x 1 x 2 p θ (x|z) p θ (y|z) h ′ 1 h ′ 2</formula><p>Figure 2: Neural networks for conditional probabilities p θ (x|z) and p θ (y|z). The gray color denotes real-valued representations while the white and black color 0-1 rep- resentations.</p><p>illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, the joint distribution of the three variables is formulated as follows:</p><formula xml:id="formula_4">p θ (x, y, z) = p θ (x, y|z)p(z)<label>(4)</label></formula><p>We begin with this distribution to elaborate the ma- jor components of VarNDRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Discourse Recognizer</head><p>The conditional distribution p(x, y|z) in Eq. <ref type="formula" target="#formula_4">(4)</ref> shows that both discourse arguments and the corre- sponding relation are generated from the latent vari- able. As shown in <ref type="figure" target="#fig_1">Figure 1</ref>, x is d-separated from y by z. Therefore the discourse x and the corre- sponding relation y is independent given the latent variable z. The joint probability can be therefore formulated as follows:</p><formula xml:id="formula_5">p θ (x, y, z) = p θ (x|z)p θ (y|z)p(z)<label>(5)</label></formula><p>We use a neural model q φ (z|x) to approximate the prior p(z) conditioned on the discourse x (see the following section). With respect to the other two conditional distributions, we parameterize them via neural networks as shown in <ref type="figure">Figure 2</ref>.</p><p>Before we describe these neural networks, it is necessary to briefly introduce how discourse rela- tions are annotated in our training data. The PDTB corpus, used as our training data, annotates implicit discourse relations between two neighboring argu- ments, namely Arg1 and Arg2. In VarNDRR, we represent the two arguments with bag-of-word rep- resentations, and denote them as x 1 and x 2 .</p><p>To model p θ (x|z) (the bottom part in <ref type="figure">Figure 2</ref>), we project the representation of the latent variable z ∈ R dz onto a hidden layer:</p><formula xml:id="formula_6">h 1 = f (W h 1 z + b h 1 ) h 2 = f (W h 2 z + b h 1 )<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">h 1 ∈ R d h 1 , h 2 ∈ R d h</formula><p>2 , W * is the transfor- mation matrix, b * is the bias term, d u denotes the dimensionality of vector representations of u and f (·) is an element-wise activation function, such as tanh(·), which is used throughout our model.</p><p>Upon this hidden layer, we further stack a Sig- moid layer to predict the probabilities of correspond- ing discourse arguments:</p><formula xml:id="formula_8">x 1 = Sigmoid(W x 1 h 1 + b x 1 ) x 2 = Sigmoid(W x 2 h 2 + b x 2 )<label>(7)</label></formula><p>here, x 1 ∈ R dx 1 and x 2 ∈ R dx 2 are the real- valued representations of the reconstructed x 1 and x 2 respectively. <ref type="bibr">2</ref> We assume that p θ (x|z) is a mul- tivariate Bernoulli distribution because of the bag- of-word representation. Therefore the logarithm of p(x|z) is calculated as the sum of probabilities of words in discourse arguments as follows:</p><formula xml:id="formula_9">log p(x|z) = i x 1,i log x 1,i + (1 − x 1,i ) log(1 − x 1,i ) + j x 2,j log x 2,j + (1 − x 2,j ) log(1 − x 2,j )<label>(8)</label></formula><p>where u i,j is the jth element in u i . In order to estimate p θ (y|z) (the top part in <ref type="figure">Fig- ure 2)</ref>, we stack a softmax layer over the multilayer- perceptron-transformed representation of the latent variable z:</p><formula xml:id="formula_10">y = SoftMax(W y MLP(z) + b y )<label>(9)</label></formula><p>y ∈ R dy , and d y denotes the number of discourse relations. MLP projects the representation of latent variable z into a d m -dimensional space through four internal layers, each of which has dimension d m .</p><p>Suppose that the true relation is y ∈ R dy , the log- arithm of p(y|z) is defined as:</p><formula xml:id="formula_11">log p(y|z) = dy i=1 y i log y i<label>(10)</label></formula><p>2 Notice that the equality of</p><formula xml:id="formula_12">dx 1 = dx 2 , d h 1 = d h 2</formula><p>is not necessary though we assume so in our experiments.</p><formula xml:id="formula_13">µ x 1 h 1 log σ 2 h 2 x 2 y h y q ′ φ (z|x) q φ (z|x, y)</formula><p>Figure 3: Neural networks for Gaussian parameters µ and log σ in the approximated posterior q φ (z|x, y) and prior q φ (z|x).</p><p>In order to precisely estimate these conditional probabilities, our model will force the representation z of the latent variable to encode semantic informa- tion for both the reconstructed discourse x (Eq. <ref type="formula" target="#formula_9">(8)</ref>) and predicted discourse relation y (Eq. <ref type="formula" target="#formula_0">(10)</ref>), which is exactly what we want.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Latent Approximator</head><p>For the joint distribution in Eq. <ref type="formula" target="#formula_5">(5)</ref>, we can define a variational lower bound that is similar to Eq. (2). The difference lies in two aspects: the approximate prior q φ (z|x) and posterior q φ (z|x, y). We model both distributions as a multivariate Gaussian distri- bution with a diagonal covariance structure:</p><formula xml:id="formula_14">N (z; µ, σ 2 I)</formula><p>The mean µ and s.d. σ of the approximate distribu- tion are the outputs of the neural network as shown in <ref type="figure">Figure 3</ref>, where the prior and posterior have dif- ferent conditions and independent parameters.</p><p>Approximate Posterior q φ (z|x, y) is modeled to condition on both observed variables: the discourse arguments x and relations y. Similar to the calcula- tion of p θ (x|z), we first transform the input x and y into a hidden representation:</p><formula xml:id="formula_15">h 1 = f (W h 1 x 1 + b h 1 ) h 2 = f (W h 2 x 2 + b h 2 ) h y = f (W hy y + b hy )<label>(11)</label></formula><p>where</p><formula xml:id="formula_16">h 1 ∈ R d h 1 , h 2 ∈ R d h 2 and h y ∈ R d hy . 3</formula><p>We then obtain the Gaussian parameters of the posterior µ and log σ 2 through linear regression:</p><formula xml:id="formula_17">µ = W µ 1 h 1 + W µ 2 h 2 + W µy h y + b µ log σ 2 = W σ 1 h 1 + W σ 2 h 2 + W σy h y + b σ<label>(12)</label></formula><p>where µ, σ ∈ R dz . In this way, this posterior ap- proximator can be efficiently computed.</p><p>Approximate Prior q φ (z|x) is modeled to condi- tion on discourse arguments x alone. This is based on the observation that discriminative models are able to obtain promising results using only x. There- fore, assuming the discourse arguments encode the prior information for discourse relation recognition is meaningful.</p><p>The neural model for prior q φ (z|x) is the same as that (i.e. Eq <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>) for posterior q φ (z|x, y) (see <ref type="figure">Figure 3)</ref>, except for the absence of discourse relation y. For clarity , we use µ and σ to denote the mean and s.d. of the approximate prior.</p><p>With the parameters of Gaussian distribution, we can access the representation z using different sam- pling strategies. However, traditional sampling ap- proaches often breaks off the connection between recognizer and approximator, making the optimiza- tion difficult. Instead, we employ the reparameteri- zation trick ) as in Eq. (3). During training, we sam- ple the latent variable using˜zusing˜ using˜z = µ + σ ; during testing, however, we employ the expectation of z in the approximate prior distribution, i.e. set˜zset˜ set˜z = µ to avoid uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parameter Learning</head><p>We employ the Monte Carlo method to estimate the expectation over the approximate posterior, that is E q φ (z|x,y) [log p θ (x, y|z)]. Given a training instance (x (t) , y (t) ), the joint training objective is defined:</p><formula xml:id="formula_18">L(θ, φ) −KL(q φ (z|x (t) , y (t) )||q φ (z|x (t) )) + 1 L L l=1 log p θ (x (t) , y (t) |˜z|˜z (t,l) )<label>(13)</label></formula><p>where˜zwhere˜ where˜z (t,l) = µ (t) + σ (t) (l) and (l) ∼ N (0, I)</p><p>L is the number of samples. The first term is the KL divergence of two Gaussian distributions which can be computed and differentiated without estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Parameter Learning Algorithm of VarNDRR.</head><p>Inputs: A, the maximum number of iterations; M , the number of instances in one batch; L, the number of samples; Maximizing this objective will minimize the differ- ence between the approximate posterior and prior, thus making the setting˜zsetting˜ setting˜z = µ during testing rea- sonable. The second term is the approximate ex- pectation of E q φ (z|x,y) [log p θ (x, y|z)], which is also differentiable.</p><formula xml:id="formula_19">θ, φ ← Initialize parameters repeat D ← getRandomMiniBatch(M) ← getRandomNoiseFromStandardGaussian() g ← θ,φ L(θ</formula><note type="other">, φ; D, ) θ, φ ← parameterUpdater(θ, φ; g) until convergence of parameters (θ, φ) or reach the maximum iteration A Relation #Instance Number Train</note><p>As the objective function in Eq. <ref type="formula" target="#formula_0">(13)</ref> is differen- tiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We used the largest hand-annotated discourse cor- pus PDTB 2.0 <ref type="bibr">5</ref>   al., 2013; Zhang et al., 2015), we used sections 2- 20 as our training set, sections 21-22 as the test set. Sections 0-1 were used as the development set for hyperparameter optimization. In PDTB, discourse relations are annotated in a predicate-argument view. Each discourse connective is treated as a predicate that takes two text spans as its arguments. The discourse relation tags in PDTB are arranged in a three-level hierarchy, where the top level consists of four major semantic classes: TEMPORAL (TEM), CONTINGENCY (CON), EX- PANSION (EXP) and COMPARISON (COM). Be- cause the top-level relations are general enough to be annotated with a high inter-annotator agreement and are common to most theories of discourse, in our experiments we only use this level of annotations.</p><p>We formulated the task as four separate one- against-all binary classification problems: each top level class vs. the other three discourse relation classes. We also balanced the training set by resam- pling training instances in each class until the num- ber of positive and negative instances are equal. In contrast, all instances in the test and development set are kept in nature. The statistics of various data sets is listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>We tokenized all datasets using Stanford NLP Toolkit 6 . For optimization, we employed the Adam <ref type="bibr">6</ref> http://nlp.stanford.edu/software/corenlp.shtml algorithm ( <ref type="bibr" target="#b10">Kingma and Ba, 2014</ref>) to update param- eters. With respect to the hyperparameters M, L, A and the dimensionality of all vector representations, we set them according to previous <ref type="bibr">work (Kingma and Welling, 2014;</ref>) and pre- liminary experiments on the development set. Fi- nally, we set M = 16, A = 1000,</p><formula xml:id="formula_20">L = 1, d z = 20, d x 1 = d x 2 = 10001, d h 1 = d h 2 = d h 1 = d h 2 = d m = d hy = 400, d y = 2</formula><p>for all experiments. <ref type="bibr">7</ref> . All parameters of VarNDRR are initialized by a Gaus- sian distribution (µ = 0, σ = 0.01). For Adam, we set β 1 = 0.9, β 2 = 0.999 with a learning rate 0.001. Additionally, we tied the following parame- ters in practice:</p><formula xml:id="formula_21">W h 1 and W h 2 , W x 1 and W x 2 .</formula><p>We compared VarNDRR against the following two different baseline methods:</p><p>• SVM: a support vector machine (SVM) classi- fier 8 trained with several manual features.</p><p>• SCNN: a shallow convolutional neural network proposed by <ref type="bibr" target="#b28">Zhang et al. (2015)</ref>.</p><p>We also provide results from two state-of-the-art systems:</p><p>• Rutherford and Xue (2015) convert explicit discourse relations into implicit instances.</p><p>• Ji and Eisenstein (2015) augment discourse representations via entity connections. </p><note type="other">20 -92.55 32.95638 21 -89.57 25.26767 22 -86.76 38.05668 23 -84.18 7.506053 24 -81.77 49.31774 25 -79.56 29.11051 26 -77.86 28.06122 27 -75.62 14.74926 28 -73.78 32.99145 29 -72.12 24.42589 30 -70.57 40.37685 31 -69.09 27.44722 32 -67.72 26.64418 33 -66.42 25.56391 34 -65.28 37.69231 35 -64.09 0 36 -63.03 42.11687 37 -62.01 0 38 -61.03 34.80278 39 -60.12 35.76642 40 -59.21 39.21085 41 -58.42 39.11439 42 -57.67 32.78195 43 -56.94 34.12463 44 -56.24 42.22821 45 -55.53 42.36902 46 -54.95 28.40909 47 -54.29 41.14173 48 -53.69 42.89044 49</note><p>-53.1 40.85189 50 -52.63 40 51 -52.08 40.1122 52 -51.58 41.76049 53 -51.11 43.67301 54 -50.63 44.73976      <ref type="bibr" target="#b23">Rutherford and Xue, 2014)</ref>. In order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cut- off of 5 to remove rare features, following <ref type="bibr" target="#b14">Lin et al. (2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Classification Results</head><p>Because the development and test sets are imbal- anced in terms of the ratio of positive and negative instances, we chose the widely-used F1 score as our major evaluation metric. In addition, we also pro- vide the precision, recall and accuracy for further analysis. <ref type="table" target="#tab_2">Table 2</ref> summarizes the classification re- sults.</p><p>From <ref type="table" target="#tab_2">Table 2</ref>, we observe that the proposed VarN- DRR outperforms SVM on COM/EXP/TEM and SCNN on EXP/COM according to their F1 scores. Although it fails on CON, VarNDRR achieves the best result on EXP/COM among these three mod- els. Overall, VarNDRR is competitive in compar- ison with these two baselines. With respect to the accuracy, our model does not yield substantial im- provements over the two baselines. This may be be- cause that we used the F1 score rather than the accu- racy, as our selection criterion on the development set. With respect to the precision and recall, our model tends to produce relatively lower precisions but higher recalls. This suggests that the improve- ments of VarNDRR in terms of F1 scores mostly benefits from the recall values.</p><p>Comparing with the state-of-the-art results of pre- vious work <ref type="bibr" target="#b8">(Ji and Eisenstein, 2015;</ref><ref type="bibr" target="#b24">Rutherford and Xue, 2015)</ref>, VarNDRR achieves comparable results in term of the F1 scores. Specifically, VarNDRR out- performs <ref type="bibr">Xue (2015) on EXP, and</ref><ref type="bibr" target="#b8">Eisenstein (2015)</ref> on TEM. However, the accu- racy of our model fails to surpass these models. We argue that this is because both baselines use many manual features designed with prior human knowl- edge, but our model is purely neural-based.</p><p>Additionally, we find that the performance of our model is proportional to the number of training in- stances. This suggests that collecting more training instances (in spite of the noises) may be beneficial to our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Variational Lower Bound Analysis</head><p>In addition to the classification performance, the ef- ficiency in learning and inference is another concern for variational methods. <ref type="figure" target="#fig_4">Figure 4</ref> shows the training procedure for four tasks in terms of the variational lower bound on the training set. We also provide F1 scores on the development set to investigate the relations between the variational lower bound and recognition performance.</p><p>We find that our model converges toward the vari- ational lower bound considerably fast in all exper- iments (within 100 epochs), which resonates with the previous findings ). However, the change trend of the F1 score does not follow that of the lower bound which takes more time to converge. Particularly to the four discourse relations, we further observe that the change paths of the F1 score are completely dif- ferent. This may suggest that the four discourse re- lations have different properties and distributions.</p><p>In particular, the number of epochs when the best F1 score reaches is also different for the four dis- course relations. This indicates that dividing the im- plicit DRR into four different tasks according to the type of discourse relations is reasonable and better than performing DRR on the mixtures of the four relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There are two lines of research related to our work: implicit discourse relation recognition and varia- tional neural model, which we describe in succes- sion.</p><p>Implicit  <ref type="bibr" target="#b1">Braud and Denis, 2015)</ref>, Brown cluster pairs and co-reference patterns <ref type="bibr" target="#b23">(Rutherford and Xue, 2014</ref>). With these features, <ref type="bibr" target="#b17">Park and Cardie (2012)</ref> perform feature set optimization for better feature combination.</p><p>Different from feature engineering, predicting discourse connectives can indirectly help the rela- tion classification ( <ref type="bibr" target="#b29">Zhou et al., 2010;</ref><ref type="bibr" target="#b18">Patterson and Kehler, 2013)</ref>. In addition, selecting explicit dis- course instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement ( <ref type="bibr" target="#b26">Wang et al., 2012;</ref><ref type="bibr" target="#b13">Lan et al., 2013;</ref><ref type="bibr" target="#b0">Braud and Denis, 2014;</ref><ref type="bibr" target="#b5">Fisher and Simmons, 2015;</ref><ref type="bibr" target="#b24">Rutherford and Xue, 2015)</ref>. Very re- cently, neural network models have been also used for implicit DRR due to its capability for represen- tation learning <ref type="bibr" target="#b8">(Ji and Eisenstein, 2015;</ref><ref type="bibr" target="#b28">Zhang et al., 2015)</ref>. Despite their successes, most of them focus on the discriminative models, leaving the field of genera- tive models for implicit DRR a relatively uninvesti- gated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by <ref type="bibr" target="#b9">Ji et al. (2016)</ref>. However, our work differs from theirs significantly, which can be summarized in the following three aspects: 1) they employ the recurrent neural network to represent the discourse arguments, while we use the simple feed- forward neural network; 2) they treat the discourse relations directly as latent variables, rather than the underlying semantic representation of discourses; 3) their model is optimized in terms of the data likeli- hood, since the discourse relations are observed dur- ing training. However, VarNDRR is optimized un- der the variational theory.</p><p>Variational Neural Model In the presence of con- tinuous latent variables with intractable posterior distributions, efficient inference and learning in di- rected probabilistic models is required.  as well as  introduce variational neural networks that employ an approximate inference model for intractable pos- terior and reparameterized variational lower bound for stochastic gradient optimization.  revisit the approach to semi-supervised learning with generative models and further develop new models that allow effective generalization from a small labeled dataset to a large unlabeled dataset. <ref type="bibr" target="#b2">Chung et al. (2015)</ref> incorporate latent variables into the hidden state of a recurrent neural network, while <ref type="bibr" target="#b6">Gregor et al. (2015)</ref> combine a novel spatial atten- tion mechanism that mimics the foveation of human eyes, with a sequential variational auto-encoding framework that allows the iterative construction of complex images.</p><p>We follow the spirit of these variational models, but focus on the adaptation and utilization of them onto implicit DRR, which, to the best of our knowl- edge, is the first attempt in this respect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have presented a variational neural discourse relation recognizer for implicit DRR. Dif- ferent from conventional discriminative models that directly calculate the conditional probability of the relation y given discourse arguments x, our model assumes that it is a latent variable from an underly- ing semantic space that generates both x and y. In order to make the inference and learning efficient, we introduce a neural discourse recognizer and two neural latent approximators as our generative and in- ference model respectively. Using the reparameteri- zation technique, we are able to optimize the whole model via standard stochastic gradient ascent algo- rithm. Experiment results in terms of classification and variational lower bound verify the effectiveness of our model.</p><p>In the future, we would like to exploit the utiliza- tion of discourse instances with explicit relations for implicit DRR. For this we can start from two direc- tions: 1) converting explicit instances into pseudo implicit instances and retraining our model; 2) de- veloping a semi-supervised model to leverage se- mantic information inside discourse arguments. Fur- thermore, we are also interested in adapting our model to other similar tasks, such as nature language inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model p θ (x|z)p θ (y|z), dashed lines denote the variational approximation q φ (z|x, y) to the posterior p(z|x, y) and q φ (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the variational lower bound (blue color) on the training set and F-score (brown color) on the development set. Horizontal axis: the epoch numbers; Vertical axis: the F1 score for relation classification (left) and the estimated average variational lower bound per datapoint (right). Features used in SVM are taken from the stateof-the-art implicit discourse relation recognition model, including Bag of Words, Cross-Argument Word Pairs, Polarity, First-Last, First3, Production Rules, Dependency Rules and Brown cluster pair (Rutherford and Xue, 2014). In order to collect bag of words, production rules, dependency rules, and cross-argument word pairs, we used a frequency cutoff of 5 to remove rare features, following Lin et al. (2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate con- text words, word pairs as well as discourse parse information into their classifier. Following this di- rection, several more powerful features have been exploited: entities (Louis et al., 2010), word em- beddings (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>(Prasad et al., 2008) (PDTB here- after). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is or- ganized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et</figDesc><table>4 Source 
code 
is 
available 
at 
https://github.com/DeepLearnXMU/VarNDRR. 
5 http://www.seas.upenn.edu/ pdtb/ 

386 

Model 

Acc 
P 
R 
F1 
R &amp; X (2015) 
-
-
-
41.00 
J &amp; E (2015) 70.27 
-
-
35.93 
SVM 
63.10 22.79 64.47 33.68 
SCNN 
60.42 22.00 67.76 33.22 
VarNDRR 
63.30 24.00 71.05 35.88 

(a) COM vs Other 

Model 
Acc 
P 
R 
F1 
(R &amp; X (2015)) 
-
-
-
53.80 
(J &amp; E (2015)) 76.95 
-
-
52.78 
SVM 
62.62 39.14 72.40 50.82 
SCNN 
63.00 39.80 75.29 52.04 
VarNDRR 
53.82 35.39 88.53 50.56 

(b) CON vs Other 

Model 
Acc 
P 
R 
F1 
(R &amp; X (2015)) 
-
-
-
69.40 
(J &amp; E (2015)) 69.80 
-
-
80.02 
SVM 
60.71 65.89 58.89 62.19 
SCNN 
63.00 56.29 91.11 69.59 
VarNDRR 
57.36 56.46 97.39 71.48 

(c) EXP vs Other 

Model 
Acc 
P 
R 
F1 
(R &amp; X (2015)) 
-
-
-
33.30 
(J &amp; E (2015)) 87.11 
-
-
27.63 
SVM 
66.25 15.10 68.24 24.73 
SCNN 
76.95 20.22 62.35 30.54 
VarNDRR 
62.14 17.40 97.65 29.54 

(d) TEM vs Other 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classification results of different models on the implicit DRR task. Acc=Accuracy, P=Precision, R=Recall, 
and F1=F1 score. 

</table></figure>

			<note place="foot" n="1"> Unless otherwise specified, all variables in the paper, e.g., x, y, z are multivariate. But for notational convenience, we</note>

			<note place="foot" n="3"> Notice that d h 1 /d h 2 are not necessarily equal to d h 1 /d h 2 .</note>

			<note place="foot" n="7"> There is one dimension in dx 1 and dx 2 for unknown words. 8 http://svmlight.joachims.org/</note>

			<note place="foot" n="1">-475.58 59.25384 2-167.78 62.2192 3-158.7 62.41888 4-150.99 71.32476</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors were supported by National Natural Sci-<ref type="figure">No. IIP2015-4)</ref>. We also thank the anony-mous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining natural and artificial examples to improve implicit discourse relation identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="1694" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing word representations for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2201" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Reyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmiň</forename><surname>Sari´csari´c</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ontology-driven discourse analysis for information extraction</title>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="59" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral semisupervised discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="89" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>abs/1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards an open-domain conversational system fully based on natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuichiro</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyomi</forename><surname>Meguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiaki</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomi</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toru</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiro</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="928" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">One vector is not enough: Entity-augmented distributed semantics for discourse relations. TACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="329" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A latent variable recurrent neural network for discourse-driven language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="332" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">AutoEncoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="476" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the Penn Discourse Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using entity features to classify implicit discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGDIAL</title>
		<meeting>of SIGDIAL<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="59" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experiments on sense annotations and sense disambiguation of discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TLT2005</title>
		<meeting>of TLT2005</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving Implicit Discourse Relation Recognition Through Feature Set Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGDIAL</title>
		<meeting>of SIGDIAL<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting the presence of discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="914" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Easily identifiable discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mridhula</forename><surname>Raghupathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hena</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind K</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CIS</publisher>
			<biblScope unit="page">884</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Technical Reports</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-AFNLP</title>
		<meeting>of ACL-AFNLP</meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discovering implicit discourse relations through brown cluster pair representation and coreference patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the inference of implicit discourse relations via classifying explicit discourse connectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="799" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluating discourse-based answer extraction for why-question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzan</forename><surname>Verberne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Boves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peterarno</forename><surname>Coppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="735" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Implicit discourse relation recognition by selecting typical training examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2757" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dependency-based discourse parser for single-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhisa</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1834" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shallow convolutional neural network for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting discourse connectives for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
