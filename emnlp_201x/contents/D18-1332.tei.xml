<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev &amp;apos;</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>1 Microsoft Way</addrLine>
									<postCode>98121</postCode>
									<settlement>Microsoft, Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield &amp;apos;</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aji</forename><forename type="middle">&amp;apos;</forename></persName>
						</author>
						<title level="a" type="main">Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2991" to="2996"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2991</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly. In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size. Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asyn-chronous stochastic gradient descent (SGD) which makes the model convergence poor. We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27% faster than an optimized baseline with negligible penalty in BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With training times measured in days, paralleliz- ing stochastic gradient descent (SGD) is valuable for making experimental progress and scaling data sizes. Synchronous SGD sums gradients com- puted by multiple GPUs into one update, equiva- lent to a larger batch size. But GPUs sit idle unless workloads are balanced, which is difficult in ma- chine translation and other natural language tasks because sentences have different lengths. Asyn- chronous SGD avoids waiting, which is faster in terms of words processed per second. However asynchronous SGD suffers from stale gradients ( <ref type="bibr" target="#b0">Abadi et al., 2016</ref>) that degrade convergence, re- sulting in an almost no improvement in time to con- vergence ( <ref type="bibr" target="#b6">Hadjis et al., 2016)</ref>. This paper makes asynchronous SGD even faster and deploys a series of convergence optimizations.</p><p>In order to achieve fastest training (and inspired by <ref type="bibr" target="#b5">Goyal et al. (2017)</ref> we increase the mini-batch size, making the matrix operations more efficient and reducing the frequency of gradient communica- tion for the optimizer step. Unlike their task (image classification), text training consumes a lot of GPU Memory <ref type="table">(Table 1)</ref> for word embedding activations making it impossible to fit mini-batches of similar magnitude as <ref type="bibr" target="#b5">Goyal et al. (2017)</ref>.</p><p>Our main contributions are as follows:</p><p>1. We introduce a delayed gradient updates which allow us to work with much larger mini- batches which would otherwise not be possi- ble due to limited GPU memory. 2. We introduce local optimizers which run on each worker to mitigate the extra staleness and convergence issues <ref type="bibr" target="#b4">(Dekel et al., 2010;</ref><ref type="bibr" target="#b8">Keskar et al., 2017)</ref> caused by large mini-batches. 3. We highlight the importance of tuning the op- timizer momentum and show how it can be used as a cooldown strategy.</p><p>VRAM τ Words WPS 3 GB 1 3080 19.5k 7 GB 1 7310 36.6k 10 GB 1 10448 40.2k 20* GB 2 20897 44.2k 30* GB 2 31345 46.0k 40* GB 4 41794 47.6k <ref type="table">Table 1</ref>: Relationship between the GPU Mem- ory (VRAM) budget for batches (* means emu- lated by summing τ smaller batches), number of source words processed in each batch and words- per-second (WPS) measured on a shallow model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experiments</head><p>This section introduces each optimization along with an intrinsic experiment on the WMT 2016 Romanian→English task <ref type="bibr" target="#b3">(Bojar et al., 2016</ref>).</p><p>The translation system is equivalent to <ref type="bibr" target="#b15">Sennrich et al. (2016)</ref>, which was the first place constrained system (and tied for first overall in the WMT16 shared task.). The model is a shallow bidirectional GRU ( <ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>) encoder-decoder trained on 2.6 million parallel sentences. Due to variable-length sentences, machine translation sys- tems commonly fix a memory budget then pack as many sentences as possible into a dynamically- sized batch. The memory allowance for mini- batches in our system is 3 GB (for an average batch size of 2633 words). Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2015)</ref> is used to perform asynchronous SGD with learn- ing rate of 0.0001. This is our baseline system. We also compare with a synchronous baseline which uses modified Adam parameters, warmup of 16000 mini-batches and inverse square root cooldown fol- lowing <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>. We used 4 Tesla P 100 GPUs in a single node with the Marian NMT framework for training <ref type="bibr" target="#b7">(Junczys-Dowmunt et al., 2018</ref>). Since we apply optimizations over asyn- chronous SGD we performed a learning rate and mini-batch-size parameter sweep over the baseline system and settled on a learning rate of 0.00045 and 10 GB memory allowance for mini-batches (average batch size of 10449 words). This is the fastest system we could train without sacrificing performance before adding our improvements. In our experiments on <ref type="table">Table 2</ref> we refer to this sys- tem as "Optimized asynchronous". All systems were trained until 5 consecutive stalls in the cross- entropy metric of the validation set. Note that some systems require more epochs to reach this criteria which indicates poor model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Larger Batches and delayed updates</head><p>This experiment aims to increase speed, in words- per-second (WPS), by increasing the batch size. Larger batches have two well-known impacts on speed: making more use of GPU parallelism and communicating less often.</p><p>After raising the batch size to the maximum that fits on the GPU, <ref type="bibr">1</ref> we emulate even larger batches by processing multiple mini-batches and summing their gradients locally without sending them to the optimizer. This still increases speed because com- munication is reduced <ref type="table">(Table 1)</ref>. We introduce parameter τ , which is the number of iterations a GPU performs locally before communicating exter- nally as if it had run one large batch. The Words-per-second (WPS) column on <ref type="table">Table 1</ref> shows the effect on corpora processing speed when applying delayed gradients updates for different values of τ . While we reduce the overall training time if we just apply delayed gradient updates we worsen the overall convergence <ref type="table">(Table 2)</ref>.</p><p>When increasing the mini-batch size τ times without touching the learning rate, we effectively do τ times less updates per epoch. On the sur- face, it might seem that these less frequent updates are counterbalanced by the fact that each update is accumulated over a larger batch. But practical optimization heuristics like gradient clipping mean that in effect we end up updating the model less often, resulting in slower convergence. <ref type="bibr" target="#b5">Goyal et al. (2017)</ref> recommend scaling the learning rate lin- early with the mini-batch size in order to maintain convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Warmup</head><p>Goyal et al. <ref type="formula">(2017)</ref> point out that just increasing the learning rate performs poorly for very large batch sizes, because when the model is initialized at a random point, the training error is large. Large error and large learning rate result in bad "jerky" updates to the model and it can't recover from those. <ref type="bibr" target="#b5">Goyal et al. (2017)</ref> suggest that initially model updates should be small so that the model will not be pushed in a suboptimal state. Afterwards we no longer need to be so careful with our updates. <ref type="bibr" target="#b5">Goyal et al. (2017)</ref> lower the initial learning rate and gradually increase it over a number of mini- batches until it reaches a predefined maximum. This technique is also adopted in the work of <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>. This is the canonical way to perform warmup for neural network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Lowering initial learning rate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Local optimizers</head><p>We propose an alternative warm up strategy and compare it with the canonical method. Since we emulate large batches by running multiple smaller batches, it makes sense to consider whether to op- timize locally between each batch by adapting the concept of local per-worker optimizers from <ref type="bibr" target="#b18">Zhang et al. (2014)</ref>. In asynchronous SGD setting each GPU has a full copy of the model as well as the mas- ter copy of 1/N th of the parameters in its capacity as parameter server. We use the local optimizers to update the local model shard in between delayed gradient updates, which helps mitigate staleness.</p><p>Unlike prior work, we also update the shard of the global model that happens to be on the same GPU. Local updates are almost free because we avoid remote device communication.</p><p>Updating the parameter shard of the global model bears some resemblance to the Hogwild method (Recht et al., 2011) as we don't synchro- nize the updates to the shard, however, global up- dates are still synchronised. As before, once every τ iterations we run a global optimizer that updates the sharded parameter set and then distributes the updated model across all devices. Any local model divergences are lost at this point. We found that this strategy improves model convergence in the early epochs but tends to be harmful later on. We hypoth- esize that initially partial model updates reduce staleness, but when the model starts to converge, local optimizers introduce extra noise in the train- ing, which is harmful. We use local optimizers purely as a warmup strategy, turning them off after the initial phase of the training. Empirically, we found that we can get the best convergence by us- ing them for the first 4000 mini-batches that each device sees. On <ref type="table">Table 2</ref> we compare and contrast the two warmup strategies. By itself learning-rate warmup offers slower convergence but to a better point compared to local optimizers. The reader may notice that if we apply delayed gradient updates, the effective batch size that the global optimizer deals with is τ times larger than the mini-batch size on which the local optimizers runs. Therefore we use τ times lower learning rate for the local optimizers compared to the global optimizers. Momentum tuning is not a well explored area in deep learning. Most researchers simply use the de- fault values for momentum for a chosen optimizer ( <ref type="bibr" target="#b6">Hadjis et al., 2016</ref>) (in the case of NMT, this is usually Adam). <ref type="bibr" target="#b6">Hadjis et al. (2016)</ref> argue that this is an oversight especially when it comes to asyn- chronous SGD, because the asynchronisity adds extra implicit momentum to the training which is not accounted for. Because of this, asynchronous SGD has been deemed ineffective, as without mo- mentum tuning, the observed increase in training speed is negated by the lower convergence rate, re- sulting in near-zero net gain ( <ref type="bibr" target="#b0">Abadi et al., 2016</ref>). However, <ref type="bibr" target="#b6">Hadjis et al. (2016)</ref> show that after per- forming a grid search over momentum values, it is possible to achieve convergence rates typical for synchronous SGD even when working with many asynchronous workers. The downside of momen- tum tuning is that we can't offer rule-of-thumb values, as they are individually dependent on the optimizer used, the neural model, the number of workers and the batch size. In our experiments, we lowered the overall momentum and in addition per- formed momentum cooldown where we reduced the momentum of our optimizer (Adam) after the first few thousand batches. <ref type="table">Table 2</ref> shows the effect of modifying momentum values. When using just delayed gradient updates, training is noticeably faster, but there are signifi- cant regressions in BLEU and CE (system 2). In order to mitigate those, when using delayed gradi- ent updates, we tune the momentum and apply mo- mentum cooldown on top of either of our warmup strategies. By doing this we not only further reduce training time, but also recover the loss of accuracy. Compared to the optimized baseline system (1), our best system (4) reduces the training time by 27%. Progression of the training can be seen on figures 1 and 2. Our system starts poorly compared to the baselines in terms of epoch-for-epoch convergence, but catches up in the later epochs. Due to faster training speed however, the desired BLEU score is achieved faster <ref type="figure" target="#fig_2">(Figure 2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Momentum cooldown and tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Results</head><p>Local optimizers as a warmup strategy show faster convergence compared to learning rate warmup at almost no penalty to BLEU or cross- entropy (System 4 vs system 6). Against the sys- tem used in WMT 16 ( <ref type="bibr" target="#b15">Sennrich et al., 2016)</ref>, we achieve nearly 4 times faster training time with no discernible penalty in BLEU or CE. In contrast, the other communication reducing method tested, the work of <ref type="bibr" target="#b1">Aji and Heafield (2017)</ref>, is slower than our work and achieves worse BLEU and CE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Using even larger mini-batches</head><p>We can achieve even greater processing speed by further increasing τ but we were unable to maintain the same convergence with the Romanian-English shallow model. We found that larger τ values are useful when dealing with the larger deep RNN mod-  <ref type="formula">(1)</ref> 4.97 10 35.56 50.90 40.2k (1) + Aji and Heafield <ref type="bibr">(2017)</ref> 4.32 11 35.16 52.02 41.5k (1) + delayed updates τ = 2 (2) <ref type="formula">(6)</ref> 3.98 11 35.76 50.73 44.2k <ref type="table">Table 2</ref>: Romanian-English results from our exploration and optimizations. We also compare our methods against the work of Aji and Heafield (2017) which also reduces communication. We use system (1) as our reference baseline upon which we improve. The system that achieved the best training time is bolded. els. With deep RNN models the parameters take the majority of the available VRAM leaving very little for mini-batches. In this scenario we can apply τ = 4 without negative effect towards convergence. We demonstrate the effectiveness of larger τ on Ta- ble 3. The baseline system is equivalent to the win- ning system for English-German at the WMT 2017 competition ( <ref type="bibr" target="#b14">Sennrich et al., 2017</ref>). The baseline is trained with synchronous SGD and our system uses asynchronous SGD, delayed gradient updates by a factor of 4, local optimizers and the momentum is tuned and further reduced after the first 16000 mini-batches. We found learning rate of 0.0007 to work the best. We do not report the numbers for asynchronous baseline because we were unable to achieve competitive BLEU scores without using delayed gradient updates. We speculate this is be- cause with this type of deep model, our mini-batch size is very small leading to very jerky and unstable training updates. Larger mini-batches ensure the gradients produced by different workers are going to be closer to one another. Our training progres- sion can be seen on figures 3 and 4. We show that even though we use 4 times larger mini-batches we actually manage to get lower Cross-Entropy epoch for epoch compared to the baseline <ref type="figure" target="#fig_3">(Figure 3</ref>). This coupled with out higher training speed makes our method reach the best BLEU score 1.6 times faster than the baseline <ref type="figure" target="#fig_4">(Figure 4</ref>).</p><note type="other">4.20 11 34.82 51.68 44.2k (2) + local optimizers (3) 3.66 10 35.45 51.32 44.2k (3) + momentum tuning (4) 3.66 10 35.48 50.87 44.2k (2) + warmup (5) 4.87 13 35.29 50.78 44.2k (5) + momentum tuning</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work</head><p>We use larger mini-batches and delay gradient up- dates in order to increase the speed at which the dataset is processed. The principal reason why this works is because when mini-batch size is increased n (also includes delayed updates) times, commu- nication is reduced by the same amount. This as-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Time (h) BLEU CE Baseline 51.3 25.1 47.31 Async (4) + τ = 4 39. <ref type="bibr">7</ref> 25.07 46.59  pect of our work is similar to the work of <ref type="bibr" target="#b1">Aji and Heafield (2017)</ref> where they drop the lower 99% of the gradient updates based on absolute value thus reducing the memory traffic. Compared with them we achieve faster dataset processing speed and also better model convergence as shown on <ref type="table">Table 2</ref>.</p><p>Independently from us <ref type="bibr" target="#b11">Mao et al. (2018)</ref> extend the work of <ref type="bibr" target="#b1">Aji and Heafield (2017)</ref> aiming to re- duce gradient communication without suffering any of the negative effects we have noted. In process they independently arrive to some of the methods that we use, notably tuning the momentum and applying warmup to achieve better convergence.</p><p>Independently from us Shazeer and Stern (2018) have done further exploratory work on ADAM's momentum parameters using the Transformer model ( <ref type="bibr" target="#b17">Vaswani et al., 2017</ref>) as a case study and have offered a mathematical explanation about why different stages of the training require different mo- mentum values. <ref type="bibr">2</ref> Independently from us Saunders et al. (2018) have employed delayed gradient updates in syntax NMT setting, where the sequences are much longer due to the syntax annotation and delayed updates are necessary because video RAM is limited. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future work</head><p>We show that we can increase speed and main- tain convergence rate for very large mini-batch asynchronous SGD by carefully adjusting momen- tum and applying warmup and cooldown strate- gies. While we have demonstrated our methods on GPUs, they are hardware agnostic and can be applied to neural network training on any multi- device hardware such as TPUs or Xeon Phis. We were able to achieve end-to-end training on multi- ple tasks a lot faster than the baseline systems. For our Romanian-English model, we train nearly 3X faster than the commonly used baseline and 1.5X faster over a specifically optimised baseline. When experimenting with English-German we are able to train our model 1.3X faster than the baseline model, achieving practically the same BLEU score and much better model cross-entropy.</p><p>In the future we would like to apply local opti- mizers in distributed setting where the communi- cation latency between local and remote devices varies significantly we could use local optimizers to synchronize remote models less often.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Goyal et al. (2017) and Vaswani et al. (2017) both employ cooldown strategies that lower the learning rate towards the end of training. Inspired by the work of Hadjis et al. (2016) however we decided to pursue a different cooldown strategy by modifying the momentum inside Adam's parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cross-entropy training progression per epoch for our ro-en systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLEU scores for our ro-en systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CE scores for our en-de systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: BLEU scores for our en-de systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Training times for English-German deep 
RNN system trained on WMT17 data. Our asyn-
chronous system includes the optimizations of sys-
tem (4) from Table 2. 

</table></figure>

			<note place="foot" n="1"> The Tesla P100 has 16 GB of GPU memory and we opt to use 10 GBs of mini-batches and the rest is used to store model parameters, shards, optimizers and additional system specific elements such as the cache vectors for gradient dropping (Aji and Heafield, 2017).</note>

			<note place="foot" n="2"> This work was published on 11.04.2018. 3 This work was published on 01.05.2018.</note>

			<note place="foot" n="4"> This work was published on 22.08.2018</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Adam Lopez, Sameer Bansal and Naomi Saphra for their help and comments on the paper. We thank our reviewers for their comments and suggestions. Nikolay Bogoychev was funded by an Amazon faculty research award to Adam Lopez.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">Gordon</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1605.08695</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparse communication for distributed gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Jimeno</forename><surname>Yepes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelie</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Optimal distributed online prediction using mini-batches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Yangqing Jia, and Kaiming He</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Omnivore: An optimizer for multi-device deep learning on cpus and gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadjis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mudigere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2017</title>
		<meeting>ICLR 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note>Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Don&apos;t use large mini-batches, use local sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno>abs/1808.07217</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep gradient compression: Reducing the communication bandwidth for distributed training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-representation ensembles and delayed sgd updates improve syntax-based nmt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<idno>abs/1805.00456</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Adrì a de Gispert, and Bill Byrne</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The University of Edinburgh&apos;s Neural MT Systems for WMT17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Shared Task Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edinburgh Neural Machine Translation Systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno>abs/1804.04235</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep learning with elastic averaging SGD. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1412.6651</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
