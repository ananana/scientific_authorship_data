<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
							<email>nikolaos.pappas@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">EPFL and Idiap Research Institute</orgName>
								<address>
									<addrLine>Rue Marconi 19</addrLine>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
							<email>andrei.popescu-belis@idiap.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">Idiap Research Institute</orgName>
								<address>
									<addrLine>Rue Marconi 19</addrLine>
									<postCode>CH-1920</postCode>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="455" to="466"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multiple-instance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models , namely SVR and Lasso, by more than 10% relative in terms of MSE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment analysis of texts provides a coarse- grained view of their overall attitude towards an item, either positive or negative. The recent abun- dance of user texts accompanied by real-valued la- bels e.g. on a 5-star scale has contributed to the de- velopment of automatic sentiment analysis of re- views of items such as movies, books, music or other products, with applications in social com- puting, user modeling, and recommender systems. The overall sentiment of a text towards an item often results from the ratings of several specific aspects of the item. For instance, the author of a review might have a rather positive sentiment about a movie, having particularly liked the plot and the music, but not too much the actors. De- termining the ratings of each aspect automatically is a challenging task, which may seem to require the engineering of a large number of features de- signed to capture each aspect. Our goal is to put forward a new feature-agnostic solution for ana- lyzing aspect-related ratings expressed in a text, thus aiming for a finer-grained, deeper analysis of text meaning than overall sentiment analysis.</p><p>Current state-of-the-art approaches to sentiment analysis and aspect-based sentiment analysis, at- tempt to go beyond word-level features either by using higher-level linguistic features such as POS tagging, parsing, and knowledge infusion, or by learning features that capture syntactic and seman- tic dependencies between words. Once an appro- priate feature space is found, the ratings are typi- cally modeled using a linear model, such as Sup- port Vector Regression (SVR) with 2 norm for regularization or Lasso Regression with 1 norm. By treating a text globally, these models ignore the fact that the sentences of a text have diverse con- tributions to the overall sentiment or to the attitude towards a specific aspect of an item.</p><p>In this paper, we propose a new learning model which answers the following question: "To what extent does each part of a text contribute to the prediction of its overall sentiment or the rating of a particular aspect?" The model uses multiple- instance regression (MIR), based on the assump- tion that not all the parts of a text have the same contribution to the prediction of the rating. Specif- ically, a text is seen as a bag of sentences (in- stances), each of them modeled as a word vector. The overall challenge is to learn which sentences refer to a given aspect, and how they contribute to the text's attitude towards it, but the model applies to overall sentiment analysis as well. For instance, <ref type="figure" target="#fig_0">Figure 1</ref> displays a positive global comment on a TED talk and the weights assigned to two of its sentences by MIR. Using regularized least squares, we formulate an optimization objective to jointly assign instance weights and regression hyperplane weights. Then, an instance relevance estimation method is used to predict aspect ratings, or global ones, in previ- ously unseen texts. The parameters of the model are learned using an alternating optimization pro- cedure inspired by <ref type="bibr" target="#b29">Wagstaff and Lane (2007)</ref>. Our model requires only text with ratings for training, with no particular assumption on the word fea- tures to be extracted, and provides interpretable explanations of the predicted ratings through the relevance weights assigned to sentences. We also show that the model has reasonable computational demands. The model is evaluated on aspect and sentiment rating prediction over seven datasets: five of them contain reviews with aspect labels about beers, audiobooks and toys <ref type="bibr" target="#b10">(McAuley et al., 2012)</ref>, and two contain TED talks with emotion la- bels, and comments on them with sentiment labels <ref type="bibr" target="#b15">(Pappas and Popescu-Belis, 2013)</ref>. Our model outperforms previous MIR models and two strong linear models for rating prediction, namely SVR and Lasso by more than 10% relative in terms of MSE. The improvement is observed even when the sophistication of the feature space increases.</p><p>The paper is organized as follows. Section 2 shows how our model innovates with respect to previous work on MIR and rating prediction. Sec- tion 3 formulates the problem while Section 4 de- scribes previous MIR models. Section 5 presents our MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Sec- tion 7 reports our results on rating prediction tasks, and provides examples of rating explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multiple-Instance Regression</head><p>Multiple-instance regression (MIR) belongs to the class of multiple-instance learning (MIL) prob- lems for real-valued output, and it is a variant of multiple regression where each data point may be described by more than one vectors of values. Many MIL studies focused on classification <ref type="bibr" target="#b0">(Andrews et al., 2003;</ref><ref type="bibr" target="#b2">Bunescu and Mooney, 2007;</ref><ref type="bibr" target="#b21">Settles et al., 2008;</ref><ref type="bibr" target="#b5">Foulds and Frank, 2010;</ref><ref type="bibr" target="#b32">Wang et al., 2011</ref>) while fewer focused on regression <ref type="bibr" target="#b18">(Ray and Page, 2001;</ref><ref type="bibr">Davis and others, 2007;</ref><ref type="bibr" target="#b30">Wagstaff et al., 2008;</ref><ref type="bibr" target="#b29">Wagstaff and Lane, 2007)</ref>. Related to document analysis, several MIR stud- ies have focused on news categorization <ref type="bibr" target="#b35">(Zhang and Zhou, 2008;</ref>) or web-index recommendation ( <ref type="bibr" target="#b37">Zhou et al., 2005</ref>) but, to our knowledge, no study has attempted to use MIR for aspect rating prediction or sentiment analysis with real-valued labels. MIR was firstly introduced by <ref type="bibr" target="#b18">Ray et al. (2001)</ref>, proposing an EM algorithm which assumes that one primary instance per bag is responsible for its label. <ref type="bibr" target="#b29">Wagstaff and Lane (2007)</ref> proposed to simultaneously learn a regression model and es- timate instance weights per bag for crop yield modeling (not applicable to prediction). A simi- lar method which learns the internal structure of bags using clustering was proposed by <ref type="bibr" target="#b30">Wagstaff et al. (2008)</ref> for crop yield prediction, and we will use it for comparison in the present study. Later, the method was adapted to map bags into a single- instance feature space by <ref type="bibr" target="#b36">Zhang et al. (2009)</ref>. <ref type="bibr" target="#b31">Wang et al. (2008)</ref> assumed that each bag is gener- ated by random noise around a primary instance, while <ref type="bibr" target="#b33">Wang et al. (2012)</ref> represented bag labels with a probabilistic mixture model. <ref type="bibr" target="#b5">Foulds et al. (2010)</ref> concluded that various assumptions are differently suited to different tasks, and should be stated clearly when describing an MIR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Rating Prediction from Text</head><p>Sentiment analysis aims at analyzing the polar- ity of a given text, either with classification (for discrete labels) or regression (for real-valued la- bels). Early studies introduced machine learning techniques for sentiment classification, e.g. <ref type="bibr" target="#b14">Pang et al. (2002)</ref>, including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. <ref type="bibr" target="#b28">Turney et al. (2002)</ref>. Other studies focused on subjectivity detection, i.e. whether a text span expresses opinions or not ( <ref type="bibr" target="#b34">Wiebe et al., 2004</ref>). Rating inference was defined by <ref type="bibr" target="#b12">Pang et al. (2005)</ref> as multi-class classification or regres- sion with respect to rating scales. <ref type="bibr" target="#b13">Pang and Lee (2008)</ref> discusses the large range of features engi- neered for this task, though several recent stud- ies focus on feature learning <ref type="bibr" target="#b9">(Maas et al., 2011;</ref><ref type="bibr" target="#b23">Socher et al., 2011</ref>), including the use of a deep neural network <ref type="bibr" target="#b24">(Socher et al., 2013</ref>). In contrast, we do not make any assumption about the nature or dimensionality of the feature space.</p><p>The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usu- ally requires aspect-related text segmentation, fol- lowed by prediction or summarization ( <ref type="bibr" target="#b6">Hu and Liu, 2004;</ref><ref type="bibr" target="#b40">Zhuang et al., 2006</ref>). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content mod- els ( <ref type="bibr" target="#b11">Mei et al., 2007;</ref><ref type="bibr" target="#b27">Titov and McDonald, 2008;</ref><ref type="bibr" target="#b20">Sauper et al., 2010;</ref><ref type="bibr" target="#b8">Lu et al., 2011)</ref>, or with lin- guistic features <ref type="bibr" target="#b12">(Pang and Lee, 2005;</ref><ref type="bibr" target="#b1">Baccianella et al., 2009;</ref><ref type="bibr" target="#b17">Qu et al., 2010;</ref><ref type="bibr" target="#b39">Zhu et al., 2012</ref>). Other studies have advocated joint modeling of multiple aspects <ref type="bibr" target="#b22">(Snyder and Barzilay, 2007)</ref> or multiple reviews for the same product ( <ref type="bibr" target="#b7">Li et al., 2011</ref>). <ref type="bibr" target="#b10">McAuley et al. (2012)</ref> introduced new cor- pora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detec- tion, sentiment summarization and rating predic- tion. Lastly, joint aspect identification and senti- ment classification have been used for aggregating product review snippets by <ref type="bibr" target="#b19">Sauper at al. (2013)</ref>. None of the above studies considers the multiple- instance property of text in their modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MIR Definition</head><p>Let us consider a set B of m bags with numerical labels Y as input data</p><formula xml:id="formula_0">D = {({b 1j } d n 1 , y 1 ), ..., ({b mj } d nm , y m )}, where b ij ∈ R d (for 1 ≤ j ≤ n i ) and y i ∈ R.</formula><p>Each bag B i consists of n i data points (called 'instances'), hence it is a matrix of n i d-dimensional vectors, e.g. word vectors. The challenge is to infer the label of the bag given a variable number of in- stances n i . This requires finding a set of bag rep- resentations X = {x 1 , . . . , x m } of size m where x i ∈ R d , from which the class labels can be com- puted. The goal is then to find a mapping from this representation, noted Φ : R d → R, which is able to predict the label of a given bag. Ideally, assuming that X is the best bag representation for our task, we look for the optimal regression hyper- plane Φ which minimizes a loss function L plus a regularization term Ω as follows:</p><formula xml:id="formula_1">Φ = arg min Φ L(Y, X, Φ) loss + Ω(Φ) reg.<label>(1)</label></formula><p>Since the best set of representations X for a task is generally unknown, one has to make assumptions to define it or compute it jointly with the regres- sion hyperplane Φ. Thus, the main difficulty lies in finding a good assumption for X, as we will now discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Previous MIR Assumptions</head><p>We describe here three assumptions frequently made in past MIR studies, to which we will later compare our model: aggregating all instances, keeping them as separate examples, or choosing the most representative one ( <ref type="bibr" target="#b33">Wang et al., 2012</ref>).</p><p>For each assumption, we will experiment with two state-of-the-art regression models (noted ab- stractly as f ), namely SVR ( <ref type="bibr" target="#b4">Drucker et al., 1996)</ref> and Lasso (Tibshirani, 1996) with respectively the 2 and 1 norms for regularization. The Aggregated algorithm assumes that each bag is represented as a single d-dimensional vec- tor, which is the average of its instances (hence</p><formula xml:id="formula_2">x i ∈ R d ).</formula><p>Then, a regression model f is trained on pairs of vectors and class labels, D agg = {(x i , y i ) | i = 1, . . . , m}, and the predicted class of an unlabeled bag B i = {b ij | j = 1, . . . , n i } is computed as follows:</p><formula xml:id="formula_3">ˆ y(B i ) = f (mean({b ij | j = 1, . . . , n i })) (2)</formula><p>In fact, a simple sum can also be used instead of the mean, and we observed in practice that with an appropriate regularization there is no difference on the prediction performance between these options. This baseline corresponds to the typical approach for text regression tasks, where each text sample is represented by a single vector in the feature space (e.g. BOW with counts or TF-IDF weights).</p><p>The Instance algorithm considers each of the in- stances in a bag as separate examples, by labeling each of them with the bag's label. A regression model f is learned over the training set made of all vectors of all bags, D ins = {(b ij , y i ) | j = 1, . . . , n i ; i = 1, . . . , m}, assuming that there are m labeled bags. To label a new bag B i , given that there is no representation x i , the method simply averages the predicted labels of its instances:</p><formula xml:id="formula_4">ˆ y(B i ) = mean({f (b ij ) | j = 1, . . . , n i }) (3)</formula><p>Instead of the average, the median value can also be used, which is more appropriate when the bags contain outlying instances.</p><p>The Prime algorithm assumes that a single in- stance in each bag is responsible for its label <ref type="bibr" target="#b18">(Ray and Page, 2001</ref>). This instance is called the pri- mary or prime one. The method is similar to the previous one, except that only one instance per bag is used as training data:</p><formula xml:id="formula_5">D pri = {(b p i , y i ) | i = 1, . . . , m}, where b p i</formula><p>is the prime instance of the i th bag B i and m is the number of bags. The prime instances are discovered through an itera- tive algorithm which refines the regression model f . Given an initial model f , in each iteration the algorithm selects from each bag a prime candidate which is the instance with the lowest prediction er- ror. Then, a new model is trained over the selected prime candidates, until convergence. For a new bag, the target class is computed as in Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Proposed MIR Model</head><p>We propose a new MIR model which assigns in- dividual relevance values (weights) to each in- stance of a bag, thus making fewer simplifying assumptions than previous models. We extend instance-relevance algorithms such as ( <ref type="bibr" target="#b29">Wagstaff and Lane, 2007)</ref> by supporting high-dimensional feature spaces, as required for text regression, and by predicting both the class label and the con- tent structure of previously unseen (hence unla- beled) bags. The former is achieved by minimiz- ing a regularized least squares loss (RLS) instead of solving normal equations, which is prohibitive in large spaces. The latter represents a significant improvement over Aggregated and Instance algo- rithms, which are unable to pinpoint the most rel- evant instances with respect to the label of each bag, being thus applicable only to bag label pre- diction. Similarly, Prime only identifies the prime instance when the bag is already labeled. Instead, our model learns an optimal method to aggregate instances, rather than a pre-defined one, and al- lows more degrees of freedom in the regression model than previous ones. Moreover, the weight of an instance is interpreted as its relevance both in training and prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Instance Relevance Assumption</head><p>Each bag defines a bounded region of a hyper- plane orthogonal to the y-axis (the envelope of all its points). The goal is to find a regression hy- perplane that passes through each bag B i and to predict its label by using at least one data point x i within that bounded region. Thus, the point x i is a convex combination of the points in the bag, in other words B i is represented by the weighted average of its instances b ij :</p><formula xml:id="formula_6">x i = n i j=1 ψ ij b ij , ψ ij ≥ 0 and n i j=1 ψ ij = 1 (4)</formula><p>where ψ ij is the weight of the j th instance of the i th bag. Each weight ψ ij indicates the relevance of an instance j to the prediction of the class y i of the i th bag. The constraint forces x i to fall within the bounded region of the points in bag i and guar- antees that the i th bag will influence the regressor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Modeling Bag Structure and Labels</head><p>Let us consider a set of m bags, where each bag B i is represented by its n i d-dimensional instances, i.e. B i = {b ij } d n i along with the set of target class labels for each bag, Y = {y i } N , y i ∈ R. The representation set of all B i in the feature space, X = {x 1 , . . . , x m }, x i ∈ R d , is obtained using the n i instance weights associated to each bag B i , ψ i = {ψ ij } n i , ψ ij ∈ [0, 1] which are initially unknown. Thus, we look for a linear regression model f that is able to model the target values us- ing the regression coefficients Φ ∈ R d , where X and Y are respectively the sets of training bags and their labels: Y = f (X) = Φ T X. We define a loss function according to the least squares objective dependent on X, Y , Φ and the set of weight vec- tors Ψ = {ψ 1 , . . . , ψ m } using Eq. 4 as follows:</p><formula xml:id="formula_7">L(Y, X, Ψ, Φ) = ||Y − Φ T X|| 2 2 (4) = N i=1 y i − Φ T n i j=1 ψ ij b ij 2 = N i=1 y i − Φ T (B i ψ i ) 2<label>(5)</label></formula><p>Using the above loss function, accounting for the constraints of our assumption in Eq. 4 and assum- ing 2 -norm for regularization with 1 and 2 terms for each ψ i ∈ Ψ and Φ respectively, we obtain the following least squares objective from Eq. 1:</p><formula xml:id="formula_8">arg min ψ 1 ,...,ψm,Φ m i=1 ∆ 2 i f 1 loss + 1 ||ψ i || f 1 reg. f 2 loss + 2 ||Φ|| 2 f 2 reg.</formula><p>where</p><formula xml:id="formula_9">∆ 2 i = y i − Φ T (B i ψ i ) 2 ,<label>(6)</label></formula><p>subject to ψ ij ≥ 0 ∀i, j and n i j=1 ψ ij = 1 ∀i. The selection of the 2 -norm was based on prelim- inary results showing that it outperforms 1 -norm. Other combinations of p-norm regularization can be explored for f 1 and f 2 , e.g. to learn sparser in- stance weights and denser regression coefficients or vice versa.</p><p>The above objective is non-convex and difficult to optimize because the minimization is with re- spect to all ψ 1 , . . . , ψ m and Φ at the same time. As indicated in Eq. 6 above, we will note f 1 a model that is learned from the minimization only with re- spect to ψ 1 , . . . , ψ m and f 2 a model obtained from the minimization with respect to Φ only. In Eq. 6, we can observe that if one of the two is known or held fixed, then the other one is convex and can be learned with the well-known least squares solving techniques. In Section 5.3, we will describe an al- gorithm that is able to exploit this observation.</p><p>Having computed ψ 1 , . . . , ψ m and Φ, we could predict a label for an unlabeled bag using Eq. 3, but would not be able to compute the weights of the instances. Moreover, information that has been learned about the instances during the train- ing phase would not be used during prediction. For these reasons, we introduce a third regression model f 3 with regression coefficients O ∈ R d as- suming a 2 -norm for the regularization with 3 term, which is trained on the relevance weights obtained from the Eq. 6, D w = {(b ij , ψ ij ) | i = 1, ..., m; j = 1, ..., n i }. The optimization objec- tive for the f 3 model is the following:</p><formula xml:id="formula_10">arg min O N i=1 n i j=1 ψ ij − O T b ij 2 f 3 loss function + 3 ||O|| 2 f 3 reg.<label>(7)</label></formula><p>This minimization can be easily performed with the well-known least squares solving techniques. The learned model is able to estimate the weights of the instances of an unlabeled bag during pre- diction time as:</p><formula xml:id="formula_11">ˆ ψ i = f 3 (B i ) = Ω T B i .</formula><p>Thê ψ i weights are estimations which are influenced by the relevance weights learned in our minimization objective of Eq. 6 but they are not constrained at prediction time. To obtain interpretable weights, we can convert the estimated scores to the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> interval as follows:</p><formula xml:id="formula_12">ˆ ψ i = ˆ ψ i /sum( ˆ ψ i ).</formula><p>Finally, the prediction of the label for the i th bag using the estimated instance weightsˆψweightsˆ weightsˆψ i is done as follows:</p><formula xml:id="formula_13">ˆ y = f 2 (B i ) = Φ T B i ˆ ψ i<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Learning with Alternating Projections</head><p>Algorithm 1 solves the non-convex optimization problem of Eq. 6 by using a powerful class of methods for finding the intersection of convex sets, namely alternating projections (AP). The prob- lem is firstly divided into two convex problems, namely f 1 loss function and f 2 loss function, which are then solved in an alternating fashion. Like EM algorithms, AP algorithms do not have general guarantees on their convergence rate, al- though, in practice, we found it acceptable at gen- erally fewer than 20 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 APWeights(B, Y , 1, 2, 3)</head><p>1: Initialize(ψ1, . . . , ψN , Φ, X) 2: while not converged do The algorithm takes as input the bags B i , their target class labels Y and the regularization terms 1 , 2 , 3 and proceeds as follows. First, under a fixed regression model (f 2 ), it proceeds with f 1 to the optimal assignment of weights to the in- stances of each bag (projection of Φ vectors on the ψ i space which is a n i -simplex) and com- putes its new representation set X. Second, given the fixed instance weights, it trains a new regres- sion model (f 2 ) using X (projection back to the Φ  <ref type="table">Table 1</ref>: Description of the seven datasets used for aspect, sentiment and emotion rating prediction.</p><p>space). This procedure repeats until convergence, i.e. when there is no more decrease on the training error, or until a maximum number of iterations has been reached. The regression model f 3 is trained on the weights learned from the previous steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Complexity Analysis</head><p>The overall time complexity T of Algorithm 1 in terms of the input variables, noted h = {m, ˆ n, d}, with m being the number of bags, ˆ n the average size of the bags, and d the dimensionality of the feature space (here, the size of word vectors), is derived as follows:</p><formula xml:id="formula_14">T (h) = T ap (h) + T f 3 (h) = O m(ˆ n 2 + d 2 ) + O mˆndmˆnd 2 = O m(ˆ n 2 + d 2 + ˆ nd 2 ) ,<label>(9)</label></formula><p>where T ap and T f 3 are respectively the time com- plexity of the AP procedure and of training the f 3 model. Eq. 9 shows that whenˆnwhenˆ whenˆn m, the model complexity is linear with the input bags m and al- ways quadratic with the number of features d. Previous works on relevance assignment for MIR have prohibitive complexity for high- dimensional feature spaces or numerous bags and hence they are not most appropriate for text regres- sion tasks. <ref type="bibr" target="#b29">Wagstaff and Lane (2007)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Data, Protocol and Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Aspect Rating Datasets</head><p>We use seven datasets summarized in <ref type="table">Table 1</ref>. Five publicly available datasets were built for as- pect prediction by <ref type="bibr" target="#b10">McAuley et al. (2012)</ref> -Beer- Advocate, Ratebeer (ES), RateBeer (FR), Audio- books and Toys &amp; Games -and have aspect rat- ings assigned by their creators on the respective websites. On the set of comments on TED talks from Pappas and Popescu-Belis (2013), we aim to predict two things: talk-level emotion dimen- sions assigned by viewers through voting, and comment polarity scores assigned by crowdsourc- ing. The distributions of aspect ratings per dataset are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Five datasets are in En- glish, one in Spanish (Ratebeer) and one in French (RateBeer), so our results will also demonstrate the language-independence of our method.</p><p>From every dataset we kept 1,200 texts as bags of sentences, but we also used three full-size datasets, namely Ratebeer ES (1,259 labeled re- views), Ratebeer FR (17,998) and Audiobooks (10,989). The features for each of them are word vectors with binary attributes signaling word pres- ence or absence, in a traditional bag-of-words model (BOW). The word vectors are provided with the first five datasets and we generated them for the latter two, after lowercasing and stopword removal. Moreover, for TED comments, we com- puted TF-IDF scores using the same dimension- ality as with BOW to experiment with a different feature space. The target class labels were nor- malized by the maximum rating in their scale, ex- cept for TED talks where the votes were normal- ized by the maximum number of votes over all the emotion classes for each talk, and two emotions, 'informative' and 'ok', were excluded as they are neutral ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Protocol</head><p>We compare the proposed model, noted AP- Weights, with four baseline ones -Aggre- gated, Instance, Prime (Section 4) and Clus- tering (from github.com/garydoranjr/ mcr), which is an instance relevance method pro- posed by <ref type="bibr" target="#b30">Wagstaff et al. (2008)</ref> for aspect rating prediction. First, for each aspect class, we opti- mize all methods on a development set of 25% of the data (300 randomly selected bags). Then, we perform 5-fold cross-validation for every as- pect on each entire data set and report the average error scores using the optimal hyper-parameters per method. In addition, we report for compar- ison the scores of AverageRating, which always predicts the average rating over the training set.</p><p>We report standard error metrics for regression, namely the Mean Absolute Error (MAE) and the Mean Squared Error (MSE). The former measures the average magnitude of errors in a set of predic- tions while the latter measures the average of their squares, which are defined over the test set of bags B i respectively as MAE = (</p><formula xml:id="formula_15">k i=1 |f (B i )−y i |)/k and MSE = ( k i=1 (f (B i ) − y i ) 2 )/k.</formula><p>The cross- validation scores are obtained by averaging the MAE and MSE scores on each fold.</p><p>To find the optimal hyper-parameters for each model, we perform 3-fold cross-validation on the development set using exhaustive grid-search over a fine-grained range of possible values and se- lect the ones that perform best in terms of MAE. The hyper-parameters to be optimized for the baselines (except AverageRating) are the regular- ization terms λ 2 , λ 1 of their possible regression model f , namely SVR which uses the 2 norm and Lasso which uses the 1 norm. As for AP- Weights, it relies on three regularization terms, namely 1 , 2 , 3 of the 2 -norm for f 1 , f 2 and f 3 regression models. Lastly, for the Clustering baseline, we use the f 2 regression model, which relies on 2 and the number of clusters k, opti- mized over {5, ..., 50} with step 5, for its cluster- ing algorithm, here k-Means. All the regulariza- tion terms are optimized over the same range of possible values, noted a · 10 b with a ∈ {1, . . . , 9} and b ∈ {−4, . . . , +4}, hence 81 values per term. For the regression models and evaluation proto- col, we use the scikit-learn machine learning li- brary ( <ref type="bibr" target="#b16">Pedregosa et al., 2012)</ref>. Our code and data are available in the first author's website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Aspect Rating Prediction</head><p>The results for aspect rating prediction are given in <ref type="table">Table 2</ref>. The proposed APWeights method outperforms Aggregated ( 2 ) and Aggregated ( 1 ) i.e. SVR and Lasso along with all other baselines on each case. The SVR baseline has on average 11% lower performance than APWeights in terms of MSE and about 6% in terms of MAE. Simi- larly, the Lasso baseline has on average 13% lower MSE and 8% MAE than APWeights. As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, APWeights also outperforms them for each aspect in the five review datasets. The In- stance method with 1 performed well on BeerAd- vocate and Toys &amp; Games (for MSE), and with 2 performed well on Ratebeer (ES), RateBeer (FR) and Toys &amp; Games (for MAE). Therefore, the instance-as-example assumption is quite appropri- ate for this task, however both options score be- low APWeights -by about 5% MAE, and 8%/9% MSE, respectively. The Prime method with 1 per- formed well only on the BeerAdvocate dataset and Prime with 2 only on the Toys &amp; Games dataset, always with lower scores than APWeights, namely about 9% MAE for both and 15%/18% MSE re- spectively. This suggests that the primary-instance   <ref type="table">Table 2</ref>: Performance of aspect rating prediction (the lower the better) in terms of MAE and MSE (× 100) with 5-fold cross-validation. All scores are averaged over all aspects in each dataset. The scores of the best method are in bold and the second best ones are underlined. Significant improvements (paired t-test, p &lt; 0.05) are in italics. <ref type="figure" target="#fig_5">Fig. 4</ref> shows MSE scores per aspect for three methods on five datasets.</p><note type="other">(1) 12.90 2.97 15.78 3.97 12.70 2.76 20.65 6.46 21.09 6.79 Prime (2) 14.60 3.64 15.05 3.68 12.92 2.98 20.12 6.59 20.11 6.92 Clustering (2) 13.95 3.26 15.06 3.64 12.23 2.60 20.50 6.48 20.59 6.52 APWeights (2) 12.24 2.66 14.18 3.28 11.37 2.27 18.89 5.71 18.50 5.57 APW vs. SVR (%) +16.0 +27.7 +2.0 +3.8 +7.6 +15.6 +1.0 +4.5 +2.6 +6.0 APW vs. Lasso (%) +10.1 +15.1 +11.0 +18.4 +6.8 +11.8 +6.0 +6.9 +8.1 +11.9 APW vs. 2 nd best (%) +3.3 +7.8 +1.5 +3.3 +3.7 +4.9 +1.0 +4.5 +2.6 +6.0</note><p>assumption is not the most appropriate for this task. Lastly, even though Clustering is an instance relevance method, it has similar scores to Prime, presumably because the relevances are assigned according to the computed clusters and they are not directly influenced by the task's objective.</p><p>To compare with the state-of-the-art results ob- tained by <ref type="bibr" target="#b10">McAuley et al. (2012)</ref>, we experimented with three of their full-size datasets. Splitting each dataset in half for training vs. testing, and using the optimal settings from our experiments above, we measured the average MSE over all aspects. APWeights improved over Lasso by 10%, 26% and 17% MSE respectively on each dataset -the absolute MSE scores are .038 for Lasso vs. .034 for APWeights on Ratebeer SP; .023 vs. .017 on Ratebeer FR; .063 vs. .052 on Audiobooks. Sim- ilarly, when compared to the best SVM baseline provided by the McAuley et al., our method im- proved by 32%, 43% and 35% respectively on each dataset, though it did not use their rating model. Moreover, the best model proposed by McAuley et al., which uses a joint rating model and an aspect-specific text segmenter trained on hand-labeled data, reaches MSE scores of .03, .02 and .03, which is comparable to our model that does not use these features (.034, .017, .052), though it could benefit from them in the future. Lastly, as mentioned by the same authors, predic- tors which use segmented text, for example with topic models as in ( <ref type="bibr" target="#b8">Lu et al., 2011</ref>), do not neces- sarly outperform SVR baselines; instead they have marginal or even no improvements, therefore, we did not further experiment with them. Interes-  <ref type="table">Table 3</ref>: MAE and MSE (× 100) on sentiment and emotion prediction with 5-fold c.-v. Scores on TED talks are averaged over the 12 emotions. The scores of the best method are in bold and the second best ones are underlined. Significant im- provements (paired t-test, p &lt; 0.05) are in italics.</p><p>tignly, multiple-instance learning algorithms un- der several assumptions go beyond SVR baselines with BOW and even more sophisticated features such as TF-IDF (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Sentiment and Emotion Prediction</head><p>Our method is also competitive for sentiment pre- diction over comments on TED talks, as well as for talk-level emotion prediction with 12 dimen- sions from subsets of 10 comments on each talk (see <ref type="table">Table 3</ref>). APWeights outperforms SVR and Lasso, as well as all other methods for each task. For sentiment prediction, SVR is outperformed by 11% MSE and Lasso by 5%. For emotion pre- diction (averaged over all 12 aspects), differences are smaller, at 1.6% and 2.9% respectively. These smaller differences could be explained by the fact that among the 10 most recent comments for each talk, many are not related to the emotion that the system tries to predict. As mentioned earlier, the proposed model does not make any assumption about the feature space. Thus, we examined whether the improvements it brings remain present even with a different fea- ture space, for instance based on TF-IDF instead of BOW with counts. For sentiment prediction on TED comments, we found that by changing the feature space to TF-IDF, strong baselines such as Aggregated ( 1 ) and ( 2 ), i.e. SVR and Lasso, im- prove their performance <ref type="bibr">(16.25 and 16.59 MAE; 4.16 and 3.97 MSE respectively)</ref>. However, AP- Weights still outperforms them on both MAE and MSE scores <ref type="bibr">(15.35 and 3.63</ref>), improving over SVR by 5.5% on MAE and 12.5% on MSE, and over Lasso by 7.4% on MAE and 8.5% on MSE. These promising results suggest that improve- ments with APWeights could be observed also on more sophisticated feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Interpreting the Relevance Weights</head><p>Apart from predicting ratings, the MIR scores as- signed by our model reflect the contribution of each sentence to these predictions.</p><p>To illustrate the explanatory power of our model (until a dataset for quantitative analysis becomes available), we provide examples of predictions on test data taken from the cross-validation folds above. <ref type="table">Table 5</ref>   <ref type="table" target="#tab_3">Table 4</ref>: Predicted sentiment for TED comments: y i is the actual sentiment, ˆ y i the predicted one, andˆψ andˆ andˆψ i the estimated relevance of each sentence. ment for two correctly predicted emotions on two TED talks, based on thê ψ i relevance scores, along with thê ψ i scores of the other comments, for two emotion classes: 'beautiful' and 'courageous'. These comments appear to reflect correctly the fact that the respective emotion is the majority one in each of the comments. As noted earlier, this task is quite challenging since we use only the ten most recent comments for each talk. ˆ ψi distribution inspiring "It seems to me that the idea worth spreading of this TED Talk is inspiring and key for a full life. 'No-one else is the authority on your potential. You're the only person that decides how far you go and what you're capable of.' It seems to me that teens actually think that. As a child one is all knowing and all capable. How did we get to the (...)"</p><p>beautiful "The beauty of the nature. It would be more interesting just integrates his thought and idea into a mobile device, like a mobile, so we can just turn on the nature gallery in any time. The paintings don't look incidental but genuinely thought out, random perhaps, but with a clear grand design behind the randomness. Drawing is an art where it doesn't (...)" funny "Funny story, but not as funny as a good 'knock, knock' joke. My favorite knock-knock joke of all time is Cheech &amp; Chong's 'Dave's Not Here' gag from the early 1970s. I'm still waiting for someone to top it after all these years. <ref type="bibr">[Knock, knock]</ref> 'Who is it?' the voice of an obviously stoned male answers from the other side of a door, (...)"</p><p>courageous "I was a soldier in Iraq and part of the unit represented in this documentary. I would ques- tion anyone that told you we went over there to kill Iraqi people. I spent the better part of my time in Iraq protecting the Iraqi people from insurgents who came from countries outside of Iraq to kill Iraqi people. We protected families men, women, and (...)" <ref type="table">Table 5</ref>: Two examples of top comments (according to weights ψ i ) for correctly predicted emotions in four TED talks (score 1.0) and the distribution of weights over the 10 most recent comments in each talk. from the test set of a given fold, for the comment- level sentiment prediction task. <ref type="table">The table also</ref> shows thê ψ i relevance scores assigned to each of the composing sentences, the predicted polar- ity scoresˆyscoresˆ scoresˆy i and the actual ones y i . We observe that the sentences that convey the most sentiment are assigned higher scores than sentences with less sentiment, always with respect to the global polar- ity level. These examples suggest that, given that APWeights has more degrees of freedom for inter- pretation, it is able to assign relevance to parts of a text (here, sentences) and even to words, while other models can only consider words. Hence, the assigned weights might be useful for other NLP tasks mentioned below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>This paper introduced a novel MIR model for as- pect rating prediction from text, which learns in- stance relevance together with target labels. To the best of our knowledge, this has not been consid- ered before. Compared to previous work on MIR, the proposed model is competitive and more effi- cient in terms of complexity. Moreover, it is not only able to assign instance relevances on labeled bags, but also to predict them on unseen bags.</p><p>Compared to previous work on aspect rating prediction, our model performs significantly bet- ter than BOW regression baselines (SVR, Lasso) without using additional knowledge or features. The improvements persist even when the sophis- tication of the features increases, suggesting that our contribution may be orthogonal to feature en- gineering or learning. Lastly, the qualitative eval- uation on test examples demonstrates that the pa- rameters learned by the model are not only useful for prediction, but they are also interpretable. In the future, we intend to test our model on sen- timent classification at the sentence-level, based only on document-level supervision <ref type="bibr" target="#b25">(Täckström and McDonald, 2011</ref>). Moreover, we will experi- ment with other model settings, such as regulariza- tion norms other than 2 and feature spaces other than BOW or TF-IDF. In the longer term, we plan to investigate new methods to estimate instance weights at prediction time, and to evaluate the im- pact of assigned weights on sentence ranking, seg- mentation or summarization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Analysis of a comment (bag of sentences {s 1 , ..., s j }) annotated by humans with the maximal positive sentiment score (5 stars). The weights assigned by MIR reveal that s 1 has the greatest relevance to the overall sentiment.</figDesc><graphic url="image-1.png" coords="2,77.33,62.81,204.88,144.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Visual representation for the training and testing procedure of Algorithm 1.</figDesc><graphic url="image-2.png" coords="5,309.01,500.41,214.80,81.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>have cubic time complexity with the average bag sizê n and number of features d; Zhou et al. (2009) use ker- nels, thus their complexity is quadratic with the number of bags m; and Wang et al. (2011) have cubic time wrt. d. Our formulation is thus com- petitive in terms of complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distributions of rating values per aspect rating class for the seven datasets.</figDesc><graphic url="image-3.png" coords="7,63.39,62.81,461.04,154.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>REVIEW</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MSE scores of SVR, Lasso and APWeights for each aspect over the five review datasets.</figDesc><graphic url="image-4.png" coords="9,93.47,62.80,410.62,199.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>displays the most relevant com- Sentences per commentˆψî commentˆ commentˆψicommentˆψî yi yi "Very brilliant and witty, as well as great improvisation.They are either dictated by the most extreme personalities who crave nothing but power or man- aged by politicians who are voted in by a far from gifted population." 0.52 "I am very disappointed by this, smug, cliched and missing so much information as to be almost (...)"' 0.43 1.8 1.0 "No mention of ship transport lets say 50% of all material transport, no mention of rail transport, (...)" 0.29 "I am sorry to be so negative, this just sounds like a sales pitch that he has given too many times (...)." 0.28</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top words based on Φ for predicting four emotions from comments on TED talks.</figDesc><graphic url="image-9.png" coords="10,64.08,287.18,458.04,68.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 displays four TED comments selected Class Top comment per talk (according to weights ψi)</head><label>4</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work described in this article was sup-ported by the European Union through the inEvent project FP7-ICT n. 287872 (see http://www. inevent-project.eu).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-facet rating of product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<editor>Mohand Boughanem, Catherine Berrut, Josiane Mothe, and Chantal Soule-Dupuy</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5478</biblScope>
			<biblScope unit="page" from="461" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple instance learning for sparse positive bags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th Annual International Conference on Machine Learning, ICML &apos;07<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tightly integrating relational learning and multiple-instance regression for real-valued drug activity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>Corvallis, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems</title>
		<meeting><address><addrLine>Denver, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A review of multi-instance learning assumptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining, KDD &apos;04</title>
		<meeting>the 10th ACM SIGKDD Int. Conf. on Knowledge discovery and data mining, KDD &apos;04<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating reviewer and product information for review rating prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Joint Conference on Artificial Intelligence</title>
		<meeting>the 22nd International Joint Conference on Artificial Intelligence<address><addrLine>Barcelona, Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1820" to="1825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-aspect sentiment analysis with topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IEEE International Conference on Data Mining Workshops, ICDMW &apos;11</title>
		<meeting>the 11th IEEE International Conference on Data Mining Workshops, ICDMW &apos;11<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multi-aspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Data Mining, ICDM &apos;12</title>
		<meeting>the 12th IEEE International Conference on Data Mining, ICDM &apos;12<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic sentiment mixture: modeling facets and opinions in weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Int. Conf. on the World Wide Web, WWW &apos;07</title>
		<meeting>the 16th Int. Conf. on the World Wide Web, WWW &apos;07<address><addrLine>Banff, AB</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conf. on Empirical Methods in Natural Language Processing, EMNLP &apos;02</title>
		<meeting>the ACL Conf. on Empirical Methods in Natural Language Processing, EMNLP &apos;02<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sentiment analysis of user comments for one-class collaborative filtering over TED talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Popescu-Belis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;13</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Duchesnay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Scikit-learn: Machine learning in python. CoRR, abs/1201.0490</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The bag-of-opinions method for review rating prediction from sparse text patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="913" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiple instance regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the 18th International Conference on Machine Learning, ICML &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="425" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic aggregation by joint modeling of aspects and values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="127" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incorporating content structure into text analysis applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="377" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple-instance active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS &apos;08</title>
		<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1289" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple aspect ranking using the good grief algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLTNAACL &apos;07</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLTNAACL &apos;07<address><addrLine>Rochester, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="300" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note>inburgh</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;13<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovering fine-grained sentiment with latent variable structured prediction models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd European Conference on Advances in Information Retrieval, ECIR&apos;11</title>
		<meeting>the 33rd European Conference on Advances in Information Retrieval, ECIR&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="368" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society (Series B)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web, WWW &apos;08</title>
		<meeting>the 17th international conference on World Wide Web, WWW &apos;08<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Salience assignment for multiple-instance regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiri</forename><forename type="middle">L</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terran</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2007 Workshop on Constrained Optimization and Structured Output Spaces</title>
		<meeting><address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiple-instance regression with structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiri</forename><forename type="middle">L</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terran</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Roper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Data Mining Workshops, ICDMW &apos;08</title>
		<meeting>the IEEE International Conference on Data Mining Workshops, ICDMW &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aerosol optical depth prediction from satellite observations by multiple instance regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Obradovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Vucetic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM Int. Conf. on Data Mining, SDM &apos;08</title>
		<meeting>the SIAM Int. Conf. on Data Mining, SDM &apos;08<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning instance specific distance for multiinstance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mixture model for multiple instance regression and applications in remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2226" to="2237" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning subjective language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="308" />
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">M3MIML: A maximum margin method for multi-instance multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM &apos;08. Eighth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008-12" />
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiinstance clustering with applications to multiinstance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="68" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiinstance learning based web mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="147" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-instance learning by treating instances as noni.i.d. samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Yin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-aspect rating inference with aspectbased segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="469" to="481" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Movie review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Information and Knowledge Management, CIKM &apos;06</title>
		<meeting>the 15th ACM International Conference on Information and Knowledge Management, CIKM &apos;06<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
