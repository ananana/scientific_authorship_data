<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Language Modeling with Power Low Rank Ensembles</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
							<email>apparikh@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical &amp; Computer Engineering</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="department" key="dep4">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Saluja</surname></persName>
							<email>avneesh@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical &amp; Computer Engineering</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="department" key="dep4">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical &amp; Computer Engineering</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="department" key="dep4">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical &amp; Computer Engineering</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="department" key="dep4">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit4">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Language Modeling with Power Low Rank Ensembles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1487" to="1498"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can be understood as a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modeling is the task of estimating the probability of sequences of words in a language and is an important component in, among other applications, automatic speech recognition <ref type="bibr" target="#b35">(Rabiner and Juang, 1993)</ref> and machine translation <ref type="bibr" target="#b21">(Koehn, 2010)</ref>. The predominant approach to lan- guage modeling is the n-gram model, wherein the probability of a word sequence P (w 1 , . . . , w ) is decomposed using the chain rule, and then a Markov assumption is made: P (w 1 , . . . , w ) ≈ i=1 P (w i |w i−1 i−n+1 ). While this assumption sub- stantially reduces the modeling complexity, pa- rameter estimation remains a major challenge. Due to the power-law nature of language <ref type="bibr" target="#b48">(Zipf, 1949)</ref>, the maximum likelihood estimator mas- sively overestimates the probability of rare events and assigns zero probability to legitimate word se- quences that happen not to have been observed in the training data <ref type="bibr" target="#b25">(Manning and Schütze, 1999</ref>).</p><p>Many smoothing techniques have been pro- posed to address the estimation challenge. These reassign probability mass (generally from over- estimated events) to unseen word sequences, whose probabilities are estimated by interpolating with or backing off to lower order n-gram models <ref type="bibr" target="#b5">(Chen and Goodman, 1999)</ref>.</p><p>Somewhat surprisingly, these widely used smoothing techniques differ substantially from techniques for coping with data sparsity in other domains, such as collaborative filtering <ref type="bibr" target="#b22">(Koren et al., 2009;</ref><ref type="bibr" target="#b41">Su and Khoshgoftaar, 2009</ref>) or matrix completion <ref type="bibr" target="#b4">(Candès and Recht, 2009;</ref><ref type="bibr" target="#b3">Cai et al., 2010)</ref>. In these areas, low rank approaches based on matrix factorization play a central role ( <ref type="bibr" target="#b23">Lee and Seung, 2001;</ref><ref type="bibr" target="#b37">Salakhutdinov and Mnih, 2008;</ref><ref type="bibr" target="#b24">Mackey et al., 2011</ref>). For example, in recom- mender systems, a key challenge is dealing with the sparsity of ratings from a single user, since typical users will have rated only a few items. By projecting the low rank representation of a user's (sparse) preferences into the original space, an es- timate of ratings for new items is obtained. These methods are attractive due to their computational efficiency and mathematical well-foundedness.</p><p>In this paper, we introduce power low rank en- sembles (PLRE), in which low rank tensors are used to produce smoothed estimates for n-gram probabilities. Ideally, we would like the low rank structures to discover semantic and syntactic relat- edness among words and n-grams, which are used to produce smoothed estimates for word sequence probabilities. In contrast to the few previous low rank language modeling approaches, PLRE is not orthogonal to n-gram models, but rather a gen- eral framework where existing n-gram smoothing methods such as Kneser-Ney smoothing are spe- cial cases. A key insight is that PLRE does not compute low rank approximations of the original joint count matrices (in the case of bigrams) or ten- sors i.e. multi-way arrays (in the case of 3-grams and above), but instead altered quantities of these counts based on an element-wise power operation, similar to how some smoothing methods modify their lower order distributions.</p><p>Moreover, PLRE has two key aspects that lead to easy scalability for large corpora and vocabu- laries. First, since it utilizes the original n-grams, the ranks required for the low rank matrices and tensors tend to be remain tractable (e.g. around 100 for a vocabulary size V ≈ 1 × 10 6 ) leading to fast training times. This differentiates our ap- proach over other methods that leverage an under- lying latent space such as neural networks ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b29">Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b27">Mikolov et al., 2010)</ref> or soft-class models <ref type="bibr" target="#b39">(Saul and Pereira, 1997)</ref> where the underlying dimension is required to be quite large to obtain good performance. Moreover, at test time, the probability of a se- quence can be queried in time O(κ max ) where κ max is the maximum rank of the low rank matri- ces/tensors used. While this is larger than Kneser Ney's virtually constant query time, it is substan- tially faster than conditional exponential family models <ref type="bibr" target="#b6">(Chen and Rosenfeld, 2000;</ref><ref type="bibr" target="#b7">Chen, 2009;</ref><ref type="bibr" target="#b31">Nelakanti et al., 2013</ref>) and neural networks which require O(V ) for exact computation of the nor- malization constant. See Section 7 for a more de- tailed discussion of related work.</p><p>Outline: We first review existing n-gram smoothing methods ( §2) and then present the in- tuition behind the key components of our tech- nique: rank ( §3.1) and power ( §3.2). We then show how these can be interpolated into an ensem- ble ( §4). In the experimental evaluation on English and Russian corpora ( §5), we find that PLRE out- performs Kneser-Ney smoothing and all its vari- ants, as well as class-based language models. We also include a comparison to the log-bilinear neu- ral language model <ref type="bibr" target="#b29">(Mnih and Hinton, 2007)</ref> and evaluate performance on a downstream machine translation task ( §6) where our method achieves consistent improvements in BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discount-based Smoothing</head><p>We first provide background on absolute discount- ing ( <ref type="bibr" target="#b32">Ney et al., 1994)</ref> and Kneser-Ney smooth- ing ( <ref type="bibr" target="#b20">Kneser and Ney, 1995)</ref>, two common n-gram smoothing methods. Both methods can be formu- lated as back-off or interpolated models; we de- scribe the latter here since that is the basis of our low rank approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>Let c(w) be the count of word w, and similarly c(w, w i−1 ) for the joint count of words w and w i−1 . For shorthand we will define w j i to denote the word sequence {w i , w i+1 , ..., w j−1 , w j }. Let P (w i ) refer to the maximum likelihood estimate (MLE) of the probability of word w i , and simi- larly P (w i |w i−1 ) for the probability conditioned on a history, or more generally, P (w i |w i−1 i−n+1 ). Let N − (w i ) := |{w : c(w i , w) &gt; 0}| be the number of distinct words that appear be- fore w i . More generally, let</p><formula xml:id="formula_0">N − (w i i−n+1 ) = |{w : c(w i i−n+1 , w) &gt; 0}|. Similarly, let N + (w i−1 i−n+1 ) = |{w : c(w, w i−1 i−n+1 ) &gt; 0}|.</formula><p>V denotes the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Absolute Discounting</head><p>Absolute discounting works on the idea of inter- polating higher order n-gram models with lower- order n-gram models. However, first some prob- ability mass must be "subtracted" from the higher order n-grams so that the leftover probability can be allocated to the lower order n-grams. More specifically, define the following discounted con- ditional probability:</p><formula xml:id="formula_1">P D (w i |w i−1 i−n+1 ) = max{c(w i , w i−1 i−n+1 ) − D, 0} c(w i−1 i−n+1 )</formula><p>Then absolute discounting P abs (·) uses the follow- ing (recursive) equation:</p><formula xml:id="formula_2">P abs (w i |w i−1 i−n+1 ) = P D (w i |w i−1 i−n+1 ) + γ(w i−1 i−n+1 )P abs (w i |w i−1 i−n+2 )</formula><p>where γ(w i−1 i−n+1 ) is the leftover weight (due to the discounting) that is chosen so that the con- ditional distribution sums to one:</p><formula xml:id="formula_3">γ(w i−1 i−n+1 ) = D c(w i−1 i−n+1 ) N + (w i−1 i−n+1 ).</formula><p>For the base case, we set P abs (w i ) = P (w i ).</p><formula xml:id="formula_4">Discontinuity: Note that if c(w i−1 i−n+1 ) = 0, then γ(w i−1 i−n+1 ) = 0 0 , in which case γ(w i−1 i−n+1</formula><p>) is set to 1. We will see that this discontinuity appears in PLRE as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Kneser Ney Smoothing</head><p>Ideally, the smoothed probability should preserve the observed unigram distribution:</p><formula xml:id="formula_5">P (w i ) = w i−1 i−n+1 P sm (w i |w i−1 i−n+1 ) P (w i−1 i−n+1 ) (1)</formula><p>where P sm (w i |w i−1 i−n+1 ) is the smoothed condi- tional probability that a model outputs. Unfortu- nately, absolute discounting does not satisfy this property, since it exclusively uses the unaltered MLE unigram model as its lower order model. In practice, the lower order distribution is only uti- lized when we are unsure about the higher order distribution (i.e., when γ(·) is large). Therefore, the unigram model should be altered to condition on this fact. This is the inspiration behind Kneser-Ney (KN) smoothing, an elegant algorithm with robust per- formance in n-gram language modeling. KN smoothing defines alternate probabilities P alt (·):</p><formula xml:id="formula_6">P alt D (w i |w i−1 i−n +1 ) =        P D (w i |w i−1 i−n +1 ), if n = n max{N − (w i i−n +1 )−D,0} w i N − (w i i−n +1 ) , if n &lt; n</formula><p>The base case for unigrams reduces to</p><formula xml:id="formula_7">P alt (w i ) = N − (w i ) w i N − (w i ) . Intuitively P alt (w i ) is</formula><p>proportional to the number of unique words that precede w i . Thus, words that appear in many dif- ferent contexts will be given higher weight than words that consistently appear after only a few contexts. These alternate distributions are then used with absolute discounting:</p><formula xml:id="formula_8">P kn (w i |w i−1 i−n+1 ) = P alt D (w i |w i−1 i−n+1 ) + γ(w i−1 i−n+1 )P kn (w i |w i−1 i−n+2 ) (2)</formula><p>where we set P kn (w i ) = P alt (w i ). By definition, KN smoothing satisfies the marginal constraint in Eq. 1 (Kneser and Ney, 1995).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Power Low Rank Ensembles</head><p>In n-gram smoothing methods, if a bigram count c(w i , w i−1 ) is zero, the unigram probabilities are used, which is equivalent to assuming that w i and w i−1 are independent ( and similarly for general n). However, in this situation, instead of back- ing off to a 1-gram, we may like to back off to a "1.5-gram" or more generally an order between 1 and 2 that captures a coarser level of dependence between w i and w i−1 and does not assume full in- dependence.</p><p>Inspired by this intuition, our strategy is to con- struct an ensemble of matrices and tensors that not only consists of MLE-based count informa- tion, but also contains quantities that represent lev- els of dependence in-between the various orders in the model. We call these combinations power low rank ensembles (PLRE), and they can be thought of as n-gram models with non-integer n. Our ap- proach can be recursively formulated as:</p><formula xml:id="formula_9">P plre (w i |w i−1 i−n+1 ) = P alt D 0 (w i |w i−1 i−n+1 ) + γ 0 (w i−1 i−n+1 ) Z D 1 (w i |w i−1 i−n+1 ) + ..... + γ η−1 (w i−1 i−n+1 ) Z Dη (w i |w i−1 i−n+1 ) + γ η (w i−1 i−n+1 ) P plre (w i |w i−1 i−n+2 ) ...<label>(3)</label></formula><p>where Z 1 , ..., Z η are conditional probability ma- trices that represent the intermediate n-gram or- ders 1 and D is a discount function (specified in §4). This formulation begs answers to a few crit- ical questions. How to construct matrices that represent conditional probabilities for intermedi- ate n? How to transform them in a way that generalizes the altered lower order distributions in KN smoothing? How to combine these matri- ces such that the marginal constraint in Eq. 1 still holds? The following propose solutions to these three queries:</p><p>1. Rank (Section 3.1): Rank gives us a concrete measurement of the dependence between w i and w i−1 . By constructing low rank ap- proximations of the bigram count matrix and higher-order count tensors, we obtain matri- ces that represent coarser dependencies, with a rank one approximation implying that the variables are independent.</p><p>2. Power (Section 3.2): In KN smoothing, the lower order distributions are not the original counts but rather altered estimates. We pro- pose a continuous generalization of this alter- ation by taking the element-wise power of the counts.</p><p>3. Creating the Ensemble (Section 4): Lastly, PLRE also defines a way to interpolate the specifically constructed intermediate n-gram matrices. Unfortunately a constant discount, as presented in Section 2, will not in general preserve the lower order marginal constraint (Eq. 1). We propose a generalized discount- ing scheme to ensure the constraint holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rank</head><p>We first show how rank can be utilized to construct quantities between an n-gram and an n − 1-gram.</p><p>In general, we think of an n-gram as an n th or- der tensor i.e. a multi-way array with n indices {i 1 , ..., i n }. (A vector is a tensor of order 1, a ma- trix is a tensor of order 2 etc.) Computing a spe- cial rank one approximation of slices of this tensor produces the n − 1-gram. Thus, taking rank κ ap- proximations in this fashion allows us to represent dependencies between an n-gram and n − 1-gram.</p><p>Consider the bigram count matrix B with N counts which has rank V .</p><p>Note that</p><formula xml:id="formula_10">P (w i |w i−1 ) = B(w i ,w i−1 )</formula><p>w B(w,w i−1 ) . Additionally, B can be considered a random variable that is the re- sult of sampling N tuples of (w i , w i−1 ) and ag- glomerating them into a count matrix. Assum- ing w i and w i−1 are independent, the expected value (with respect to the empirical distribution) E[B] = N P (w i )P (w i−1 ), which can be rewrit- ten as being proportional to the outer product of the unigram probability vector with itself, and is thus rank one.</p><p>This observation extends to higher order n-grams as well. Let C n be the n th order tensor where C n (w i , ...., w i−n+1 ) = c(w i , ..., w i−n+1 ). Furthermore denote C n (:, ˜ w i−1 i−n+2 , :) to be the V × V matrix slice of C n where w i−n+2 , ..., w i−1 are held fixed to a particular sequence˜wsequence˜ sequence˜w i−n+2 , ...,</p><formula xml:id="formula_11">˜ w i−1 . Then if w i is con- ditionally independent of w i−n+1 given w i−1 i−n+2 , then E[C n (:, ˜ w i−1 i−n+2 , :)] is rank one ∀ ˜ w i−1 i−n+2 .</formula><p>However, it is rare that these matrices are ac- tually rank one, either due to sampling vari- ance or the fact that w i and w i−1 are not in- dependent. What we would really like to say is that the best rank one approximation B (1) (under some norm) of B is ∝ P (w i ) P (w i−1 ). While this statement is not true under the 2 norm, it is true under generalized KL diver- gence ( <ref type="bibr" target="#b23">Lee and Seung, 2001</ref>): gKL(A||B) = ij A ij log(</p><formula xml:id="formula_12">A ij B ij ) − A ij + B ij ) .</formula><p>In particular, generalized KL divergence pre- serves row and column sums: if M (κ) is the best rank κ approximation of M under gKL then the row sums and column sums of M (κ) and M are equal <ref type="bibr" target="#b17">(Ho and Van Dooren, 2008</ref>). Leveraging this property, it is straightforward to prove the fol- lowing lemma: Lemma 1. Let B (κ) be the best rank κ ap- proximation of B under gKL. Then B (1) ∝ P (w i ) P (w i−1 ) and ∀w i−1 s.t. c(w i−1 ) = 0:</p><formula xml:id="formula_13">P (w i ) = B (1) (w i , w i−1 ) w B (1) (w, w i−1 )</formula><p>For more general n, let C n,(κ) i−1,...,i−n+2 be the best rank κ approximation of C n (:,</p><formula xml:id="formula_14">˜ w i−1 i−n+2 , : ) under gKL. Then similarly, ∀w i−1 i−n+1 s.t. c(w i−1 i−n+1 ) &gt; 0: P (w i |w i−1 , ..., w i−n+2 ) = C n,(1) i−1,...,i−n+2 (w i , w i−1 i−n+1 ) w C n,(1) i−1,...,i−n+2 (w, w i−1 i−n+1 )<label>(4)</label></formula><p>Thus, by selecting 1 &lt; κ &lt; V , we obtain count matrices and tensors between n and n − 1-grams. The condition that c(w i−1 i−n+1 ) &gt; 0 corresponds to the discontinuity discussed in §2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Power</head><p>Since KN smoothing alters the lower order distri- butions instead of simply using the MLE, vary- ing the rank is not sufficient in order to generalize this suite of techniques. Thus, PLRE computes low rank approximations of altered count matri- ces. Consider taking the elementwise power ρ of the bigram count matrix, which is denoted by B ·ρ . For example, the observed bigram count matrix and associated row sum:</p><formula xml:id="formula_15">B ·1 = 1.0 2.0 1.0 0 5.0 0 2.0 0 0 row sum → 4.0 5.0 2.0</formula><p>As expected the row sum is equal to the uni- gram counts (which we denote as u). Now con- sider B ·0.5 :</p><formula xml:id="formula_16">B ·0.5 = 1.0 1.4 1.0 0 2.2 0 1.4 0 0 row sum → 3.4</formula><p>2.2 1.4</p><p>Note how the row sum vector has been altered.</p><p>In particular since w 1 (corresponding to the first row) has a more diverse history than w 2 , it has a higher row sum (compared to in u where w 2 has the higher row sum). Lastly, consider the case when p = 0:</p><formula xml:id="formula_17">B ·0 = 1.0 1.0 1.0 0 1.0 0 1.0 0 0 row sum → 3.0 1.0 1.0</formula><p>The row sum is now the number of unique words that precede w i (since B 0 is binary) and is thus equal to the (unnormalized) Kneser Ney unigram. This idea also generalizes to higher order n-grams and leads us to the following lemma:</p><p>Lemma 2. Let B (ρ,κ) be the best rank κ ap- proximation of B ·ρ under gKL. Then ∀w i−1 s.t. c(w i−1 ) = 0:</p><formula xml:id="formula_18">P alt (w i ) = B (0,1) (w i , w i−1 ) w B (0,1) (w, w i−1 )</formula><p>For more general n, let C </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Creating the Ensemble</head><p>Recall our overall formulation in Eq. 3; a naive solution would be to set Z 1 , ..., Z η to low rank approximations of the count matrices/tensors un- der varying powers, and then interpolate through constant absolute discounting. Unfortunately, the marginal constraint in Eq. 1 will generally not hold if this strategy is used. Therefore, we propose a generalized discounting scheme where each non- zero n-gram count is associated with a different discount D j (w i , w i−1 i−n +1 ). The low rank approxi- mations are then computed on the discounted ma- trices, leaving the marginal constraint intact.</p><p>For clarity of exposition, we focus on the spe- cial case where n = 2 with only one low rank matrix before stating our general algorithm:</p><formula xml:id="formula_20">P plre (w i |w i−1 ) = P D 0 (w i |w i−1 ) + γ 0 (w i−1 ) Z D 1 (w i |w i−1 ) + γ 1 (w i−1 )P alt (w i )<label>(6)</label></formula><p>Our goal is to compute D 0 , D 1 and Z 1 so that the following lower order marginal constraint holds:</p><formula xml:id="formula_21">P (w i ) = w i−1 P plre (w i |w i−1 ) P (w i−1 )<label>(7)</label></formula><p>Our solution can be thought of as a two- step procedure where we compute the discounts D 0 , D 1 (and the γ(w i−1 ) weights as a by- product), followed by the low rank quantity Z</p><note type="other">1 . First, we construct the following intermediate en- semble of powered, but full rank terms. Let Y ρ j be the matrix such that Y ρ j (w i , w i−1 ) := c(w i , w i−1 ) ρ j . Then define</note><formula xml:id="formula_22">P pwr (w i |w i−1 ) := Y (ρ 0 =1) D 0 (w i |w i−1 ) + γ 0 (w i−1 ) Y (ρ 1 ) D 1 (w i |w i−1 ) + γ 1 (w i−1 )Y (ρ 2 =0) (w i |w i−1 )<label>(8)</label></formula><p>where with a little abuse of notation:</p><formula xml:id="formula_23">Y ρ j D j (wi|wi−1) = c(wi, wi−1) ρ j − Dj(wi, wi−1) w i c(wi, wi−1) ρ j</formula><p>Note that P alt (w i ) has been replaced with Y (ρ 2 =0) (w i |w i−1 ), based on Lemma 2, and will equal P alt (w i ) once the low rank approximation is taken as discussed in § 4.2).</p><p>Since we have only combined terms of differ- ent power (but all full rank), it is natural choose the discounts so that the result remains unchanged i.e., P pwr (w i |w i−1 ) = P (w i |w i−1 ), since the low rank approximation (not the power) will imple- ment smoothing. Enforcing this constraint gives rise to a set of linear equations that can be solved (in closed form) to obtain the discounts as we now show below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Step 1: Computing the Discounts</head><p>To ensure the constraint that P pwr (w i |w i−1 ) = P (w i |w i−1 ), it is sufficient to enforce the follow- ing two local constraints:</p><formula xml:id="formula_24">Y (ρ j ) (w i |w i−1 ) = Y (ρ j ) D j (w i |w i−1 ) + γ j (w i−1 )Y (ρ j+1 ) (w i |w i−1 ) for j = 0, 1<label>(9)</label></formula><p>This allows each D j to be solved for indepen- dently of the other {D j } j =j . Let c i,i−1 = c(w i , w i−1 ), c j i,i−1 = c(w i , w i−1 ) ρ j , and d j i,i−1 = D j (w i , w i−1 ). Expanding Eq. 9 yields that ∀w i , w i−1 : proportional to c j+1 i,i−1 satisfies Eq. 11. Furthermore it can be shown that all solutions are of this form (i.e., the linear system has a null space of exactly one). Moreover, we are interested in a particular subset of solutions where a single parameter d * (independent of w i−1 ) controls the scaling as in- dicated by the following lemma: The above lemma generalizes to longer contexts (i.e. n &gt; 2) as shown in Algorithm 1. Note that if ρ j = ρ j+1 then Algorithm 1 is equivalent to scal- ing the counts e.g. deleted-interpolation/Jelinek Mercer smoothing <ref type="bibr" target="#b19">(Jelinek and Mercer, 1980</ref>). On the other hand, when ρ j+1 = 0, Algorithm 1 is equal to the absolute discounting that is used in Kneser-Ney. Thus, depending on ρ j+1 , our method generalizes different types of interpola- tion schemes to construct an ensemble so that the marginal constraint is satisfied.</p><formula xml:id="formula_25">Lemma 3. Assume that ρ j ≥ ρ j+1 . Choose any 0 ≤ d * ≤ 1. Set d j i,i−1 = d * c j+1 i,i−1 ∀i, j.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Compute D</head><p>In: Count tensor C n , powers ρ j , ρ j+1 such that ρ j ≥ ρ j+1 , and parameter d * . Out: Discount D j for powered counts C n,(ρ j ) and associated leftover weight γ j  </p><formula xml:id="formula_26">1: Set D j (w i , w i−1 i−n+1 ) = d * c(w i , w i−1 i−n+1 ) ρ j+1 . 2: γ j (w i , w i−1 i−n+1 ) = d * w i c(w i , w i−1 i−n+1 ) ρ j+1 w i c(w i , w i−1 i−n+1 ) ρ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Step 2: Computing Low Rank Quantities</head><p>The next step is to compute low rank approxi-</p><formula xml:id="formula_27">mations of Y (ρ j ) D j</formula><p>to obtain Z D j such that the inter- mediate marginal constraint in Eq. 7 is preserved. This constraint trivially holds for the intermediate ensemble P pwr (w i |w i−1 ) due to how the discounts were derived in § 4.1. For our running bigram ex- ample, define Z</p><formula xml:id="formula_28">(ρ j ,κ j ) D j</formula><p>to be the best rank κ j ap-</p><formula xml:id="formula_29">proximation to Y (ρ j ,κ j ) D j</formula><p>according to gKL and let</p><formula xml:id="formula_30">Z ρ j ,κ j D j (w i |w i−1 ) = Z ρ j ,κ j D j (w i , w i−1 ) w i c(w i , w i−1 ) ρ j Note that Z ρ j ,κ j D j (w i |w i−1 )</formula><p>is a valid (discounted) conditional probability since gKL preserves row/column sums so the denominator remains un- changed under the low rank approximation. Then using the fact that Z (0,1) (w i |w i−1 ) = P alt (w i ) (Lemma 2) we can embellish Eq. 6 as</p><formula xml:id="formula_31">P plre (w i |w i−1 ) = P D 0 (w i |w i−1 )+ γ 0 (w i−1 ) Z (ρ 1 ,κ 1 ) D 1 (w i |w i−1 ) + γ 1 (w i−1 )P alt (w i )</formula><p>Leveraging the form of the discounts and row/column sum preserving property of gKL, we then have the following lemma (the proof is in the supplementary material):</p><p>Lemma 4. Let P plre (w i |w i−1 ) indicate the PLRE smoothed conditional probability as computed by Eq. 6 and Algorithms 1 and 2. Then, the marginal constraint in Eq. 7 holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">More general algorithm</head><p>In general, the principles outlined in the previ- ous sections hold for higher order n-grams. As- sume that the discounts are computed according to Algorithm 1 with parameter d * and Z</p><formula xml:id="formula_32">(ρ j ,κ j ) D j</formula><p>is computed according to Algorithm 2. Note that, as shown in Algorithm 2, for higher order n-grams, the Z</p><formula xml:id="formula_33">(ρ j ,κ j ) D j</formula><p>are created by taking low rank approx- imations of slices of the (powered) count tensors (see Lemma 2 for intuition). Eq. 3 can now be embellished:</p><formula xml:id="formula_34">P plre (w i |w i−1 i−n+1 ) = P alt D 0 (w i |w i−1 i−n+1 ) + γ 0 (w i−1 i−n+1 ) Z (ρ 1 ,κ 1 ) D 1 (w i |w i−1 i−n+1 ) + ..... + γ η−1 (w i−1 i−n+1 ) Z (ρη,κη) Dη (w i |w i−1 i−n+1 ) + γ η (w i−1 i−n+1 ) P plre (w i |w i−1 i−n+2 ) ...<label>(12)</label></formula><p>Lemma 4 also applies in this case and is given in Theorem 1 in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Links with KN Smoothing</head><p>In this section, we explicitly show the relation- ship between PLRE and KN smoothing. Rewrit- ing Eq. 12 in the following form:</p><formula xml:id="formula_35">P plre (w i |w i−1 i−n+1 ) = P terms plre (w i |w i−1 i−n+1 ) +γ 0:η (w i−1 i−n+1 )P plre (w i |w i−1 i−n+2 ) (13)</formula><p>where P terms plre (w i |w i−1 i−n+1 ) contains the terms in Eq. 12 except the last, and γ 0:η (w i−1 i−n+1 ) = η h=0 γ h (w i−1 i−n+1 ), we can leverage the form of the discount, and using the fact that ρ η+1 = 0 2 :</p><formula xml:id="formula_36">γ 0:η (w i−1 i−n−1 ) = d * η+1 N + (w i−1 i−n+1 ) c(w i−1</formula><p>i−n+1 ) With this form of γ(·), Eq. 13 is remarkably sim- ilar to KN smoothing (Eq.</p><formula xml:id="formula_37">2) if KN's discount pa- rameter D is chosen to equal (d * ) η+1 .</formula><p>The difference is that P alt (·) has been replaced with the alternate estimate P terms plre (w i |w i−1 i−n+1 ), which have been enriched via the low rank struc- ture. Since these alternate estimates were con- structed via our ensemble strategy they contain both very fine-grained dependencies (the origi- nal n-grams) as well as coarser dependencies (the lower rank n-grams) and is thus fundamentally different than simply taking a single matrix/tensor decomposition of the trigram/bigram matrices.</p><p>Moreover, it provides a natural way of setting d * based on the Good-Turing (GT) estimates em- ployed by KN smoothing. In particular, we can set d * to be the (η + 1) th root of the KN discount D that can be estimated via the GT estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Computational Considerations</head><p>PLRE scales well even as the order n increases. To compute a low rank bigram, one low rank ap- proximation of a V × V matrix is required. For the low rank trigram, we need to compute a low rank approximation of each slice C n,(·p) D (:, ˜ w i−1 , : ) ∀ ˜ w i−1 . While this may seem daunting at first, in practice the size of each slice (number of non-zero rows/columns) is usually much, much smaller than V , keeping the computation tractable.</p><p>Similarly, PLRE also evaluates conditional probabilities at evaluation time efficiently. As shown in Algorithm 2, the normalizer can be pre- computed on the sparse powered matrix/tensor. As a result our test complexity is O( η total i=1 κ i ) where η total is the total number of matrices/tensors in the ensemble. While this is larger than Kneser Ney's practically constant complexity of O(n), it is much faster than other recent methods for language modeling such as neural networks and conditional exponential family models where ex- act computation of the normalizing constant costs O(V ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate PLRE, we compared its performance on English and Russian corpora with several vari-ants of KN smoothing, class-based models, and the log-bilinear neural language model <ref type="bibr" target="#b29">(Mnih and Hinton, 2007)</ref>. We evaluated with perplexity in most of our experiments, but also provide results evaluated with BLEU ( <ref type="bibr" target="#b34">Papineni et al., 2002</ref>) on a downstream machine translation (MT) task. We have made the code for our approach publicly available <ref type="bibr">3</ref> .</p><p>To build the hard class-based LMs, we utilized mkcls 4 , a tool to train word classes that uses the maximum likelihood criterion <ref type="bibr" target="#b33">(Och, 1995)</ref> for classing. We subsequently trained trigram class language models on these classes (correspond- ing to 2 nd -order HMMs) using SRILM <ref type="bibr" target="#b40">(Stolcke, 2002)</ref>, with KN-smoothing for the class transition probabilities. SRILM was also used for the base- line KN-smoothed models.</p><p>For our MT evaluation, we built a hierarchi- cal phrase translation (Chiang, 2007) system us- ing cdec ( <ref type="bibr" target="#b11">Dyer et al., 2010</ref>). The KN-smoothed models in the MT experiments were compiled us- ing KenLM (Heafield, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For the perplexity experiments, we evaluated our proposed approach on 4 datasets, 2 in English and 2 in Russian. In all cases, the singletons were re- placed with "&lt;unk&gt;" tokens in the training cor- pus, and any word not in the vocabulary was re- placed with this token during evaluation. There is a general dearth of evaluation on large-scale cor- pora in morphologically rich languages such as Russian, and thus we have made the processed Large-Russian corpus available for comparison 3 .</p><p>• Small-English: APNews corpus ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref>: Train -14 million words, Dev -963,000, Test -963,000. Vocabulary-18,000 types.</p><p>• Small-Russian: Subset of Russian news com- mentary data from 2013 WMT translation task 5 : Train-3.5 million words, Dev -400,000 Test - 400,000. Vocabulary -77,000 types.</p><p>• Large-English: English Gigaword, Training - 837 million words, Dev -8.7 million, Test -8.7 million. Vocabulary-836,980 types.</p><p>• Large-Russian: Monolingual data from WMT 2013 task. Training -521 million words, Vali- dation -50,000, Test -50,000. Vocabulary-1.3 million types.</p><p>For the MT evaluation, we used the parallel data from the WMT 2013 shared task, excluding the Common Crawl corpus data. The newstest2012 and newstest2013 evaluation sets were used as the development and test sets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Small Corpora</head><p>For the class-based baseline LMs, the number of classes was selected from {32, 64, 128, 256, 512, 1024}</p><p>(Small-English) and {512, 1024} (Small-Russian). We could not go higher due to the computationally laborious process of hard clustering. For Kneser-Ney, we explore four different variants: back-off (BO-KN) interpolated (int-KN), modified back-off (BO- MKN), and modified interpolated (int-MKN). Good-Turing estimates were used for discounts. All models trained on the small corpora are of order 3 (trigrams).</p><p>For PLRE, we used one low rank bigram and one low rank trigram in addition to the MLE n- gram estimates. The powers of the intermediate matrices/tensors were fixed to be 0.5 and the dis- counts were set to be square roots of the Good Tur- ing estimates (as explained in § 4.4). The ranks were tuned on the development set. For Small- English, the ranges were {1e − 3, 5e − 3} (as a fraction of the vocabulary size) for both the low rank bigram and low rank trigram models. For Small-Russian the ranges were {5e − 4, 1e − 3} for both the low rank bigram and the low rank tri- gram models.</p><p>The results are shown in <ref type="table">Table 1</ref>. The best class- based LM is reported, but is not competitive with the KN baselines. PLRE outperforms all of the baselines comfortably. Moreover, PLRE's perfor- mance over the baselines is highlighted in Russian. With larger vocabulary sizes, the low rank ap- proach is more effective as it can capture linguistic similarities between rare and common words.</p><p>Next we discuss how the maximum n-gram or- der affects performance. <ref type="figure">Figure 1</ref> shows the rela- tive percentage improvement of our approach over int-MKN as the order is increased from 2 to 4 for both methods. The Small-English dataset has a rather small vocabulary compared to the number of tokens, leading to lower data sparsity in the bi- gram. Thus the PLRE improvement is small for order = 2, but more substantial for order = 3. On the other hand, for the Small-Russian dataset, the vocabulary size is much larger and consequently the bigram counts are sparser. This leads to sim-   <ref type="table">Table 1</ref>: Perplexity results on small corpora for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1024(3) BO-KN(3) int-KN(3) BO-MKN(3) int-MKN(3) PLRE(3) Small-English Dev</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small-Russian</head><p>Small-English <ref type="figure">Figure 1</ref>: Relative percentage improvement of PLRE over int-MKN as the maximum n-gram or- der for both methods is increased.</p><p>ilar improvements for all orders (which are larger than that for Small-English).</p><p>On both these datasets, we also experimented with tuning the discounts for int-MKN to see if the baseline could be improved with more careful choices of discounts. However, this achieved only marginal gains (reducing the perplexity to 98.94 on the Small-English test set and 259.0 on the Small-Russian test set).</p><p>Comparison to LBL <ref type="bibr" target="#b29">(Mnih and Hinton, 2007)</ref>: <ref type="bibr" target="#b29">Mnih and Hinton (2007)</ref> evaluate on the Small-English dataset (but remove end markers and concatenate the sentences). They obtain per- plexities 117.0 and 107.8 using contexts of size 5 and 10 respectively. With this preprocessing, a 4- gram (context 3) PLRE achieves 108.4 perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Large Corpora</head><p>Results on the larger corpora for the top 2 per- forming methods "PLRE" and "int-MKN" are pre- sented in <ref type="table" target="#tab_3">Table 2</ref>. Due to the larger training size, we use 4-gram models in these experiments. How- ever, including the low rank 4-gram tensor pro- vided little gain and therefore, the 4-gram PLRE only has additional low rank bigram and low rank trigram matrices/tensors. As above, ranks were tuned on the development set. For Large-English, the ranges were {1e − 4, 5e − 4, 1e − 3} (as a frac- tion of the vocabulary size) for both the low rank     <ref type="table">Table 4</ref>: Results on English-Russian translation task (mean ± stdev). See text for details.</p><p>bigram and low rank trigram models. For Small- Russian the ranges were {1e−5, 5e−5, 1e−4} for both the low rank bigram and the low rank trigram models. For statistical validity, 10 test sets of size equal to the original test set were generated by ran- domly sampling sentences with replacement from the original test set. Our method outperforms "int- MKN" with gains similar to that on the smaller datasets. As shown in <ref type="table" target="#tab_4">Table 3</ref>, our method obtains fast training times even for large datasets. <ref type="table">Table 4</ref> presents results for the MT task, trans- lating from English to Russian 7 . We used MIRA ( <ref type="bibr" target="#b8">Chiang et al., 2008)</ref> to learn the feature weights. To control for the randomness in MIRA, we avoid retuning when switching LMs -the set of feature weights obtained using int-MKN is the same, only the language model changes. The procedure is repeated 10 times to control for op- timizer instability <ref type="bibr" target="#b10">(Clark et al., 2011</ref>). Unlike other recent approaches where an additional fea- ture weight is tuned for the proposed model and used in conjunction with KN smoothing ( <ref type="bibr" target="#b43">Vaswani et al., 2013)</ref>, our aim is to show the improvements that PLRE provides as a substitute for KN. On av- erage, PLRE outperforms the KN baseline by 0.16 BLEU, and this improvement is consistent in that PLRE never gets a worse BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Machine Translation Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Recent attempts to revisit the language model- ing problem have largely come from two direc- tions: Bayesian nonparametrics and neural net- works. <ref type="bibr" target="#b42">Teh (2006)</ref> and <ref type="bibr" target="#b12">Goldwater et al. (2006)</ref> discovered the connection between interpolated Kneser Ney and the hierarchical Pitman-Yor pro- cess. These have led to generalizations that ac- count for domain effects  and unbounded contexts ( .</p><p>The idea of using neural networks for language modeling is not new <ref type="bibr" target="#b26">(Miikkulainen and Dyer, 1991)</ref>, but recent efforts <ref type="bibr" target="#b29">(Mnih and Hinton, 2007;</ref><ref type="bibr" target="#b27">Mikolov et al., 2010</ref>) have achieved impressive performance. These methods can be quite expen- sive to train and query (especially as the vocab- ulary size increases). Techniques such as noise contrastive estimation ( <ref type="bibr" target="#b15">Gutmann and Hyvärinen, 2012;</ref><ref type="bibr" target="#b30">Mnih and Teh, 2012;</ref><ref type="bibr" target="#b43">Vaswani et al., 2013</ref>), subsampling (Xu et al., 2011), or careful engi- neering approaches for maximum entropy LMs (which can also be applied to neural networks) <ref type="bibr" target="#b46">(Wu and Khudanpur, 2000</ref>) have improved train- ing of these models, but querying the probabil- ity of the next word given still requires explicitly normalizing over the vocabulary, which is expen- sive for big corpora or in languages with a large number of word types. <ref type="bibr" target="#b30">Mnih and Teh (2012)</ref> and <ref type="bibr" target="#b43">Vaswani et al. (2013)</ref> propose setting the normal- ization constant to 1, but this is approximate and thus can only be used for downstream evaluation, not for perplexity computation. An alternate tech- nique is to use word-classing <ref type="bibr" target="#b13">(Goodman, 2001;</ref><ref type="bibr" target="#b28">Mikolov et al., 2011</ref>), which can reduce the cost of exact normalization to O( √ V ). In contrast, our approach is much more scalable, since it is triv- ially parallelized in training and does not require explicit normalization during evaluation.</p><p>There are a few low rank approaches <ref type="bibr" target="#b39">(Saul and Pereira, 1997;</ref><ref type="bibr" target="#b0">Bellegarda, 2000;</ref><ref type="bibr" target="#b18">Hutchinson et al., 2011</ref>), but they are only effective in restricted set- tings (e.g. small training sets, or corpora divided into documents) and do not generally perform comparably to state-of-the-art models. <ref type="bibr" target="#b36">Roark et al. (2013)</ref> also use the idea of marginal constraints for re-estimating back-off parameters for heavily- pruned language models, whereas we use this con- cept to estimate n-gram specific discounts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>= 0</head><label>0</label><figDesc>(11) Note that Eq. 11 decouples across w i−1 since the only d j i,i−1 terms that are dependent are the ones that share the preceding context w i−1 . It is straightforward to see that setting d j i,i−1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proof.</head><label></label><figDesc>Clearly this choice of d j i,i−1 satisfies Eq. 11. The largest possible value of d j i,i−1 is c j+1 i,i−1 . ρ j ≥ ρ j+1 , implies c j i,i−1 ≥ c j+1 i,i−1 . Thus the inequality constraints are met. It is then easy to verify that γ takes the above form.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean perplexity results on large corpora, 
with standard deviation. 

Dataset 
PLRE Training Time 
Small-English 3.96 min ( order 3) / 8.3 min (order 4) 
Small-Russian 4.0 min (order 3) / 4.75 min (order 4) 
Large-English 
3.2 hrs (order 4) 
Large-Russian 
8.3 hrs (order 4) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>PLRE training times for a fixed parameter 
setting 6 . 8 Intel Xeon CPUs were used. 

Method 
BLEU 
int-MKN(4) 
17.63 ± 0.11 
PLRE(4) 
17.79 ± 0.07 
Smallest Diff 
PLRE+0.05 
Largest Diff 
PLRE+0.29 

</table></figure>

			<note place="foot" n="1"> with a slight abuse of notation, let Z D j be shorthand for Z j,D j</note>

			<note place="foot" n="2"> for derivation see proof of Lemma 4 in the supplementary material</note>

			<note place="foot" n="3"> http://www.cs.cmu.edu/∼apparikh/plre.html 4 http://code.google.com/p/giza-pp/ 5 http://www.statmt.org/wmt13/training-monolingualnc-v8.tgz</note>

			<note place="foot" n="6"> As described earlier, only the ranks need to be tuned, so only 2-3 low rank bigrams and 2-3 low rank trigrams need to be computed (and combined depending on the setting). 7 the best score at WMT 2013 was 19.9 (Bojar et al., 2013)</note>

			<note place="foot" n="8"> Conclusion We presented power low rank ensembles, a technique that generalizes existing n-gram smoothing techniques to non-integer n. By using ensembles of sparse as well as low rank matrices and tensors, our method captures both the fine-grained and coarse structures in word sequences. Our discounting strategy preserves the marginal constraint and thus generalizes Kneser Ney, and under slight changes can also extend other smoothing methods such as deleted-interpolation/JelinekMercer smoothing. Experimentally, PLRE convincingly outperforms Kneser-Ney smoothing as well as class-based baselines.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by NSF IIS1218282, NSF IIS1218749, NSF IIS1111142, NIH R01GM093156, the U. S. Army Research Labo-ratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533, the NSF Graduate Research Fellowship Program under Grant No. 0946825 (NSF Fellowship to APP), and a grant from Ebay Inc. (to AS).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large vocabulary speech recognition with multispan statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jerome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bellegarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="84" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuowei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization. Foundations of Computational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="717" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of smoothing techniques for me models. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="50" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Shrinking exponential language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="468" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online large-margin training of syntactic and structural translation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="233" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpolating between types and tokens by estimating power-law generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;01)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="561" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">KenLM: faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland; United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with fixed row and column sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Diep</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Van Dooren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">429</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1020" to="1025" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low rank language models for small training sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="489" to="492" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interpolated estimation of markov source parameters from sparse data. Pattern recognition in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Divide-and-conquer matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lester</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1107.0789</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">999</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Natural language processing with modular pdp networks and distributed lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">G</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="343" to="399" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luk</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</title>
		<meeting>the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured penalties for log-linear language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kumar Nelakanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On Structuring Probabilistic Dependencies in Stochastic Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Maximum-likelihoodschätzung von wortkategorien mit verfahren der kombinatorischen optimierung. Bachelor&apos;s thesis (Studienarbeit)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Erlangen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fundamentals of speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biing-Hwang</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Smoothed marginal distribution constraints for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using Markov chain Monte Carlo</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregate and mixed-order markov models for statistical language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second conference on empirical methods in natural language processing</title>
		<meeting>the second conference on empirical methods in natural language processing<address><addrLine>Somerset</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="81" to="89" />
		</imprint>
	</monogr>
	<note>New Jersey</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SRILM-An Extensible Language Modeling Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference in Spoken Language Processing</title>
		<meeting>the International Conference in Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in artificial intelligence</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian language model based on pitman-yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="607" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A stochastic memoizer for sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Archambeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lancelot</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1129" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient training methods for maximum entropy language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="114" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Efficient subsampling for training complex language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asela</forename><surname>Gunawardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1128" to="1136" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Human behaviour and the principle of least-effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949" />
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
