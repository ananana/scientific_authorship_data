<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph and Text Jointly Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research § Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research § Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research § Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research § Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph and Text Jointly Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1591" to="1601"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We examine the embedding approach to reason new relational facts from a large-scale knowledge graph and a text corpus. We propose a novel method of jointly embedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be handled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task show that jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs such as Freebase ( <ref type="bibr" target="#b1">Bollacker et al., 2008)</ref> and WordNet <ref type="bibr" target="#b11">(Miller, 1995)</ref> have be- come important resources for many AI &amp; NLP ap- plications such as Q &amp; A. Generally, a knowledge graph is a collection of relational facts that are of- ten represented in the form of a triplet (head en- tity, relation, tail entity), e.g., "(Obama, Born-in, Honolulu)". An urgent issue for knowledge graph- s is the coverage, e.g., even the largest knowledge graph of Freebase is still far from complete.</p><p>Recently, targeting knowledge graph comple- tion, a promising paradigm of embedding was pro- posed, which is able to reason new facts only from the knowledge graph <ref type="bibr" target="#b2">(Bordes et al., 2011;</ref><ref type="bibr" target="#b14">Socher et al., 2013;</ref><ref type="bibr" target="#b16">Wang et al., 2014</ref>). Generally, in this series of methods, each entity is represented as a k-dimensional vector and each relation is characterized by an operation in k so that a candidate fact can be asserted by sim- ple vector operations. The embeddings are usually learnt by minimizing a global loss function of all the entities and relations in the knowledge graph. Thus, the vector of an entity may encode global information from the entire graph, and hence scor- ing a candidate fact by designed vector operations plays a similar role to long range "reasoning" in the graph. However, since this requires the vectors of both entities to score a candidate fact, this type of methods can only complete missing facts for which both entities exist in the knowledge graph. However, a missing fact often contains entities out of the knowledge graph (called out-of-kb for short in this paper), e.g., one or both entities are phras- es appearing in web text but not included in the knowledge graph yet. How to deal with these fact- s is a significant obstacle to widely applying the embedding paradigm.</p><p>In addition to knowledge embedding, anoth- er interesting approach is the word embedding method word2vec ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>), which shows that learning word embeddings from an unlabeled text corpus can make the vectors con- necting the pairs of words of some certain relation almost parallel, e.g., vec("China") − vec("Beijing") ≈ vec("Japan") − vec("Tokyo"). However, it does not know the exact relation be- tween the pairs. Thus, it cannot be directly applied to complete knowledge graphs.</p><p>The capabilities and limitations of knowledge embedding and word embedding have inspired us to design a mechanism to mosaic the knowledge graph and the "word graph" together in a vector space so that we can score any candidate relation- al facts between entities and words <ref type="bibr">1</ref> . Therefore, we propose a novel method to jointly embed enti- ties and words into the same vector space. In our solution, we define a coherent probabilistic model for both knowledge and text, which is composed of three components: the knowledge model, text model, and alignment model. Both the knowledge model and text model use the same core transla- tion assumption for the fact modeling: a candidate fact (h, r, t) is scored based on h + r − t. The only difference is, in the knowledge model the re- lation r is explicitly supervised and the goal is to fit the fact triplets, while in the text model we as- sume any pair of words h and t that concur in some text windows are of certain relation r but r is a hid- den variable, and the goal is to fit the concurring pairs of words. The alignment model guarantees the embeddings of entities and words/phrases lie in the same space and impels the two models to en- hance each other. Two mechanisms of alignment are introduced in this paper: utilizing names of en- tities and utilizing Wikipedia anchors. This way of jointly embedding knowledge and text can be con- sidered to be semi-supervised knowledge embed- ding: the knowledge graph provides explicit su- pervision of facts while the text corpus provides much more "relation-unlabeled" pairs of words.</p><p>We conduct extensive large scale experiments on Freebase and Wikipedia corpus, which show jointly embedding brings promising improve- ments to the accuracy of predicting facts, com- pared to separately embedding the knowledge graph and the text corpus, respectively. Particu- larly, jointly embedding enables the prediction of a candidate fact with out-of-kb entities, which can not be handled by any existing embedding meth- ods. We also use embeddings to provide a prior score to help fact extraction on the benchmark da- ta set of Freebase+NYTimes and also observe very promising improvements. Meanwhile, concerning the quality of word embeddings, experiments on the analogical reasoning task show that jointly em- bedding is comparable to or slightly better than word2vec (Skip-Gram).</p><p>Knowledge Embedding. A knowledge graph is embedded into a low-dimensional continuous vec- tor space while certain properties of it are pre- served <ref type="bibr" target="#b2">(Bordes et al., 2011;</ref><ref type="bibr" target="#b14">Socher et al., 2013;</ref><ref type="bibr" target="#b4">Chang et al., 2013;</ref><ref type="bibr" target="#b16">Wang et al., 2014</ref>). Generally, each entity is represented as a point in that space while each relation is inter- preted as an operation over entity embeddings. For instance, <ref type="bibr">TransE (Bordes et al., 2013</ref>) interprets a relation as a translation from the head entity to the tail entity. The embedding representations are usu- ally learnt by minimizing a global loss function in- volving all entities and relations so that each entity embedding encodes both local and global connec- tivity patterns of the original graph. Thus, we can reason new facts from learnt embeddings. Word Embedding. Generally, word embeddings are learned from a given text corpus without su- pervision by predicting the context of each word or predicting the current word given its contex- t ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b9">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>). Al- though relations between words are not explicitly modeled, continuous bag-of-words (CBOW) and Skip-gram ( <ref type="bibr" target="#b9">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>) learn word embeddings capturing many syntactic and semantic relations between words where a relation is also represented as the trans- lation between word embeddings. Relational Facts Extraction. Another pivotal channel for knowledge graph completion is ex- tracting relational facts from external sources such as free text <ref type="bibr" target="#b12">(Mintz et al., 2009;</ref><ref type="bibr" target="#b13">Riedel et al., 2010;</ref><ref type="bibr" target="#b7">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b15">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b18">Zhang et al., 2013;</ref><ref type="bibr" target="#b6">Fan et al., 2014</ref>). This se- ries of methods focuses on identifying local text patterns that express a certain relation and making predictions based on them. However, they have not fully utilized the evidences from a knowledge graph, e.g., knowledge embedding is able to rea- son new facts without any external sources. Ac- tually, knowledge embedding is very complemen- tary to traditional extraction methods, which was first confirmed by . To es- timate the plausibility of a candidate fact, they added scores from embeddings to scores from an extractor, which showed significant improvemen- t. However, as pointed out in the introduction, their knowledge embedding method cannot pre- dict facts involving out-of-kb entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Jointly Embedding Knowledge and Text</head><p>We will first describe the notation used in this pa- per. A knowledge graph ∆ is a set of triplets in the form (h, r, t), h, t ∈ E and r ∈ R where E is the entity vocabulary and R is a collection of pre- defined relations. We use bold letters h, r, t to de- note the corresponding embedding representation- s of h, r, t. A text corpus is a sequence of words drawn from the word vocabulary V. Note that we perform some preprocessing to detect phrases in the text and the vocabulary here already includes the phrases. For simplicity's sake, without spe- cial explanation, when we say "word(s)", it means "word(s)/phrase(s)". Since we consider triplets in- volving not only entities but also words, we denote I = E ∪ V. Additionally, we denote anchors by A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling</head><p>Our model is composed of three components: the knowledge model, text model, and alignment model. Before defining the component models, we first define the element model for a fact triplet. In- spired by TransE, we also represent a relation r as a vector r ∈ k and score a fact triplet (h, r, t) by z(h, r, t) = b − 1 2 h + r − t 2 where b is a constant for bias designated for adjusting the scale for better numerical stability and b = 7 is a sensi- ble choice. z(h, r, t) is expected to be large if the triplet is true. Based on the same element model of fact, we define the component models as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Knowledge Model</head><p>We define the following conditional probability of a fact (h, r, t) in a knowledge graph:</p><formula xml:id="formula_0">Pr(h|r, t) = exp{z(h, r, t)} ˜ h∈I exp{z( ˜ h, r, t)}<label>(1)</label></formula><p>and we have named our model pTransE (Proba- bilistic TransE) to show respect to TransE. We also define Pr(r|h, t) and Pr(t|h, r) in the same way by choosing corresponding normalization terms respectively. We define the likelihood of observ- ing a fact triplet as:</p><p>L f (h, r, t) = log Pr(h|r, t)+ log Pr(t|h, r)</p><p>+ log Pr(r|h, t)</p><p>The goal of the knowledge model is to maximize the conditional likelihoods of existing fact triplets in the knowledge graph:</p><formula xml:id="formula_2">L K = (h,r,t)∈∆ L f (h, r, t)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Text Model</head><p>We propose the following key assumption for modeling text, which connects word embedding and knowledge embedding: there are relations between words although we do not know what they are.</p><p>Relational Concurrence Assumption. If two words w and v concur in some context, e.g., a win- dow of text, then there is a relation r wv between the two words. That is, we can state the triplet of (w, r wv , v) is a fact.</p><p>We define the conditional probability Pr(w|r wv , v) following the same formulation of Eq. <ref type="formula" target="#formula_0">(1)</ref> to model why two words concur in some context. In contrast to knowledge embedding, here r wv is a hidden variable rather than explicitly supervised.</p><p>The challenge is to deal with the hidden variable r wv . Obviously, without any more assumption- s, the number of distinct r wv is around |V| × ¯ N , where ¯ N is the average number of unique word- s concurred with each word. This number is ex- tremely large. Thus it is almost impossible to esti- mate a vector for each r wv . And the problem is ac- tually ill-posed. We need to constrain the freedom degree of r wv . Here we use auxiliary variables to reduce the size of variables we need to estimate: let w = w + r wv , then</p><formula xml:id="formula_3">z(w, r wv , v) z(w , v) = b − 1 2 w − v 2 (4)</formula><p>and</p><formula xml:id="formula_4">Pr(w|r wv , v) Pr(w|v) = exp{z(w , v)} ˜ w∈V exp{z( ˜ w , v)} (5)</formula><p>In this way we need to estimate vectors w and w for each word w, and a total of 2 × |V| vectors.</p><p>The goal of the text model is to maximize the likelihood of the concurrences of pairs of words in text windows:</p><formula xml:id="formula_5">L T = (w,v)∈C</formula><p>n wv log Pr(w|v).</p><p>In the above equation, C is all the distinct pairs of words concurring in text windows of a fixed size. And n wv is the number of concurrences of the pair (w, v). Interestingly, as explained in Sec.(3.3), this text model is almost equivalent to Skip-Gram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Alignment Model</head><p>If we only have the knowledge model and text model, the entity embeddings and word embed- dings will be in different spaces and any comput- ing between them is meaningless. Thus we need mechanisms to align the two spaces into the same one. We propose two mechanisms in this paper: u- tilizing Wikipedia anchors, and utilizing names of entities. Alignment by Wikipedia Anchors. This mod- el is based on the connection between Wikipedia and Freebase: for most Wikipedia (English) pages, there is an unique corresponding entity in Free- base. As a result, for most of the anchors in Wikipedia, each of which refers to a Wikipedi- a page, we know that the surface phrase v of an anchor actually refers to the Freebase entity e v . Thus, we define a likelihood for this part of an- chors as Eq. <ref type="formula" target="#formula_6">(6)</ref> but replace the word pair (w, v) with the word-entity pair (w, e v ), i.e., using the corresponding entity e v rather than the surface word v in Eq. <ref type="formula">(5)</ref>:</p><formula xml:id="formula_7">L AA = (w,v)∈C,v∈A log Pr(w|e v ) (7)</formula><p>where A denotes the set of anchors.</p><p>In addition to Wikipedia anchors, we can also use an entity linking system with satisfactory per- formance to produce the pseudo anchors.</p><p>Alignment by Names of Entities. Another way is to use the names of entities. For a fact triplet (h, r, t) ∈ ∆, if h has a name w h and w h ∈ V, then we will generate a new triplet of (w h , r, t) and add it to the graph. Similarly, we also add (h, r, w t ) and (w h , r, w t ) into the graph if the names exist and belong to the word vocabulary. We call this sub-graph containing names the name graph and define a likelihood for the name graph by observ- ing its triplets:</p><formula xml:id="formula_8">L AN = (h,r,t)∈∆ I [w h ∈V ∧ wt∈V] ·L f (w h , r, w t )+ I [w h ∈V] · L f (w h , r, t) + I [wt∈V] · L f (h, r, w t )<label>(8)</label></formula><p>Both alignment models have advantages and disadvantages. Alignment by names of entities is straightforward and does not rely on additional da- ta sources. The number of triplets generated by the names is also large and can significantly change the results. However, this model is risky. On the one hand, the name of an entity is ambiguous be- cause different entities sometimes have the same name so that the name graph may contaminate the knowledge embedding. On the other hand, an en- tity often has several different aliases when men- tioned in the text but we do not have the complete set, which will break the semantic balance of word embedding. For example, for the entity Apple In- c., suppose we only have the standard name "Ap- ple Inc." but do not have the alias "apple". And for the entity Apple that is fruit, suppose we have the name "apple" included in the name graph. Then the vector of the word "apple" will be biased to the concept of fruit rather than the company. But if no name graph intervenes, the unsupervised word embedding is able to learn a vector that is closer to the concept of the company due to the polarities. Alignment by anchors relies on the additional data source of Wikipedia anchors. Moreover, the num- ber of matched Wikipedia anchors (∼40M) is rela- tively small compared to the total number of word pairs (∼2.0B in Wikipedia) and hence the contri- bution is limited. However, the advantage is that the quality of the data is very high and there are no ambiguity/completeness issues.</p><p>Considering the above three component models together, the likelihood we maximize is:</p><formula xml:id="formula_9">L = L K + L T + L A<label>(9)</label></formula><p>where</p><formula xml:id="formula_10">L A could be L AA or L AN or L AA + L AN .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Approximation to the Normalizers</head><p>It is difficult to directly compute the normalizers in Pr(h|r, t) (or Pr(t|h, r), Pr(r|h, t)) and Pr(w|v) as the normalizers sum over |I| or |V| terms where both |I| and |V| reach tens of millions. To pre- vent having to exactly calculate the normalizer- s, we use negative sampling (NEG) ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>) to transform the original objective, i.e., Eq. <ref type="formula" target="#formula_9">(9)</ref> to a simple objective of the binary classifi- cation problem-differentiating the observed data from noise. First, we define: (i) the probability of a given triplet (h, r, t) to be true (D = 1); and (ii) the probability of a given word pair (w, v) to co-occur (D = 1):</p><formula xml:id="formula_11">Pr(D = 1|h, r, t) = σ(z(h, r, t))<label>(10)</label></formula><formula xml:id="formula_12">Pr(D = 1|w, v) = σ(z(w , v))<label>(11)</label></formula><p>where σ(x) = 1 1+exp{−x} and D ∈ {0, 1}.</p><p>Instead of maximizing log Pr(h|r, t) in Eq.(2), we maximize: log Pr(1|h, r, t)</p><formula xml:id="formula_13">+ c i=1 E ˜ h i ∼Prneg( ˜ h i ) [Pr(0|˜h0|˜ 0|˜h i , r, t)]<label>(12)</label></formula><p>where c is the number of negative examples to be discriminated for each positive example. NEG guarantees that maximizing Eq. <ref type="formula" target="#formula_0">(12)</ref> can approxi- mately maximize log Pr(h|r, t). Thus, we also re- place log Pr(r|h, t), log Pr(t|r, h) in Eq. <ref type="formula" target="#formula_1">(2)</ref>, and log Pr(w|v) in Eq.(6) in the same way by choosing corresponding negative distributions respectively. As a result, the objectives of both the knowledge model L K (Eq. <ref type="formula" target="#formula_2">(3)</ref>) and text model L T (Eq. <ref type="formula" target="#formula_6">(6)</ref>) are free from cumbersome normalizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Optimization</head><p>We use stochastic gradient descent (SGD) to max- imize the simplified objectives.</p><p>Knowledge model. ∆ is randomly tra- versed multiple times. When a positive example (h, r, t) ∈ ∆ is considered, to maximize (12), we construct c negative triplets by sampling elements from an uniform distribution over I and replacing the head of (h, r, t). The transformed objective of log Pr(r|h, t) is maximized in the same manner, but by sampling from a uniform distribution over R and corrupting the relation of (h, r, t). After a mini-batch, computed gradients are used to update the involved embeddings.</p><p>Text model. The text corpus is traversed one or more times. When current word v and a context word w are considered, c words are sampled from the unigram distribution raised to the 3/4rd power and regarded as negative examples ( ˜ w, v) that are never concurrent. Then we compute and update the related gradients.</p><p>Alignment model. L AA and L AN are absorbed by the text model and knowledge model respec- tively, since anchors are considered to predict con- text given an entity and the name graph are homo- geneous to the original knowledge graph.</p><p>Joint. All three component objectives are si- multaneously optimized. To deal with large-scale data, we implement a multi-thread version with shared memory. Each thread is in charge of a por- tion of the data (either knowledge or text corpus), and traverses through them, calculates gradients and commits the update to the global model and is stored in a block of shared memory. For the sake of efficiency, no lock is used on the shared memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Connections to Related Models</head><p>TransE. <ref type="figure" target="#fig_0">(Bordes et al., 2013)</ref> proposed to mod- el a relation r as a translation vector r ∈ k which is expected to connect h and t with low error if (h, r, t) ∈ ∆. We also follow it. How- ever, TransE uses a margin based ranking loss {{h+r−t 2 +γ− ˜ h+r− ˜ t 2 } + . It is not a proba- bilistic model and hence it needs to restrict the nor- m of either entity embedding and/or relation em- bedding.  intuitively addresses this problem by simply normalizing the entity em- beddings to the unit sphere before computing gra- dients at each iteration. We define pTransE as a probabilistic model, which doesn't need addition- al constraints on the norms of embeddings of en- tities/words/relations, and thus eliminates the nor- malization operations. <ref type="bibr">Skip-gram. (Mikolov et al., 2013a;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>) defines the probability of the concurrence of two words in a window as:</p><formula xml:id="formula_14">Pr(w|v) = exp{w T v}˜w∈V v}˜ v}˜w∈V exp{˜wexp{˜ exp{˜w T v}<label>(13)</label></formula><p>which is based on the inner product, while our text model (Eqs. <ref type="formula">(4)</ref>, <ref type="formula">(5)</ref>) is based on distance. If we constrain w = 1 for each w, then w T v = 1 − 1 2 w − v 2 . It is easy to see that our text model is equivalent to Skip-gram in this case. Our distance-based text model is directly derived from the triplet fact model, which clearly explains why it is able to make the pairs of entities of a certain relation parallel in the vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We empirically evaluate and compare related mod- els with regards to three tasks: triplet classifica- tion ( <ref type="bibr" target="#b14">Socher et al., 2013)</ref>, improving relation ex- traction ( , and the analogi- cal reasoning task ( <ref type="bibr" target="#b9">Mikolov et al., 2013a</ref>). The related models include: for knowledge embed- ding alone, <ref type="bibr">TransE (Bordes et al., 2013)</ref>, pTransE (proposed in this paper); for word embedding alone, Skip-gram (Mikolov et al., 2013b); for both <ref type="table">Table 2</ref>: Data: the number of e − e, w − e, e − w, w − w triplets/analogies where w represents the out-of-kb entity, which is regarded as word and replaced by its corresponding entity name. knowledge and text, we use "respectively" to re- fer to the embeddings learnt by TransE/pTransE and Skip-gram, respectively, "jointly" to refer to our jointly embedding method, in which "anchor" and "name" refer to "Alignment by Wikipedia An- chors" and "Alignment by Names of Entities", re- spectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>To learn the embedding representations of entities and words, we use a knowledge graph, a text cor- pus, and some connections between them.</p><p>Knowledge. We adopt Freebase as our knowl- edge graph. First, we remove the user profiles, version control, and meta data, leaving 52,124,755 entities, 4,490 relations, and 204,120,782 triplet- s. We call this graph main facts. Then we held out 8,331,147 entities from main facts and regard them as out-of-kb entities. Under such a setting, from main facts, we held out all the triplets in- volving out-of-kb entities, as well as 24,610,400 triplets that don't contain out-of-kb entities. Held- out triplets are used for validation and testing; the remaining triplets are used for training. See <ref type="table" target="#tab_0">Table  1</ref> for the statistics.</p><p>We regard out-of-kb entities as words/phrases and thus divide the held-out triplets into four type- s: no out-of-kb entity (e − e), the head is out-of-kb entity but the tail is not (w − e), the tail is out-of- kb entity but the head is not (e − w), and both the head and tail are out-of-kb entities (w − w). Then we replace the out-of-kb entities among the held- out triplets by their corresponding entity names. The mapping from a Freebase entity identifier to its name is done through the Freebase predicate- "/type/object/name". Since some entity names are not present in our vocabulary V, we remove triplets involving these names (see <ref type="table">Table 2</ref>). In such a way, besides the missing edges between ex- isting entities, the related models can be evaluated on triplets involving words/phrases as their head and/or tail.</p><p>Text. We adopt the Wikipedia (English) cor- pus. After removing pages designated for nav- igation, disambiguation, or discussion purpos- es, there are 3,469,024 articles left. We ap- ply sentence segmentation, tokenization, Part-of- Speech (POS) tagging, and named entity recog- nition (NER) to these articles using Apache OpenNLP package 2 . Then we conduct some sim- ple chunking to acquire phrases: if several con- secutive tokens are identically tagged as "Loca- tion"/"Person"/"Organization", or covered by an anchor, we combine them as a chunk. After the preprocessing, our text corpus contain 73,675,188 sentences consisting of 1,522,291,723 chunks. A- mong them, there are around 20 millions distinct chunks, including words and phrases. We filter out punctuation and rare words/phrases that occur less than three times in the text corpus, reducing |V| to 5,240,003.</p><p>Alignment. One of our alignment models need- s Wikipedia anchors. There are around 45 million such anchors in our text corpus and 41,970,548 of them refer to entities in E. Another mechanism u- tilizes the name graph constructed through names of entities. Specifically, for each training triplet (h, r, t), suppose h and t have entity names w h and w t , respectively and w h , w t ∈ V, the train- ing triplet contributes (w h , r, w t ), (w h , r, t), and (h, r, w t ) to the name graph. There are 81,753,310 triplets in our name graphs. Note that there is no overlapping between the name graph and held-out triplets of e − w, w − e, and w − w types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Triplet Classification</head><p>This task judges whether a triplet (h, r, t) is true or false, i.e., binary classification of a triplet.</p><p>Evaluation protocol. Following the same pro- tocol in <ref type="bibr">NTN (Socher et al., 2013)</ref>, for each true triplet, we construct a false triplet for it by ran- domly sampling an element from I to corrupt its head or tail. Since |E| is significantly larger than |V| in our data, sampling from a uniform distri- bution over I will let triplets involving no word dominate the false triplets. To avoid that, when we corrupt the head of (h, r, t), if h ∈ E, h is sam- pled from E while if h ∈ V, h is sampled from V. The same rule is applied when we corrupt the tail of (h, r, t). In this way, for each of the four types of triplets, we ensure the number of true triplets is equal to that of false ones.</p><p>To classify a triplet (h, r, t), we first use the con- sidered methods to score it. TransE scores it by −|h + r − t|. Our models score it by Pr(D = 1|h, r, t) (see Eq. <ref type="formula" target="#formula_0">(10)</ref>). Then the considered meth- ods label a triplet (h, r, t) as true if its score is larger than the relation-specific threshold of r, as false otherwise. The relation-specific thresholds are chosen to maximize the classification accura- cy over the validation set.</p><p>We report the classification accuracy. Addition- ally, we rank all the testing triplets by their scores in descending order. Then we draw a precision- recall (PR) curve based on this ranking and report the area under the PR curve.</p><p>Implementation. We implement TransE (Bor- des et al., 2013), Skip-gram ( <ref type="bibr" target="#b9">Mikolov et al., 2013a)</ref>, and our models.</p><p>First, we train TransE and pTransE over our training triplets with embedding dimension k in {50, 100, 150}. Adhering to ( , we use the fixed learning rate α in {0.005, 0.01, 0.05} for TransE during its 300 e- pochs. For pTransE, we use the number of neg- ative examples per positive example c among {5, 10}, the learning rate α among {0.01, 0.025} where α decreases along with its 40 epochs. The optimal configurations of TransE are: k = 100, α = 0.01. The optimal configurations of pTransE are: k = 100, c = 10, and α = 0.025.</p><p>Then we train Skip-gram with the embedding dimension k in {50, 100, 150}, the max skip-range s in {5, 10}, the number of negative examples per positive example c in {5, 10}, and learning rate α = 0.025 linearly decreasing along with the 6 epochs over our text corpus. Popular words whose frequencies are larger than 10 −5 are subsampled according to the trick proposed in <ref type="figure" target="#fig_0">(Mikolov et al.,  2013b)</ref>. The optimal configurations of Skip-gram are: k = 150, s = 5, and c = 10.</p><p>Combining entity embeddings and word em- beddings learnt by pTransE and Skip-gram respec- tively, "respectively" model can score all types of held-out triplets. For our jointly embedding mod- el, we consider various alignment mechanisms and use equal numbers of threads for knowledge mod- el and text model. The best configurations of "jointly" model are: k = 150, s = 5, c = 10, and α = 0.025 which linearly decreases along with the 6 epochs of traversing text corpus.</p><p>Results. We first illustrate the comparison be- tween TransE and pTransE over e − e type triplet- s in <ref type="table" target="#tab_2">Table 3</ref>. Observing the scores assigned to true triplets by TransE, we notice that triplets of popular relations generally have larger scores than those of rare relations. In contrast, pTransE, as a probabilistic model, assigns comparable scores to true triplets of both popular and rare relations. When we use a threshold to separate true triplets from false triplets of the same relation, there is no obvious difference between the two models. How- ever, when all triplets are ranked together, assign- ing scores in a more uniform scale is definitely an advantage. Thus, the contradiction stems from the different training strategies of the two models and the consideration of relation-specific thresholds.</p><p>Classification accuracies over various types of held-out triplets are presented in <ref type="table" target="#tab_3">Table 4</ref>. The "jointly" model outperforms the "respectively" model no matter which alignment mechanism(s) are used. Actually, for the "respectively" model, there is no interaction between entity embeddings and word embeddings during training and thus it- s predictions, over triplets that involve both enti- ty and word at the same time, are not much bet- ter than random guessing. It is also a natural re- sult that alignment by names is more effective than alignment by anchors. The number of anchors is much smaller than the number of overall chunks in our text corpus. In addition, the number of en- tities mentioned by anchors is very limited com- pared with |E|. Thus, interactions brought in by anchors are not as significant as that of the name graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improving Relation Extraction</head><p>It has been shown that embedding models are very complementary to extractors . However, some entities detected from text are out- of-kb entities. In such a case, triplets involving these entities cannot be handled by any existing knowledge embedding method, but our jointly em- bedding model can score them. As our model can cover more candidate triplets provided by extrac- tors, it is expected to provide more significant im- provements to extractors than any other embed- ding model. We confirm this point as follow. Evaluation protocol. For relation extraction, we use a public dataset-NYT+FB ( <ref type="bibr" target="#b13">Riedel et al., 2010)</ref>  <ref type="bibr">3</ref> , which distantly labels the NYT corpus by Freebase facts. We consider ( <ref type="bibr" target="#b12">Mintz et al., 2009</ref>) and <ref type="bibr">Sm2r (Weston et al., 2013)</ref> as our extractors to provide candidate triplets as well as their plau- sibilities estimated according to text features.</p><p>For embedding, we first held out triplets from our training set that appear in the test set of NYT+FB. Then we train TransE, pTransE and the "jointly" model on the remaining training triplets as well as on our text corpus. Then we use these models to score each candidate triplet in the same 3 http://iesl.cs.umass.edu/riedel/ecml/ way as the previous triplet classification experi- ment.</p><p>For combination, we first divide each candidate triplet into one of these categories: e − e, e − w, w − e, w − w, and "out-of-vocabulary". Be- cause there is no embedding model that can score triplets involving out-of-vocabulary word/phrase, we just ignore these triplets.Please note that, for our jointly embedding model, there are no "out- of-vocabulary" triplets if we include the NYT cor- pus for training. We use the embedding models to score candidate triplets and combine the scores given by the embedding model with scores given by the extractors. For each type e−e, e−w, w −e, w − w and their union (i.e. all), we rank the candi- date triplets by their revisited scores and draw PR curve to observe which embedding method pro- vides the most significant improvements to the ex- tractors.</p><p>Implementation. For ( <ref type="bibr" target="#b12">Mintz et al., 2009</ref>), we use the implementation in ( <ref type="bibr" target="#b15">Surdeanu et al., 2012)</ref>  <ref type="bibr">4</ref> . We implement Sm2r by ourselves with the best hy- perparameters introduced in ( ). For TransE, pTransE, and the "jointly" model, we use the same implementations, scoring schemes, and optimal configurations as the triplet classifica- tion experiment.</p><p>To combine extractors with embedding mod- els, we consider two schemes. Since Mintz s- cores candidate triplets in a probabilistic man- ner, we linearly combine its scores with the s- cores given by pTransE or the "jointly" mod- el: β Pr Mintz +(1 − β) Pr pTransE/Jointly where β is enumerated from 0 to 1 with 0.025 as a search step. On the other hand, neither Sm2r nor TransE is a probabilistic model. Thus, we combine Sm2r with TransE or the "jointly" model ac- cording to the scheme proposed in  where for each candidate (h, r, t), if r =r δ(Score(h, r, t) &lt; Score(h, r , t)) is less than τ , we increase Score Sm2r (h, r, t) by p. We search for the best β, τ , and p on another dataset- Wikipedia corpus distantly labeled by Freebase.</p><p>Result. We present the PR curves in <ref type="figure" target="#fig_0">Fig. (1,  2)</ref>. Over candidate triplets provided by either Mintz or Sm2r, the "jointly" model is consis- tently comparable with the "knowledge" model (TransE/pTransE) over e − e triplets while it out- performs the "knowledge" model by a consider- able margin over triplets of other types. These results confirm the advantage of jointly embed- ding and are actually straightforward results of our triplet classification experiment because the only difference is that the triplets here are provided by the extractor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analogical Reasoning Task</head><p>We compare our method with Skip-gram on this task to observe and study the influences of both knowledge embedding and alignment mechanisms on the quality of word embeddings. Evaluation protocol. We use the same pub- lic datasets as in ( <ref type="bibr" target="#b10">Mikolov et al., 2013b</ref>): 19,544 word analogies 5 ; 3,218 phrase analogies 6 . We al- so construct analogies from our held-out triplet- s (see <ref type="table">Table 2</ref>) by first concatenating two entity pairs of the same relation to form an analogy and then replacing the entities by corresponding entity names, e.g., "(Obama, Honolulu, David Beckham, London)" where the relation is "Born-in". Following <ref type="bibr" target="#b10">(Mikolov et al., 2013b</ref>), we only con- sider analogies that consist of the top-K most fre- quent words/phrases in the vocabulary. For each analogy denoted by (h 1 , t 1 , h 2 , t 2 ), we enumer- ate all the top-K most frequent words/phrases w except for h 1 , t 1 , h 2 , and calculate the distance (Cosine/Euclidean according to specific model) between h 2 + (t 1 − h 1 ) and w. Ordering all these words/phrases by their distances in ascend- ing order, we obtain the rank of the correct an- swer t 2 . Finally, we report Hits@10 (i.e., the pro- portion of correct answers whose ranks are not larger than 10) and accuracy (i.e., Hits@1). For word analogies and constructed analogies, we set K = 200, 000; while for phrase analogies, we set K = 1, 000, 000 to recall sufficient analogies.</p><p>Implementation.</p><p>For Skip-gram and the "Jointly" (anchor/name/anchor+name) model, we use the same implementations and optimal config- urations as the triplet classification experiment.</p><p>Results. Jointly embedding using Wikipedi- a anchors for alignment consistently outperforms Skip-gram <ref type="bibr">(Table 5, 6, 7)</ref> showing that the influ- ence of knowledge embedding, injected into word embedding through Wikipedia anchors, is benefi- cial. The vector of an ambiguous word is often a mixture of its several meanings but, in a specific context, the word is disambiguated and refers to a specific meaning. Using global word embedding to predict words within a specific context may pol- lute the embeddings of surrounding words. Align- ment by anchors enables entity embeddings to al- leviate the propagation of ambiguities and thus im- proves the quality of word embeddings.</p><p>Using entity names for alignment hurts the per- formance of analogies of words and phrases (Ta- ble 5, 6). The main reason is that these analo- gies are popular facts frequently mentioned in tex- t while a name graph forces word embeddings to satisfy both popular and rare facts. Another rea- son stems from the versatility of mentioning an entity. Consider "(Japan, yen, Europe, euro)" for example. Knowledge embedding is supposed to give significant help to completing this analogy as "/location/country/currency"∈ R. However, the entity of Japanese currency is named "Japanese yen" rather than "yen" and thus the explicit trans- lation learnt from knowledge embedding is not di- rectly imposed on the word embedding of "yen". In contrast, using entity names for alignment im- proves the performances on constructed analogies <ref type="table" target="#tab_5">(Table 7)</ref>. Since there is a relation r ∈ R for each constructed analogy (w h 1 , w t 1 , w h 2 , w t 2 ), al- though neither (w h 1 , r, w t 1 ) nor (w h 2 , r, w t 2 ) is present in the name graph, other facts involving these words act on the vectors of these words, in the same manner of traditional knowledge embed- ding.</p><p>Overall, any high-quality entity linking system can be used to further improve the performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Improving Relation Extraction: PR curves of Mintz alone or combined with knowledge (pTransE) / jointly model over (a) e − e, (b) w − e, (c) e − w, (d) w − w, and (e) all triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Improving Relation Extraction: PR curves of Sm2r alone or combined with knowledge (TransE) / jointly model over (a) e − e, (b) w − e, (c) e − w, (d) w − w, and (e) all triplets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Data: triplets used in our experiments.</head><label>1</label><figDesc></figDesc><table>#R 
#E 
#Triplet (Train/Valid/Test) 

4,490 43,793,608 123,062,855 40,528,963 40,528,963 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Triplet Classification: comparison be- tween TransE and pTransE over e − e triplets. Method Accuracy (%) Area under PR curve</head><label>3</label><figDesc></figDesc><table>TransE 
93.1 
0.86 
pTransE 
93.4 
0.97 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Triplet classification: accuracy (%) over various types of triplets.</head><label>4</label><figDesc></figDesc><table>Type 
e − e 
w − e e − w w − w 
all 

respectively 
93.4 
52.1 
51.4 
71.0 
77.5 
jointly (anchor) 
94.4 
67.0 
66.7 
79.8 
81.9 
jointly (name) 
94.5 
80.5 
80.0 
89.0 
87.7 
jointly (anchor+name) 
95.0 
82.0 
81.5 
90.0 
88.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 6 : Phrases Analogical Reasoning Task.</head><label>6</label><figDesc></figDesc><table>Method 
Accuracy (%) Hits@10 (%) 

Skip-gram 
18.0 
56.1 
Jointly (anchor) 
27.6 
65.0 
Jointly (name) 
11.3 
40.6 
Jointly (anchor+name) 
18.3 
54.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Constructed Analogical Reasoning 
Task. 

Method 
Accuracy (%) Hits@10 (%) 

Skip-gram 
10.5 
14.1 
Jointly (anchor) 
10.5 
14.3 
Jointly (name) 
11.5 
16.2 
Jointly (anchor+name) 
11.6 
16.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 : Words Analogical Reasoning Task.</head><label>5</label><figDesc></figDesc><table>Method 
Accuracy (%) 
Hits@10 (%) 
Semantic Syntactic Total 
Semantic Syntactic Total 

Skip-gram 
71.4 
69.0 
70.0 
90.4 
89.3 
89.8 
Jointly (anchor) 
75.3 
68.3 
71.2 
91.5 
88.9 
89.9 
Jointly (name) 
54.5 
54.2 
59.0 
75.8 
86.5 
82.1 
Jointly (anchor+name) 
56.5 
65.7 
61.9 
78.1 
87.6 
83.6 

</table></figure>

			<note place="foot" n="1"> We do not distinguish between &quot;words&quot; and &quot;phrases&quot;, i.e., &quot;words&quot; means &quot;words/phrases&quot;. 2 Related Work</note>

			<note place="foot" n="2"> https://opennlp.apache.org</note>

			<note place="foot" n="4"> http://nlp.stanford.edu/software/ mimlre.shtml</note>

			<note place="foot" n="5"> code.google.com/p/word2vec/source/ browse/trunk/questions-words.txt 6 code.google.com/p/word2vec/source/ browse/trunk/questions-phrases.txt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we introduced a novel method of jointly embedding knowledge graphs and a text corpus so that entities and words/phrases are rep-resented in the same vector space. In such a way, our method can perform prediction on any can-didate facts between entities/words/phrases, going beyond previous knowledge embedding methods, which can only predict facts whose entities exist in knowledge graph. Extensive, large-scale exper-iments show that the proposed method is very ef-fective at reasoning new facts. In addition, we also provides insights into word embedding, especially on the capability of analogical reasoning. In this aspect, we empirically observed some hints that jointly embedding also helps word embedding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="301" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-relational latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1602" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th</title>
		<meeting>the 49th</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the TwentyEighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1366" to="1371" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards accurate distant supervision for relational facts extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="810" to="815" />
		</imprint>
	</monogr>
	<note>Sofia</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
