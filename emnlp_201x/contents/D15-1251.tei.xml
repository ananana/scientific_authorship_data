<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Optimization of Text Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Optimization of Text Representations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>When applying machine learning to problems in NLP, there are many choices to make about how to represent input texts. They can have a big effect on performance , but they are often uninteresting to researchers or practitioners who simply need a module that performs well. We apply sequential model-based optimization over this space of choices and show that it makes standard linear models competitive with more sophisticated, expensive state-of-the-art methods based on latent variables or neural networks on various topic classification and sentiment analysis problems. Our approach is a first step towards black-box NLP systems that work with raw text and do not require manual tuning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>NLP researchers and practitioners spend a consid- erable amount of time comparing machine-learned models of text that differ in relatively uninteresting ways. For example, in categorizing texts, should the "bag of words" include bigrams, and is tf-idf weighting a good idea? In learning word embed- dings, distributional similarity approaches have been shown to perform competitively with neural network models when the hyperparameters (e.g., context window, subsampling rate, smoothing con- stant) are carefully tuned ( <ref type="bibr" target="#b13">Levy et al., 2015</ref>). These choices matter experimentally, often leading to big differences in performance, with little consistency across tasks and datasets in which combination of choices works best. Unfortunately, these differ- ences tell us little about language or the problems that machine learners are supposed to solve.</p><p>We propose that these decisions can be auto- mated in a similar way to hyperparameter selection (e.g., choosing the strength of a ridge or lasso regu- larizer). Given a particular text dataset and classi- fication task, we show a technique for optimizing over the space of representational choices, along with other "nuisances" that interact with these de- cisions, like hyperparameter selection. For exam- ple, using higher-order n-grams means more fea- tures and a need for stronger regularization and more training iterations. Generally, these decisions about instance representation are made by humans, heuristically; our work seeks to automate them, not unlike <ref type="bibr" target="#b4">Daelemans et al. (2003)</ref>, who proposed to use genetic algorithms to optimize representational choices.</p><p>Our technique instantiates sequential model- based optimization <ref type="bibr">(SMBO;</ref><ref type="bibr" target="#b7">Hutter et al., 2011</ref>). SMBO and other Bayesian optimization ap- proaches have been shown to work well for hyper- parameter tuning <ref type="bibr" target="#b2">(Bergstra et al., 2011;</ref><ref type="bibr" target="#b6">Hoffman et al., 2011;</ref><ref type="bibr" target="#b18">Snoek et al., 2012)</ref>. Though popular in computer vision ( <ref type="bibr" target="#b3">Bergstra et al., 2013</ref>), these techniques have received little attention in NLP.</p><p>We apply it to logistic regression on a range of topic and sentiment classification tasks. Consis- tently, our method finds representational choices that perform better than linear baselines previously reported in the literature, and that, in some cases, are competitive with more sophisticated non-linear models trained using neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation and Notation</head><p>Let the training data consist of a collection of pairs</p><formula xml:id="formula_0">d train = d.i 1 , d.o 1 , . . . , d.i n , d</formula><p>.o n , where each input d.i ∈ I is a text document and each output d.o ∈ O, the output space. The overall training goal is to maximize a performance func- tion f (e.g., classification accuracy, log-likelihood,</p><formula xml:id="formula_1">F 1 score, etc.) of a machine-learned model, on a held-out dataset, d dev ∈ (I × O) n .</formula><p>Classification proceeds in three steps: first, x : I → R N maps each input to a vector rep- resentation. Second, a predictive model (typi- cally, its parameters) is learned from the inputs (now transformed into vectors) and outputs: L :</p><formula xml:id="formula_2">(R N × O) n → (R N → O). Finally, the resulting classifier c : I → O is fixed as L(d train ) • x (i.</formula><p>e., the composition of the representation function with the learned mapping).</p><p>Here we consider linear classifiers of the form c(d.i) = arg max o∈O w o x(d.i), where the param- eters w o ∈ R N , for each output o, are learned using logistic regression on the training data. We let w denote the concatenation of all w o . Hence the parameters can be understood as a function of the training data and the representation function x. The performance function f , in turn, is a func- tion of the held-out data d dev and x-also w and d train , through x. For simplicity, we will write "f (x)" when the rest are clear from context.</p><p>Typically, x is fixed by the model designer, per- haps after some experimentation, and learning fo- cuses on selecting the parameters w. For logistic regression and many other linear models, this train- ing step reduces to convex optimization in N |O| dimensions-a solvable problem that is costly for large datasets and/or large output spaces. In seek- ing to maximize f with respect to x, we do not wish to carry out training any more times than necessary.</p><p>Choosing x can be understood as a problem of selecting hyperparameter values. We therefore turn to Bayesian optimization, a family of techniques that can be used to select hyperparameter values intelligently when solving for parameters (w) is costly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bayesian Optimization</head><p>Our approach is based on sequential model-based optimization (SMBO; <ref type="bibr" target="#b7">Hutter et al., 2011</ref>). It itera- tively chooses representation functions x. On each round, it makes this choice through a probabilistic model of f , then evaluates f -we call this a "trial." As in any iterative search algorithm, the goal is to balance exploration of options for x with exploita- tion of previously-explored options, so that a good choice is found in a small number of trials.</p><p>More concretely, in the tth trial, x t is selected using an acquisition function A and a "surrogate" probabilistic model p t . Second, f is evaluated given x t -an expensive operation which involves training to learn parameters w and assessing per- formance on the held-out data. Third, the surrogate model is updated. See Algorithm 1; details on A and p t follow.</p><p>Acquisition Function. A good acquisition func- tion returns high values for x when either the value f (x) is predicted to be high, or the uncertainty about f (x)'s value is high; balancing between these is the classic tradeoff between exploitation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 SMBO algorithm</head><p>Input: number of trials T , target function f p 1 = initial surrogate model Initialize y * for t = 1 to T do</p><formula xml:id="formula_3">x t ← arg max x A(x; p t , y * ) y t ← evaluate f (x t )</formula><p>Update y * Estimate p t given x 1:t and y 1:t end for and exploration. We use a criterion called Expected Improvement (EI; Jones, 2001), 1 which is the ex- pectation (under the current surrogate model p t ) that f (x) = y will exceed f (x * ) = y * : Surrogate Model. As a surrogate model, we use a tree-structured Parzen estimator (TPE; Bergstra et al., 2011). 2 This is a nonparametric approach to density estimation. We seek to estimate p t (y | x) where y = f (x), the performance function that is expensive to compute exactly. The TPE approach</p><formula xml:id="formula_4">A(x; p t , y * ) = ∞ −∞ max(y − y * , 0)p t (y | x)dy</formula><formula xml:id="formula_5">seeks p t (y | x) ∝ p t (y) · p &lt; t (x), if y&lt;y * p ≥ t (x), if y≥y *</formula><p>, where p &lt; t and p ≥ t are densities estimated using observa- tions from previous trials that are less than and greater than y * , respectively. In TPE, y * is defined as some quantile of the observed y from previous trials; we use 15-quantiles.</p><p>As shown by <ref type="bibr" target="#b2">Bergstra et al. (2011)</ref>, the Ex- pected Improvement in TPE can be written as:</p><formula xml:id="formula_6">Hyperparameter Values nmin {1, 2, 3} nmax</formula><p>{nmin , . . . , 3} weighting scheme {tf, tf-idf, binary} remove stop words?</p><p>{True, False} regularization {1, 2} regularization strength [10 −5 , 10 5 ] convergence tolerance [10 −5 , 10 −3 ] <ref type="table">Table 1</ref>: The set of hyperparameters considered in our ex- periments. The top half are hyperparameters related to text representation, while the bottom half are logistic regression hyperparameters, which also interact with the chosen repre- sentation.</p><formula xml:id="formula_7">A(x; p t , y * ) ∝ γ + p &lt; t (x) p ≥ t (x) (1 − γ) −1</formula><p>, where γ = p t (y &lt; y * ), fixed at 0.15 by definition of y * (above). Here, we prefer x with high probability under p ≥ t (x) and low probability under p &lt; t (x). To maximize this quantity, we draw many candidates according to p ≥ t (x) and evaluate them according to p &lt; t (x)/p ≥ t (x). Note that p(y) does not need to be given an explicit form. To compute p &lt; t (x) and p ≥ t (x), we associate each hyperparameter with a node in the graphical model and multiply individ- ual probabilities at every node-see Bergstra et al. (2011) for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We fix L to logistic regression. We optimize text representation based on the types of n-grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation. See <ref type="table">Table 1</ref> for a complete list.</p><p>Note that even with this limited number of options, the number of possible combinations is huge, 3 so exhaustive search is computationally ex- pensive. In all our experiments for all datasets, we limit ourselves to 30 trials per dataset. The only preprocessing we applied was downcasing.</p><p>We always use a development set to evaluate f (x) during learning and report the final result on an unseen test set. We summarize the hyperparam- eters selected by our method, and the accuracies achieved (on test data) in <ref type="table">Table 5</ref>. We discuss com- parisons to baselines for each dataset in turn. For each of our datasets, we select supervised, non- ensemble classification methods from previous lit- erature as baselines. In each case, we emphasize comparisons with the best-published linear method Stanford sentiment treebank (Socher et al., 2013)- <ref type="table" target="#tab_1">Table 2</ref>. A sentence-level sentiment analysis dataset of rottentomatoes.com movie re- views: http://nlp.stanford.edu/sentiment. We use the binary classification task where the goal is to pre- dict whether a review is positive or negative (no neutral). Our logistic regression model outperforms the baseline SVM reported by <ref type="bibr" target="#b21">Socher et al. (2013)</ref>, who used only unigrams but did not specify the weighting scheme for their SVM baseline. While our result is still below the state-of-the-art based on the the recursive neural tensor networks <ref type="bibr" target="#b21">(Socher et al., 2013</ref>) and the paragraph vector ( <ref type="bibr" target="#b12">Le and Mikolov, 2014</ref>), we show that logistic regression is comparable with recursive and matrix-vector neu- ral networks <ref type="bibr" target="#b19">(Socher et al., 2011;</ref><ref type="bibr" target="#b20">Socher et al., 2012</ref>   <ref type="bibr">4</ref> Our method is on par with the second- best of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines. They varied the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency. Our method achieved the best perfor- mance with binary weighting, which they did not consider.</p><p>IMDB movie reviews (Maas et al., 2011)- <ref type="table" target="#tab_3">Table 3</ref>. A binary sentiment analysis dataset of highly polar IMDB movie reviews:</p><p>http://ai.stanford.edu/~amaas/data/sentiment.</p><p>The results parallel those for Amazon electronics; our method comes close to convolutional neural networks <ref type="bibr" target="#b8">(Johnson and Zhang, 2015)</ref>, which are state-of-the-art. <ref type="bibr">5</ref> It outperforms SVMs and feed-forward neural networks, the restricted Boltzmann machine approach presented by <ref type="bibr" target="#b5">Dahl et al. (2012)</ref>, and compressive feature learning ( <ref type="bibr" target="#b16">Paskov et al., 2013</ref>   <ref type="bibr" target="#b0">Bansal et al., 2008;</ref><ref type="bibr" target="#b28">Yessenalina et al., 2010</ref>), we consider the task to predict the vote ("yea" or "nay") for the speaker of each speech segment (speaker-based speech-segment classification).</p><p>Our method outperforms the best results of <ref type="bibr" target="#b28">Yessenalina et al. (2010)</ref>, which use a multi-level structured model based on a latent-variable SVM. We show comparisons to two weaker baselines as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. SVM-link 71.28 Min-cut 75.00 SVM-SLE 77.67 LR (this work) 78.59 <ref type="table">Table 4</ref>: Comparisons on the congress vote dataset. SVM- link exploits link structures <ref type="figure">(Thomas et al., 2006)</ref>; the min-cut result is from <ref type="bibr" target="#b0">Bansal et al. (2008)</ref>; and SVM-SLE result is reported by <ref type="bibr" target="#b28">Yessenalina et al. (2010)</ref>. Test size = 1, 175.</p><p>20 Newsgroups (Lang, 1995) all topics- <ref type="table">Table 6</ref>. 20 Newsgroups is a benchmark topic classifica- tion dataset: http://qwone.com/~jason/20Newsgroups. There are 20 topics in this dataset. Our method outperforms state-of-the-art methods including the distributed structured output model <ref type="bibr" target="#b22">(Srikumar and Manning, 2014)</ref>. <ref type="bibr">7</ref> The strong logistic regression baseline from <ref type="bibr" target="#b16">Paskov et al. (2013)</ref> uses all 5-grams, heuristic normalization, and elastic net regulariza- tion; our method found that unigrams and bigrams, with binary weighting and 2 penalty, achieved far better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Discriminative RBM 76.20 LR-{1, 2, 3, 4, 5}-grams 82.80 Compressive feature learning 83.00 Distributed structured output 84.00 LR (this work) 87.84 <ref type="table">Table 6</ref>: Comparisons on the 20 Newsgroups dataset for classifying documents into all topics. The disriminative RBM result is from <ref type="bibr" target="#b11">Larochelle and Bengio (2008)</ref>; compressive feature learning and LR-5-grams results are from <ref type="bibr" target="#b16">Paskov et al. (2013)</ref>, and the distributed structured output result is from <ref type="bibr" target="#b22">Srikumar and Manning (2014)</ref>. Test size = 9, 052.  <ref type="table">Table 5</ref>: Classification accuracies and the best hyperparameters for each of the datasets in our experiments. "Acc" shows accuracies for our logistic regression model. "Min" and "Max" correspond to the min n-grams and max n-grams respectively. "Reg." is the regularization type, "Strength" is the regularization strength, and "Conv." is the convergence tolerance. For regularization strength, we round it to the nearest integer for readability.</p><p>method achieves 86.3% and 92.1% using slightly different representations (see <ref type="table">Table 5</ref>). The last task is to classify related science documents into four science topics (sci.crypt, sci.electronics, sci.space, sci.med; test size = 1, 899). We were not able to find previous results that are comparable to ours on this task; we include our result (95.82%) to enable further comparisons in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Optimized representations. For each task, the chosen representation is different. Out of all possi- ble choices in our experiments <ref type="table">(Table 1)</ref>, each of them is used by at least one of the datsets <ref type="table">(Table 5)</ref>. For example, on the Congress vote dataset, we only need to use bigrams, whereas on the Amazon elec- tronics dataset we need to use {1, 2, 3}-grams. The binary weighting scheme works well for most of the datasets, except the sentence-level sentiment analysis task, where the tf-idf weighting scheme was selected. 2 regularization was best in all cases but one. We do not believe that an NLP expert would be likely to make these particular choices, except through the same kind of trial-and-error pro- cess our method automates efficiently.</p><p>Number of trials. We ran 30 trials for each dataset in our experiments. <ref type="figure">Figure 1</ref> shows each trial accuracy and the best accuracy on develop- ment data as we increase the number of trials for two datasets. We can see that 30 trials are gener- ally enough for the model to obtain good results, although the search space is large.</p><p>Transfer learning and multitask setting. We treat each dataset independently and create a sep- arate model for each of them. It is also possible to learn from previous datasets (i.e., transfer learn- ing) or to learn from all datasets simultaneously (i.e., multitask learning) to improve performance. This has the potential to reduce the number of trials Figure 1: Classification accuracies on development data for Stanford sentiment treebank (left) and congressional vote (right) datasets. In each plot, the green solid line indicates the best accuracy found so far, while the dotted orange line shows accuracy at each trial. We can see that in general the model is able to obtain reasonably good representation in 30 trials. required even further. See <ref type="bibr" target="#b1">Bardenet et al. (2013)</ref>, <ref type="bibr" target="#b24">Swersky et al. (2013)</ref>, and <ref type="bibr" target="#b29">Yogatama and Mann (2014)</ref> for more about how to perform Bayesian optimization in these settings.</p><p>Beyond supervised learning. Our framework could also be extended to unsupervised and semi- supervised models. For example, in document clus- tering (e.g., k-means), we also need to construct representations for documents. Log-likelihood might serve as a performance function. A range of random initializations might be considered. Inves- tigation of this approach for nonconvex problems is an exciting area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We used Bayesian optimization to optimize choices about text representations for various categoriza- tion problems. Our technique identifies settings for a standard linear model (logistic regression) that are competitive with far more sophisticated meth- ods on topic classification and sentiment analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where x * is chosen depending on the surrogate model, discussed below. (For now, think of it as a strongly-performing "benchmark" discovered in earlier iterations.) Other options for the acquisition function include maximum probability of improve- ment (Jones, 2001), minimum conditional entropy (Villemonteix et al., 2009), Gaussian process up- per confidence bound (Srinivas et al., 2010), or a combination of them (Hoffman et al., 2011).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Stanford sentiment trial accuracy 0 5 10 15 20 25 30 50 60 70 80 Congress vote trial accuracy 0 5 10 15 20 25 30 40 50 60 70 80</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Method 
Acc. 
Na¨ıveNa¨ıve Bayes 
81.8 
SVM 
79.4 
Vector average 
80.1 
Recursive neural networks 
82.4 
LR (this work) 
82.4 
Matrix-vector RNN 
82.9 
Recursive neural tensor networks 85.4 
Paragraph vector 
87.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparisons on the Stanford sentiment treebank 
dataset. Scores are as reported by Socher et al. (2013) and Le 
and Mikolov (2014). Test size = 6, 920. 

Amazon electronics (McAuley and Leskovec, 
2013)-Table 3. A binary sentiment analy-
sis dataset of Amazon electronics product re-
views: http://riejohnson.com/cnn data.html. The best-
performing methods on this dataset are based on 
convolutional neural networks (Johnson and Zhang, 
2015). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparisons on the Amazon electronics and IMDB 
reviews datasets. SVM results are from Wang and Manning 
(2012), the RBM (restricted Bolzmann machine) result is from 
Dahl et al. (2012), NN and CNN results are from Johnson 
and Zhang (2015), and LR-{1, 2, 3, 4, 5}-grams and compres-
sive feature learning results are from Paskov et al. (2013). 
Test size = 20, 000 for both datasets. 

Congressional 
vote 
(Thomas 
et 
al., 
2006)-Table 4. A dataset of transcripts 
from the U.S. Congressional debates: 
http://www.cs.cornell.edu/~ainur/sle-data.html. Similar 
to previous work (Thomas et al., 2006; </table></figure>

			<note place="foot" n="1"> EI is the most widely used acquisition function that has been shown to work well on a range of tasks. 2 Another common approach to the surrogate is the Gaussian process (Rasmussen and Williams, 2006; Hoffman et al., 2011; Snoek et al., 2012). Like Bergstra et al. (2011), our preliminary experiments found the TPE to perform favorably. Further TPE&apos;s tree-structured configuration space is advantageous, because it allows nested definitions of hyperparameters, which we exploit in our experiments (e.g., only allows bigrams to be chosen if unigrams are also chosen).</note>

			<note place="foot" n="3"> It is actually infinite since the reg. strength and conv. tolerance are continuous values, but we could discretize them. (often an SVM with a linear kernel with representation selected by experts) and the best-published method overall. In the following, &quot;SVM&quot; always means &quot;linear SVM.&quot; All methods were trained and evaluated on the same training/testing splits as baselines; in cases where standard development sets were not available, we used a random 20% of the training data as a development set.</note>

			<note place="foot" n="4"> These are convolutional neural networks with a rectifier activation function, trained under 2 regularization with stochastic gradient descent. The authors also consider an extension based on parallel CNN that we do not include here.</note>

			<note place="foot" n="5"> As noted, semi-supervised and ensemble methods are excluded for a fair comparison. 6 This approach is based on minimum description length, using unlabeled data to select a set of higher-order n-grams to use as features.</note>

			<note place="foot" n="7"> This method was designed for structured prediction, but Srikumar and Manning (2014) also applied it to classification. It attempts to learn a distributed representation for features and for labels. The authors used unigrams and did not discuss the weighting scheme. 8 They also report a na¨ıvena¨ıve Bayes/SVM ensemble achieving 87.9% and 91.2%.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank several reviewers for their helpful feedback. This work was supported by the Defense Advanced Research Projects Agency through grant FA87501420244 and com-puting resources provided by Amazon. This research was completed while NAS was at CMU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The power of negative thinking: Exploiting label disagreement in the min-cut classification framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clair</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collaborative hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matyas</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balazs</forename><surname>Kegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Bardenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Yoshua Bengio, and Balazs Kegl</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combined optimization of feature selection and algorithm parameters in machine learning of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Naudts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML</title>
		<meeting>of ECML</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training restricted Boltzmann machines on word observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Portfolio allocation for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LION</title>
		<meeting>of LION</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A taxonomy of global optimization methods based on response surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="345" to="385" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification using discriminative restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys</title>
		<meeting>of RecSys</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Paskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larrochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Sham Kakade, and Matthias Seeger</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Get out the vote: Determining support or opposition from congressional floor-debate transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An informational approach to the global optimization of expensive-to-evaluate functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Villemonteix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="534" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-level structured models for document sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient transfer learning method for automatic hyperparameter tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
