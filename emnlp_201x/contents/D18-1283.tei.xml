<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Commonsense Justification for Action Explanation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sari</forename><surname>Saba-Sadiya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Commonsense Justification for Action Explanation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2627" to="2637"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2627</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the gen-erative Conditional Variational Autoencoder (CVAE) that models object relations/attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and justification. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To make AI more accessible, transparent, and trustworthy, recent years have seen an increas- ing effort on Explainable AI (XAI) which devel- ops explainable models that attempts to explain the agent's decision making behaviors while main- taining a high-level of performance. Two types of explanation have been explored by the research community: introspective explanation which ad- dresses the process of decision making (e.g., how a decision is made) and justification explanation which gathers evidence to support a certain de- cision ( <ref type="bibr" target="#b19">Park et al., 2018;</ref><ref type="bibr" target="#b1">Biran and McKeown, 2017)</ref>. In this paper we focus on justification ex- planation, particularly identifying commonsense evidence to justify the prediction of an action. The key question we are addressing is: when an AI agent makes a prediction about an action in the world, how can the system justify its prediction that makes sense to the human?</p><p>Humans have tremendous commonsense knowledge about actions in the world (e.g., key constituents of an action) which allows them to quickly recognize and infer actions in the environment from millions of available features <ref type="bibr" target="#b21">(Rensink, 2000)</ref>. As a first step in our investigation, we initiated a human study to observe the kind of commonsense reasoning used by humans to justify the prediction of an action. From this study, we identified several dimensions of commonsense evidence which is commonly used to explain an action. Motivated by this study, we frame our task as follows: given all the symbolic descriptions of the perceived physical world (e.g., object relations and attributes as a result of vision or other processing), the goal is to identify a small set of descriptions which can justify an action prediction in line with humans' commonsense knowledge about that action. The lack of commonsense knowledge is a major bottleneck in artificial agents which jeopardizes the common ground between humans and agents for successful communication. If artificial agents ever become partners with humans in joint tasks, the ability to learn and acquire commonsense evidence for action justification is crucial.</p><p>To address this problem, we developed an ap- proach based on the generative Conditional Vari- ational Autoencoder (CVAE). This approach mod- els the perceived attributes/relations as latent vari- ables and jointly learns a performer which pre- dicts actions based on attributes/relations and a explainer which selects a subset of at- tributes/relations as commonsense evidence to jus- tify the action prediction. Our empirical results on a subset of the Visual Genome data ( <ref type="bibr" target="#b15">Krishna et al., 2016)</ref> have shown that, compared to a typ- ical attention-based model, CVAE has a signifi- cantly higher explanation ability in terms of iden- tifying correct commonsense evidence to justify the predicted action. When adding the supervision of commonsense evidence during training, both the explainability and the performance (i.e., action prediction) are further improved.</p><p>As commonsense evidence is intuitive to hu- mans, the agent's ability to select the right kind of commonsense evidence will allow the human and the agent to come to a common understanding of actions and their justifications, in other words, common ground. To evaluate the role of common- sense evidence in facilitating common ground, we conducted additional human subject studies. In these experiments, the agent is given a set of im- ages and applies our models to predict actions and select commonsense evidence to justify the prediction. For each image, the agent communi- cates the selected commonsense evidence to the human. The human, who does not have access to the original image, makes a guess on the action only based on the communicated evidence. The agreement between the action guessed by the hu- man and the action predicted by the agent is used to measure how well the selected commonsense evidence serves to bring the human and the agent to a common ground of perceived actions. Our ex- perimental results have shown that the common- sense evidence selected by CVAE leads to a signif- icantly higher common ground.</p><p>The contributions of this paper are three folds. First we identified several key dimensions of com- monsense evidence, from a human's perspective, to justify concrete actions in the physical environ- ment. These dimensions provide a basis for jus- tification explanation that is aligned with human's commonsense knowledge about the action. Sec- ond we proposed a method using CVAE to jointly learn to predict actions and select commonsense evidence as action justification. CVAE naturally models the generation process of both actions and commonsense evidence. Inferring commonsense evidence is equivalent to the posterior inference of the CVAE model, which is flexible and powerful by incorporating actions as context. Our experi- mental results have shown a higher explainability of CVAE in action justification without sacrificing performance. Finally our dataset of commonsense evidence for action explanation is available to the community 1 . It can serve as a benchmark for fu- ture work on this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Advanced machine learning such as deep learning approaches have shown effectiveness in many ap- plications, however, they often lack transparency and interpretability. This makes it difficult for hu- mans to understand the agent's capabilities and limitations. To address this problem, there is a growing interest in Explainable AI. For example, previous work has applied high-precision rules to explain classifiers' decisions ( <ref type="bibr" target="#b22">Ribeiro et al., 2016</ref><ref type="bibr" target="#b23">Ribeiro et al., , 2018</ref>. For Convolutional Neural Networks (CNNs), recent work attempts to explain model behaviors by mining semantic meanings of fil- ters ( <ref type="bibr">Zhang et al., 2017a,b)</ref> or by generating lan- guage explanations <ref type="bibr" target="#b10">(Hendricks et al., 2016;</ref><ref type="bibr" target="#b19">Park et al., 2018</ref>). An increasing amount of work on the Visual Question Answering (VQA) task <ref type="bibr" target="#b0">(Antol et al., 2015;</ref><ref type="bibr" target="#b18">Lu et al., 2016</ref>) has also looked into more interpretable approaches, for exam- ple, by utilizing attention-based models <ref type="bibr" target="#b7">(Fukui et al., 2016)</ref> or reasoning based on explicit evi- dence ( <ref type="bibr" target="#b27">Wang et al., 2017)</ref>.</p><p>Specifically for action understanding, recent work explicitly models commonsense knowledge including causal relations ( <ref type="bibr" target="#b8">Gao et al., 2016;</ref><ref type="bibr" target="#b6">Forbes and Choi, 2017;</ref><ref type="bibr" target="#b31">Zellers and Choi, 2017;</ref><ref type="bibr" target="#b9">Gao et al., 2018</ref>) related to concrete actions, which can facilitate action explanation. Commonsense knowledge can be acquired from image annota- tions ( <ref type="bibr" target="#b30">Yatskar et al., 2016)</ref> or learned from vi- sual abstraction <ref type="bibr" target="#b26">(Vedantam et al., 2015)</ref>. Differ- ent from the above work, our work here focuses on learning to acquire commonsense evidence for action justification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Study on Justification Explanation</head><p>While there is a rich literature on explanations in Psychology, Philosophy, and Linguistics, partic- ularly for higher-level events and decision mak- ing <ref type="bibr" target="#b25">(Thagard, 2000;</ref><ref type="bibr" target="#b17">Lombrozo, 2012;</ref><ref type="bibr" target="#b5">Dennett, 1987)</ref>, explanations for recognition of lower-level concrete physical actions (e.g., drink, brush, cook, etc.) occurred in our daily life are rarely studied. One possible reason is that we humans are so intu- itive in recognizing these actions, which are often taken for granted without the need for any further explanation. However, despite recent advances, the ability to recognize and understand actions in the real world is extremely challenging for arti- ficial agents. Thus it becomes important for the agent to have an ability to explain and justify its action prediction. What can be used to justify an action prediction, and more importantly, in a hu- man understandable way? To address this ques- tion, we initiated a human study to examine what kind of evidence humans would gather in justify- ing their recognition of an action perceived from the physical world.</p><p>More specifically, we selected a set of 12 short video clips (each about 14 seconds) from the Mi- crosoft Research Video to Text dataset ( <ref type="bibr" target="#b28">Xu et al., 2016)</ref>. For each video clip, we asked human sub- jects to explain why they think a certain action is happening in the video. The answers were col- lected via an online interface. A total of about 140 responses from 67 Michigan State University en- gineering students were collected. From the data, we identified the following categories of evidence commonly used by the subjects in their justifica- tions. Most responses contain multiple categories of explanation.</p><p>• Transitive-Relation. This kind of expla- nation does not directly focus on the struc- tural relations between an action and its par- ticipants, but rather transits to the relation between the participant of the action and other related evidence. For example, using a woman wears an apron to justify the cook ac- tion. In the collected responses, 64% of them used transitive relations.</p><p>• Subaction-Relation.</p><p>Lower-level sub- actions are used to justify a higher-level ac- tion. For example, the action is cook because there are sub-actions like cutting and heating meat. Almost 75% of the responses used sub- actions.</p><p>• Spatial-Relation. Spatial relations between the participants of the action play an impor- tant role. For example, the knife is on the cutting board is used to explain cooking; and the water is in the bottle to explain drinking. Around 15% of responses are in this category.</p><p>• Effect-Attribute. A change in the state of an object, in other words the effect state af- ter the action, is often used as evidence. For example, cucumber in small pieces is used as the evidence for chop. Over 28% of the re- sponses are in this category.</p><p>• Associated-Attribute. Other attributes asso- ciated with the participants of the action, but not the effect state of the participants as a result of the action (20%). While these at- tributes are not directly related to the action, they are linked to the action by association. For example, banana is sliced is used as evi- dence to justify blend.</p><p>• Other. Participants have also cited other commonsense such as the "definition" of the action (5%), or the manner associated with different sub actions(12%).</p><p>Most of the above categories can be potentially perceived and represented through symbolic de- scriptions such as logic predicates to capture ob- ject attributes and relations between objects. This study has motivated us to collect additional data (Section 4) and formulate the task of common- sense justification as described in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Collection</head><p>Motivated by the human study described above, we created a dataset based on the Visual Genome (VG) data ( <ref type="bibr" target="#b15">Krishna et al., 2016</ref>) for our investiga- tion. Each image in the VG dataset is annotated with bounding boxes, attributes of the bounding boxes, and relations between the bounding boxes. The available annotation provides an ideal setup for us to focus on commonsense justification. In this work, we are interested in the concrete physical actions that involve physical objects that can be perceived. We selected ten frequently oc- curred concrete actions: feed, pull, ride, drink, chop, brush, fry, bake, blend, eat and manually identified a set of images from the VG dataset de- picting these actions. This has led to a dataset of 853 images with annotated ground-truth actions.</p><p>We conducted a crowd-source study to collect responses from the crowd in terms of common-  sense evidence for action justification. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, for each image, we showed to the crowd (through Amazon Mechanical Turk) the im- age itself, the ground-truth action, and a list of relations/attributes. The workers were instructed to select the relations/attributes that were deemed to justify the corresponding action. We randomly assigned three workers to each image. The rela- tions or attributes that were selected by the major- ity (two or more) workers were considered gold commonsense evidence for action justification. <ref type="table" target="#tab_0">Table 1</ref> shows the average number of rela- tions/attributes available (i.e., Rel# and Att#) for the corresponding images for each verb. It also shows the number of relations/attributes se- lected by the workers as commonsense evidence (i.e., Gold Rel# and Gold Att#). The average number of relations and attributes in each image for different actions varies slightly. However, only a small percentage of them are considered com- monsense evidence. What's interesting is that the percentage of attributes considered good evidence is significantly less than the percentage of the re- lations. The sparsity of gold relations/attributes shows that it's a challenging task to learn an ex- plainer for a target action.</p><p>We further inspected the selected gold com- monsense relations and attributes. As shown in <ref type="table" target="#tab_1">Table 2</ref>, they nicely fall into the categories of commonsense evidence discussed in Section 3. The ratios of Transitive-Relation are similar across different actions.</p><p>The ratios of Subaction-Relation and Spatial-Relation vary for different verbs. For instance, ride, bake, blend tend to be justified by spatial relations more often than sub-actions. In addition, feed, pull, ride are rarely justified by Effect-Attribute while chop is mainly explained by the effect state of its direct object. These results will provide insight for generating justification explanations for a variety of verbs in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Method</head><p>Before we formulate the problem, we will first give some formal definitions. The set of rela- tions R is defined as {r 1 , r 2 , ..., r m } where each r i is a tuple (r p i , r s i , r o i ) corresponding to the pred- icate, subject, and object; and the set of attributes E is represented as {e 1 , e 2 , ..., e n } where each e i is a tuple (e o i , e p i ) corresponding to the object and attribute. We introduce z as a discrete vector (z 1 , z 2 , ..., z m+n ) where z i ∈ {0, 1} represents the hidden explainable variable. z is interpreted as an evidence selector: z i = 1 means the cor- responding relation/attribute justifies the target ac- tion a. We define A as the vocabulary of target ac- tions. Based on all these definitions, our goal is to jointly select evidence z and predict target action a ∈ A. In other words, to learn the probability p(a, z|R, E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Conditional Variational Autoencoder</head><p>The varational autoencoder( VAE) <ref type="bibr" target="#b14">(Kingma and Welling, 2013</ref>) is proposed as a generative model to combine the power of both directed continuous or discrete graphical models and neural network with latent variables. The VAE models the gener- ative process of a random variable x as following: first the latent variable z is generated from a prior probability distribution p(z), then a data sample x is generated from a conditional probability dis- tribution p(x|z). The CVAE ( <ref type="bibr" target="#b34">Zhao et al., 2017</ref>) is a natural extension of VAE: Both the prior dis- tribution and conditional distribution now are con- ditioned on an additional context c: p(z|c) and p(x|z, c).</p><p>In our task, we decompose the inference prob- lem p(a, z|R, E) into two smaller problems. The first sub-problem is to infer p(a|R, E), which is a performer. The second problem is to in- fer p(z|a, R, E) which is an explainer. These two problems are closely coupled, hence we model them jointly. The probability distribution p(a|R, E) can be written as :</p><formula xml:id="formula_0">p(a|R, E) = z p θ (a|z, R, E)p(z|R, E)</formula><p>Directly optimizing this conditional probability is not feasible. Usually the Evidence Lower Bound (ELBO) ( <ref type="bibr" target="#b24">Sohn et al., 2015</ref>) is optimized, which can be derived as the following:</p><formula xml:id="formula_1">ELBO(a, R, E; θ, φ) = − KL(q φ (z|a, R, E)||p θ (z|R, E)) + E q φ (z|a,R,E) [log p θ (a|z, R, E)] ≤ log p(a|R, E)<label>(1)</label></formula><p>The first KL divergence term is to minimize the distance between the posterior distribution and the prior distribution. The second term is to maximize the expectation of the target action based on the posterior latent distribution.</p><p>In most previous work using VAE, there is no explicit meaning for the hidden representation z, thus it's hard for humans to interpret. For exam- ple, z is simply assumed as a Gaussian distribu- tion or a categorical distribution. In order to have a more explicit representation for the purpose of explanation, our latent discrete variable z is used to indicate whether the corresponding relation or attribute can be used for justifying the action.</p><p>The whole system architecture is shown in <ref type="figure" target="#fig_1">Fig- ure 2</ref>. From an image, we first extract a candidate relation set R and an attribute set E. Every rela- tion r and attribute e are embedded using a Gated Recurrent Neural Network ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>).</p><formula xml:id="formula_2">r emb = GRU([r p , r s , r o ]) e emb = GRU([e o , e p ])</formula><p>The action a is represented by a GloVe embed- ding (Pennington et al., 2014), followed by an- other non-linear layer:</p><formula xml:id="formula_3">a emb = ReLU(W i a glove + b i )</formula><p>where a glove ∈ R k is the pre-trained GloVe em- bedding. Then the latent variable z can be calcu- lated as:</p><formula xml:id="formula_4">q φ (z|a, R, E) = softmax(W z [U; a emb ] + b z )</formula><p>where U = [r emb 1 , ..., r emb m , e emb 1 , ..., e emb n ] and [U, a emb ] means the concatenation of U and a emb . and W z ∈ R 2×2k as we assume each z i belongs to one of the two classes {0, 1}.</p><p>The prior distribution can be calculated as:</p><formula xml:id="formula_5">p θ (z|R, E) = softmax(W z U + b z )</formula><p>The KL divergence between the prior random variable z prior from p θ (z|R, E) and the posterior random variable z posterior from q φ (z|a, R, E) is:</p><formula xml:id="formula_6">KL(zprior, zposterior) = −pi log pi p i − (1 − pi) log 1 − pi 1 − p i here z prior ∼ Bern (p i ), z posterior ∼ Bern p i .</formula><p>Another challenge is that z is a discrete vari- able which blocks the gradient and makes the end- to-end training infeasible. Gumbel-Softmax ( <ref type="bibr" target="#b11">Jang et al., 2016</ref>) is a re-parameterization trick to deal with the discrete variables in the neural network. We use this trick to sample discrete z. Then we do a weighted sum pooling between discretized z and U:</p><formula xml:id="formula_7">h z = ReLU( i z i * U i ) h = ReLU(W h h z + b h ) p θ (a|z, R, E) = softmax(Wh + b)</formula><p>During training, we also add a sparsity regulariza- tion on the latent variable z besides the ELBO. So our final training objective is</p><formula xml:id="formula_8">L CV AE = − ELBO(a, R, E; θ, φ) + β KL(q φ (z|a, R, E)|| Bern(0))<label>(2)</label></formula><p>During testing, we have two objectives. First we want to infer the target action a, which can be computed through sampling:</p><formula xml:id="formula_9">p(a|R, E) = z p θ (z|R, E)p θ (a|z, R, E) ≈ 1 S S s=1 p θ (a|z s , R, E)<label>(3)</label></formula><p>where z s ∼ p(z|R, E) and S is the number of samples. After obtaining the predicted ac- tionâtionˆtionâ, the posterior explanation is inferred as q φ (z|â, R, E). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Embedding</head><p>Attribute Embedding r1: (hold, hand, knife) …… rm: (on, knife, cutting-board) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Conditional Variational Autoencoder</head><p>with Supervision (CVAE+SV)</p><p>In this setting, we assume we have the supervision for the discrete latent variable z, which is more like a multi-task setting. We optimize both the action prediction loss and the evidence selection loss. The final loss function is defined as:</p><formula xml:id="formula_10">L SV = λL CV AE + (1 − λ)L evidence</formula><p>where</p><formula xml:id="formula_11">L evidence = − k (z k log p(ˆ z k )+(1−z k ) log(1−p(ˆ z k )))</formula><p>in which z k ∈ {0, 1} is the ground truth label, ˆ z k is the predicted label and λ is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation on Action Explanation</head><p>To evaluate our model, we randomly split our dataset (853 images) into 60% for training, 20% for validation, and 20% for test. For all the mod- els we use the Adam optimizer ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with a starting learning rate 1e-4. All other hyperparameters are tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baseline: Attention Model</head><p>We use an attention-based model as a baseline, which is similar to the model originally proposed for document classification . The architecture is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Different from the CVAE-based method, this model directly learns a context parameter instead of learning from the posterior action context. The attention is cal- culated as:</p><formula xml:id="formula_12">α i = exp(u T i v) j exp(u T i v)</formula><p>where v is the context parameter, and u i is the GRU embedding of the corresponding rela- tion/attribute. The learned attention weights are used for the selection of commonsense evidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Metrics and Comparison</head><p>Our evaluation compares the performance from the following models:</p><p>• Baseline. The attention model presented in Section 6.1.</p><p>• CVAE. The conditional variational autoen- coder model presented in Section 5.1.</p><p>• CVAE+SV. The CVAE model with supervi- sion as presented in Section 5.2.</p><p>• Upper Bound. We also calculate the upper bound of the CVAE model using the human annotated gold evidence.</p><p>For each of the above model, evaluate model performance on both action prediction (i.e., per- former) and action justification (i.e., explainer)</p><p>• Performer: Accuracy is used to measure the percentage of actions that are correctly pre- dicted by the model.</p><p>• Explainer: As discussed in Section 5, the binary random variable z is used to capture commonsense evidence. The probability of each z represents the model's belief that the corresponding evidence supports the action decision. As we hope to rank the gold ev- idence higher, the Mean Average Precision (MAP) metric is calculated for evaluating ev- idence selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation Results</head><p>The results are shown in <ref type="table" target="#tab_2">Table 3</ref>. Since the Upper Bound method directly uses the human annotated gold evidence, its MAP for selecting evidence is always 1.0. The CVAE model outperforms the attention- based model in both action prediction and evi- dence selection. This indicates that the CVAE model can incorporate a better guidance for evi- dence selection during the training process. One possible explanation is that the CVAE model in- corporates the target action as the context during learning instead of directly learning a context pa- rameter. Furthermore, after adding the evidence supervision, the CVAE+SV model gives even bet- ter performance in both action prediction and evi- dence selection. We notice that for the CVAE+SV model, its action prediction accuracy is approach- ing the upper bound 91.8%, however the evidence selection MAP is still far from the upper bound even with supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Semi-Supervised Learning</head><p>Although we have shown that adding supervision on the latent variable z improves the model perfor- mance, collecting this label information through human annotation is usually time consuming and expensive. In this section, we explore how semi- supervised learning can help to alleviate this diffi- culty. As a generative model, VAE has shown its advantage on semi-supervised learning ( . Following the method in ( ), our semi-supervised learning loss function is defined as:</p><formula xml:id="formula_13">L = (a,R,E,z)∼p l L SV + (a,R,E)∼pu L CV AE</formula><p>where L SV is defined in section 5.2 and L CV AE is detailed in section 5.1. In other words, the data sample with evidence label is fed to L SV , other- wise is fed to L CV AE .</p><p>The results are shown in <ref type="figure" target="#fig_4">Figure 4</ref> where the x-axis shows the ratio of labeled examples. The incremental Naive CVAE+SV model only uses the labeled evidence examples while the Semi CVAE model also uses unlabeled evidence exam- ples. The figure shows that the Semi CVAE  <ref type="figure">Figure 5</ref>: The experimental setup for the human subject study examining the role of commonsense justification towards common ground.</p><p>model outperforms the Naive CVAE+SV model. This indicates that the semi-supervised method can improve the evidence selection by making use of unlabeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Commonsense Justification towards Common Ground</head><p>In human-agent communication, the success of communication is largely dependent on common ground which captures shared knowledge, beliefs, or past experience <ref type="bibr" target="#b4">(Clark, 1996)</ref>. As common- sense evidence what humans use to justify ac- tions, To validate this hypothesis, we conducted a human-subject experiment to examine the role of commonsense justification in facilitating common ground. <ref type="figure">Figure 5</ref> shows the setup of our experiment. The agent is provided with an image and applies vari- ous models (e.g., CVAE) to jointly predict the ac- tion and identify commonsense evidence. The hu- man is provided with a list of six action choices and does not have access to the image. The agent communicates to the human only the identified commonsense evidence and the human makes a guess on the action from the candidate list purely based on the communicated evidence. The idea is that, if the human and the agent share the same be- liefs about evidence to justify an action, then the action guessed by the human should be the same as the action predicted by the agent. Generating Distracting Verbs. For each image, the human is provided with a list of six action/verb candidates. To generate this list, we mix four dis- tracting verbs with the ground-truth action verb plus a default Other. Most of the distracting verbs come from the concrete action verbs made available by ( <ref type="bibr" target="#b9">Gao et al., 2018)</ref>. We first manu- ally filtered out the verbs which have the same meaning with the ground-truth verb. We then se- lected two groups of distracting verbs: an easy group (where the distracting verbs have larger dis- tance from the ground-truth verb in the embedding space, with an average similarity of 0.284) and a hard group (more close to the ground-truth verbs with an average similarity of 0.479). The temper- ature based softmax distribution <ref type="bibr" target="#b2">(Chorowski and Jaitly, 2016</ref>) was used to sample the easy and the hard distracting verbs based on the pre-trained GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>) embedding cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experiment Setup</head><p>Process. A total of 170 images were used in this experiment, and 24 workers from AMT par- ticipated in our study. For each image, we ap- plied three different models: Attention base- line, CVAE, and CVAE+SV to generate the com- monsense evidence. An upper bound based on gold commonsense evidence was also measured. Note that, the agent has no knowledge of the hu- man's action choices when generating the com- monsense evidence. Theory of mind is an impor- tant aspect in human-agent communication. Incor- porating human's action choices in justifying ac- tion is an interesting however a different problem which requires different solutions. In this paper, we only focus on the situation where the mind of the human is opaque to the agent.</p><p>For each model and each image under the easy or hard configurations, the top five predicted com- monsense evidence (associated with the predicted action) were shown to a worker. The the worker was requested to select the most probable action from the distracting list only based on these five pieces of evidence. We randomly assigned three workers to each image. The majority of three se- lections was considered as the final answer. If all three selections disagreed, one worker's choice was randomly selected as the final answer.</p><p>Metrics for Common Ground. We use the agree- ment between the action guessed by the human and the action predicted by the agent to mea- sure how well the selected commonsense evidence serves to bring the human and the agent to a com- mon ground of perceived actions. More formally, as shown in <ref type="figure">Figure 5</ref>, given an image, suppose its ground-truth action is A, the action predicted by the agent/machine is A m , and the action guessed by the human is A h , the Common Ground is de- fined as: A m = A h = A. Here we also enforce that the predicted action should be the same as the ground-truth action. The percentage of trials based on different models that have led to a com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVAE CVAE+SV Gold</head><p>Gold Action: Bake Am: Eat Ah: Bake</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Am: Bake Ah: Bake</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Am: Bake Ah: Bake</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Am: Bake Ah: Bake</head><p>• The bread is next to the bread.</p><p>• The bread is on the rack.</p><p>• The bread is on the pan.</p><p>• The man has keys.</p><p>• The man has the band.</p><p>• The bread is on the rack.</p><p>• The bread is on the pan.</p><p>• The bread is on the tray.</p><p>• The bread is next to the bread.</p><p>• The bread is baked.</p><p>• The bread is baked.</p><p>• The bread is next to the bread.</p><p>• The person is pushing the tray.</p><p>• The bread is on the pan.</p><p>• The bread is on the rack.</p><p>• The bread is on the tray.</p><p>• The person is pushing the tray.</p><p>• The bread is baked. • The baby has a mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Action: Brush</head><p>• The baby has a hand.</p><p>• The baby has eyeballs.</p><p>• The baby has fingers.</p><p>• The baby has a nose.</p><p>• The hand holds the toothbrush.</p><p>• The toothbrush is in the mouth.</p><p>• The baby has a mouth.</p><p>• The baby has fingers.</p><p>• The baby has a nose.</p><p>• The hand holds the toothbrush.</p><p>• The toothbrush is in the mouth.</p><p>• The baby has eyeballs.</p><p>• The baby has a mouth.</p><p>• The baby has a hand.</p><p>• The toothbrush is in the mouth.</p><p>• The hand holds the toothbrush.  mon ground is measured and compared. <ref type="table" target="#tab_4">Table 4</ref> shows the comparison results among vari- ous models and the upper bound where the gold commonsense evidence provided to the human. It's not surprising that performance on common ground is worse in the hard configuration as the distracting verbs are more similar to the target action. The CVAE-based method is better than the attention-based method in facilitating common ground. <ref type="figure" target="#fig_6">Figure 6</ref> shows two examples of the top five pre- dicted evidence under different models. For each model, it also shows the agent predicted action (A m ) and the human guessed action (A h ). In both examples, all models were able to establish a com- mon ground except for the attention-based model. The evidence selected by the CVAE+SV model is clearly more accurate than the CVAE model and is more close to the ground-truth evidence. The second example shows that although the attention- based model predicts a correct target action, it fails to convey correct commonsense evidence to estab- lish a common ground with the human.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper describes an approach for action justi- fication using commonsense evidence. As demon- strated in our experiments, commonsense evi- dence is selected to align with humans' justifica- tion of an action and is therefore critical in es- tablishing a common ground between humans and agents.</p><p>For all experiments in this paper, we use the an- notated relations/attributes from the original Vi- sual Genome data. As the state-of-the-art re- call@50 on the relation detection with a lim- ited vocabulary is only around 20% ( <ref type="bibr" target="#b16">Liang et al., 2018)</ref>. Using annotated relations and attributes al- lows us to focus on the study of commonsense ev- idence and its role in action justification and com- mon ground. Nevertheless, our proposed method has the potential to handle the erroneous rela- tions/entities, e.g., as a result of vision processing, for example, by avoiding to select erroneous re- lations as they do not correlate with actions and other indicative relations/attributes. Our future work will extend the model and findings from this work to vision processing that will not only iden- tify commonsense evidence but also explain where and how in the perceived environment the evi- dence is gathered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of commonsense evidence selected by the crowd (in bold) from the list of relations and attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System architecture for the CVAE model. The dotted region is only used during the model training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The system architecture for attention-based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evidence selection MAP for semi-supervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Two examples of the common ground study based on different models. In each example, a ranked list of commonsense evidence generated by different models is shown. A m captures the action predicted by the agent. A h captures the action guessed by the human based on the selected commonsense evidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : The average number of available relations/attributes and the average number of annotated commonsense evidence relations/attributes across the corresponding images for each verb in the dataset.</head><label>1</label><figDesc></figDesc><table>feed 
pull 
ride 
drink 
chop 
brush 
fry 
bake 
blend 
eat 

Rel# 
15.49 
± 7.55 

14.62 
± 9.36 

12.42 
± 7.18 

15.16 
± 9.89 

12.00 
± 7.22 

15.40 
± 8.93 

14.02 
± 7.02 

13.31 
± 7.27 

14.37 
± 6.37 

15.08 
± 6.87 

Gold Rel# 
2.79 
± 1.28 

1.86 
± 0.84 

1.69 
± 0.83 

2.41 
± 1.14 

2.41 
± 1.66 

2.26 
± 1.08 

2.72 
± 2.06 

2.25 
± 1.69 

2.56 
± 1.84 

2.52 
± 1.08 

Att# 
12.48 
± 7.11 

13.60 
± 7.52 

12.20 
± 7.13 

10.86 
± 6.52 

15.09 
± 6.82 

12.31 
± 8.91 

15.31 
± 7.16 

13.44 
± 6.84 

15.22 
± 7.18 

11.98 
± 6.50 

Gold Att# 
0.26 
± 0.48 

0.20 
± 0.45 

0.13 
± 0.40 

0.30 
± 0.56 

1.60 
± 1.33 

0.22 
± 0.49 

0.91 
± 1.26 

0.93 
± 1.06 

0.15 
± 0.40 

0.41 
± 0.70 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Distributions of the categories of commonsense evidence relations/attributes for each verb. 

feed pull ride drink chop brush 
fry 
bake blend 
eat 
Transitive-Relation 
0.10 0.14 0.15 
0.11 
0.11 
0.13 
0.12 0.18 
0.15 
0.09 
Subaction-Relation 
0.45 0.46 0.13 
0.32 
0.29 
0.39 
0.17 0.11 
0.09 
0.43 
Spatial-Relation 
0.45 0.40 0.72 
0.57 
0.60 
0.48 
0.71 0.71 
0.76 
0.48 
Effect-Attribute 
0.0 
0.0 
0.0 
0.14 
0.82 
0.05 
0.53 0.34 
0.22 
0.27 
Associated-Attribute 
1.0 
1.0 
1.0 
0.86 
0.18 
0.95 
0.47 0.66 
0.78 
0.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Action prediction accuracy and evidence se- lection MAP.</head><label>3</label><figDesc></figDesc><table>Action Accuracy Evidence MAP 
Attention 
0.789 
0.442 
CVAE 
0.835 
0.572 
CVAE+SV 
0.871 
0.690 
Upper Bound 
0.918 
1.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results from the human subject study on com-
mon ground. 

Attenton CVAE CVAE+SV Gold 
Easy 
0.665 
0.776 
0.818 
0.888 
Hard 
0.576 
0.718 
0.788 
0.841 

</table></figure>

			<note place="foot" n="1"> The dataset is available at https://github.com/ yangshao/Commonsense4Action</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by IIS-1617682 from the National Science Foundation and the DARPA XAI program under a subcontract from UCLA (N66001-17-2-4029).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Humancentric justification of machine learning predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Using language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">952</biblScope>
			<biblScope unit="page" from="274" to="296" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The intentional Stance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dennett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03799</idno>
		<title level="m">Verb physics: Relative physical knowledge of actions and objects</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Physical causality of action verbs in grounded language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1814" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What action causes this? towards naive physical action-effect prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating visual explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual relationship detection with deep structural ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kongming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence</title>
		<meeting>the ThirtySecond AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Explanation and abductive inference. Oxford handbook of thinking and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Lombrozo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="260" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical question-image coattention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08129</idno>
		<title level="m">Multimodal explanations: Justifying decisions and pointing to the evidence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ronald A Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="17" to="42" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">why should i trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anchors: High-precision modelagnostic explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic networks and explanatory coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thagard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science Quarterly</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2542" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The vqa-machine: Learning how to use existing vision algorithms to answer new questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Chunhua Shen, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stating the obvious: Extracting visual common sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Zero-shot activity recognition with verb attribute induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09468</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Interpreting cnn knowledge via an explanatory graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01785</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00935</idno>
		<title level="m">Interpretable convolutional neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
