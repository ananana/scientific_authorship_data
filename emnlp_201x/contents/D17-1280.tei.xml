<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NITE: A Neural Inductive Teaching Framework for Domain-Specific NER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjian</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NITE: A Neural Inductive Teaching Framework for Domain-Specific NER</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2652" to="2657"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In domain-specific NER, due to insufficient labeled training data, deep models usually fail to behave normally. In this paper , we proposed a novel Neural Inductive TEaching framework (NITE) to transfer knowledge from existing domain-specific NER models into an arbitrary deep neu-ral network in a teacher-student training manner. NITE is a general framework that builds upon transfer learning and multiple instance learning, which collaboratively not only transfers knowledge to a deep student network but also reduces the noise from teachers. NITE can help deep learning methods to effectively utilize existing resources (i.e., models, labeled and unla-beled data) in a small domain. The experiment resulted on Disease NER proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neu-ral network nearly over 30% in terms of F1-score.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain-specific Named Entity Recognition (DNER), which aims to identify domain specific entity mentions and their categories, plays an important role in domain document classification, retrieval and content analysis. It is also a foun- dation for further level of complex information extraction tasks, serves as cornerstone in the knowledge computing process of transforming data into machine readable knowledge ( . Domain-specific NER is a challeng- ing problem. For example, in biomedical domain, the number of unseen biomedical entity mentions (such as disease names, chemical names), their abbreviations or acronyms, as well as multiple names of the same entity is growing fast with the rapid increase of biomedical literatures and clinical records. However, the performance of a learning based NER system relies heavily on data annotation, which is quite expensive. The situation is even worse in domain-specific NER systems, since their data annotation requires the engage of domain experts. Therefore, in many special domains, only trained models or APIs are available, while their training data are private and inaccessible. On the other hand, due to insufficient labeled training data, deep models usually fail to behave normally in such domain, and state-of-the-art methods in these domains are usually dominated by rule based deductive methods or shallow model with hand-crafted features. However, the way of pre-defining useful domain specific hand-crafted features or rules are usually unavailable to the public.</p><p>In this paper, we proposed a novel Neural In- ductive TEaching framework (NITE) to transfer knowledge from existing models into an arbitrary deep neural network. The idea of NITE is mainly borrowed from Transfer learning <ref type="bibr" target="#b14">(Pan and Yang, 2010)</ref> where previously learned knowledge can aid current situation and solve problems with bet- ter solutions. In NITE, existing NER models be- have like inefficient teachers to teach a deep neu- ral network (we called student network) to iden- tify named entities by giving it concrete exam- ples. The knowledge transferred from these mod- els is their posterior distributions on unlabeled data. These teachers are inefficient because they transfer not only useful information, but also er- rors to the student. The inputs of student network can be twofold, one is a small proportion from human labeled ground truth data (optional, like text book), and another is a large proportion from teachers, which is always noisy and less trustable.</p><p>In such case, a student is overwhelmed and often inferior to the teachers, therefore in NITE, we in- troduced Multiple Instance Learning (MIL) trick ( <ref type="bibr" target="#b3">Dietterich et al., 1997;</ref><ref type="bibr" target="#b1">Babenko, 2008)</ref> to reduce the input noise during the model training.</p><p>In summary, NITE is a general framework that can help deep learning methods to make the best use of existing resources (i.e., models, labeled and unlabeled data). The experiment results on Disease NER (DNER) proved that without using any labeled data, NITE can significantly boost the performance of a CNN-bidirectional LSTM-CRF NER neural network <ref type="bibr" target="#b12">(Ma and Hovy, 2016)</ref>, which trained on NCBI training dataset nearly over 30% in terms of F1-score. It also outperformed the teacher model, which proved the correctness of our hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Inductive Teaching Framework</head><p>In this section we will define our NITE framework step by step, and apply it to Disease NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inductive Teaching</head><p>Inductive teaching means teaching student by ex- amples, our inductive teaching method builds upon teacher-student models ( <ref type="bibr" target="#b0">Ba and Caruana, 2014</ref>) and knowledge distillation ( <ref type="bibr" target="#b5">Hinton et al., 2015)</ref>. The main idea of our method is to transfer discriminative knowledge from well-trained exist- ing models (teachers) to a new and more capable model (student). The student learns by imitating the teachers' behaviors, and the teaching process can be defined as follows:</p><p>Let x = {w 1 , w 2 , . . . , w |x| } be an input sen- tence of |x| words, where w k is the kth word in x. If l k is the corresponding 3-dimensional one hot IOB (In-Out-Begin) vector for w k , then the NER labeling sequence of x can be defined as</p><formula xml:id="formula_0">y = {l 1 , l 2 , . . . , l |x| }.</formula><p>For a given sentence x i , we further define the posterior distribution of a teacher as y ft i = f t (y i |x i ), while the posterior distribution of a stu- dent network can be defined as y fs i = f s (y i |x i ; θ), where θ is the parameters of the student network. During training, we measure the similarity be- tween y ft and y fs with KL-divergence, and min- imize their difference. Therefore, for a given x i , we optimize:</p><formula xml:id="formula_1">min θ n i=1 D KL (f t (y i |x i )||f s (y i |x i ; θ)) (1) , where D KL (P ||Q) = j P j log P j Q j</formula><p>is the KL-divergence. This equation can be optimized through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) up- date rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multiple Instance Learning</head><p>Multiple Instance Learning is an effective train- ing method that can help to train a supervised model to alleviate the wrong label problem ( <ref type="bibr" target="#b15">Riedel et al., 2010;</ref><ref type="bibr" target="#b6">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b16">Surdeanu et al., 2012</ref>). Instead of predicting labels for each indi- vidual training sample, the objective of MIL is to predict the labels (positive or negative) of the un- seen bags, where each bag contains a fixed number of instances (samples). The standard MIL assump- tion assumes that a bag is positively labeled if at least one instance in a bag is positive, and is nega- tively labeled if all instances in a bag are negative. MIL is generally used in training a binary classi- fier, to apply MIL in NITE, we redefine the label of a bag as the quality (correctness) of its contain- ing samples. Thus, in NITE, a bag is positively labeled if at least one instance in it is labeled cor- rectly. Furthermore, it is inappropriate to evaluate the correctness of IOB label (i.e., l k ) of each word (i.e., w k ), since the IOB sequence y i of a sentence x i is generated dependently. Therefore, we choose sentence x i as our MIL instance, and the correct- ness of x i is evaluated by the likelihood probabil- ity of all words with correct BIO tags. In general, our MIL can be formally defined as follows:</p><p>Randomly allocate training samples in a mini- batch B into M bags, i.e., B = {B 1 , B 2 , . . . , B M } with their corresponding labels {z 1 , z 2 , . . . , z M }, where z m ∈ {−1, 1}. For bag B m , it contains K instances, i.e., B m = {x 1 , x 2 , . . . , x K }, where x i is a sentence with its posterior evaluation y fs i . During the training, given a bag B m , if z m = 1, which means B m is a positive bag. In order to reduce the noise, our MIL learner will select the most correct instance y fs i * , which has the maximum likelihood among all other instances (i.e., sen- tence) in the bag B m . That is P (z m = 1|B m ) = P (y fs i * ) = arg max i {P (y fs i |x i )}, where 1 ≤ i ≤ K, x i ∈ B m . If z m = −1, which means B m is a negative bag, in order to better detect such negative bags, our MIL learner should select the most violated instance for learning, which is also the instance with maximum likelihood. Thus, the bag label z (which indicates the sentence is labeled correctly or incorrectly) is actually integrated out, since no matter what the value z is, MIL in NITE will always select the instance with the highest likelihood probability. Finally the MIL in NITE can be summarized as:</p><formula xml:id="formula_2">P (z m |B m ) = P (y fs i * ) = arg max i=1:K {P (y fs i |x i )}<label>(2)</label></formula><p>In summary, MIL in NITE can be regarded as a mechanism for posterior selection, or regulariza- tion on posterior distribution of a student network. Therefore, MIL only affects the model training, and it will not affect the testing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Teacher Model &amp; Student Network</head><p>Theoretically, the teacher model of NITE can be any existing well-trained model, while the student network can be an arbitrary deep neural network. In this paper, we focus on domain-specific NER, and more specifically on Disease NER, which is a small but typical domain that is suffering from insufficient labeled training data.</p><p>There are many existing DNER systems, and the most well-known systems are BANNER ( <ref type="bibr" target="#b10">Leaman et al., 2008)</ref>, and DNorm ( <ref type="bibr" target="#b9">Leaman et al., 2013)</ref>. BANNER is an open-source biomedical NER system implemented using conditional ran- dom fields (CRFs) ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>). While, DNorm uses supervised semantic indexing, is trained with pairwise learning to rank, to score the mentions returned by BANNER. Therefore, DNorm can be regarded as an extension of BAN- NER, and the whole system depends on hand- crafted features such as word spelling features and orthographic features. DNorm is the state-of-the- art DNER system, and therefore we adopt DNorm as our teacher model.</p><p>For the student network, we are looking for state-of-the-art solutions in general NER. There are many studies on applying complex deep learn- ing models on general NER or other sequence labeling tasks. Without any feature engineering trick, deep models have achieved comparable or better performances than many other traditional methods. More recently, <ref type="bibr" target="#b12">Ma and Hovy (2016)</ref> proposed a method that concatenated CNN, bidi- rectional LSTM, and CRF successively to form an end to end deep NER model (CLC for short). CLC achieved state-of-the-art performance in general NER, and therefore we take the CLC as our stu- dent network, <ref type="figure">Fig. 1</ref> shows the overall architecture of our student network.  <ref type="figure">Figure 1</ref>: the Flowchart of the Student Network</p><p>As shown in <ref type="figure">Fig. 1</ref>, the character-level embed- dings are generated by CNN layers, then are con- catenated with pre-trained word embeddings, and finally fed into the bidirectional LSTM layer. The bidirectional LSTM is efficient to capture syntac- tic and semantic information both preceding and following simultaneously. Its output vectors are fed into the CRFs layer for IOB sequence label- ing. It uses maximum conditional likelihood es- timation to choose parameters during the finally CRFs training process, and its likelihood can be given as follows:</p><formula xml:id="formula_3">P (y fs i |x i ) = arg max y∈Y(x i ) P (y|x i )<label>(3)</label></formula><p>, where Y(x i ) denotes the set of possible label se- quences for x i . Eq. 3 can be solved efficiently by adopting the Viterbi algorithm. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the whole NITE-NER training pro- cess. For each training iteration, training sam- ples in a mini-batch are randomly allocated into M bags, and then fed into the student network f s . For bag B m , the student network will gener- ate posterior evaluation y fs i for each input instance x i ∈ B m respectively. Then the MIL module will select the best sample y ft i * from all K instances ac- cording to Eq. 3 and 2. Finally, NITE will retrieve posterior evaluation y ft i * from the teacher, and up- date θ based on Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we designed several experiments to testify our hypothesis of inductive teaching as well as evaluate our NITE framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Corpus</head><p>Although NITE is a supervised learning frame- work, the discriminative knowledge of student net- </p><p>. output for each in bag (3). find * arg max : || (5). output * (5). output * || ; ;</p><p>(1). input bag work is learned indirectly from the teacher mod- els, therefore NITE can be trained without any la- beled data.</p><p>To evaluate the efficiency of the NITE frame- work, we trained two DNER models on NCBI dis- ease corpus <ref type="bibr" target="#b4">(Do˘ gan et al., 2014;</ref><ref type="bibr" target="#b7">Islamaj Dogan and Lu, 2012)</ref>. One is the well-known DNorm model, which is the state-of-the-art method in disease NER. Another one is the bi-directional LSTM-CNN-CRF NER neural network i.e., CLC <ref type="bibr" target="#b12">(Ma and Hovy, 2016)</ref>, which has the state-of-the- art performance in general NER task. The CLC architecture also serves as our student network.</p><p>The NCBI disease corpus is a widely used data corpus with disease name and related concept an- notations in biomedical research field. The cor- pus is an extension of the AZDC corpus ( <ref type="bibr" target="#b11">Leaman et al., 2009</ref>) which was annotated only with dis- ease mentions. The detailed characteristics of the NCBI disease corpus as well as how we partition the data are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Setup</head><p>The experiment's setup is as follows:</p><p>Our NITE-DNER is trained without any labeled data, we randomly sampled 2,000 unlabeled ab- stracts of biomedical literature from PubMed as our training data. The DNorm model is served as the teacher model in the NITE framework.</p><p>In student network, we initialized charac- ter embeddings with uniform samples from</p><formula xml:id="formula_5">[− 3 d , + 3 d ],</formula><p>where we set the dimension d =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NCBI</head><p>Train Validate <ref type="table" target="#tab_2">Test  # of documents  593  100  100  # of sentences  5661  791  961  # of disease  5148  791  961  Specific Disease  2959  409  556  Disease Class  781  127  121  Modifier  1292  218  264  Composite Mention  116  37  20   Table 1</ref>: The description of the NCBI corpus as training, validating and testing sets for the recog- nition of disease named entity 30. We use 30 filters with window length 3 in CNN and 200 hidden states in bi-directional LSTM. In training procedure we set initial learn- ing rate η 0 = 0.015 with decay rate ρ = 0.05, the learning rate is updated as</p><formula xml:id="formula_6">η t = η 0 /(1.0 + ρn),</formula><p>where n is the number of epochs. We use a fixed dropout rate 0.5 at CNN and both input and output vectors of bi-directional LSTM to mitigate over- fitting. For MIL we set the bag size K = 5 with mini-batch size 30. We implemented neural net- works on a GeForce GTX 1080 using Theano.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussion</head><p>We evaluated all three DNER methods on the NCBI test set in terms of precision, recall and F1- score. All the measurements are based on exact location of extracted disease mentions in the given test sentences.</p><p>Method  The experiment results are presented in <ref type="table" target="#tab_2">Table 2</ref>. As shown in <ref type="table" target="#tab_2">Table 2</ref>, although the complex CLC network is the state-of-the-art method in general NER, it behaves poorly in domain-specific NER task due to insufficient labeled training data. How- ever, with the help of our NITE framework, its performance is significantly boosted, and reached the comparable level of DNorm. This proved that knowledge transfer in NITE is efficient and impor- tant in training a deep model of domain-specific NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>In this paper, we proposed a general framework, NITE, and demonstrated its efficiency in transfer- ring DNER knowledge into an end to end deep NER model. Although we only proposed a so- lution for DNER, it could be easily applied to other domain-specific NER problems (e.g., chem- ical, gene, and protein) or even applications other than NER. The experiment results suggested that NITE can be very helpful on training a deep model when other resources are available. For future work, a NITE architecture with more than one teacher could be considered. Moreover, as men- tioned in ( <ref type="bibr" target="#b18">Zhou et al., 2017)</ref>, crowd knowledge can be used to reshape deep learning features. Our framework can also incorporate crowd knowledge easily, in which the teachers can be human crowds, and then the NITE can employs active learning <ref type="bibr" target="#b13">(Olsson, 2009)</ref> or lifelong machine learning <ref type="bibr" target="#b2">(Chen and Liu, 2016)</ref> to progressively polishing the stu- dent model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The training process of NITE-NER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Performance comparisons.</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple instance learning: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>View Article PubMed/NCBI Google Scholar</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="145" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Solving the multiple instance problem with axis-parallel rectangles. Artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas G Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-Pérez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="31" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ncbi disease corpus: a resource for disease name recognition and concept normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj Do˘ Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Rezarta Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<title level="m">BioNLP: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>chapter An improved corpus of disease mentions in PubMed citations. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on machine learning, ICML</title>
		<meeting>the eighteenth international conference on machine learning, ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dnorm: disease name normalization with pairwise learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rezarta</forename><surname>Islamaj Do˘ Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page">474</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Banner: an executable survey of advances in biomedical named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graciela</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific symposium on biocomputing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="652" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enabling recognition of diseases in biomedical text with machine learning: corpus and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Symposium on Languages in Biology and Medicine</title>
		<meeting>the 2009 Symposium on Languages in Biology and Medicine</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A literature survey of active machine learning in the context of natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Olsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Disambiguating named entitieswith deep supervised learning via crowd labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le-Kui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si-Liang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Ting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Challenges and opportunities: from big data to knowledge inai 2.0. Frontiers of Information Technology &amp; Electronic Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yue-Ting Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-He</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
