<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale Reordering Model for Statistical Machine Translation using Dual Multinomial Logistic Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Alrajeh</surname></persName>
							<email>, asrajeh@kacst.edu.sa</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Electronics and Computer Science</orgName>
								<orgName type="institution">University of Southampton Southampton</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Research Institute</orgName>
								<orgName type="department" key="dep2">King Abdulaziz City for Science and Technology (KACST) Riyadh</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-scale Reordering Model for Statistical Machine Translation using Dual Multinomial Logistic Regression</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1758" to="1763"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Phrase reordering is a challenge for statistical machine translation systems. Posing phrase movements as a prediction problem using contextual features modeled by maximum entropy-based classifier is superior to the commonly used lexicalized reordering model. However, Training this discriminative model using large-scale parallel corpus might be computationally expensive. In this paper, we explore recent advancements in solving large-scale classification problems. Using the dual problem to multinomial logistic regression, we managed to shrink the training data while iterating and produce significant saving in computation and memory while preserving the accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phrase reordering is a common problem when translating between two grammatically different languages. Analogous to speech recognition sys- tems, statistical machine translation (SMT) sys- tems relied on language models to produce more fluent output. While early work penalized phrase movements without considering reorderings aris- ing from vastly differing grammatical structures across language pairs like Arabic-English <ref type="bibr" target="#b2">(Koehn, 2004a</ref>), many researchers considered lexicalized reordering models that attempted to learn orienta- tion based on the training corpus <ref type="bibr" target="#b15">(Tillmann, 2004;</ref><ref type="bibr" target="#b5">Kumar and Byrne, 2005;</ref><ref type="bibr">Koehn et al., 2005</ref>).</p><p>Building on this, some researchers have bor- rowed powerful ideas from the machine learning literature, to pose the phrase movement problem as a prediction problem using contextual input fea- tures whose importance is modeled as weights of a linear classifier trained by entropic criteria. The approach (so called maximum entropy classifier or simply MaxEnt) is a popular choice ( <ref type="bibr" target="#b19">Zens and Ney, 2006;</ref><ref type="bibr" target="#b17">Xiong et al., 2006;</ref><ref type="bibr" target="#b7">Nguyen et al., 2009;</ref><ref type="bibr" target="#b16">Xiang et al., 2011</ref>). Max-margin structure classifiers were also proposed ( <ref type="bibr" target="#b8">Ni et al., 2011</ref>). Alternatively, <ref type="bibr">Cherry (2013)</ref> proposed recently us- ing sparse features optimize the translation quality with the decoder instead of training a classifier in- dependently.</p><p>While large-scale parallel corpus is advanta- geous for improving such reordering model, this improvement comes at a price of computational complexity. This issue is particularly pronounced when discriminative models are considered such as maximum entropy-based model due to the re- quired iterative learning.</p><p>Advancements in solving large-scale classifica- tion problems have been shown to be effective such as dual coordinate descent method for linear support vector machines ( <ref type="bibr">Hsieh et al., 2008)</ref>. Sim- ilarly, <ref type="bibr" target="#b18">Yu et al. (2011)</ref> proposed a two-level dual coordinate descent method for maximum entropy classifier.</p><p>In this work we explore the dual problem to multinomial logistic regression for building large- scale reordering model (section 3). One of the main advantages of solving the dual problem is providing a mechanism to shrink the training data which is a serious issue in building such large- scale system. We present empirical results com- paring between the primal and the dual problems (section 4). Our approach is shown to be fast and memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline System</head><p>In statistical machine translation, the most likely translation e best of an input sentence f can be found by maximizing the probability p(e|f ), as follows:</p><formula xml:id="formula_0">e best = arg max e p(e|f ).<label>(1)</label></formula><p>A log-linear combination of different models (features) is used for direct modeling of the poste- rior probability p(e|f ) ( <ref type="bibr" target="#b12">Papineni et al., 1998;</ref><ref type="bibr" target="#b10">Och and Ney, 2002</ref>):</p><formula xml:id="formula_1">e best = arg max e n i=1 λ i h i (f , e) (2)</formula><p>where the feature h i (f , e) is a score function over sentence pairs. The translation model and the language model are the main features in any sys- tem although additional features h(.) can be inte- grated easily (such as word penalty). State-of-the- art systems usually have around ten features.</p><p>The language model, which ensures fluent translation, plays an important role in reordering; however, it has a bias towards short translations <ref type="bibr" target="#b4">(Koehn, 2010)</ref>. Therefore, a need for developing a specific model for the reordering problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lexicalized Reordering Model</head><p>Adding a lexicalized reordering model consis- tently improved the translation quality for sev- eral language pairs ( <ref type="bibr">Koehn et al., 2005</ref>). Re- ordering modeling involves formulating phrase movements as a classification problem where each phrase position considered as a class <ref type="bibr" target="#b15">(Tillmann, 2004)</ref>. Some researchers classified phrase move- ments into three categories (monotone, swap, and discontinuous) but the classes can be extended to any arbitrary number <ref type="bibr">(Koehn and Monz, 2005</ref>). In general, the distribution of phrase orientation is:</p><formula xml:id="formula_2">p(o k | ¯ f i , ¯ e i ) = 1 Z h( ¯ f i , ¯ e i , o k ) .<label>(3)</label></formula><p>This lexicalized reordering model is estimated by relative frequency where each phrase pair ( ¯ f i , ¯ e i ) with such an orientation (o k ) is counted and then normalized to yield the probability as fol- lows:</p><formula xml:id="formula_3">p(o k | ¯ f i , ¯ e i ) = count( ¯ f i , ¯ e i , o k ) o count( ¯ f i , ¯ e i , o) .<label>(4)</label></formula><p>The orientation of a current phrase pair is de- fined with respect to the previous target phrase. <ref type="bibr">Galley and Manning (2008)</ref> extended the model to tackle long-distance reorderings. Their hierarchi- cal model enables phrase movements that are more complex than swaps between adjacent phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multinomial Logistic Regression</head><p>Multinomial logistic regression (MLR), also known as maximum entropy classifier <ref type="bibr" target="#b19">(Zens and Ney, 2006)</ref>, is a probabilistic model for the multi- class problem. The class probability is given by:</p><formula xml:id="formula_4">p(o k | ¯ f i , ¯ e i ) = exp(w k φ( ¯ f i , ¯ e i )) k exp(w k φ( ¯ f i , ¯ e i )) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">φ( ¯ f i , ¯ e i )</formula><p>is the feature vector of the i-th phrase pair. An equivalent notation to</p><formula xml:id="formula_6">w k φ( ¯ f i , ¯ e i ) is w f (φ( ¯ f i , ¯ e i ), o k ) where w is a long vector composed of all classes parameters (i.e. w = [w 1 . . . w K ] ) and f (., .</formula><p>) is a joint feature vec- tor decomposed via the orthogonal feature rep- resentation ( <ref type="bibr" target="#b14">Rousu et al., 2006</ref>). This repre- sentation simply means there is no crosstalk be- tween two different feature vectors. For example,</p><formula xml:id="formula_7">f (φ( ¯ f i , ¯ e i ), o 1 ) = [φ( ¯ f i , ¯ e i ) 0 . . . 0]</formula><p>. The model's parameters can be estimated by minimizing the following regularized negative log-likelihood P(w) as follows <ref type="bibr">(Bishop, 2006</ref>):</p><formula xml:id="formula_8">min w 1 2σ 2 K k=1 w k 2 − N i=1 K k=1˜p k=1˜ k=1˜p ik log p(o k | ¯ f i , ¯ e i ) (6)</formula><p>Here σ is a penalty parameter and˜pand˜ and˜p is the em- pirical distribution where˜pwhere˜ where˜p ik equals zero for all</p><formula xml:id="formula_9">o k = o i .</formula><p>Solving the primal optimization problem (6) us- ing the gradient:</p><formula xml:id="formula_10">∂P(w) ∂w k = w k σ 2 − N i=1˜p i=1˜ i=1˜p ik − p(o k | ¯ f i , ¯ e i ) φ( ¯ f i , ¯ e i ),<label>(7)</label></formula><p>do not constitute a closed-form solution. In our experiments, we used stochastic gradient decent method (i.e. online learning) to estimate w which is shown to be fast and effictive for large-scale problems <ref type="bibr">(Bottou, 2010)</ref>. The method approxi- mates <ref type="formula" target="#formula_10">(7)</ref> by a gradient at a single randomly picked phrase pair. The update rule is:</p><formula xml:id="formula_11">w k = w k − η i k P i (w),<label>(8)</label></formula><p>where η i is a positive learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Dual Problem</head><p>Lebanon and Lafferty (2002) derived an equiva- lent dual problem to (6). Introducing Lagrange multipliers α, the dual becomes</p><formula xml:id="formula_12">min w 1 2σ 2 K k=1 w k (α) 2 + N i=1 K k=1 α ik log α ik , s.t. K k=1 α ik = 1 and α ik ≥ 0 , ∀i, k,<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">w k (α) = σ 2 N i=1 (˜ p ik − α ik )φ( ¯ f i , ¯ e i )<label>(10)</label></formula><p>As mentioned in the introduction, <ref type="bibr" target="#b18">Yu et al. (2011)</ref> proposed a two-level dual coordinate de- scent method to minimize D(α) in (9) but it has some numerical difficulties. <ref type="bibr">Collins et al. (2008)</ref> proposed simple exponentiated gradient (EG) al- gorithm for Conditional Random Feild (CRF). The algorithm is applicable to our problem, a special case of CRF. The rule update is:</p><formula xml:id="formula_14">α ik = α ik exp(−η i ik D(α)) k α ik exp(−η i ik D(α))<label>(11)</label></formula><p>where</p><formula xml:id="formula_15">ik D(α) ≡ ∂D(α) ∂α ik = 1 + log α ik + w y (α) φ( ¯ f i , ¯ e i ) − w k (α) φ( ¯ f i , ¯ e i ) .<label>(12)</label></formula><p>Here y represents the true class (i.e. o y = o i ). To improve the convergence, η i is adaptively ad- justed for each example. If the objective function (9) did not decrease, η i is halved for number of tri- als ( <ref type="bibr">Collins et al., 2008)</ref>. Calculating the function difference below is the main cost in EG algorithm,</p><formula xml:id="formula_16">D(α ) − D(α) = K k=1 α ik log α ik − α ik log α ik − K k=1 (α ik − α ik )w k (α) φ( ¯ f i , ¯ e i ) + σ 2 2 φ( ¯ f i , ¯ e i ) 2 K k=1 (α ik − α ik ) 2 .<label>(13)</label></formula><p>Clearly, the cost is affordable because w k (α) is maintained throughout the algorithm as follows:</p><formula xml:id="formula_17">w k (α ) = w k (α) − σ 2 (α ik − α ik )φ( ¯ f i , ¯ e i )<label>(14)</label></formula><p>Following <ref type="bibr" target="#b18">Yu et al. (2011)</ref>, we initialize α ik as follows:</p><formula xml:id="formula_18">α ik = (1 − ) if o k = o i ; K−1 else.<label>(15)</label></formula><p>where is a small positive value. This is because the objective function <ref type="formula" target="#formula_12">(9)</ref> is not well defined at α ik = 0 due to the logarithm appearance. Finally, the optimal dual variables are achieved when the following condition is satisfied for all ex- amples ( <ref type="bibr" target="#b18">Yu et al., 2011)</ref>:</p><formula xml:id="formula_19">max k ik D(α) = min k ik D(α)<label>(16)</label></formula><p>This condition is the key to accelerate EG al- gorithm. Unlike the primal problem (6), the dual variables α ik are associated with each example (i.e. phrase pair) therefore a training example can be disregarded once its optimal dual variables ob- tained. More data shrinking can be achieved by tolerating a small difference between the two val- ues in (16). Algorithm 1 presents the overall pro- cedure (shrinking step is from line 6 to 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Shrinking stochastic exponentiated gradient method for training the dual problem</head><p>Require:</p><formula xml:id="formula_20">training set S = {φ( ¯ f i , ¯ e i ), o i } N i=1</formula><p>1: Given α and the corresponding w(α) 2: repeat 3:</p><p>Randomly pick i from S</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Claculate ik D(α) ∀k by <ref type="formula" target="#formula_0">(12)</ref> 5:</p><formula xml:id="formula_21">v i = max k ik D(α) − min k ik D(α) 6:</formula><p>if v i ≤ then for t = 1 to maxTrial do end for 19: until v i ≤ ∀i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We used MultiUN which is a large-scale parallel corpus extracted from the United Nations website <ref type="bibr">(Eisele and Chen, 2010)</ref>. We have used Arabic and English portion of MultiUN where the English side is about 300 million words.</p><p>We simplify the problem by classifying phrase movements into three categories (monotone, swap, discontinuous). To train the reordering models, we used GIZA++ to produce word align- ments <ref type="bibr" target="#b9">(Och and Ney, 2000</ref>). Then, we used the extract tool that comes with the Moses toolkit ( <ref type="bibr" target="#b1">Koehn et al., 2007</ref>) in order to extract phrase pairs along with their orientation classes.</p><p>As shown in <ref type="table">Table 1</ref>, each extracted phrase pair is represented by linguistic features as follows:</p><p>• Aligned source and target words in a phrase pair. Each word alignment is a feature.</p><p>• Words within a window around the source phrase to capture the context. We choose ad- jacent words of the phrase boundary.</p><p>The extracted phrase pairs after filtering are 47,227,789. The features that occur more than 10 times are 670,154.</p><p>Sentence pair: f :</p><formula xml:id="formula_22">f 1 f 2 1 f 3 f 4 f 5 2 f 6 3 . e : e 1 1 e 2 e 3 3 e 4 e 5 2 .</formula><p>Extracted phrase pairs ( ¯ f , ¯ e) :</p><formula xml:id="formula_23">¯ f i ||| ¯ e i ||| o i ||| alignment ||| context f 1 f 2 ||| e 1 ||| mono ||| 0-0 1-0 ||| f 3 f 3 f 4 f 5 ||| e 4 e 5 ||| swap ||| 0-1 2-0 ||| f 2 f 6 f 6 ||| e 2 e 3 ||| other ||| 0-0 0-1 ||| f 5</formula><p>All linguistic features:</p><p>1. f 1 &amp;e 1 2. f 2 &amp;e 1 3. f 3 4. f 3 &amp;e 5 5. f 5 &amp;e 4 6. f 2 7. f 6 8. f 6 &amp;e 2 9. f 6 &amp;e 3 10. f 5</p><p>Bag-of-words representation:</p><p>a phrase pair is represented as a vector where each feature is a discrete number (0=not exist). <ref type="table">Table 1</ref>: A generic example of the process of phrase pair extraction and representation.</p><formula xml:id="formula_24">φ( ¯ f i , ¯ e i ) 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">3 4 5 6 7 8 9 10</head><formula xml:id="formula_25">φ( ¯ f 1 , ¯ e 1 ) = 1 1 1 0 0 0 0 0 0 0 φ( ¯ f 2 , ¯ e 2 ) = 0 0 0 1 1 1 1 0 0 0 φ( ¯ f 3 , ¯ e 3 ) = 0 0 0 0 0 0 1 1 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification</head><p>We trained our reordering models by both primal and dual classifiers for 100 iterations. For the dual MLR, different shrinking levels have been tried by varying the parameter () in Algorithm 1. <ref type="table">Table 2</ref> reports the training time and classification error rate of these models.</p><p>Training the dual MLR with moderate shrinking level (i.e. = 0.1) is almost four times faster than training the primal one. Choosing larger value for () leads to faster training but might harm the per- formance as shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier</head><p>Training Time Error Rate Primal MLR 1 hour 9 mins 17.81% Dual MLR :0.1 18 minutes 17.95% Dual MLR :1.0 13 minutes 21.13% Dual MLR :0.01 22 minutes 17.89% <ref type="table">Table 2</ref>: Performance of the primal and dual MLR based on held-out data. <ref type="figure" target="#fig_2">Figure 1</ref> shows the percentage of active set dur- ing training dual MLR with various shrinking lev- els. Interestingly, the dual MLR could disregard more than 99% of the data after a couple of iter- ations. For very large corpus, the data might not fit in memory and training primal MLR will take long time due to severe disk-swapping. In this sit- uation, using dual MLR is very beneficial. As the data size decreases, each iteration takes far less computation time (see <ref type="table">Table 2</ref> for total time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation</head><p>We used the Moses toolkit ( <ref type="bibr" target="#b1">Koehn et al., 2007)</ref> with its default settings to build three phrase-based translation systems. They differ in how their re- ordering models were estimated. The language model is a 5-gram with interpolation and Kneser- Ney smoothing <ref type="bibr">(Kneser and Ney, 1995)</ref>. We tuned the system by using MERT technique <ref type="bibr" target="#b11">(Och, 2003)</ref>.</p><p>As commonly used in statistical machine trans- lation, we evaluated the translation performance by BLEU score ( <ref type="bibr" target="#b13">Papineni et al., 2002</ref>). The test sets are NIST MT06 and MT08 where the En- glish sides are 35,481 words (1056 sentences) and 116,840 words (3252 sentences), respectively. <ref type="table">Ta- ble 3</ref> shows the BLEU scores for the translation systems. We also computed statistical significance for the models using the paired bootstrap resam- pling method <ref type="bibr" target="#b3">(Koehn, 2004b</ref>  <ref type="table">Table 3</ref>: BLEU scores for Arabic-English transla- tion systems with different reordering models (*: better than the lexicalized model with at least 95% statistical significance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In training such system with large data sizes and big dimensionality, computational complexity be- come a serious issue. In SMT, maximum entropy- based reordering model is often introduced as a better alternative to the commonly used lexical- ized one. However, training this discriminative model using large-scale corpus might be compu- tationally expensive due to the iterative learning.</p><p>In this paper, we propose training the model using the dual MLR with shrinking method. It is almost four times faster than the primal MLR (also know as MaxEnt) and much more memory- efficient. For very large corpus, the data might not fit in memory and training primal MLR will take long time due to severe disk-swapping. In this sit- uation, using dual MLR is very beneficial. The proposed method is also useful for many classi- fication problems in natural language processing that require large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Christopher M. <ref type="bibr">Bishop. 2006</ref>. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secau- cus, NJ, USA. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Percentage of active set in dual MLR. As the data size decreases, each iteration takes far less computation time (see Table 2 for total time).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Translation System 
MT06 MT08 
Baseline + Lexical. model 
30.86 
34.22 
Baseline + Primal MLR 
31.37* 34.85* 
Baseline + Dual MLR :0.1 31.36* 34.87* 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">IWSLT speech translation evaluation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Spoken Language Translation</title>
		<meeting>International Workshop on Spoken Language Translation<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2007 Demo and Poster Sessions</title>
		<meeting>the ACL 2007 Demo and Poster Sessions<address><addrLine>Alexandra Constantin, and Evan Herbst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pharaoh: a beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th Conference of the Association for Machine Translation in the Americas (AMTA)</title>
		<meeting>6th Conference of the Association for Machine Translation in the Americas (AMTA)<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<editor>Dekang Lin and Dekai Wu</editor>
		<meeting>EMNLP 2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local phrase reordering models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-10" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting and maximum likelihood for exponential models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>T.G. Dietterich, S. Becker, and Z. Ghahramani</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving a lexicalized hierarchical reordering model using maximum entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Vinh Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Le</forename><surname>Shimazu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Machine Translation Summit</title>
		<meeting>the Twelfth Machine Translation Summit<address><addrLine>MT Summit XII</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>International Association for Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploitation of machine learning techniques in modelling phrase movements for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2011-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics (ACL)</title>
		<meeting>the 38th Annual Meeting of the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum likelihood and discriminative training of direct translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kernel-based learning of hierarchical multilabel classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandor</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1601" to="1626" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unigram orientation model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL: Short Papers</title>
		<meeting>HLT-NAACL: Short Papers</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving reordering for statistical machine translation with smoothed priors and syntactic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
	<note>Sydney, July. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual coordinate descent methods for logistic regression and maximum entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Lan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative reordering models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the Workshop on Statistical Machine Translation</title>
		<meeting>on the Workshop on Statistical Machine Translation<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
