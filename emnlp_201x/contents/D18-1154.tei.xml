<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Event Salience Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Event Salience Identification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1226" to="1236"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Identifying the salience (i.e. importance) of discourse units is an important task in language understanding. While events play important roles in text documents, little research exists on analyzing their saliency status. This paper empirically studies the Event Salience task and proposes two salience detection models based on content similarities and discourse relations. The first is a feature based salience model that incorporates similarities among discourse units. The second is a neural model that captures more complex relations between discourse units. Tested on our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neu-ral model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic extraction of prominent information from text has always been a core problem in lan- guage research. While traditional methods mostly concentrate on the word level, researchers start to analyze higher-level discourse units in text, such as entities ( <ref type="bibr" target="#b17">Dunietz and Gillick, 2014</ref>) and events ( <ref type="bibr" target="#b11">Choubey et al., 2018)</ref>.</p><p>Events are important discourse units that form the backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the "importance" of events. For example, <ref type="figure" target="#fig_0">Figure 1</ref> is a news excerpt describing a debate around a jurisdiction process: "trial" is central as the main discussing topic, while "war" is not. Researchers are aware of the need to identify central events in applications like detecting salient relations ( <ref type="bibr" target="#b47">Zhang et al., 2015)</ref>, and identifying cli- max in storyline <ref type="bibr" target="#b44">(Vossen and Caselli, 2015)</ref>. Gener- ally, the salience of discourse units is important for language understanding tasks, such as document analysis ( <ref type="bibr" target="#b4">Barzilay and Lapata, 2008)</ref>, information retrieval ( <ref type="bibr" target="#b45">Xiong et al., 2018)</ref>, and semantic role labeling <ref type="bibr" target="#b9">(Cheng and Erk, 2018)</ref>. Thus, proper mod- els for finding important events are desired.</p><p>In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In <ref type="figure" target="#fig_0">Figure 1</ref>, the "trial" event is connected to many other events: "charge" is pressed before "trial"; "trial" is being "delayed".</p><p>We present two salience detection systems based on the observations. First is a feature based learn- ing to rank model. Beyond basic features like fre- quency and discourse location, we design features using cosine similarities among events and enti- ties, to estimate the content organization <ref type="bibr" target="#b20">(Grimes, 1975)</ref>: how lexical meaning of elements relates to each other. Similarities from within-sentence or across the whole document are used to capture interactions on both local and global aspects ( §4). The model significantly outperforms a strong "Fre- quency" baseline in our experiments.</p><p>However, there are other discourse relations be- yond lexical similarity. <ref type="figure" target="#fig_0">Figure 1</ref> showcases some: the script relation ( <ref type="bibr" target="#b41">Schank and Abelson, 1977)</ref>  <ref type="bibr">1</ref> between "charge" and "trial", and the frame re- lation ( <ref type="bibr" target="#b1">Baker et al., 1998</ref>) between "attacks" and "trial" ("attacks" fills the "charges" role of "trial"). Since it is unclear which ones contribute more to salience, we design a Kernel based Centrality Esti- mation (KCE) model ( §5) to capture salient specific interactions between discourse units automatically.</p><p>In KCE, discourse units are projected to embed- dings, which are trained end-to-end towards the salience task to capture rich semantic information. A set of soft-count kernels are trained to weigh salient specific latent relations between discourse units. With the capacity to model richer relations, KCE outperforms the feature-based model by a large margin ( §7.1). Our analysis shows that KCE is exploiting several relations between discourse units: including script and frames <ref type="table" target="#tab_6">(Table 5)</ref>. To further understand the nature of KCE, we conduct an intrusion test ( §6.2), which requires a model to identify events from another document. The test shows salient events form tightly related groups with relations captured by KCE.</p><p>The notion of salience is subjective and may vary from person to person. We follow the empirical ap- proaches used in entity salience research <ref type="bibr" target="#b17">(Dunietz and Gillick, 2014</ref>). We consider the summarization test: an event is considered salient if a summary written by a human is likely to include it, since events about the main content are more likely to appear in a summary. This approach allows us to create a large-scale corpus ( §3).</p><p>In this paper, we make three main contributions. First, we present two event salience detection sys- tems, which capture rich relations among discourse units. Second, we observe interesting connections between salience and various discourse relations ( §7.1 and <ref type="table" target="#tab_6">Table 5</ref>), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for fu- ture research. Our code, dataset and models are publicly available 2 .</p><p>1 Scripts are prototypical sequences of events: a restaurant script normally contains events like "order", "eat" and "pay".</p><p>2 https://github.com/hunterhector/ EventSalience</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Events have been studied on many aspects due to their importance in language. To name a few: event detection ( <ref type="bibr" target="#b23">Li et al., 2013;</ref><ref type="bibr" target="#b32">Nguyen and Grishman, 2015;</ref><ref type="bibr" target="#b33">Peng et al., 2016</ref>), coreference ( <ref type="bibr" target="#b26">Lu and Ng, 2017)</ref>, temporal analysis ( <ref type="bibr" target="#b15">Do et al., 2012;</ref><ref type="bibr" target="#b6">Chambers et al., 2014</ref>), sequenc- ing ( ), script induction ( <ref type="bibr" target="#b7">Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b2">Balasubramanian et al., 2013;</ref><ref type="bibr" target="#b39">Rudinger et al., 2015;</ref><ref type="bibr" target="#b34">Pichotta and Mooney, 2016)</ref>.</p><p>However, studies on event salience are prema- ture. Some previous work attempts to approximate event salience with word frequency or discourse position <ref type="bibr" target="#b44">(Vossen and Caselli, 2015;</ref><ref type="bibr" target="#b47">Zhang et al., 2015)</ref>. Parallel to ours, <ref type="bibr" target="#b11">Choubey et al. (2018)</ref> pro- pose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closed- domain documents, using several oracle event at- tributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most important event per document.</p><p>There is a small but growing line of work on entity salience <ref type="bibr" target="#b17">(Dunietz and Gillick, 2014;</ref><ref type="bibr" target="#b16">Dojchinovski et al., 2016;</ref><ref type="bibr" target="#b45">Xiong et al., 2018;</ref><ref type="bibr" target="#b35">Ponza et al., 2018)</ref>. In this work, we study the case for events.</p><p>Text relations have been studied in tasks like text summarization, which mainly focused on co- hesion ( <ref type="bibr" target="#b21">Halliday and Hasan, 1976)</ref>. Grammati- cal cohesion methods make use of document level structures such as anaphora relations <ref type="bibr" target="#b3">(Baldwin and Morton, 1998</ref>) and discourse parse trees <ref type="bibr" target="#b28">(Marcu, 1999)</ref>. Lexical cohesion based methods focus on repetitions and synonyms on the lexical level <ref type="bibr" target="#b42">(Skorochod'ko, 1971;</ref><ref type="bibr" target="#b31">Morris and Hirst, 1991;</ref><ref type="bibr" target="#b18">Erkan and Radev, 2004</ref>). Though sharing similar intu- itions, our proposed models are designed to learn richer semantic relations in the embedding space.</p><p>Comparing to the traditional summarization task, we focus on events, which are at a different granu- larity. Our experiments also unveil interesting phe- nomena among events and other discourse units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Event Salience Corpus</head><p>This section introduces our approach to construct a large-scale event salience corpus, including meth- ods for finding event mentions and obtaining saliency labels. The studies are based on the Anno- tated New York Times corpus <ref type="bibr" target="#b40">(Sandhaus, 2008)</ref>, a newswire corpus with expert-written abstracts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Automatic Corpus Creation</head><p>Event Mention Annotation: Despite many anno- tation attempts on events ( <ref type="bibr" target="#b36">Pustejovsky et al., 2002;</ref><ref type="bibr" target="#b5">Brown et al., 2017)</ref>, automatic labeling of them in general domain remains an open problem. Most of the previous work follows empirical approaches. For example, Chambers and Jurafsky (2008) con- sider all verbs together with their subject and ob- ject as events. <ref type="bibr" target="#b14">Do et al. (2011)</ref> additionally in- clude nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet ( <ref type="bibr" target="#b1">Baker et al., 1998)</ref>.</p><p>There are two main challenges in labeling event mentions. First, we need to decide which lexi- cal items are event triggers. Second, we have to disambiguate the word sense to correctly identify events. For example, the word "phone" can re- fer to an entity (a physical phone) or an event (a phone call event). We use FrameNet to solve these problems. We first use a FrameNet based parser: Semafor ( <ref type="bibr" target="#b13">Das and Smith, 2011)</ref>, to find and disam- biguate triggers into frame classes. We then use the FrameNet ontology to select event mentions.</p><p>Our frame based selection method follows the Vendler classes <ref type="bibr" target="#b43">(Vendler, 1957)</ref>, a four way clas- sification of eventuality: states, activities, accom- plishments and achievements. The last three classes involve state change, and are normally considered as events. Following this, we create an "event- evoking frame" list using the following procedure:</p><p>1. We keep frames that are subframes of Event and Process in the FrameNet ontology. 2. We discard frames that are subframes of state, entity and attribute frames, such as Entity, At- tributes, Locale, etc. 3. We manually inspect frames that are not sub- frames of the above-mentioned ones (around 200) to keep event related ones (including sub- frames), such as Arson, Delivery, etc. This gives us a total of 569 frames. We parse the documents with Semafor and consider predicates that trigger a frame in the list as candidates. We finish the process by removing the light verbs <ref type="bibr">3</ref> and reporting events 4 from the candidates, similar to previous research ( <ref type="bibr" target="#b37">Recasens et al., 2013)</ref>. Salience Labeling: For all articles with a human written abstract (around 664,911) in the New York Times Annotated Corpus, we extract event men- tions. We then label an event mention as salient if we can find its lemma in the corresponding abstract ( <ref type="bibr" target="#b30">Mitamura et al. (2015)</ref> showed that lemma match- ing is a strong baseline for event coreference.). For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, event mentions in bold and red are found in the abstract, thus labeled as salient.</p><p>Data split is detailed in <ref type="table">Table 1</ref> and §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation Quality</head><p>While the automatic method enables us to create a dataset at scale, it is important to understand the quality of the dataset. For this purpose, we have conducted two small manual evaluation study. Our lemma-based salience annotation method is based on the assumption that lemma matching being a strong detector for event coreference. In order to validate this assumption, one of the authors manually examined 10 documents and identified 82 coreferential event mentions pairs between the text body and the abstract. The automatic lemma rule identifies 72 such pairs: 64 of these matches human decision, producing a precision of 88.9% (64/72) and a recall of 78% (64/82). There are 18 coreferential pairs missed by the rule.</p><p>The next question is: is an event really important if it is mentioned in the abstract? Although prior work ( <ref type="bibr" target="#b17">Dunietz and Gillick, 2014)</ref> shows that the as- sumption to be valid for entities, we study the case for events. We asked two annotators to manually annotate 10 documents (around 300 events) using a 5-point Likert scale for salience. We compute the agreement score using Cohen's Kappa <ref type="bibr" target="#b12">(Cohen, 1960</ref>). We find the task to be challenging for hu- man: annotators don't agree well on the 5-point scale (Cohens Kappa = 0.29). However, if we col- lapse the scale to binary decisions, the Kappa be- tween the annotators raises to 0.67. Further, the Kappa between each annotator and automatic la- bels are 0.49 and 0.42 respectively. These agree- ment scores are also close to those reported in the entity salience tasks <ref type="bibr" target="#b17">(Dunietz and Gillick, 2014</ref>).</p><p>While errors exist in the automatic annotation process inevitably, we find the error rate to be rea- sonable for a large-scale dataset. Further, our study indicates the difficulties for human to rate on a finer scale of salience. We leave the investigation of continuous salience scores to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feature-Based Event Salience Model</head><p>This section presents the feature-based model, in- cluding the features and the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Features</head><p>Our features are summarized in <ref type="table" target="#tab_1">Table 2</ref>. Basic Discourse Features: We first use two ba- sic features similar to <ref type="bibr" target="#b17">Dunietz and Gillick (2014)</ref>: Frequency and Sentence Location. Frequency is the lemma count of the mention's syntactic head word ( <ref type="bibr" target="#b27">Manning et al., 2014)</ref>. Sentence Loca- tion is the sentence index of the mention, since the first few sentences are normally more impor- tant. These two features are often used to estimate salience ( <ref type="bibr" target="#b4">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b44">Vossen and Caselli, 2015</ref>). Content Features: We then design several lexical similarity features, to reflect Grimes' content relat- edness <ref type="bibr" target="#b20">(Grimes, 1975)</ref>. In addition to events, the relations between events and entities are also im- portant. For example, <ref type="figure" target="#fig_0">Figure 1</ref> shows some related entities in the legal domain, such as "prosecutors" and "court". Ideally, they should help promote the salience status for event "trial".</p><p>Lexical relations can be found both within- sentence (local) or across sentence (global) <ref type="bibr" target="#b21">(Halliday and Hasan, 1976)</ref>. We compute the local part by averaging similarity scores from other units in the same sentence. The global part is com- puted by averaging similarity scores from other units in the document. All similarity scores are computed using cosine similarities on pre-trained embeddings ( <ref type="bibr" target="#b29">Mikolov et al., 2013</ref>).</p><p>These lead to 3 content features: Event Voting, the average similarity to other events in the docu- ment; Entity Voting, the average similarity to en- tities in the document; Local Entity Voting, the average similarity to entities in the same sentence. Local event voting is not used since a sentence often contains only 1 event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model</head><p>A Learning to Rank (LeToR) model <ref type="bibr" target="#b24">(Liu, 2009)</ref> is used to combine the features. Let ev i denote the ith event in a document d. Its salience score is computed as:</p><formula xml:id="formula_0">f (ev i , d) = W f · F (ev i , d) + b (1)</formula><p>where F (ev i , d) is the features for ev i in d (Ta- ble 2); W f and b are the parameters to learn. The model is trained with pairwise loss:</p><formula xml:id="formula_1">ev + ,ev − ∈d max(0, 1 − f (ev + , d) + f (ev − , d)), (2) w.r.t. y(ev + , d) = +1 &amp; y(ev − , d) = −1. y(ei, d) = +1, if ei is a salient entity in d, −1, otherwise.</formula><p>where ev + and ev − represent the salient and non- salient events; y is the gold standard function. Learning can be done by standard gradient meth- ods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Neural Event Salience Model</head><p>As discussed in §1, the salience of discourse units is reflected by rich relations beyond lexical simi- larities, for example, script ("charge" and "trial") and frame (a "trial" of "attacks"). The relations between these words are specific to the salience task, thus difficult to be captured by raw cosine scores that are optimized for word similarities. In this section, we present a neural model to exploit the embedding space more effectively, in order to capture relations for event salience estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Kernel-based Centrality Estimation</head><p>Inspired by the kernel ranking model ( <ref type="bibr" target="#b46">Xiong et al., 2017)</ref>, we propose Kernel-based Centrality Estima- tion (KCE), to find and weight semantic relations of interests, in order to better estimate salience. Formally, given a document d, the set of anno- tated events V = {ev 1 , . . . ev i . . . , ev n }, KCE first </p><formula xml:id="formula_2">ΦK (evi, V) = {φ1( − → evi, V), . . . ,<label>(3)</label></formula><formula xml:id="formula_3">φ k ( − → evi, V), . . . , φK ( − → evi, V)}, φ k ( − → evi, V) = ev j ∈V exp − (cos( − → evi, − → evj) − µ k ) 2 2σ 2 k .<label>(4)</label></formula><p>Name Description</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frequency</head><p>The frequency of the event lemma in document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Location</head><p>The location of the first sentence that contains the event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Voting</head><p>Average cosine similarity with other events in document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity Voting</head><p>Average cosine similarity with other entities in document. Local Entity Voting Average cosine similarity with entities in the sentence. </p><formula xml:id="formula_4">φ k ( − → ev i , V)</formula><p>is the k-th Gaussian kernel with mean µ k and variance σ 2 k . It models the interactions be- tween events in its kernel range defined by µ k and σ k . Φ K (ev i , V) enforces multi-level interactions among events -relations that contribute similarly to salience are expected to be grouped into the same kernels. Such interactions greatly improve the ca- pacity of the model with negligible increase in the number of parameters. Empirical evidences ( <ref type="bibr" target="#b46">Xiong et al., 2017)</ref> have shown that kernels in this form are effective to learn weights for task-specific term pairs.</p><p>The final salience score is computed as:</p><formula xml:id="formula_5">f (ev i , d) = W v · Φ K (ev i , V) + b,<label>(5)</label></formula><p>where W v is learned to weight the contribution of the certain relations captured by each kernel. We then use the exact same learning objective as in equation <ref type="formula">(2)</ref>. The pairwise loss is first back- propagated through the network to update the ker- nel weights W v , assigning higher weights to rele- vant regions. Then the kernels use the gradients to update the embeddings, in order to capture the meaningful discourse relations for salience.</p><p>Since the features and KCE capture different as- pects, combining them may give superior perfor- mance. This can be done by combining the two vectors in the final linear layer:</p><formula xml:id="formula_6">f (evi, d) = Wv · ΦK (evi, V) + W f · F (evi, d) + b (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Integrating Entities into KCE</head><p>KCE is also used to model the relations between events and entities. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the entity "court" is a frame element of the event "trial"; "United States" is a frame element of the event "war". It is not clear which pair contributes more to salience. We again let KCE to learn it.</p><p>Formally, let E be the list of entities in the doc- ument, i.e. E = {en 1 , . . . , en i , . . . , en n }, where en i is the ith entity in document d. KCE extracts the kernel features about entity-event relations as follows:</p><formula xml:id="formula_7">ΦK (evi, E) = {φ1( − → evi, E), . . . ,<label>(7)</label></formula><formula xml:id="formula_8">φ k ( − → evi, E), . . . , φK ( − → evi, E)}, φ k ( − → evi, E) = en j ∈E exp − (cos( − → evi, −→ enj) − µ k ) 2 2σ 2 k<label>(8)</label></formula><p>similarly, en i is embedded by: en i Emb − −− → − → en i , which is initialized by pre-trained entity embed- dings.</p><p>We reach the full KCE model by combining all the vectors using a linear layer:</p><formula xml:id="formula_9">f (ev i , d) = W e · Φ K (ev i , E) + W v · Φ K (ev i , V) + W f · F (ev i , d) + b<label>(9)</label></formula><p>The model is again trained by equation (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Methodology</head><p>This section describes our experiment settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Event Salience Detection</head><p>Dataset: We conduct our experiments on the salience corpus described in §3. Among the 664,911 articles with abstracts, we sample 10% of the data as the test set and then randomly leave out another 10% documents for development. Over- all, there are 4359 distinct event lexical items, at a similar scale with previous work (Chambers and Ju- rafsky, 2008; Do et al., 2011). The corpus statistics are summarized in <ref type="table">Table 1</ref>. Input: The inputs to models are the documents and the extracted events. The models are required to rank the events from the most to least salience. Baselines: Three methods from previous re- searches are used as baselines: Frequency, Loca- tion and PageRank. The first two are often used to simulate saliency ( <ref type="bibr" target="#b4">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b44">Vossen and Caselli, 2015)</ref>. The Frequency baseline ranks events based on the count of the headword lemma; the Location baseline ranks events using the order of their appearances in discourse. Ties are broken randomly. Similar to entity salience ranking with PageRank scores ( <ref type="bibr" target="#b45">Xiong et al., 2018)</ref>, our PageRank baseline runs PageRank on a fully connected graph whose nodes are the events in documents. The edges are weighted by the embedding similarities between event pairs. We conduct supervised PageRank on this graph, using the same pairwise loss setup as in KCE. We report the best performance obtained by linearly combining Frequency with the scores obtained after a one-step random walk. Evaluation Metric: Since the importance of events is on a continuous scale, the boundary be- tween "important" and "not important" is vague. Hence we evaluate it as a ranking problem. The metrics are the precision and recall value at 1, 5 and 10 respectively. It is adequate to stop at 10 since there are less than 9 salient events per doc- ument on average <ref type="table">(Table 1)</ref>. We also report Area Under Curve (AUC). Statistical significance values are tested by permutation (randomization) test with p &lt; 0.05. Implementation Details: We pre-trained word embeddings with 128 dimensions on the whole Annotated New York Times corpus using Word2Vec ( <ref type="bibr" target="#b29">Mikolov et al., 2013)</ref>. Entities are ex- tracted using the TagMe entity linking toolkit <ref type="bibr" target="#b19">(Ferragina and Scaiella, 2010)</ref>. Words or entities that appear only once in training are replaced with spe- cial "unknown" tokens.</p><p>The hyper-parameters of the KCE kernels follow previous literature <ref type="bibr" target="#b46">(Xiong et al., 2017)</ref>. There is one exact match kernel (µ = 1, σ = 1e −3 ) and ten soft-match kernels evenly distributed between (−1, 1), i.e. µ ∈ {−0.9, −0.7, . . . , 0.9}, with the same σ = 0.1.</p><p>The parameters of the models are optimized by Adam ( <ref type="bibr" target="#b22">Kingma and Ba, 2015)</ref>, with batch size 128. The vectors of entities are initialized by the pre-trained embeddings. Event embeddings are initialized by their headword embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Event Intrusion Test: A Study</head><p>KCE is designed to estimate salience by modeling relations between discourse units. To better under- stand its behavior, we design the following event intrusion test, following the word intrusion test used to assess topic model quality ( <ref type="bibr" target="#b8">Chang et al., 2009)</ref>. Event Intrusion Test: The test will present to a model a set of events, including: the origins, all events from one document; the intruders, some events from another document. Intuitively, if events inside a document are organized around the core content, a model capturing their relations well should easily identify the intruder(s).</p><p>Specifically, we take a bag of unordered events {O 1 , O 2 , . . . , O p }, from a document O, as the ori- gins. We insert into it intruders, events drawn from another document, I: {I 1 , I 2 , . . . , I q }. We ask a model to rank the mixed event set M = {O 1 , I 1 , O 2 , I 2 , . . .}. We expect a model to rank the intruders I i below the origins O i . Intrusion Instances: From the development set, we randomly sample 15,000 origin and intruding document pairs. To simplify the analysis, we only take documents with at least 5 salient events. The intruder events, together with the entities in the same sentences, are added to the origin document. Metrics: AUC is used to quantify ranking quality, where events in O are positive and events in I are negative. To observe the ranking among the salient origins, we compute a separate AUC score between the intruders and the salient origins, denoted as SA- AUC. In other words, SA-AUC is the AUC score on the list with non-salient origins removed. Experiments Details: We take the full KCE model to compute salient scores for events in the mixed event set M , which are directly used for ranking. Frequency is recounted. All other features <ref type="table" target="#tab_1">(Table 2)</ref> are set to 0 to emphasize the relational aspects, We experiment with two settings: 1. adding only the salient intruders. 2. adding only the non-salient intruders. Under both settings, the intruders are added one by one, allowing us to observe the score change regarding the number of intruders added. For comparison, we add a Frequency baseline, that directly ranks events by the Frequency feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation Results</head><p>This section presents the evaluations and analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Event Salience Performance</head><p>We summarize the main results in <ref type="table" target="#tab_3">Table 3</ref>. Baselines: Frequency is the best performing base- line. Its precision at 1 and 5 are higher than 40%. PageRank performs worse than Frequency on all   the precision and recall metrics. Location performs the worst.</p><formula xml:id="formula_10">Method P@01 P@05 P@10 AUC Location 0.3555 -0.3077 -0.2505 -0.5226 - PageRank 0.3628 -0.3438 -0.3007 -0.5866 - Frequency 0.4542 -0.4024 -0.3445 -0.5732 - LeToR 0.4753 † +4.64% 0.4099 † +1.87% 0.3517 † +2.10% 0.6373 † +11.19% KCE (-EF) 0.4420 −2.69% 0.4038 +0.34% 0.3464 † +0.54% 0.6089 † +6.23% KCE (-E) 0.4861 † ‡ +7.01% 0.4227 † ‡ +5.04% 0.3603 † ‡ +4.58% 0.6541 † ‡ +14.12% KCE 0.5049 † ‡ +11.14% 0.4277 † ‡ +6.29% 0.3638 † ‡ +5.61% 0.6557 † ‡ +14.41% Method R@01 R@05 R@10 W/T/L Location 0.0807 -0.2671 -0.3792 - -/-/- PageRank 0.0758 -0.2760 -0.4163 - -/-/- Frequency 0.0792 -0.2846 -0.4270 - -/-/- LeToR 0.0836 † +5.61% 0.2980 † +4.70% 0.4454 † +4.31% 8037 / 48493 / 6770 KCE (-EF) 0.0714 −9.77% 0.2812 −1.18% 0.4321 † +1.20% 6936 / 48811 / 7553 KCE (-E) 0.0925 † ‡ +16.78% 0.3172 † ‡ +11.46% 0.4672 † ‡ +9.</formula><p>Feature Based: LeToR outperforms the baselines significantly on all metrics. Particularly, its P@1 value outperforms the Frequency baseline the most (4.64%), indicating a much better estimation on the most salient event. In terms of AUC, LeToR outperforms Frequency by a large margin (11.19% relative gain).</p><p>Feature Ablation: To understand the contribution of individual features, we conduct an ablation study of various feature settings in <ref type="table" target="#tab_4">Table 4</ref>. We gradu- ally add feature groups to the Frequency baseline. The combination of Location (sentence location) and Frequency almost sets the performance for the whole model. Adding each voting feature individu- ally produces mixed results. However, adding all voting features improves all metrics. Though the margin is small, 4 of them are statistically signifi- cant over Frequency+Location.</p><p>Kernel Centrality Estimation: The KCE model further beats LeToR significantly on all metrics, by around 5% on AUC and precision values, and by around 10% on the recall values. Notably, the P@1 score is much higher, reaching 50%. The large relative gain on all the recall metrics and the high performance on precision show that KCE works really well on the top of the rank list.</p><p>Kernel Ablation: To understand the source of per- formance gain of KCE, we conduct an ablation study by removing its components: -E removes of entity kernels; -EF removes the entity kernels and the features. We observe a performance drop in both cases. Without entities and features, the model only using event information still performs similarly to Frequency. The drops are also a reflec- tion of the small number of events (≈ 60 per docu- ment) comparing to entities (≈ 200 per document).   <ref type="figure" target="#fig_2">Fig- ure 2</ref>. Surprisingly, the salient decisions are not linearly related, nor even positively correlated to the weights. In fact, besides the "Exact Match" bin, the highest absolute weights actually appear at 0.3 and -0.3. This implies that embedding sim- ilarities do not directly imply salience, breaking some assumptions of the feature based model and PageRank. Case Study: We inspect some pairs of events and entities in different kernels and list some ex- amples in <ref type="table" target="#tab_6">Table 5</ref>. The pre-trained embeddings are changed a lot. Pairs of units with different raw similarity values are now placed in the same bin. The pairs in <ref type="table" target="#tab_3">Table 3</ref> exhibit interesting types of relations: e.g.,"arrest-charge" and "attack-kill" form script-like chains; "911 attack" forms a quasi- identity relation <ref type="bibr" target="#b38">(Recasens et al., 2010</ref>) with "at- tack"; "business" and "increase" are candidates as frame-argument structure. While these pairs have different raw cosine similarities, they are all useful in predicting salience. KCE learns to gather these relations into bins assigned with higher weights, which is not achieved by pure embedding based methods. The KCE has changed the embedding space and the scoring functions significantly from the original space after training. This partially ex- plains why the raw voting features and PageRank are not as effective. The left figure shows that KCE successfully finds the non-salient intruders. The SA-AUC is higher than 0.8. Yet the AUC scores, which include the rankings of non-salience events, are rather close to random. This shows that the salient events in the origin documents form a more cohesive group, making them more robust against the intruders; the non-salient ones are not as cohesive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Intrusion Test Results</head><p>In both settings, KCE produces higher SA-AUC than Frequency at the first 30%. However, in set- ting 2, KCE starts to produce lower SA-AUC than Frequency after 30%, then gradually drops to 0.5 (random). This phenomenon is expected since the asymmetry between origins and intruders allow KCE to distinguish them at the beginning. When all intruders are added, KCE performs worse be- cause it relies heavily on the relations, which can be also formed by the salient intruders. This phe- nomenon is observed only on the salient intruders, which again confirms the cohesive relations are found among salient events.</p><p>In conclusion, we observe that the salient events form tight groups connected by discourse rela- tions while the non-salient events are not as related. The observations imply that the main scripts in documents are mostly anchored by small groups of salient events (such as the "Trial" script in Example 1). Other events may serve as "back- grounds" <ref type="bibr" target="#b10">(Cheung et al., 2013</ref>). Similarly, <ref type="bibr" target="#b11">Choubey et al. (2018)</ref> find that relations like event corefer- ence and sequence are important for saliency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We propose two salient detection models, based on lexical relatedness and semantic relations. The feature-based model with lexical similarities is ef- fective, but cannot capture semantic relations like scripts and frames. The KCE model uses kernels and embeddings to capture these relations, thus outperforms the baselines and feature-based mod- els significantly. All the results are tested on our newly created large-scale event salience dataset. While the automatic method inevitably introduces noises to the dataset, the scale enables us to study complex event interactions, which is infeasible via costly expert labeling.</p><p>Our case study shows that the salience model finds and utilize a variety of discourse relations: script chain (attack and kill), frame argument re- lation (business and increase), quasi-identity (911 attack and attack). Such complex relations are not as prominent in the raw word embedding space. The core message is that a salience detection mod- ule automatically discovers connections between salience and relations. This goes beyond prior cen- tering analysis work that focuses on lexical and syntax and provide a new semantic view from the script and frame perspective.</p><p>In the intrusion test, we observe that the small number of salient events are forming tight con- nected groups. While KCE captures these relations quite effectively, it can be confused by salient in- trusion events. The phenomenon indicates that the salient events are tightly connected, which form the main scripts of documents.</p><p>This paper empirically reveals many interest- ing connections between discourse phenomena and salience. The results also suggest that core script information may reside mostly in the salient events. Limited by the data acquisition method, this paper only models discourse salience as binary decisions. However, salience value may be continuous and may even have more than one aspects. In the fu- ture, we plan to investigate these complex settings. Another direction of study is large-scale semantic relation discovery, for example, frames and scripts, with a focus on salient discourse units.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples annotations. Underlying words are annotated event triggers; the red bold ones are annotated as salient.</figDesc><graphic url="image-1.png" coords="1,307.28,222.54,218.27,126.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>embed an event into vector space: ev i Emb − −− → − → ev i . The embedding function is initialized with pre- trained embeddings. It then extract K features for each ev i :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learned Kernel Weights of KCE</figDesc><graphic url="image-2.png" coords="8,72.00,566.16,218.26,116.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 plots</head><label>3</label><figDesc>Figure 3 plots results of the intrusion test. The left figure shows the results of setting 1: adding nonsalient intruders. The right one shows the results of setting 2: adding salient intruders. The AUC is 0.493 and the SA-AUC is 0.753 if all intruders are added. The left figure shows that KCE successfully finds the non-salient intruders. The SA-AUC is higher than 0.8. Yet the AUC scores, which include the rankings of non-salience events, are rather close to random. This shows that the salient events in the origin documents form a more cohesive group, making them more robust against the intruders; the non-salient ones are not as cohesive. In both settings, KCE produces higher SA-AUC than Frequency at the first 30%. However, in setting 2, KCE starts to produce lower SA-AUC than Frequency after 30%, then gradually drops to 0.5 (random). This phenomenon is expected since the asymmetry between origins and intruders allow KCE to distinguish them at the beginning. When all intruders are added, KCE performs worse because it relies heavily on the relations, which can be also formed by the salient intruders. This phenomenon is observed only on the salient intruders, which again confirms the cohesive relations are found among salient events. In conclusion, we observe that the salient events form tight groups connected by discourse relations while the non-salient events are not as related. The observations imply that the main scripts in documents are mostly anchored by small groups of salient events (such as the "Trial" script in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Intruder study results. X-axis shows the percentage of intruders inserted. Y-axis is the AUC score scale. The left and right figures are results from salient and non-salient intruders respectively. The blue bar is AUC. The orange shaded bar is SA-AUC. The line shows the SA-AUC of the frequency baseline.</figDesc><graphic url="image-3.png" coords="9,72.00,62.81,222.24,133.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Event Salience Features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Event salience performance. (-E) and (-F) marks removing Entity information and Features from 
the full KCM model. The relative performance differences are computed against Frequency. W/T/L 
are the number of documents a method wins, ties, and loses compared to Frequency.  † and  ‡ mark the 
statistically significant improvements over Frequency  † , LeToR  ‡ respectively. 

Feature Groups 
P@1 
P@5 
P@10 
R@1 
R@5 
R@10 
AUC 

Loc 
0.3548 
0.3069 
0.2497 
0.0807 0.2671 0.3792 
0.5226 
Frequency 
0.4536 
0.4018 
0.3440 
0.0792 0.2846 0.4270 
0.5732 

+ Loc 
0.4734 
0.4097 
0.3513 
0.0835 0.2976 0.4436 
0.6354 
+ Loc + Event 
0.4726 
0.4101  † 0.3516 
0.0831 0.2969 0.4431 
0.6365  † 
+ Loc + Entity 
0.4739 
0.4100 
0.3518 
0.0812 0.2955 0.4418 
0.6374 
+ Loc + Entity + Event 
0.4739 
0.4100 
0.3518  † 0.0832 0.2974 0.4452  † 0.6374  † 
+ Loc + Entity + Event + Local 0.4754  † 0.4100 
0.3517  † 0.0837 0.2981 0.4454  † 0.6373  † 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Feature Ablation Results. + sign indicates the additional features to Frequency. Loc is the 
sentence location feature. Event is the event voting feature. Entity is the entity voting feature. Local 
is the local entity voting feature.  † marks the statistically significant improvements over + Loc. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Similarities between event entity pairs. 
Word2vec shows the cosine similarity in pre-
trained embeddings. KCE lists their closest kernel 
mean after training. (E) marks entities. 

The study indicates that the relational signals and 
features contain different but both important infor-
mation. 
Discussion: The superior results of KCE demon-
strate its effectiveness in predicting salience. So 
what additional information does it capture? We re-
visit the changes made by KCE: 1. it adjusts the em-
beddings during training. 2. it introduces weighted 
soft count kernels. However, the PageRank base-
line also does embedding tuning but produces poor 
results, thus the second change should be crucial. 
We plot the learned kernel weights of KCE in </table></figure>

			<note place="foot" n="3"> Light verbs carry little semantic information: &quot;appear&quot;, &quot;be&quot;, &quot;become&quot;, &quot;do&quot;, &quot;have&quot;, &quot;seem&quot;, &quot;do&quot;, &quot;get&quot;, &quot;give&quot;, &quot;go&quot;, &quot;have&quot;, &quot;keep&quot;, &quot;make&quot;, &quot;put&quot;, &quot;set&quot;, &quot;take&quot;. 4 Reporting verbs are normally associated with the narrator: &quot;argue&quot;, &quot;claim&quot;, &quot;say&quot;, &quot;suggest&quot;, &quot;tell&quot;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported by DARPA grant FA8750-18-2-0018 funded under the AIDA pro-gram and National Science Foundation (NSF) grant IIS-1422676. Any opinions, findings, and conclu-sions in this paper are the authors and do not nec-essarily reflect the sponsors. We thank the anony-mous reviewers whose suggestions helped clarify this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting Subevent Structure for Event Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4553" to="4558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<title level="m">The berkeley framenet project. Proceeding ACL &apos;98 Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating Coherent Event Schemas at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E</forename><surname>Mausam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic Coreference-Based Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas S Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Empirical Methods in Natural Language Processing (EMNLP-3)</title>
		<meeting>the Third Conference on Empirical Methods in Natural Language Processing (EMNLP-3)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling Local Coherence: An Entity-Based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the ACL</title>
		<meeting>the 43rd Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Rich Event Ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Susan Windisch Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Obrst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Events and Stories in the News Workshop</title>
		<meeting>the Events and Stories in the News Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dense Event Ordering with a Multi-Pass Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Narrative Event Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL &apos;08 Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading Tea Leaves: How Humans Interpret Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="288" to="296" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Implicit Argument Prediction with Event Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic Frame Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jc Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Identifying the Most Dominant Event in a News Article by Mining Event Coreference Relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Prafulla Kumar Choubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A coefficient of agreement for nominal scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational and Psychological Measurement</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="46" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-Supervised Frame-Semantic Parsing for Unknown Predicates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT &apos;11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1435" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Minimally supervised event causality identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Quang Xuan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;11 Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint Inference for Event Timeline Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Quang Xuan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL &apos;12 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="677" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Crowdsourced corpus with entity salience annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dojchinovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kliegr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vitvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation, LREC</title>
		<meeting>the 10th International Conference on Language Resources and Evaluation, LREC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3307" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A New Entity Salience Task with Millions of Training Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dunietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Association for Computational Linguistics</title>
		<meeting>the European Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="205" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LexRank : Graph-based Lexical Centrality as Salience in Text Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2010</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Thread of Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Evans</forename><surname>Grimes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Halliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruqaiya</forename><surname>Hasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
		<respStmt>
			<orgName>Cohesion in English</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint Event Extraction via Structured Prediction with Global Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised Within-Document Event Coreference using Information Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4539" to="4544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint Learning for Event Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="90" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP Natural Language Processing Toolkit. Proceedings of 52nd Annual Meeting of the ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discourse Trees are Good Indicators of Importance in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Automatic Text Summarization</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="123" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2tth Advances in Neural Information Processing Systems 2013 (NIPS 2013)</title>
		<meeting>the 2tth Advances in Neural Information Processing Systems 2013 (NIPS 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overview of TAC KBP 2015 Event Nugget Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TAC KBP 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lexical cohesion computed by thesaural relations as an indicator of the structure of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="21" to="48" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Event Detection and Domain Adaptation with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="365" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Event Detection and Co-reference with Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using Sentence-Level LSTM Language Models for Script Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="279" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">SWAT: A System for Detecting Salient Wikipedia Entities in Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ponza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Saurí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Setzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename></persName>
		</author>
		<title level="m">The TIMEBANK Corpus. Natural Language Processing and Information Systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4592</biblScope>
			<biblScope unit="page" from="647" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Same Referent, Different Words: Unsupervised Mining of Opaque Coreferent Mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Typology of Near-Identity Relations for Coreference (NIDENT)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antònia Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Language Resources and Evaluation (LREC-2010)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to predict script events from domainspecific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Fourth Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The New York Times Annotated Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scripts, Plans, Goals and Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<publisher>Lawrence Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive Method of Automatic Abstracting and Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skorochod&amp;apos;ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IFIP Congress</title>
		<meeting>the IFIP Congress</meeting>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Verbs and times. The Philosophical Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Vendler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="143" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Storylines for structuring massive streams of news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piek</forename><surname>Vossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computing News Story Lines</title>
		<meeting>the First Workshop on Computing News Story Lines</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">End-to-End Neural Ad-hoc Ranking with Kernel Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Exploiting Parallel News Streams for Unsupervised Event Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="117" to="129" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
