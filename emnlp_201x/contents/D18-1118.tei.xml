<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Mutual Modulation for Visual Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Research Center for Brain-inspired Intelligence</orgName>
								<orgName type="institution">CASIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<country>CAS. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Mutual Modulation for Visual Reasoning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="975" to="980"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>975</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Visual reasoning is a special visual question answering problem that is multi-step and compositional by nature, and also requires intensive text-vision interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end visual reasoning model. CMM includes a multi-step comprehension process for both question and image. In each step, we use a Feature-wise Linear Modulation (FiLM) technique to enable textual/visual pipeline to mutually control each other. Experiments show that CMM significantly out-performs most related models, and reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR, collected from both synthetic and natural languages. Ab-lation studies confirm that both our multi-step framework and our visual-guided language modulation are critical to the task. Our code is available at https://github. com/FlamingHorizon/CMM-VR.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is a challenging task in artificial intelligence to perform reasoning with both textual and visual in- puts. Visual reasoning task is designed for re- searches in this field. It is a special visual ques- tion answering (VQA) ( <ref type="bibr" target="#b2">Antol et al., 2015</ref>) prob- lem, requiring a model to infer the relations be- tween entities in both image and text, and gen- erate a textual answer to the question correctly. Unlike other VQA tasks, questions in visual rea- soning often contain extensive logical phenomena, and refer to multiple entities, specific attributes and complex relations. Visual reasoning datasets such as CLEVR ( <ref type="bibr" target="#b9">Johnson et al., 2017a</ref>) and NLVR ( <ref type="bibr" target="#b18">Suhr et al., 2017</ref>) are built on unbiased, synthetic images, with either complex synthetic questions or natural-language descriptions, facilitating in- depth analyses on reasoning ability itself. Most previous visual reasoning models focus on using the question to guide the multi-step comput- ing on visual features (which can be defined as a image-comprehension "program"). Neural Mod- ule Networks (NMN) ( <ref type="bibr">Andreas et al., 2016a,b)</ref> and Program Generator + Execution Engine (PG+EE) <ref type="bibr" target="#b10">(Johnson et al., 2017b</ref>) learn to com- bine specific image-processing modules, guided by question semantics. Feature-modulating meth- ods like FiLM <ref type="bibr" target="#b4">(De Vries et al., 2017;</ref><ref type="bibr" target="#b14">Perez et al., 2018</ref>) control image-comprehension process using modulation-parameters generated from the ques- tion, allowing models to be trained end-to-end. However, the image-comprehension program in visual reasoning tasks can be extremely long and sophisticated. Using a single question represen- tation to generate or control the whole image- comprehension process raises difficulties in learn- ing. Moreover, since information comes from multiple modalities, it is not intuitive to assume that one (language) is the "program generator", and the other (image) is the "executor". One way to avoid making this assumption is to perform multiple steps of reasoning with each modality being generator and executor alternately in each step. For these two reasons, we propose Cascaded Mutual Modulation <ref type="figure" target="#fig_0">(Figure 1</ref>), a novel visual rea- soning model to solve the problem that previous "program-generating" models lack a method to use visual features to guide multi-step reasoning on language logics. CMM reaches state-of-the- arts on two benchmarks: CLEVR (complex syn- thetic questions) and NLVR (natural-language). <ref type="bibr" target="#b14">Perez et al. (2018)</ref> proposed FiLM as an end- to-end feature-modulating method. The orig- inal ResBlock+GRU+FiLM structure uses sin- gle question representation, and conditions all image-modulation-parameters on it, without suf- ficiently handling multi-step language logics. In contrast, we modulate both image and language features alternately in each step, and condition the modulation-parameters on the representations from previous step. We design an image-guided language attention pipeline and use it in combina- tion with FiLM in our CMM framework, and sig- nificantly outperform the original structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Other widely-cited works on CLEVR/NVLR include Stacked Attention Networks (SAN) ( <ref type="bibr" target="#b21">Yang et al., 2016</ref>  <ref type="bibr" target="#b16">(Santoro et al., 2017)</ref>. The recent CAN model <ref type="bibr" target="#b7">(Hudson and Manning, 2018</ref>) also uses multiple question representations and has strong performances on CLEVR. However, these representations are not modulated by the visual part as in our model. In other VQA tasks, <ref type="bibr">DAN (Nam et al., 2017</ref>) is the only multi-step dual framework related to ours. For comparison, in every time step, DAN computes textual and visual attention in parallel with the same key-vector, while we perform tex- tual attention and visual modulation (instead of at- tention) in a cascaded manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We review and extend FiLM in Section 3.1-3.2, and introduce CMM model in Section 3.3-3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visual Modulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perez et al. (2018) proposed Feature-wise Linear Modulation (FiLM), an affine transformation on intermediate outputs of a neural network (v stands for visual):</head><formula xml:id="formula_0">F iLM v (F i,c |γ i,c , β i,c ) = γ i,c F i,c + β i,c , (1)</formula><p>where <ref type="formula">F</ref>  </p><formula xml:id="formula_1">γ i , β i = M LP i (q i−1 ).<label>(2)</label></formula><p>MLP stands for fully connected layers with lin- ear activations. The weights and biases are not shared among all steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Modulation</head><p>In each step i, we also apply FiLM to modulate every language "feature map". If the full question representation is a D × T matrix, a question "fea- ture map" f i,d is defined as a 1 × T vector gather- ing T features along a single dimension. D is the hidden-state dimension of language encoder, and T is a fixed maximum length of word sequences.</p><formula xml:id="formula_2">F iLM l (f i,d |γ i,d , β i,d ) = γ i,d f i,d + β i,d , (3)</formula><p>where l stands for language. Concatenated modulation-parameters γ i and β i (D × 1) are con- ditioned on the visual features V i computed in the same step:</p><formula xml:id="formula_3">γ i , β i = g m (V i ),<label>(4)</label></formula><p>where g m (Section 3.4) is an interaction func- tion that converts 3-d visual features to language- modulation-parameters. The weights in g m are shared among all N steps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cascaded Mutual Modulation</head><p>The whole pipeline of our model is built up with multiple steps. In each step i (N in total), previous question vector q i−1 and the visual features V i−1 are taken as input; q i and V i are computed as out- put. Preprocessed questions/images are encoded by language/visual encoders to form q 0 and V 0 .</p><p>In each step, we cascade a FiLM-ed ResBlock with a modulated textual-attention. We feed V i−1 into the ResBlock modulated by parameters from q i−1 to compute V i , and then control the tex- tual attention process with modulation-parameters from V i , to compute the new question vector q i . <ref type="figure" target="#fig_3">(Figure 2</ref>, middle).</p><p>Each ResBlock contains a 1 × 1 convolution, a 3 × 3 convolution, a batch-normalization (Ioffe and Szegedy, 2015) layer before FiLM modula- tion, followed by a residual connection ( . <ref type="figure" target="#fig_3">(Figure 2</ref>, right. We keep the same Res- Block structure as ( <ref type="bibr" target="#b14">Perez et al., 2018)</ref>). To be con- sistent with (Johnson et al., 2017b; <ref type="bibr" target="#b14">Perez et al., 2018)</ref>, we concatenate the input visual features V i−1 of each ResBlock i with two "coordinate feature maps" scaled from −1 to 1, to enrich rep- resentations of spatial relations. All CNNs in our model use ReLU as activation functions; batch- normalization is applied before ReLU.</p><p>After the ResBlock pipeline, we apply lan- guage modulation on the full language features {h 1 , ..., h T } (D × T ) conditioned on V i and rewrite along the time dimension, yielding:</p><formula xml:id="formula_4">e i,t = F iLM l (h t |g m (V i )),<label>(5)</label></formula><p>and compute visual-guided attention weights:</p><formula xml:id="formula_5">α i,t = softmax t (W att i e i,t + b att i ),<label>(6)</label></formula><p>and weighted summation over time:</p><formula xml:id="formula_6">q i = T t=1 α i,t h t .<label>(7)</label></formula><p>In equation <ref type="formula" target="#formula_5">(6)</ref>, W att i ∈ R 1×D and b att i ∈ R 1×1 are network weights and bias; h t is the t-th lan- guage state vector (D × 1), computed using a bi- directional GRU ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>) from word embeddings {w 1 , ..., w T }. In each step i, the lan- guage pipeline does not re-compute h t , but re- modulate it as e i,t instead. <ref type="figure" target="#fig_3">(Figure 2</ref>, top.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Feature Projections</head><p>We use a function g p to project the last visual fea- tures V N into a final representation:</p><formula xml:id="formula_7">u f inal = g p (V N ).<label>(8)</label></formula><p>g p includes a convolution with K 1 × 1 ker- nels, a batch-normalization afterwards, followed by global max pooling over all pixels (K = 512).</p><p>We also need a module g m (equation <ref type="formula" target="#formula_3">(4)</ref>) to compute language-modulations with V i , since V i is 3-d features (not a weighted-summed vector as in traditional visual-attention). We choose g m to have the same structure as g p , except that K equals to the total number of modulation-parameters in each step. This design is critical (Section 4.3).</p><p>We use a fully connected layer with 1024 ReLU hidden units as our answer generator. It takes u f inal as input, and predicts the most probable an- swer in the answer vocabulary.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We are the first to achieve top results on both datasets (CLEVR, NLVR) with one structure. See Appendix for more ablation and visualization re- sults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CLEVR</head><p>CLEVR ( <ref type="bibr" target="#b9">Johnson et al., 2017a</ref>) is a commonly- used visual reasoning benchmark containing 700,000 training samples, 150,000 for validation and test. Questions in CLEVR cover several typ- ical elements of reasoning: counting, comparing, querying the memory, etc. Many well-designed models on VQA have failed on CLEVR, revealing the difficulty to handle the multi-step and compo- sitional nature of logical questions.</p><p>On CLEVR dataset, we embed the question words into a 200-dim continuous space, and use a bi-directional GRU with 512 hidden units to gen- erate 1024-dim question representations. Ques- tions are padded with NULL token to a maximum length T = 46. As the first-step question vector in CMM, q 0 can be arbitrary RNN hidden state in the set {h 1 , ..., h T } <ref type="figure">(Section 3.3)</ref>. We choose the one at the end of the unpadded question.</p><p>In each ResBlock, the feature map number C is set to 128. Images are pre-processed with a ResNet101 network pre-trained on ImageNet ( <ref type="bibr" target="#b15">Russakovsky et al., 2015</ref>) to extract 1024 × 14 × 14 visual features (this is also common practice on CLEVR). We use a trainable one-layer CNN with 128 kernels (3×3) to encode the extracted features into V 0 <ref type="bibr">(128 × 14 × 14)</ref>. Convolutional paddings are used to keep the feature map size to be 14 × 14 through the visual pipeline.</p><p>We train the model with an ADAM ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) optimizer using a learning rate of 2.5e-4 and a batch-size of 64 for about 90 epoches, and switch to an SGD with the same learning rate and 0.9 momentum, fine-tuning for another 20 epoches. SGD generally brings around 0.3 points gains to CMM on CLEVR.</p><p>We achieve 98.6% accuracy with single model (4-step), significantly better than FiLM and other related work, only slightly lower than CAN, but CAN needs at least 8 model-blocks for &gt;98% (and 12 for best). We achieve state-of-the-art of 99.0% with ensemble of 4/5/6 step CMM models. <ref type="table">Table  1</ref> shows test accuracies on all types of questions. The main improvements over program-generating models come from "Counting" and "Compare Numbers", indicating that CMM framework sig- nificantly enhances language (especially numeric) reasoning without sophisticated memory design like CAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NLVR</head><p>NLVR ( <ref type="bibr" target="#b18">Suhr et al., 2017</ref>) is a visual reason- ing dataset proposed by researchers in NLP field. NLVR has 74,460 samples for training, 5,940 for validation and 5,934 for public test. In each sam- ple, there is a human-posed natural language de- scription on an image with 3 sub-images, and re- quires a false/true response.</p><p>We use different preprocessing methods on NLVR. Before training, we reshape NLVR images into 14 × 56 raw pixels and use them directly as visual inputs V 0 . For language part, we correct some obvious typos among the rare words (fre- quency &lt; 5) in the training set, and pad the sen- tences to a maximum length of 26. Different from CLEVR, LSTM works better than GRU on the real-world questions. For training, we use ADAM with a learning rate of 3.5e-4 and a batch-size of 128 for about 200 epoches, without SGD fine- tuning.</p><p>Our model (3-step, 69.9%) outperforms all pro- posed models on both validation and public test set, showing that CMM is also suitable for real- world languages <ref type="table" target="#tab_2">(Table 2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We list CMM ablation results in  <ref type="table" target="#tab_3">Table 3</ref>: Ablation studies on CLEVR/NLVR. g m -CNN means using 2-layer-CNN with 3 × 3 kernels, followed by concatenation and MLP, as g m . BN means batch- normalization in g m . NS means not sharing weights. "FiLM-hyp" uses all the same hyper-parameters as the 3-step CMM (both use GRU as question encoder).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">A Case Study</head><p>We select an image-question pair from the vali- dation set of CLEVR for visualization. In <ref type="table" target="#tab_5">Table  4</ref>, we visualize the multi-step attention weights on question words, and the distribution of argmax po- sition in the global max-pooling layer of g p (equiv- According to NLVR rules, we will run on the unreleased test set (Test-U) in the near future.  alent to the last visual "attention map" although there isn't explicit visual attention in our image- comprehension pipeline). On the bottom right is the original image, and on the top right is the dis- tribution of argmax positions in the global max- pooling, multiplied by the original image. Our model attends to phrases "same shape as" and "brown object" in the first two reasoning steps. These phrases are meaningful because "same shape as" is the core logic in the question, and "brown object" is the key entity to generat- ing the correct answer. In the last step, the model attends to the phrase "is there". This implicitly classifies the question into question-type "exist", and directs the answer generator to answer "no" or "yes". The visual map, guided by question-based modulation parameters, concentrates on the green and brown object correctly.</p><p>The result shows that visual features can guide the comprehension of question logics with textual modulation. On the other hand, question-based modulation parameters enable the ResBlocks to filter out irrelative objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose CMM as a novel visual reasoning model cascading visual and textual modulation in each step. CMM reaches state-of-the-arts on visual reasoning benchmarks with both synthetic and real-world languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Connections and differences between previous "program-generating" works and our model: other models generate/control multi-step image-comprehension processes with single question representation, while we put more attention on language logics and let multi-modal information modulate each other in each step. The question and image are taken as a visual-reasoning example from CLEVR dataset.</figDesc><graphic url="image-1.png" coords="1,329.95,222.54,172.90,124.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>), NMN (Andreas et al., 2016b), N2NMN (Hu et al., 2017), PG+EE (Johnson et al., 2017b) and Relation Networks (RN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>i,c is the c-th feature map (C in to- tal) generated by Convolutional Neural Networks (CNN) in the i-th image-comprehension step. Modulation-parameters γ i,c and β i,c can be condi- tioned on any other part of network (in their work the single question representation q). If the output tensor V i of a CNN block is of size C × H × W , then F i,c is a single slice of size 1×H ×W . H and W are the height and width of each feature map. Unlike (Perez et al., 2018), in each step i, we compute a new question vector q i . Modulation- parameters γ i and β i (C × 1 vectors, γ i = [γ i,1 , ...,γ i,C ], etc.) are conditioned on the previ- ous question vector q i−1 instead of a single q:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Details in CMM step i (middle), with a modulated ResBlock (right) and a modulated textual attention pipeline (top). Visual and textual features modulate each other in each step to compute new representations.</figDesc><graphic url="image-2.png" coords="3,149.95,62.81,297.65,157.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Model 
Dev Test-P 
Text only 
56.6 
57.2 
Image only 
55.4 
56.1 
CNN+RNN (Suhr et al., 2017) 
56.6 
58.0 
NMN (Andreas et al., 2016b) 
63.1 
66.1 
FiLM (our run) 
59.0 
61.3 
CNN-BiAtt (Tan and Bansal, 2018) 66.9 
69.7 
CMM-3-steps (ours) 
68.0 
69.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Accuracies on valid and test set of NLVR.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Abla-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Visualization of CMM intermediate outputs 
on a sample from CLEVR validation set. We colour 
the largest attention weight with dark gray, and top four 
attention weights in the rest with light gray. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the reviewers for their insightful com-ments. This work is supported by the National Natural Science Foundation of China <ref type="formula">(61602479)</ref> and the Strategic Priority Research Program of the Chinese Academy of Sciences (XDBS01070000).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6597" to="6607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno>abs/1704.05526</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1988" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transparency by design: Closing the gap between performance and interpretability in visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mascharka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soklaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention networks for multimodal reasoning and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="299" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11361</idno>
		<title level="m">Ddrprog: A clevr differentiable dynamic reasoning programmer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object ordering with bidirectional matchings for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A dataset and architecture for visual reasoning with a working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Guangyu Robert Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jing</forename><surname>Ganichev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sussillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06092</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
