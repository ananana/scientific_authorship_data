<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Natural Language Does Not Emerge &apos;Naturally&apos; in Multi-Agent Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>2 Virginia Tech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>2 Virginia Tech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Natural Language Does Not Emerge &apos;Naturally&apos; in Multi-Agent Dialog</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2962" to="2967"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task &amp; Talk reference game between two agents as a testbed, we present a sequence of &apos;negative&apos; results culminating in a &apos;positive&apos; one-showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not inter-pretable or compositional. In essence, we find that natural language does not emerge &apos;naturally&apos;, despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One fundamental goal of artificial intelligence (AI) is the development of goal-driven dialog agents -specifically, agents that can perceive their environment (through vision, audition, or other sensors), and communicate with humans or other agents in natural language towards a goal.</p><p>While historically such agents have been based on slot filling ( <ref type="bibr" target="#b12">Lemon et al., 2006</ref>), the domi- nant paradigm today is neural dialog models <ref type="bibr" target="#b1">(Bordes and Weston, 2016;</ref><ref type="bibr" target="#b22">Weston, 2016;</ref><ref type="bibr" target="#b18">Serban et al., 2016a</ref>,b) trained on large quantities of data.</p><p>Perhaps somewhat counterintuitively, this current paradigm treats dialog as a static supervised learn- ing problem, rather than as the interactive agent learning problem that it naturally is. Specifi- cally, a typical pipeline is to collect a large dataset of human-human dialog ( <ref type="bibr" target="#b14">Lowe et al., 2015;</ref><ref type="bibr" target="#b3">Das et al., 2017a;</ref><ref type="bibr" target="#b5">de Vries et al., 2017;</ref><ref type="bibr" target="#b16">Mostafazadeh et al., 2017)</ref>, inject a machine in the middle of a di- alog from the dataset, and supervise it to mimic the human response. While this teaches the agent cor- relations between symbols, it does not convey the functional meaning of language, grounding (map- ping physical concepts to words), compositional- ity (combining knowledge of simpler concepts to describe richer concepts), or aspects of planning (why are we having this conversation?).</p><p>An alternative paradigm that has a long history <ref type="bibr" target="#b24">(Winograd, 1971;</ref><ref type="bibr" target="#b10">Kirby et al., 2014</ref>) and is wit- nessing a recent resurgence ( <ref type="bibr" target="#b21">Wang et al., 2016;</ref><ref type="bibr" target="#b6">Foerster et al., 2016;</ref><ref type="bibr" target="#b20">Sukhbaatar et al., 2016;</ref><ref type="bibr" target="#b8">Jorge et al., 2016;</ref><ref type="bibr" target="#b11">Lazaridou et al., 2017;</ref><ref type="bibr" target="#b7">Havrylov and Titov, 2017;</ref><ref type="bibr">Das et al., 2017b</ref>) -is situated language learning. A number of recent works have proposed reinforce- ment learning techniques for learning the com- munication protocols of agents situated in virtual environments in a completely end-to-end man- ner -from perceptual input (e.g. pixels) to com- munication (discrete symbols without any pre- specified meanings) to action (e.g. signaling in ref- erence games or navigating in an environment) -and have simultaneously found the emergence of grounded human-interpretable (often composi- tional) language among agents, without any hu- man supervision or pretraining, simply to succeed at the task.</p><p>In this short paper, we study the following ques- tion -what are the conditions that lead to the emergence of human-interpretable or composi- tional grounded language? Our key finding is that  natural language does not emerge 'naturally' in multi-agent dialog, despite independently reported successful demonstrations in recent literature.</p><p>Specifically, in a sequence of 'negative' results culminating in a 'positive' one, we find that while agents always successfully invent communication protocols and languages to achieve their goals with near-perfect accuracies, the invented lan- guages are decidedly not compositional, inter- pretable, or 'natural'; and that it is possible to coax the invented languages to become more and more human-like and compositional by increasing re- strictions on how two agents may communicate.</p><p>Related work and novelty. The starting point for our investigation is the recent work of <ref type="bibr">Das et al. (2017b)</ref>, who proposed a cooperative refer- ence game between two agents, where communi- cation is necessary to accomplish the goal due to an information asymmetry. Our key contribution over <ref type="bibr">Das et al. (2017b)</ref> is an exhaustive study of the conditions that must be present before com- positional grounded language emerges, and subtle but important differences in execution -tabular Q- Learning (which does not scale) vs. REINFORCE (which does), and generalization to novel environ- ments (not studied in prior work). In the spirit of , we hope our findings shed more light into the interpretability of languages in- vented in cooperative multi-agent settings, place recent work in appropriate context, and inform fruitful directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Task &amp; Talk Game</head><p>Our testbed is a reference game <ref type="bibr">(Task &amp; Talk)</ref> be- tween two agents, Q-BOT and A-BOT. The game is grounded in a synthetic world of objects com- prised of three attributes -color, style, and shape -each with four possible values for a total of 4 × 4 × 4 = 64 objects. <ref type="figure" target="#fig_1">Fig. 1a</ref> shows some ex- ample instances from this set.</p><p>Task &amp; Talk plays out over multiple rounds of dia- log. At the start, A-BOT is given an instance (e.g.</p><p>(green, dotted, square)) unseen by Q-BOT, and Q- BOT is assigned a task G (unknown to A-BOT) consisting of two attributes for Q-BOT to discover from A-BOT (e.g. (color, style)). For two rounds, Q-BOT and A-BOT exchange utterances from fi- nite vocabularies V Q and V A , with Q-BOT speak- ing first. The game culminates in Q-BOT guessing a pair of attribute values (e.g. (green, dotted)) and both agents are rewarded identically based on the accuracy of this prediction.</p><p>Note that the Task &amp; Talk game setting involves an informational asymmetry between the agents -A- BOT sees the object while Q-BOT does not; sim- ilarly Q-BOT knows the task while A-BOT does not. Thus, a two-way communication is neces- sary for success. Without this asymmetry, A-BOT could simply convey the target attributes from the task without Q-BOT having to speak. Such a set- ting has been widely studies in economics and game theory as the classic Lewis Signaling (LS) game <ref type="bibr" target="#b13">(Lewis, 2008)</ref>. By necessitating dialog be- tween agents, we are able ground both V A and V Q in our final setting (Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modeling Q-BOT and A-BOT</head><p>We formalize Q-BOT and A-BOT as agents oper- ating in a partially observable world and optimize their policies using deep reinforcement learning.</p><p>States and Actions. Each agent observes its own input (task G for Q-BOT and object in- stance I for A-BOT) and the output of the other agent as a stochastic environment. At the beginning of round t, Q-BOT observes state s t Q =[G, q 1 , a 1 , . . . , q t−1 , a t−1 ] and acts by utter- ing some token q t from its vocabulary V Q . Simi- larly, A-BOT observes the history and this new ut- terance as state s t A =[I, q 1 , a 1 , . . . , q t−1 , a t−1 , q t ] and emits a response a t from V A . At the last round, Q-BOT takes a final action by predicting a pair of</p><formula xml:id="formula_0">attribute valuesˆwvaluesˆ valuesˆw G = ( ˆ w G 1 , ˆ w G 2 )</formula><p>to solve the task. Cooperative Reward. Both Q-BOT and A-BOT are rewarded identically based on the accuracy of Q-BOT's predictionˆwpredictionˆ predictionˆw G , receiving a positive re- ward of R=1 if the prediction matches ground truth and a negative reward of R=−10 otherwise. We arrive at these values empirically.</p><p>Policy Networks.</p><p>We model Q-BOT and A-BOT as operating under stochastic policies π Q (q t |s Q t ; θ Q ) and π A (a t |s A t ; θ A ) respectively, which we instantiate as LSTM-based models. We use lower case characters (e.g. s Q t ) to denote the strings (e.g. Q-BOT's token at round t), and up- per case S Q t to denote the corresponding vector as encoded by the model. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, Q-BOT is modeled with three modules -speaking, listening, and prediction. The task G is received as a 6-dimensional one-hot en- coding over the space of possible tasks and em- bedded via the listener LSTM. At each round t, the speaker network models the probability of output utterances q t ∈ V Q based on the state S Q t−1 . This is modeled as a fully-connected layer followed by a softmax that transforms S Q t−1 to a distribution over V Q . After receiving the reply a t from A-BOT, the listener LSTM updates the state by processing both tokens of the dialog exchange. In the final round, the prediction LSTM is unrolled twice to produce Q-BOT's prediction based on the final state S Q T and the task G. As before, task G is fed in one-hot to the prediction LSTM for two time steps, resulting in a pair of outputs used as the predictionˆwpredictionˆ predictionˆw G .</p><p>Analogously, A-BOT is modeled as a combination of a speaker network, a listener LSTM, and an in- stance encoder. Like in Q-BOT, the speaker net- work models the probability of utterances a t ∈ V A given the state S A t and the listener LSTM updates the state S A t based on dialog exchanges. The in- stance encoder embeds each one-hot attribute vec- tor via a linear layer and concatenates all three en- codings to obtain a unified instance representation.</p><p>Learning Policies with REINFORCE. We train these models using the popular REINFORCE (Williams, 1992) policy gradient algorithm. Note that while the game is fully-cooperative, we do not assume full observability of one agent by another, opting instead to treat one agent as part of the un- known stochastic environment when updating the other. During training, we sample 1000 two round dialog episodes per batch and update policy pa- rameters with Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2015</ref>) based on these approximate gradients. Our code is pub- licly available 1 .</p><p>1 github.com/batra-mlp-lab/lang-emerge</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Road to Compositionality</head><p>This section details our key observation -that while the agents always successfully invent a lan- guage to solve the game with near-perfect accu- racies, the invented languages are decidedly not compositional, interpretable, or 'natural' (e.g. A- BOT ignoring Q-BOT's utterances and simply en- coding every object with a unique symbol if the vocabulary is sufficiently large). In our setting, the language being compositional simply amounts to the ability of the agents to communicate the com- positional atoms of a task (e.g. shape or color) and an instance (e.g. square or blue) independently.</p><p>Through this section, we present a series of set- tings that get progressively more restrictive to coax the agents towards adopting a compositional language, providing analysis of the learned lan- guages developed along the way. <ref type="table">Table 1</ref> summa- rizes results for all settings. In all experiments, optimal policies (achieving near-perfect rewards) were found. For each setting, we provide qualita- tive analysis of the learned languages and report their ability to generalize to unseen instances. We use 80% of the object-instances for training and the remaining 20% to evaluate these learned poli- cies. Further, greedy argmax policies are used at evaluation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overcomplete Vocabularies</head><p>We begin with the simplest setting where both A- BOT and Q-BOT are given arbitrarily large vocab- ularies. We find that when |V A | is greater than the number of instances (64), the learned policy simply has A-BOT ignore what Q-BOT asks and instead convey the instance using a single sym- bol, e.g. token 1≡(red, square, filled). Notice that this means no 'dialog' is necessary and amounts to each agent having a codebook that maps symbols to object instances.</p><p>Perhaps as expected, the generalization of this lan- guage to unseen instances is quite poor (success rate: 25.6%). The adopted strategy of mapping in- stances to token pairs fails for test instances con- taining novel combinations of attributes for which the agents lack an agreed-upon code from training.</p><p>It seems clear that like in human communication ( <ref type="bibr" target="#b17">Nowak et al., 2000</ref>), a limited vocabulary that cannot possibly encode the richness of the world seems to be necessary for non-trivial dialog to emerge. We explore such a setting next.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attribute &amp; Value Vocabulary</head><p>Since our world has 3 attributes (shape/color/ style) and 4 + 4 + 4 = 12 possible settings of their states, one may believe that the intuitive choice of |V Q | = 3 and |V A | = 12 will be enough to circum- vent the 'cheating' enumeration strategy from the previous experiment. Surprisingly, we find that the new language learned in this setting is not only de- cidedly non-compositional but also very difficult to interpret! We present two salient observations.</p><p>We observe that Q-BOT uses only the first round to convey the task to A-BOT by encoding tasks in an order-independent fashion e.g. the (style,color) and (color,style) tasks are both expressed as the ut- terance Z in the first round. Consequentially, mul- tiple rounds of dialog are rended unnecssary and the second round is inconsistent across instances even for the same task.</p><p>Given the task from Q-BOT in the first round, A- BOT only needs to identify one of the 4×4=16 at- tribute pairs for a given task. Rather than ground its symbols into individual states, A-BOT follows a 'set partitioning' strategy, i.e. A-BOT identifies a pair of attributes with a unique combinations of round 1 and 2 utterances (i.e. the round 2 utter- ance has no meaning independent from round 1). Thus, symbols are reused across tasks to describe different attributes (i.e. symbols do not have in- dividual consistent groundings). This 'set parti- tioning' strategy is consistent with known results from game theory on Nash equilibria in 'cheap talk' games <ref type="bibr" target="#b2">(Crawford and Sobel, 1982)</ref>.</p><p>This strategy has improved generalization to un- seen instances because it is able to communicate the task; however, it fails on unseen attribute value combinations because it is not compositional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Memoryless A-BOT, Minimal Vocabulary</head><p>The key problem with the previous setting is that A-BOT's utterances mean different things based on the round of dialog (a 1 = 1 is different from a 2 = 1). Essentially, the communication protocol is overparameterized and we must limit it further.</p><p>First, we limit A-BOT's vocabulary to |V A |=4 to reduce the number of 'synonyms' the agents learn. Second, we remove A-BOT's memory by reseting the state vector S A at each time step, which elim- inates its ability to enumerate all attribute pairs.</p><p>These restrictions result in a learned language that grounds individual symbols into attributes and their states. For example, Q-BOT learns that Y → shape, X → color, and Z → style. Q-BOT does not however learn to always utter these symbols in the same order as the task, e.g. asking for shape first for both (color, shape) and (shape, color).</p><p>Notice that this is perfectly valid as Q-BOT can later re-arrange the attributes in the task desired or- der. Similarly, A-BOT learns mappings to attribute values for each attribute query that remain consis- tent regardless of round (i.e. when asked for color, 1 always means blue).</p><p>This is similar to learned languages reported in re- cent works and is most closely related to <ref type="bibr">Das et al. (2017b)</ref>, who solve this problem by taking away Q-BOT's state rather than A-BOT's memory. Their approach can be interpreted as Q-BOT 'forgetting' the task after interacting with A-BOT. However, this behavior of Q-BOT to remember the task only during dialog but not while predicting is somewhat unnatural compared to our setting.</p><p>Tab. 2 enumerates the learnt groundings for both the agents. Given this mapping, we can predict a plausible dialog between the agents for any unseen instance and task combination. Notice that this is possible only due to the compositionality in the emergent language between the two agents. For example, consider solving (shape, color) for an in- stance (red, square, filled) from <ref type="figure" target="#fig_3">Fig. 2(b)</ref>. Q-BOT queries Y (shape) and X (color) across two rounds, and receives 2 (square) and 4 (red) as answers.</p><p>Intuitively, this consistently grounded and compo- sitional language has the greatest ability to gen- eralize among the settings we have explored, cor- rectly answering 74.4% of the held out instances. We note that errors in this setting seem to largely be due to A-BOT giving an incorrect answers de- spite Q-BOT asking the correct questions to ac- complish the task. A plausible reason could be the model approximation error stemming from the instance encoder as test instances are unseen and have novel attribute combinations. <ref type="figure" target="#fig_3">Fig. 2(b)</ref> shows the dialog for the instance (red, square, filled) and task (shape, color). Q-BOT queries Y (shape) and (color) across two rounds,   and receives 2 (square) and 4 (red) as answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evolution of Language Timeline</head><p>To gain further insight into the languages learned, we create a language evolution plot in <ref type="figure" target="#fig_3">Fig. 2</ref>. Specifically, at regular intervals during policy learning, we construct 'dialog trees'. A dialog tree enumerates all plausible dialogs between the two agents (q 1 , a 1 , q 2 , a 2 ), as a tree. The root node is q 1 , and at any node, we go deeper by choosing a branch based on the next utterance in the dialog. Since Task &amp; Talk runs for two rounds, our dialog trees are 4 layers deep with |V A | 2 |V Q | 2 leaves. No- tice that a single input instance could potentially result in different dialogs depending on the task. Hence, we consider (instance, task) pairs and as- sign each to a leaf by traversing the tree according to the resulting dialog. At some point in the learn- ing, the nodes become and stay 'pure' (the com- mon trend among all (instance, task) at the node stays constant till the end of training), at which point we can say that the agents have learned this dialog subsequence.</p><p>Construction. After constructing dialog trees at regular intervals, we identify 'concepts' at each node/leaf using the dialog tree of the completely trained model, which achieves a perfect accuracy on train set. A concept is simply the common trend among all the (instance, task) tuples either assigned to a leaf or contained within the sub- tree with a node as root. Next, given a resultant concept for each of the node/leaf, we backtrack in time and check for the first occurrence when only tuples which satisfy the corresponding con- cept are assigned to that particular node/leaf. In other words, we compute the earliest time when a node/leaf is 'pure' with respect to its final learned concept. Finally, we plot these leaves/nodes and the associated concept with their backtracked time to get <ref type="figure" target="#fig_3">Fig. 2</ref>.</p><p>Observations. We highlight key observations be- low: (a) The agents ground most of the tasks ini- tially at around epoch 20. Specifically, Q-BOT assigns Y to both (shape, style), (style, shape), (shape,color) and (color, shape), while (color, style) is mapped to Z. Hence, Q-BOT learns its first token very early into the training procedure. (b) The only other task (style, color) is grounded to- wards the end (around epoch 170) using X, leading to an immediate convergence. (c) We see a strong correlation between improvement in performance and when agents learn a language grounding. In particular, there is an improvement from 40% to 80% within a span of 25 epochs where most of the grounding is achieved, as seen from <ref type="figure" target="#fig_3">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In conclusion, we presented a sequence of 'neg- ative' results culminating in a 'positive' one - showing that while most invented languages are effective (i.e. achieve near-perfect rewards), they are decidedly not interpretable or compositional.</p><p>Our goal is simply to improve understanding and interpretability of invented languages in multi- agent dialog, place recent work in context, and in- form fruitful directions for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>b) Q-BOT (left) and A-BOT (right) policy networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Task &amp; Talk: The testbed for our study is cooperative 2-player game, Task &amp; Talk, grounded in a synthetic world of objects with 4 shapes × 4 colors × 4 styles. The two agents, Q-BOT and A-BOT, are modeled as neural networks, and their policies are learned via REINFORCE. We find that the languages invented by the two agents are typically not 'natural'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Setting</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Evolution of Language: timeline shows groundings learned by the agents during training, overlaid on the accuracy. Note that Q-BOT learns encodings for all tasks early (around epoch 20) except (style, color). Improvement in accuracy is strongly correlated with groundings learnt. (b) Example dialogs for memoryless A-BOT, minimal vocabulary setting ( §4.3.</figDesc><graphic url="image-6.png" coords="5,440.74,65.45,81.63,117.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Emergence of compositional grounding for language 
learnt by the agents. A-BOT (Tab. 2a) learns consistent map-
ping across rounds, depending on the query attribute. Token 
grounding for Q-BOT (Tab. 2b) depends on the task at hand. 
Though compositional, Q-BOT does not necessarily query at-
tribute in the order of task, but instead re-arranges accord-
ingly at prediction time as it contains memory. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to Communicate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="https://blog.openai.com/learning-to-communicate/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07683</idno>
		<title level="m">Learning End-to-End Goal-Oriented Dialog</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Strategic information transmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1431" to="51" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual Dialog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06585</idno>
		<title level="m">Stefan Lee, and Dhruv Batra. 2017b. Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emergence of language with multi-agent games: Learning to communicate with sequences of symbols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhii</forename><surname>Havrylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to play guess who? and inventing a grounded language as a consequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Jorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Gustavsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterated learning and the evolution of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurobiology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="108" to="114" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-agent cooperation and the emergence of (natural) language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Stuttle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convention: A philosophical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGDIAL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04908</idno>
		<title level="m">Emergence of grounded compositional language in multi-agent populations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><forename type="middle">P</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08251</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The evolution of syntactic communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Plotkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">A A</forename><surname>Jansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="issue">6777</biblScope>
			<biblScope unit="page" from="495" to="498" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06069</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning language games through interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Sida I Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06045</idno>
		<title level="m">Dialog-based language learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Procedures as a representation for data in a computer program for understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
