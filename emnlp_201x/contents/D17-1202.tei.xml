<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shortest-Path Graph Kernels for Document Similarity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><forename type="middle">Rousseaú</forename><surname>Rousseaú</surname></persName>
							<email>rousseau@lix.polytechnique.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Giannis NikolentzosÉcole Nikolentzos´NikolentzosÉcole Polytechnique and AUEB</orgName>
								<orgName type="laboratory" key="lab2">Polykarpos MeladianosÉcole Meladianos´MeladianosÉcole Polytechnique and AUEB</orgName>
								<orgName type="laboratory" key="lab3">Michalis VazirgiannisÉcole Vazirgiannis´VazirgiannisÉcole Polytechnique and AUEB</orgName>
								<orgName type="institution" key="instit1">Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit2">IMIS / RC ATHENA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Stavrakas</surname></persName>
							<email>yannis@imis.athena-innovation.gr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Giannis NikolentzosÉcole Nikolentzos´NikolentzosÉcole Polytechnique and AUEB</orgName>
								<orgName type="laboratory" key="lab2">Polykarpos MeladianosÉcole Meladianos´MeladianosÉcole Polytechnique and AUEB</orgName>
								<orgName type="laboratory" key="lab3">Michalis VazirgiannisÉcole Vazirgiannis´VazirgiannisÉcole Polytechnique and AUEB</orgName>
								<orgName type="institution" key="instit1">Ecole Polytechnique</orgName>
								<orgName type="institution" key="instit2">IMIS / RC ATHENA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Shortest-Path Graph Kernels for Document Similarity</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1890" to="1900"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present a novel document similarity measure based on the definition of a graph kernel between pairs of documents. The proposed measure takes into account both the terms contained in the documents and the relationships between them. By representing each document as a graph-of-words, we are able to model these relationships and then determine how similar two documents are by using a modified shortest-path graph kernel. We evaluate our approach on two tasks and compare it against several baseline approaches using various performance metrics such as DET curves and macro-average F1-score. Experimental results on a range of datasets showed that our proposed approach outperforms traditional techniques and is capable of measuring more accurately the similarity between two documents.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, we have witnessed a tremendous growth in the volume of textual documents avail- able on the Web. With this rapid increase in the number of available content, new opportunities for knowledge extraction have arisen. Many text min- ing tasks such as information retrieval, text catego- rization and document clustering involve the direct comparison of two documents. It is thus crucial to be able to determine accurately how similar two documents are by defining a document similarity measure.</p><p>Generally speaking, a similarity measure is a real-valued function that quantifies the common information shared by two objects (in our case documents). Determining the similarity between two documents is not a trivial task. Whether two documents are similar or different is not always clear and may vary from application to applica- tion.</p><p>Similarity measures that make use of the vector- space model ( <ref type="bibr" target="#b34">Salton et al., 1975</ref>) treat words in a document as if they were independent of one an- other, which is not realistic. In fact, words relate to one another to form meaningful phrases and to develop ideas. It is known that the human brain utilizes these relations between words to facilitate understanding ( <ref type="bibr" target="#b1">Altmann and Steedman, 1988)</ref>. In general, we assume that two terms are related if they co-occur together in a small context, typi- cally a phrase or a window of specific size, which resulted in n-gram features in many text mining tasks (an n-gram is a sequence of n terms in this paper). But n-grams correspond to sequences of words and thus fail to capture word inversion and subset matching (e. g., "article about news" vs. "news article"). To take into account these statis- tical relations, we propose to represent each doc- ument as a graph-of-words instead. And then, in order to measure the similarity between two doc- uments, we capitalize on recent advances in graph kernels. Kernels can be thought of as measures of similarity between pairs of objects ( <ref type="bibr" target="#b35">Schölkopf and Smola, 2002)</ref>. A graph kernel is a kernel func- tion that measures the similarity between pairs of graphs.</p><p>Our aim in this paper is neither to define a sim- ilarity measure for only a certain category of doc- uments based on background knowledge and fea- tures specific to that field nor to improve similar- ity estimation by using external knowledge. In-stead, we propose to define a similarity measure that does not incorporate any background or ex- ternal knowledge. Hence it is, without changes, applicable to all types of textual documents even if they come from different areas. The method takes as input a pair of documents and automati- cally computes how similar they are to each other based solely on their content.</p><p>The rest of this paper is organized as follows. Section 2 provides an overview of the related work and elaborates our contribution. Section 3 pro- vides a detailed description of our proposed graph- of-words kernel. Section 4 evaluates the proposed approach on a wide range of tasks. Finally, Sec- tion 5 summarizes the work and presents potential future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review the related work published in the areas of document similarity, graph kernels, kernel-based text categorization and graph-based text categorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document Similarity</head><p>There has been a variety of similarity measures de- fined to assess how close two objects are to each other, including documents. Let &lt; d 1 , d 2 &gt; be a pair of documents and D 1 (resp. D 2 ) the set of terms in d 1 (resp. d 2 ). Common similarity mea- sures discussed by Manning (1999) are defined as follows:</p><formula xml:id="formula_0">M atching(d 1 , d 2 ) = |D 1 ∩ D 2 | Dice(d 1 , d 2 ) = 2 |D 1 ∩ D 2 | |D 1 | + |D 2 | Jaccard(d 1 , d 2 ) = |D 1 ∩ D 2 | |D 1 ∪ D 2 | Overlap(d 1 , d 2 ) = |D 1 ∩ D 2 | min(|D 1 |, |D 2 |) Cosine(d 1 , d 2 ) = |D 1 ∩ D 2 | |D 1 | × |D 2 |</formula><p>The terms might be processed unigrams as well as processed n-grams present in the text. The set of operations described above are equivalent to vector operations when representing d 1 and d 2 as binary vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Kernels</head><p>Graph kernels are instances of the R-convolution kernels introduced by <ref type="bibr" target="#b16">Haussler (1999)</ref>. Convo- lution kernels have been proposed as a princi- pled way of designing kernels on structured ob- jects, such as sequences, trees and graphs. Graph kernels compute the similarity between pairs of graphs, based on common substructures they share. A wide variety of substructures has been proposed, such as random walks <ref type="bibr" target="#b38">Vishwanathan et al., 2010)</ref>, shortest paths (Borgwardt and ), subtrees <ref type="bibr" target="#b31">(Ramon and</ref><ref type="bibr">Gärtner, 2003), cycles (Horváth et al., 2004</ref>), and graphlets <ref type="bibr" target="#b36">(Shervashidze et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Kernel-based Text Categorization</head><p>In recent years, there has been a great deal of work in using kernel methods, such as SVMs for text classification <ref type="bibr" target="#b19">(Joachims, 1998;</ref><ref type="bibr" target="#b10">Dumais et al., 1998)</ref>. Such work concentrates on building spe- cialized kernels aimed at measuring similarity be- tween documents. We outline some of these ap- proaches below.</p><p>The works closest to ours are the ones reported by <ref type="bibr" target="#b22">Lodhi et al. (2002)</ref> and by <ref type="bibr" target="#b8">Cancedda et al. (2003)</ref>. <ref type="bibr">Lodhi et al.</ref> propose the use of string ker- nels as an alternative to the vector-space model. The feature space is generated by any ordered sub- sequence of characters found in the text not neces- sarily contiguously. Each subsequence consists of a specific number of characters and is weighted by an exponentially decaying factor of its full length in the text. Due to the enormous amount of com- putation needed to compute this feature vector, the authors present a dynamic programming tech- nique, which allows the efficient calculation of the kernel values. Our work differs from theirs in that we use graph kernels instead of sequence kernels, and we concentrate on the word level in- stead of the character level. Cancedda et al. mod- ified their string kernel to work with sequences of words rather than characters. Two sequences of words are considered similar if they have many common words in a given order. The similarity between two documents is assessed by the num- ber of matching word sequences. Non-contiguous occurrences are penalized according to the number of gaps they contain. The proposed kernel is more appealing as it is more computationally efficient and it takes advantage of the standard linguistic preprocessing techniques. This approach differs in fundamental respects from our work since we represented documents as graphs-of-words in or- der to model word co-occurrence rather than se-quences of words and we used a graph kernel in- stead of a sequence kernel to measure the similar- ity between pairs of documents. Other text cate- gorization works use kernels that measure the se- mantic similarity between concepts extracted from the text ( <ref type="bibr" target="#b3">Bleik et al., 2013;</ref><ref type="bibr" target="#b40">Wang and Domeniconi, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Graph-based Text Categorization</head><p>Our work is also related to methods that repre- sent documents as graphs and perform graph min- ing tasks to achieve improved classification per- formance. These methods either extract frequent subgraphs which are then used to produce fea- ture vectors for the documents ( <ref type="bibr" target="#b18">Jiang et al., 2010;</ref> or they determine term weights to be used in the vector-space model based on centrality criteria or random walks <ref type="bibr" target="#b15">(Hassan et al., 2007;</ref><ref type="bibr" target="#b23">Malliaros and Skianis, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Graph Kernel for Document Similarity</head><p>In this section, we first discuss the essential def- initions from graph theory. We then present our graph-of-words model for representing tex- tual documents. And finally, we define our cus- tom Shortest-Path Graph Kernel (SPGK) capable of measuring the similarity between pairs of doc- uments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Concepts</head><p>Let G = (V, E) be an undirected and unweighted graph consisting of a set V of vertices and a set E of edges between them. In this paper, we will denote by n the number of vertices and by m the number of edges. A labeled graph is a graph with labels on ver- tices and/or edges. Given a set of labels L, : V → L is a function that assigns labels to the and/or edges of the graph. In our case, we deal with fully-labeled graphs as labels are assigned both to vertices and to edges.</p><p>A graph G can be represented by its adjacency matrix A. The (i, j) th entry of A is 1 if the edge (v i , v j ) between vertices v i and v j exists, and 0 otherwise.</p><p>A walk in a graph G is a sequence of vertices v 1 , v 2 , . . . , v k+1 where v i ∈ V and (v i , v i+1 ) ∈ E for 1 ≤ i ≤ k. The length of the walk is equal to the number of edges in the sequence, i. e. k in the above case. A walk in which v i = v j ⇔ i = j is called a path. In other words, a path is a walk without repetition of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph-of-words</head><p>We chose to represent each textual document as a statistical graph-of-words, following earlier ap- proaches in keyword extraction ( <ref type="bibr" target="#b28">Ohsawa et al., 1998;</ref><ref type="bibr" target="#b27">Mihalcea and Tarau, 2004</ref>) and more re- cent ones in ad hoc IR ( <ref type="bibr" target="#b2">Blanco and Lioma, 2012;</ref><ref type="bibr" target="#b33">Rousseau and Vazirgiannis, 2013)</ref> and in summa- rization ( <ref type="bibr" target="#b26">Meladianos et al., 2015)</ref>.</p><p>The construction of each graph is preceded by a preprocessing phase where standard text process- ing tasks such as tokenization, stopword, punctua- tion and special character removal, and stemming are performed. The processed document is then transformed into an unweighted, undirected graph whose vertices represent unique terms and whose edges represent co-occurrences between the con- nected terms within a fixed-size window (hence the statistical denomination). The graph-of-words representation of text provides enhanced model- ing capabilities compared to the bag-of-words rep- resentation. Besides the terms (vertices), it also models the relationships between them (edges). All the words present in a document have some relationships with one another, modulo a window size outside of which the relationship is not taken into consideration, and graphs are able to capture these dependencies. The extended modeling capa- bilities, however, come with an increase in com- plexity.</p><p>An example of a document represented as an unweighted undirected graph is given in <ref type="figure" target="#fig_0">Figure 1</ref>. The source text comes from Shakespeare's play "Hamlet": "to be or not to be: that is the ques- tion". For illustration purposes, only the colon is removed and no other text processing tasks are performed. The size of the window is set to 2, i. e. it captures bigram relationships. Hence, each word (vertex) is connected with an edge with its previous and its next word, if any.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shortest-Path Graph-of-words Kernel (SPGK)</head><p>Our proposed approach measures the similar- ity between two textual documents by represent- ing them as graphs-of-words, transforming these graphs into other graphs, and using graph ker- nels to calculate the similarity of the new graphs. Specifically, we capitalize on the shortest-path graph kernel (Borgwardt and ) and we modify it to compare the graph representations of pairs of documents.</p><p>The first step of our proposed approach is to transform the graph-of-words representation of each document G into another graph C whose ver- tices are connected with an edge only if the short- est distance between them is not greater than a variable d. The emerging graph contains the same set of vertices as the graph-of-words from which it was generated. However, there exist edges only between vertices that are connected by a path of length at most d. Every node in C is labeled by the term that it represents, while every edge between two vertices is labeled by the shortest distance be- tween these vertices given that it is no greater than d. Specifically, the label of an edge e that links two vertices whose shortest path is p is set equal to label(e) = 1 /p. For d = 1, the emerging network is equivalent in a structural sense to its correspond- ing graph-of-words. For greater values of d, it is very likely that the number of edges of the graph will have increased compared to its predecessor.</p><p>The commonly-used unigram bag-of-words representation assumes that words in a document are independent of one another. Although sim- ilarity measures based on this assumption have shown to work well in practice in many fields, it is not rational to completely ignore word order and word dependence. Hence, the distance between two terms in a document determines their rela- tionship. This led us to explore alternative doc- ument similarity metrics that take into account the co-occurrence of words in the documents. More specifically, we assume that two terms are re- lated given that they appear together inside a win- dow. The underlying assumption is that each word present in a document has some relationship with the other words that are close to it. We set the size of the window over the processed text equal to 2. Therefore, in our graph-of-words representation of a document, each term is linked with its preceding and its following term with an edge. In our trans- formed graphs, terms are not only connected with terms that are next to them, but also with terms that are close to their neighbors (d = 2), with lower label values, and close to neighbors of their neigh- bors (d = 3), with even lower label values. Param- eter d determines how far from the initial terms we allow the paths to go. Our intuition is that given an initial term, terms that are close to terms that are close to the initial term or beyond, may have also some relation with the initial term, and the strength of this relation decreases as the shortest path length increases. Therefore, although the pro- posed kernel does not incorporate any knowledge of the language being used, it does capture some statistical information and is thus capable of out- performing metrics based on the unigram and even n-gram vector-space model.</p><p>To determine the edge labels in the new graph C, we can perform depth-first search (DFS) or breadth-first search (BFS) traversals from each vertex in the graph, limiting the depth to d. The complexity for calculating paths of length up to d from a source vertex to all other vertices using either DFS or BFS is at most O(b d ), where b is the average branching factor. The branching fac- tor depends on the average degree of the vertices of the graphs-of-words G which, in its turn, de- pends on the selected size of the sliding window. For W = 2, the average degree of the vertices will be typically only slightly above 2 and the branch- ing factor will be only slightly above 1. Calcu- lating paths of length up to d for all vertices takes thus O(nb d ) time. This still yields reasonable time complexity estimates for small values of d.</p><p>After our original graphs have been transformed into the graphs described above, we can measure their similarity using the following kernel:</p><p>Definition 1 (Custom shortest-path graph kernel). Let G 1 , G 2 denote two graph-of-words represen- tations of two textual documents d 1 , d 2 that are transformed into graphs C 1 , C 2 through the pro- cess described above. The proposed Shortest- Path Graph Kernel (SPGK) on C 1 = (V 1 , E 1 ) and C 2 = (V 2 , E 2 ) is defined as follows:</p><formula xml:id="formula_1">k(d 1 , d 2 ) = v 1 ∈V 1 ,v 2 ∈V 2 k node (v 1 , v 2 ) + e 1 ∈E 1 ,e 2 ∈E 2 k (1) walk (e 1 , e 2 ) norm</formula><p>(1) where k node is a positive definite kernel for com- paring two vertices, k</p><p>walk a positive definite ker- nel for comparing two edge walks of length 1 in C (i. e. up to d in G) and norm a normalization factor described next.</p><p>The similarity value generated by our custom shortest-path graph kernel is equal to the sum over the kernel values of all pairs of vertices on the transformed graphs plus the sum over the kernel values of all pairs of edge walks of length 1 over a positive normalization factor. The k node kernel is a function for comparing two vertices. In practice, we use a delta kernel defined as:</p><formula xml:id="formula_3">k node (v 1 , v 2 ) = 1 if (v 1 ) = (v 2 ), 0<label>otherwise (2)</label></formula><p>but other works have considered distances in word embeddings for instance to account for word sim- ilarity at the cost of having to compare every node of a graph to every other nodes of the other graph ( <ref type="bibr" target="#b37">Srivastava et al., 2013</ref>). The normalization factor is introduced because the nominator of the proposed kernel depends on the length of the compared documents. Specifi- cally, given the adjacency matrices of the trans- formed graph representations of two documents A 1 , A 2 where the value of each entry in the adja- cency matrix is set equal to the label of the corre- sponding edge, and the diagonal matrices D 1 , D 2 with diagonal entries set to 1 if the correspond- ing term exists in the corresponding document, we first compute the matrices M 1 , M 2 as shown be- low:</p><formula xml:id="formula_4">M 1 = A 1 + D 1 M 2 = A 2 + D 2</formula><p>and we then compute the normalization factor us- ing the following formula:</p><formula xml:id="formula_5">norm = M 1 F × M 2 F</formula><p>where · F is the Frobenius norm for matrices.</p><p>The k</p><p>walk kernel can be expressed as the prod- uct of kernels on vertices and edges along the walk. Only walks of length 1 in C are consid- ered, therefore, k <ref type="bibr">(1)</ref> walk can be calculated in terms of the original vertex, the destination vertex, and the edge connecting them.</p><p>Definition 2 (Custom edge walk kernel). Let u 1 , v 1 be two vertices of graph C 1 (u 1 , v 1 ∈ V 1 ) and e 1 the edge connecting them. Let also u 2 , v 2 be two vertices of graph C 2 (u 2 , v 2 ∈ V 2 ) and e 2 the edge connecting them. The edge walk kernel is defined as follows:</p><formula xml:id="formula_7">k (1) walk (e 1 , e 2 ) = k node (u 1 , u 2 ) × k edge (e 1 , e 2 ) ×k node (v 1 , v 2 )<label>(3)</label></formula><p>where k node is the kernel function defined above and k edge is a kernel function for comparing two edges defined as follows:</p><formula xml:id="formula_8">k edge (e 1 , e 2 ) =      (e 1 ) × (e 2 ) if e 1 ∈ E 1 ∧ e 2 ∈ E 2 , 0 otherwise<label>(4)</label></formula><p>The measure of similarity between two graphs depends on the kernel values corresponding to the vertices and edges that compose each walk, while the matching between two vertices or two edges is determined by comparing their labels. The val- ues of our kernel function lie in the interval <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. It takes a value equal to 0 for documents with no common terms and a value equal to 1 for identical documents.</p><p>Lemma 1. SPGK is a valid kernel.</p><p>Proof. Based on the proofs presented in (Borg- wardt and ) and ( ), we show that our custom shortest-path graph kernel is positive definite. The k node kernel is a delta kernel, which is known to be positive def- inite ( <ref type="bibr" target="#b35">Schölkopf and Smola, 2002</ref>) and therefore a valid kernel. The k edge kernel is also a delta ker- nel multiplied by a positive real number. Since the multiplication of a kernel by a positive constant preserves positive definiteness, this kernel is also valid. Regarding the k <ref type="bibr">(1)</ref> walk kernel, it is positive definite as the point-wise multiplication of posi- tive definite kernels (k node , k edge ) preserves posi- tive definiteness. The</p><formula xml:id="formula_9">v 1 ∈V 1 ,v 2 ∈V 2 k node (v 1 , v 2 )</formula><p>function is the sum of valid kernel functions, hence, it is also positive definite. Regarding the</p><formula xml:id="formula_10">e 1 ∈E 1 ,e 2 ∈E 2 k (1)</formula><p>walk (e 1 , e 2 ) function, it is a walk kernel that takes into account only walks of length 1 on the transformed graphs and is zero-extended to the whole set of pairs of walks that do not sat- isfy the above constraint. Therefore, kernel values for walks with length greater than 1 are set to zero. This zero-extension is known to preserve positive definiteness <ref type="bibr" target="#b16">(Haussler, 1999)</ref>. This function is a convolution kernel, which is proven to be positive definite <ref type="bibr" target="#b16">(Haussler, 1999</ref>). Finally, the kernel is di- vided by a positive constant and its positive defi- niteness is preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Run Time Complexity</head><p>We now determine the time complexity of our pro- posed kernel for measuring the similarity between two documents. Let us assume that the graph-of- words representations of the two documents con- sist of n vertices each. To determine the shortest paths of length at most d from a root vertex to all other vertices, we need O(b d ) time when using a graph traversal algorithm (depth-first or breadth- first search). There are also n vertices in the trans- formed graph, hence, the transformation will re- quire O(nb d ) time for each graph. In order to de- termine the kernel value, it is necessary to com- pute the value of k <ref type="formula" target="#formula_2">(1)</ref> walk for all pairs of edges be- tween the two transformed graphs. The number of edges in the transformed graph can be at most n 2 in the case all the shortest paths in the origi- nal graph are no longer than d. Thus, there are at most n 2 · n 2 = n 4 pairs of edges. However, due to the label enrichment that has been applied to the vertices of the transformed graphs, the num- ber of matching nodes in the two graphs has been radically reduced and the number of pairs of edges that have to be considered is also reduced. Specifi- cally, we have to consider n 2 pairs of edges as only paths between vertices whose label is the same in the two graphs are considered. The kernel value can thus be computed in O(n 2 + nb d ) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Alternative Computation Method for d = 1</head><p>In the case we consider only the common paths of length 1, there is a more efficient algorithm to compute the kernel values. The common paths of length 1 correspond to common edges between the graph representations of the documents. The emerging kernel takes into account the number of common vertices (terms) between the two graphs and the number of common edges (terms co- occurring in the same window) as well. More specifically, given two documents d 1 and d 2 , the adjacency matrices of their graph representations A 1 , A 2 where each entry in the adjacency matrix is set to 1 if the corresponding edge exists in the graph and the diagonal matrices D 1 , D 2 with di- agonal entries set to 1 if the corresponding term exists in the document, we first compute the ma- trices M 1 , M 2 as described previously and then we compute the kernel value using the following formula:</p><formula xml:id="formula_11">k(d 1 , d 2 ) = M 1 • M 2 M 1 F × M 2 F (5)</formula><p>where (·•·) is the Hadamard or element-wise prod- uct between matrices.</p><p>If n is the number of unique node labels, i. e. the length of the vocabulary, and m the number of edges, the computation of the kernel values re- quires O(n + m) time in the worst case scenario. For the baseline similarity measures, with unigram features, the computational cost is O(n) time but it goes up as we consider higher order n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Evaluation</head><p>In this section, we present the experiments we con- ducted to evaluate and validate our proposed ker- nel between documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>To assess the effectiveness of the different ap- proaches, we employed a set of well-known eval- uation metrics inherited from Information Re- trieval: accuracy, macro-average F1-score and for the story link detection task DET curves ( <ref type="bibr" target="#b25">Martin et al., 1997</ref>).</p><p>The DET curve is a variant of the ROC curve that plots the missed detection probability (P miss = f n /(tp+fn)) versus the false alarm prob- ability (P f a = f p /(tn+fp)) for various system op- erating points, which allows someone to get a greater insight into the effectiveness of the eval- uated approaches. A method is considered to per- form best at thresholds that correspond to points that are close to the lower-left of the graph (i. e. lower error probabilities) and the area under the curve should be minimal.</p><p>For the story link detection experiments, we also computed the normalized C Det costs, the  <ref type="table">Table 1</ref>: Summary of the 5 datasets that were used in our text categorization experiments.</p><note type="other">Dataset # training # test # classes vocabulary avg. terms avg.</note><p>standard performance measure of TDT as de- scribed in ( <ref type="bibr" target="#b11">Fiscus and Wheatley, 2004</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets</head><p>We evaluate the SPGK and the baselines on 5 stan- dard datasets for text categorization: (1) WebKB: Web pages collected from Computer Science de- partments of various Universities manually classi- fied into 7 categories (we removed Web pages that belong to the classes "staff", "department" and "other") ( <ref type="bibr" target="#b9">Craven et al., 1998)</ref>.  <ref type="table">Table 1</ref> shows statistics of the datasets that were used for the evaluation. For the Story Link Detection task, we employed the TDT-5 corpus that contains stories from var- ious newswire sources ( <ref type="bibr" target="#b13">Glenn et al., 2006;</ref><ref type="bibr" target="#b14">Graff and Kong, 2006</ref>). We only used the English part of the dataset for our experiments consisting of 221, 306 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>The similarity measure presented in this paper is best suited for settings where the concept of a pre- defined corpus does not exist. For example, it could find applications in plagiarism detection and in cases where independent pairs of documents must be compared to each other. In such settings, due to the absense of a corpus, we cannot learn mappings of terms to a vector space (i. e. word em- beddings) or use methods that take advantage of the corpus to increase their performance. Hence, our set of baselines includes methods that take as input two documents and output their similarity. More specifically, the performance of our pro- posed kernel was compared to the performances of three baseline kernels based on similarity mea- sures between pairs of documents &lt; d 1 , d 2 &gt; in the n-gram feature space (up to 4-grams):</p><p>1. The linear kernel, which uses the dot product as similarity measure: k dp (</p><formula xml:id="formula_12">d 1 , d 2 ) = d 1 · d 2</formula><p>where d is the n-gram feature vector associated with the document d;</p><p>2. Cosine, which measures the cosine of the an- gle between the two vectors:</p><formula xml:id="formula_13">k c ( d 1 , d 2 ) = d 1 · d 2 d 1 ×× d 2</formula><p>where · is the L 2 −norm.</p><p>3. Tanimoto coefficient (also known as Jaccard co- efficient), which measures the intersection of features divided by their union:</p><formula xml:id="formula_14">k tc ( d 1 , d 2 ) = d 1 · d 2 d 1 2 + d 2 2 − d 1 · d 2</formula><p>In the task of text categorization, we also com- pared the proposed kernel against the so-called Dynamic Convolutional Neural Network (DCNN) which is capable of generating representations for larger pieces of text such as sentences and docu- ments ( <ref type="bibr" target="#b20">Kalchbrenner et al., 2014</ref>) and a convolu- tional neural network (CNN) architecture that has recently showed state-of-the-art results on many NLP sentence classification tasks <ref type="bibr" target="#b21">(Kim, 2014)</ref>. We used two variants of the CNN: (1) a model where all words are initialized to random vec- tors and are kept static during training (CNN static,rand), and (2) a model where again all words are initialized to random vectors, but are modified during training (CNN non-static,rand).</p><p>The second model as well as DCNN have access to the whole corpus to generate word/document em- beddings. Hence, it is not fair in a sense to com- pare the proposed kernel against these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>WebKB News Subjectivity Amazon Polarity Accuracy F1-score Accuracy F1-score Accuracy F1-score Accuracy F1-score Accuracy F1-score  <ref type="table">Table 2</ref>: Performance of the 6 approaches in text categorization. * indicates statistical significance in accuracy improvement at p &lt; 0.05 using the micro sign test against the Cosine (n = 2) baseline of the same column. &gt; 1 day indicates that the computation did not finish after 1 day.</p><formula xml:id="formula_15">Dot product n = 1 0.9026</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Text Categorization</head><p>To perform text categorization, for all methods ex- cept the DCNN and the two CNNs, we employed a Support Vector Machine (SVM) classifier <ref type="bibr" target="#b7">(Boser et al., 1992)</ref>. It is interesting to note that all we need to train an SVM classifier is the kernel ma- trix of the training examples. We optimized the parameter C of the SVM by performing 10-fold cross-validation on the training set. We then made predictions on the test set using the optimal value of C. For DCNN the dimensionality of the gen- erated embeddings was set to 100, while for the two CNNs it was set to 300. For DCNN and the two CNNs, the number of training epochs was set to 25. All similarity measures were coded in Python 1 . For each value of the parameter d, we obtain a new kernel and in turn the resultant kernel matrix contains different values. To study the effect of parameter d on the classification performance, we performed tests for values of d ranging from 1 to 4. We did not further increase the value of d since in most cases, for values greater than 4, the perfor- mance of the classifier stayed the same. <ref type="table">Table 2</ref> shows the performance of the baseline methods and the proposed shortest-path graph ker- nel (SPGK), on the five datasets. Bold font marks the best performance in a column, while * indi-cates statistical significance in accuracy improve- ment at p &lt; 0.05 using the micro sign test <ref type="bibr" target="#b41">(Yang and Liu, 1999</ref>) against the Cosine (n = 2) baseline of the same column. We chose to test for signifi- cance against that measure, as it corresponds to the best-performing baseline. On all datasets except one (News), SPGK outperforms the other three similarity measures and the neural network archi- tectures. In addition, the results show a statisti- cally significant improvement of at least one of our kernels over the Cosine (n = 2) approach on all datasets except two (News, Amazon). In general, our kernel is followed in performance by Cosine, Tanimoto, Dot Product in that order. The three neural network architectures fail to outperform the proposed kernel even on a single dataset. Further- more, the approaches that make use of the whole corpus to generate embeddings (DCNN and CNN non-static,rand) do not seem to gain any advantage from having access to the whole dataset. This may be due to the fact that the size of the datasets is not large enough for learning high-quality representa- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Story Link Detection</head><p>Story link detection, as defined by the Topic De- tection and Tracking (TDT) research program <ref type="bibr" target="#b0">(Allan, 2002)</ref>, is the task of determining whether two stories, such as news articles and radio broadcasts, are "linked" by the same event. According to TDT,  an event is something that happens at some spe- cific time and place and two stories are "linked" if they discuss the same event.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we plot the DET curves compar- ing the proposed approaches. For clarity, we only plot one curve for our SPGK approach (d = 1) since the plots overlapped, and the best perform- ing curve for each of the baseline approaches. It is clear that our approach outperforms the base- lines over the whole set of operating points. We also searched for the threshold values for which each approach maximizes its performance. Our next step was to compare the four systems in terms of detection effectiveness at that optimal thresh- old. <ref type="table" target="#tab_2">Table 3</ref> illustrates the normalized C det of the proposed methods and the baselines. We can see that the proposed methods are better than baseline methods in terms of the normalized C det metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a graph kernel for mea- suring the similarity between pairs of documents. The graph-of-words representation of textual doc- uments allows us to model relationships between terms in documents and, hence, to go beyond the limits of the vector-space model. At the same time, it allows us to measure the similarity be- tween two documents by comparing their graph representations using kernel functions. The effec- tiveness of the proposed kernel was empirically tested on two different tasks, namely text catego- rization and story link detection. The proposed measure showed improved performance on both tasks compared to the baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of the graph representation of a textual document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 )</head><label>2</label><figDesc>News: News extracted from RSS feeds of popular newspaper websites classified into 7 categories based on the taxonomies of their publishing websites (Vitale et al., 2012). (3) Subjectivity: Subjective and objective sentences corresponding to movie reviews from Rotten Tomatoes and to plot sum- maries gathered from the Internet Movie Database respectively (Pang and Lee, 2004). (4) Amazon: Product reviews over four different sub-collections (Blitzer et al., 2007). (5) Polarity: Positive and negative snippets acquired from Rotten Toma- toes (Pang and Lee, 2005).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: DET curves for all similarity measures on story link detection track. Similarity measure (C det )norm Dot product 0.3908 Cosine 0.0953 Tanimoto coefficient 0.1453</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of all similarity measures in 
story link detection. 

</table></figure>

			<note place="foot" n="1"> Code available at: http://www.db-net.aueb. gr/nikolentzos/code/spgk.zip</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introduction to Topic Detection and Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topic Detection and Tracking</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Interaction with context during human sentence processing. Cognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="191" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph-based term weighting for information retrieval. Information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="54" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text Categorization of Biomedical Data Sets using Graph Kernels and a Controlled Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bleik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Computational Biology and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1211" to="1217" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, Bollywood, Boomboxes and Blenders: Domain Adaptation for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE International Conference on Data Mining</title>
		<meeting>the 5th IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Training Algorithm for Optimal Margin Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Workshop on Computational Learning Theory</title>
		<meeting>the 5th Annual Workshop on Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word-Sequence Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1059" to="1082" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Renders</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Extract Symbolic Knowledge from the World Wide Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dipasquo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence</title>
		<meeting>the 10th Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive Learning Algorithms and Representations for Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 7th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the TDT 2004 Evaluation and Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wheatley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TDT Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Graph Kernels: Hardness Results and Efficient Alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glenn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maeda</surname></persName>
		</author>
		<title level="m">TDT5 Topics and Annotations. Linguistic Data Consortium (LDC)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<title level="m">TDT5 Multilingual Text. Linguistic Data Consortium (LDC)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RandomWalk Term Weighting for Improved Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Semantic Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="421" to="439" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolution Kernels on Discrete Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<idno>UCSC-CRL-99-10</idno>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Santa Cruz</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cyclic Pattern Kernels for Predictive Graph Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Text classification using graph mining-based feature extraction. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zito</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Text Categorization with Support Vector Machines: Learning with Many Relevant Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Convolutional Neural Network for Modelling Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text Classification using String Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="444" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph-Based Term Weighting for Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Malliaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting>the 2015 International Conference on Advances in Social Networks Analysis and Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1473" to="1479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The DET Curve in Assessment of Detection Task Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Degeneracy-based Real-Time Sub-Event Detection in Twitter Stream</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Stavrakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th AAAI Conference on Web and Social Media</title>
		<meeting>the 9th AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="248" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TextRank: Bringing Order into Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">KeyGraph: Automatic Indexing by Co-occurrence Graph based on Building Construction Metaphor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohsawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Digital Libraries Conference</title>
		<meeting>the Advances in Digital Libraries Conference</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="12" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Sentimental Education: Sentiment Analysis using Subjectivity Summarization based on Minimum Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Expressivity versus Efficiency of Graph Kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Mining Graphs, Trees and Sequences</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Text Categorization as a Graph Classification Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kiagias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1702" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-ofword and TW-IDF: New Approach to Ad Hoc IR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Vector Space Model for Automatic Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient Graphlet Kernels for Large Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 12th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A WalkBased Semantically Enriched Tree Kernel Over Distributed Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<title level="m">Graph Kernels. The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Classification of Short Texts by Deploying Topical Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="376" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building Semantic Kernels for Text Classification using Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="713" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Re-examination of Text Categorization Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 22nd International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
