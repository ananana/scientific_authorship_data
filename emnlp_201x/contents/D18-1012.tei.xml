<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Game-Based Video-Context Dialogue</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>{ram, mbansal}@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Game-Based Video-Context Dialogue</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="125" to="136"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>125</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Current dialogue systems focus more on tex-tual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications , we introduce a new video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models , e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrase-matching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent in- teractions with humans have been a long-standing goal of artificial intelligence and machine learn- ing. There has been a lot of important previ- ous work in this field for decades <ref type="bibr" target="#b62">(Weizenbaum, 1966;</ref><ref type="bibr" target="#b23">Isbell et al., 2000;</ref><ref type="bibr" target="#b43">Rambow et al., 2001;</ref><ref type="bibr" target="#b44">Rieser et al., 2005;</ref><ref type="bibr" target="#b18">Georgila et al., 2006</ref>; <ref type="bibr" target="#b45">Rieser and Lemon, 2008;</ref><ref type="bibr" target="#b46">Ritter et al., 2011</ref>), includ-  ing recent work on introduction of large textual- dialogue datasets (e.g., <ref type="bibr" target="#b32">Lowe et al. (2015)</ref>; <ref type="bibr" target="#b51">Serban et al. (2016)</ref>) and end-to-end neural network based models ( <ref type="bibr" target="#b55">Sordoni et al., 2015;</ref><ref type="bibr" target="#b59">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b56">Su et al., 2016;</ref><ref type="bibr" target="#b33">Luan et al., 2016;</ref><ref type="bibr" target="#b27">Li et al., 2016;</ref><ref type="bibr">Serban et al., 2017a,b)</ref>.</p><p>Current dialogue tasks are usually focused on the textual or verbal context (conversation his- tory). In terms of multimodal dialogue, speech- based spoken dialogue systems have been widely explored ( <ref type="bibr" target="#b14">Eckert et al., 1997;</ref><ref type="bibr">Young, 2000;</ref><ref type="bibr" target="#b25">Janin et al., 2003;</ref><ref type="bibr" target="#b4">Celikyilmaz et al., 2017;</ref><ref type="bibr" target="#b63">Wen et al., 2015;</ref><ref type="bibr" target="#b56">Su et al., 2016;</ref><ref type="bibr" target="#b39">Mrkši´Mrkši´c et al., 2016)</ref>, as well as work on gesture and hap- tics based dialogue ( <ref type="bibr" target="#b26">Johnston et al., 2002;</ref><ref type="bibr" target="#b3">Cassell, 1999;</ref><ref type="bibr" target="#b16">Foster et al., 2008)</ref>. In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work intro- duced the visual dialogue task ( <ref type="bibr" target="#b10">Das et al., 2017;</ref><ref type="bibr" target="#b60">de Vries et al., 2017;</ref><ref type="bibr" target="#b38">Mostafazadeh et al., 2017)</ref>. However, the visual context in these tasks is lim-ited to one static image. Moreover, the interac- tions are between two speakers with fixed roles (one asks questions and the other answers).</p><p>Several situations of real-world dialogue among humans involve more 'dynamic' visual context, i.e., video-style information of the world moving around us (both spatially and temporally). Fur- ther, several human conversations involve more than two speakers, with changing roles. In order to develop such dynamically-visual multimodal dialogue models, we introduce a new 'many- speaker, video-context chat' testbed, along with a new dataset and models for the same. Our dataset is based on live-broadcast soccer (FIFA- 18) game videos from the 'Twitch.tv' live video streaming platform, along with the spontaneous, many-speaker live chats about the game. This challenging testbed allows us to develop dialogue models where the generated response is required to be relevant to the temporal and spatial events in the live video, as well as be relevant to the chat history (with potential impact towards video- grounded applications such as personal assistants, intelligent tutors, and human-robot collaboration).</p><p>We also present several strong discriminative and generative baselines that learn to retrieve and generate bimodal-relevant responses. We first present a triple-encoder discriminative model to encode the video, chat history, and response, and then classify the relevance label of the response. We then improve over this model via tridirec- tional attention flow (TriDAF). For the generative models, we model bidirectional attention flow be- tween the video and textual chat context encoders, which then decodes the response. We evaluate these models via retrieval ranking-recall, phrase- matching metrics, as well as human evaluation studies. We also present dataset analysis as well as model ablations and attention visualizations to understand the contribution of the video vs. chat modalities and the model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early dialogue systems had components of nat- ural language (NL) understanding unit, dia- logue manager, and NL generation unit <ref type="bibr" target="#b2">(Bates, 1995)</ref>. Statistical learning methods were used for automatic feature extraction ( <ref type="bibr" target="#b13">Dowding et al., 1993;</ref><ref type="bibr" target="#b37">Mikolov et al., 2013)</ref>, dialogue managers incorporated reward-driven reinforcement learn- ing ( <ref type="bibr">Young et al., 2013;</ref><ref type="bibr" target="#b53">Shah et al., 2016)</ref>, and the generation units have been extended with seq2seq neural network models ( <ref type="bibr" target="#b59">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b51">Serban et al., 2016;</ref><ref type="bibr" target="#b33">Luan et al., 2016</ref>).</p><p>In addition to the focus on textual dialogue con- text, using multimodal context brings more poten- tial for having real-world grounded conversations. For example, spoken dialogue systems have been widely explored ( <ref type="bibr" target="#b20">Gurevych and Strube, 2004;</ref><ref type="bibr" target="#b18">Georgila et al., 2006;</ref><ref type="bibr" target="#b14">Eckert et al., 1997;</ref><ref type="bibr">Young, 2000;</ref><ref type="bibr" target="#b25">Janin et al., 2003;</ref><ref type="bibr" target="#b11">De Mori, 2007;</ref><ref type="bibr" target="#b63">Wen et al., 2015;</ref><ref type="bibr" target="#b56">Su et al., 2016;</ref><ref type="bibr" target="#b39">Mrkši´Mrkši´c et al., 2016;</ref><ref type="bibr" target="#b22">Hori et al., 2016;</ref><ref type="bibr" target="#b6">Celikyilmaz et al., 2015</ref><ref type="bibr" target="#b4">Celikyilmaz et al., , 2017</ref>, as well as gesture and haptics based dialogue ( <ref type="bibr" target="#b26">Johnston et al., 2002;</ref><ref type="bibr" target="#b3">Cassell, 1999;</ref><ref type="bibr" target="#b16">Foster et al., 2008)</ref>. Additionally, dialogue sys- tems for digital personal assistants are also well explored ( <ref type="bibr" target="#b40">Myers et al., 2007;</ref><ref type="bibr" target="#b47">Sarikaya et al., 2016;</ref><ref type="bibr" target="#b8">Damacharla et al., 2018</ref>). In the visual modal- ity direction, some important recent attempts have been made to use static image based context in di- alogue systems ( <ref type="bibr" target="#b10">Das et al., 2017;</ref><ref type="bibr" target="#b60">de Vries et al., 2017;</ref><ref type="bibr" target="#b38">Mostafazadeh et al., 2017)</ref>, who proposed the 'visual dialog' task, where the human can ask questions on a static image, and an agent interacts by answering these questions based on the previ- ous chat context and the image's visual features. Also, <ref type="bibr" target="#b5">Celikyilmaz et al. (2014)</ref> used visual display information for on-screen item resolution in utter- ances for improving personal digital assistants.</p><p>In contrast, we propose to employ dynamic video-based information as visual context knowl- edge in dialogue models, so as to move to- wards video-grounded intelligent assistant appli- cations. In the video+language direction, previ- ous work has looked at video captioning ( <ref type="bibr" target="#b58">Venugopalan et al., 2015</ref>) as well as Q&amp;A and fill-in- the-blank tasks on videos ( <ref type="bibr" target="#b57">Tapaswi et al., 2016;</ref><ref type="bibr" target="#b24">Jang et al., 2017;</ref><ref type="bibr" target="#b35">Maharaj et al., 2017)</ref> and interactive 3D environments ( <ref type="bibr" target="#b9">Das et al., 2018;</ref><ref type="bibr" target="#b64">Yan et al., 2018;</ref><ref type="bibr" target="#b19">Gordon et al., 2017;</ref><ref type="bibr" target="#b0">Anderson et al., 2017)</ref>. There has also been early related work on generating sportscast commen- taries from simulation (RoboCup) soccer videos represented as non-visual state information <ref type="bibr" target="#b7">(Chen and Mooney, 2008</ref>). Also, <ref type="bibr" target="#b30">Liu et al. (2016a)</ref> presented some initial ideas on robots learning grounded task representations by watching and in- teracting with humans performing the task (i.e., by converting human demonstration videos to Causal And-Or graphs). On the other hand, we propose a new video-chat dataset where the dialogue models need to generate the next re- sponse in the sequence of chats, conditioned both on the raw video features as well as the pre- vious textual chat history. Moreover, our new dataset presents a many-speaker conversation set- ting, similar to previous work on meeting un- derstanding and Computer Supported Cooperative Work (CSCW) ( <ref type="bibr" target="#b25">Janin et al., 2003;</ref><ref type="bibr" target="#b61">Waibel et al., 2001;</ref><ref type="bibr" target="#b48">Schmidt and Bannon, 1992</ref>). In the live video stream direction, <ref type="bibr" target="#b17">Fu et al. (2017)</ref> and <ref type="bibr" target="#b42">Ping and Chen (2017)</ref> used real-time comments to pre- dict the frame highlights in a video, and <ref type="bibr" target="#b1">Barbieri et al. (2017)</ref> presented emotes and troll prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Twitch-FIFA Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Collection and Processing</head><p>For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA- 18) games along with the users' live chat conver- sations about the game. This dataset has videos in- volving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towards multimodal video-based dialogue data. An example is shown in <ref type="figure" target="#fig_1">Fig. 1</ref> (and an original screenshot example in <ref type="figure" target="#fig_2">Fig. 2</ref>), where the users perform a complex 'many- speaker', 'multimodal' dialogue. Overall, we col- lected 49 FIFA-18 game videos along with their users' chat, and divided them into 33 videos for training, 8 videos for validation, and 8 videos for testing. Each such video is several hours long, pro- viding a good amount of data <ref type="table" target="#tab_1">(Table 2)</ref>.</p><p>To extract triples (instances) of video context, chat context, and response from this data, we di- vide these videos based on the fixed time frames instead of fixed number of utterances in order to maintain conversation topic clusters (because of the sparse nature of chat utterances count over the time). First, we use 20-sec context windows to extract the video clips and users utterances in Relevance to Video+Chat filtered response wins 34% 1st response wins 3% Non-distinguishable 63% (56 both-good, 7 both-bad) <ref type="table">Table 1</ref>: Human evaluation of our dataset, comparing our filtered responses versus the first response in the window (for relevance w.r.t. video and chat contexts).</p><p>this time frame, and use it as our video and chat contexts, resp. Next, the chat utterances in the immediately-following 10-sec window (response window) that do not overlap with the next in- stance's context window are considered as poten- tial responses. 1 Hence, there are only two in- stances (triples) in a 60-sec long video, i.e., 20-sec video+chat context window and 10-sec response window, and there is no overlap between the in- stances. Now, out of these potential responses, to only allow the response that has at least some good coherence and relevance with the chat context's topic, we choose the first (earliest) response that has high similarity with some other utterance in this response window (using 0.5 BLEU-4 thresh- old, based on manual inspection). <ref type="bibr">2</ref> Human Quality Evaluation of Data Filtering Process: To evaluate the quality of the responses that result from our filtering process described above, we performed an anonymous (randomly shuffled w/o identity) human comparison between the response selected by our filtering process vs. the first response from the response window with- out any filtering, based on relevance w.r.t. video and chat context. <ref type="table">Table 1</ref> presents the results on 100 sample size, showing that humans in a blind- test found 90% (34+56) of our filtered responses as valid responses, verifying that our response se- lection procedure is reasonable. Furthermore, out of these 90% valid responses, we found that 55% are chat-only relevant, 11% are video-only rele- vant, and 24% are both video+chat relevant. In order to make the above procedure safe and to make the dataset more challenging, we also dis- courage frequent responses (top-20 most-frequent  generic utterances) unless no other response satis- fies the similarity condition, hence suppressing the frequent responses. <ref type="bibr">3</ref> If we couldn't find any utter- ance based on the multi-response matching pro- cedure described above, then we just consider the first utterance in the 10-second window as the re- sponse. <ref type="bibr">4</ref> We also make sure that the chat context window has at least 4 utterances, otherwise we exclude that context window and also the corre- sponding response window from the dataset. After all this processing, our final resulting dataset con- tains 10, 510 samples in training, 2, 153 samples in validation, and 2, 780 samples in test. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset Analysis</head><p>Dataset Statistics <ref type="table" target="#tab_1">Table 2</ref> presents the full statis- tics on train, validation, and test sets of our Twitch-FIFA dataset, after the filtering process de- scribed in Sec. 3.1. As shown, the average chat context length in the dataset is around 68 words, and the average response length is 6.3 words.</p><p>Chat Context Size <ref type="figure">Fig. 3</ref> presents the study of number of utterances in the chat context vs. the number of such training samples. As we limit the minimum number of utterances to 4, chat context with less than 4 utterances is not present in the dataset. From the <ref type="figure">Fig. 3</ref>, it is clear that as the num- ber of utterances in the chat context increases, the number of such training samples decrease. Frequent Words <ref type="figure">Fig. 4</ref> presents the top-20 fre- quent words (excluding stop words) and their cor- responding frequency in our Twitch-FIFA dataset. Most of these frequent words are related to soccer vocabulary. Also, some of these frequent words are twitch emotes (e.g. 'kappa', 'inceptionlove').</p><p>3 Note that this filtering suppresses the performance of simple frequent-response baseline described in Sec. 4.1. <ref type="bibr">4</ref> Other preprocessing steps include: omit the utterances in the response window which refer to a speaker name out of the current chat context; remove non-representative utter- ances, e.g., those with hyperlinks; replace (anonymize) all the user identities mentioned in the utterances with a com- mon tag (i.e., anonymizing due to similar intuitions from the Q&amp;A community ( <ref type="bibr" target="#b21">Hermann et al., 2015)</ref>). <ref type="bibr">5</ref> Note that this is substantially larger than or comparable to most current video captioning datasets. We plan to further extend our dataset based on diverse games and video types. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>Let v = {v 1 , v 2 , .., v m } be the video context frames, u = {u 1 , u 2 , .., u n } be the textual chat (utterance) context tokens, and r = {r 1 , r 2 , .., r k } be response tokens generated (or retrieved).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>Our simple non-trained baselines are Most- Frequent-Response (re-rank the candidate re- sponses based on their frequency in the training set), Chat-Response-Cosine (re-rank the candidate responses based on their similarity score w.r.t. the chat context), and Nearest-Neighbor (find the K- best similar chat contexts in the training set, take their corresponding responses, and then re-rank the candidate responses based on mean similar- ity score w.r.t. this K-best response set). For trained baselines, we use logistic regression and Naive Bayes methods. We use the final state of a Twitch-trained RNN Language Model to represent the chat context and response. Please see supple- mentary for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discriminative Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Triple Encoder</head><p>For our simpler discriminative model, we use a 'triple encoder' to encode the video context, chat context, and response (see <ref type="figure" target="#fig_4">Fig. 5</ref>), as an exten- sion of the dual encoder model in <ref type="bibr" target="#b32">Lowe et al. (2015)</ref>. The task here is to predict the given train- ing triple (v, u, r) as positive or negative. Let h v f , h u f , and h r f be the final state information of the video, chat, and response LSTM-RNN (bidirec- tional) encoders respectively; then the probability of a positive training triple is defined as follows:</p><formula xml:id="formula_0">p(v, u, r; θ) = σ([h v f ; h u f ] T W h r f + b) (1)</formula><p>where W and b are trainable parameters. Here, W can be viewed as a similarity matrix which will bring the context [h v f ; h u f ] into the same space as the response h r f , and get a suitable similarity score. For optimizing our discriminative model, we use max-margin loss function similar to <ref type="bibr" target="#b36">Mao et al. (2016)</ref> and <ref type="bibr" target="#b24">Yu et al. (2017)</ref>. Given a positive training triple (v, u, r), let the corresponding neg- ative training triples be (v , u, r), (v, u , r), and (v, u, r ), i.e., one modality is wrong at a time in each of these three (see Sec. 5 for the negative ex- ample selection). The max-margin loss is:</p><formula xml:id="formula_1">L(θ) =</formula><p>[max(0, M + log p(v , u, r) − log p(v, u, r))</p><p>+ max(0, M + log p(v, u , r) − log p(v, u, r))</p><formula xml:id="formula_2">+ max(0, M + log p(v, u, r ) − log p(v, u, r))]<label>(2)</label></formula><p>where the summation is over all the training triples in the dataset. M is a tunable margin hyperparam- eter between positive and negative training triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Tridirectional Attention Flow (TriDAF)</head><p>Our tridirectional attention flow model learns stronger joint spaces between the three modalities in a mutual-information way. We use bidirectional attention flow mechanisms (Seo et al., 2017) be- tween the video and chat contexts, between the video context and the response, as well as between the chat context and the response, hence enabling attention flow across all three modalities, as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>. We name this model Tridirectional At- tention Flow or TriDAF. We will next discuss the bidirectional attention flow mechanism between video and chat contexts, but the same formula- tion holds true for bidirectional attention between video context and response, and between chat con- text and response. Given the video context hidden . . . . . . . . . . . .</p><p>response-to-video attention chat-to-video attention . . . . . .</p><p>video-to-chat attention response-to-chat attention video-to-response attention chat-to-response attention state h v i and chat context hidden state h u j at time steps i and j respectively, the bidirectional atten- tion mechanism is based on the similarity score:</p><formula xml:id="formula_3">S (v,u) i,j = w T S (v,u) [h v i ; h u j ; h v i h u j ]<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">S (v,u) i,j</formula><p>is a scalar, w S (v,u) is a trainable parameter, and denote element-wise multi- plication. The attention distribution from chat context to video context is defined as α i: = sof tmax(S i: ), hence the chat-to-video context vector c v←u i = j α i,j h u j . Similarly, the attention distribution from video context to chat context is defined as β j: = sof tmax(S :j ), hence the video- to-chat context vector c u←v j = i β j,i h v i . We then compute similar bidirectional attention flow mechanisms between the video context and response, and between the chat context and re- sponse. Then, we concatenate each hidden state and its corresponding context vector from other two modalities, e.g., </p><formula xml:id="formula_5">e s i = V v a tanh(W v a ˆ h v i + b v a )<label>(4)</label></formula><p>where V v a , W v a , and b v a are trainable self-attention parameters. The final representation vector of the full video context after self-attention isˆcisˆ isˆc v = i α s i ˆ h v i . Similarly, the final representation vec- tors of the chat context and the response arê c u andˆcandˆ andˆc r , respectively. Finally, the probability that the given training triple (v, u, r) is positive is:</p><formula xml:id="formula_6">p(v, u, r; θ) = σ([ˆ c v ; ˆ c u ] T W ˆ c r + b)<label>(5)</label></formula><p>Again, here also we use max-margin loss (Eqn. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generative Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Seq2seq with Attention</head><p>Our simpler generative model is a sequence-to- sequence model with bilinear attention mechanism (similar to <ref type="bibr" target="#b34">Luong et al. (2015)</ref>). We have two en- coders, one for encoding the video context and another for encoding the chat context, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. We combine the final state informa- tion from both encoders and give it as initial state to the response generation decoder. The two en- coders and the decoder are all two-layer LSTM- RNNs. Let h v i and h u j be the hidden states of video and chat encoders at time step i and j re- spectively. At each time step t of the decoder with hidden state h r t , the decoder attends to parts of video and chat encoders and uses the combined information to generate the next token. Let α t and β t be the attention weight distributions for video and chat encoders respectively with video context vector c v t = i α t,i h v i and chat context vector c u t = j β t,j h u j . The attention distribution for video encoder is defined as (and the same holds for chat encoder):</p><formula xml:id="formula_7">e t,i = h r t T W v a h v i ; α t = softmax(e t )<label>(6)</label></formula><p>where W v a is a trainable parameter. Next, we con- catenate the attention-based context information (c v t and c u t ) and decoder hidden state (h r t ), and do a non-linear transformation to get the final hidden statê h r t as follows:</p><formula xml:id="formula_8">ˆ h r t = tanh(W c [c v t ; c u t ; h r t ])<label>(7)</label></formula><p>where W c is again a trainable parameter. Fi- nally, we project the final hidden state informa- tion to vocabulary size and give it as input to a softmax layer to get the vocabulary distribution p(r t |r 1:t−1 , v, u; θ). During training, we minimize the cross-entropy loss defined as follows:</p><formula xml:id="formula_9">L XE (θ) = − t log p(r t |r 1:t−1 , v, u; θ) (8)</formula><p>where the final summation is over all the training triples in the dataset. Further, to train a stronger generative model with negative training examples (which teaches the model to give higher generative decoder prob- ability to the positive response as compared to all the negative ones), we use a max-margin loss (sim- ilar to Eqn. 2 in Sec. 4.2.1):</p><formula xml:id="formula_10">LMM(θ) = [max(0, M + log p(r|v , u) − log p(r|v, u))</formula><p>+ max(0, M + log p(r|v, u ) − log p(r|v, u))</p><formula xml:id="formula_11">+ max(0, M + log p(r |v, u) − log p(r|v, u))]<label>(9)</label></formula><p>where the summation is over all the training triples in the dataset. Overall, the final joint loss func- tion is a weighted combination of cross-entropy loss and max-margin loss:</p><formula xml:id="formula_12">L(θ) = L XE (θ) + λL MM (θ)</formula><p>, where λ is a tunable hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Bidirectional Attention Flow (BiDAF)</head><p>The stronger version of our generative model extends the two-encoder-attention-decoder model above to add bidirectional attention flow (BiDAF) mechanism ( <ref type="bibr" target="#b49">Seo et al., 2017</ref>) between video and chat encoders, as shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. Given the hid- den states h v i and h u j of video and chat encoders at time step i and j, the final hidden states after the BiDAF arê</p><formula xml:id="formula_13">h v i = [h v i ; c v←u i ] andˆhandˆ andˆh u j = [h u i ; c u←v j ]</formula><p>(similar to as described in Sec. 4.2.2), respectively. Now, the decoder attends over these final hidden states, and the rest of the decoder process is simi- lar to Sec 4.3.1 above, including the weighted joint cross-entropy and max-margin loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Evaluation We first evaluate both our discrimi- native and generative models using retrieval-based recall@k scores, which is a concrete metric for such dialogue generation tasks ( <ref type="bibr" target="#b32">Lowe et al., 2015</ref>). For our discriminative models, we simply rerank the given responses (in a candidate list of size 10, based on 9 negative examples; more details below) Models r@1 r@2 r@5 BASELINES Most-Frequent-Response 10.0 16.0 20.9 Naive Bayes 9.6 20.9 51.5 Logistic Regression 10.8 21.8 52.5 Nearest Neighbor 11.4 22.6 53.2 Chat-Response-Cosine 11.4 22.0 53.</p><note type="other">2 DISCRIMINATIVE MODEL Dual Encoder (C) 17.1 30.3 61.9 Dual Encoder (V) 16.3 30.5 61.1 Triple Encoder (C+V) 18.1 33.6 68.5 TriDAF+Self Attn (C+V) 20.7 35.3 69.4 GENERATIVE MODEL Seq2seq +Attn (C) 14.8 27.3 56.6 Seq2seq +Attn (V) 14.8 27.2 56.7 Seq2seq + Attn (C+V)</note><p>15.7 28.0 57.0 Seq2seq + Attn + BiDAF (C+V) 16.5 28.5 57.7 <ref type="table">Table 3</ref>: Performance of our baselines, discriminative models, and generative models for recall@k metrics on our Twitch-FIFA test set. C and V represent chat and video context, respectively.</p><p>in the order of the probability score each response gets from the model. If the positive response is within the top-k list, then the recall@k score is 1, otherwise 0, following previous Ubuntu-dialogue work ( <ref type="bibr" target="#b32">Lowe et al., 2015)</ref>. For the generative mod- els, we follow a similar approach, but the rerank- ing score for a candidate response is based on the log probability score given by the generative models' decoder for that response, following the setup of previous visual-dialog work ( <ref type="bibr" target="#b10">Das et al., 2017)</ref>. In our experiments, we use recall@1, recall@2, and recall@5 scores. For complete- ness, we also report the phrase-matching metric scores: METEOR <ref type="bibr" target="#b12">(Denkowski and Lavie, 2014</ref>) and ROUGE <ref type="bibr" target="#b28">(Lin, 2004</ref>) for our generative mod- els. We also present human evaluation.</p><p>Training Details For negative samples, during training, for every positive triple (video, chat, response) in the training set, we sample 3 ran- dom negative triples. For validation/test, we sam- ple 9 random negative responses elsewhere from the validation/test set. Also, the negative sam- ples don't come from the video corresponding to the positive response. More details of negative samples and other training details (e.g., dimen- sion/vocab sizes, visual feature details, validation- based hyperparamater tuning and model selec- tion), are discussed in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Human Evaluation of Dataset</head><p>First, the overall human quality evaluation of our dataset (shown in Table 1) demonstrates that it contains 90% responses relevant to video and/or chat context. Next, we also do a blind hu- man study on the recall-based setup (on a set of 100 samples from the validation set), where we anonymize the positive response by randomly mixing it with 9 tricky negative responses in the retrieval list, and ask the user to select the most relevant response for the given video and/or chat context. We found that human performance on this task is around 55% recall@1, demonstrating that this 10-way-discriminative recall-based task setup is reasonably challenging for humans, 7 but also that there is a lot of scope for future model improvements because the chance baseline is only 10% and the best-performing model so far (see Sec. 6.3) achieves only 22% recall@1 (on dev set), and hence there is a large 33% gap. <ref type="table">Table 3</ref> displays all our primary results. We first discuss results of our simple non-trained and trained baselines (see Sec. 4.1). The 'Most- Frequent-Response' baseline, which just ranks the 10-sized response retrieval list based on their fre- quency in the training data, gets only around 10% recall@1. 8 Our other non-trained baselines: 'Chat-Response-Cosine' and 'Nearest Neighbor', which ranks the candidate responses based on (Twitch-trained RNN encoder's vector) cosine similarity with chat-context and K-best training contexts' response vectors, respectively, achieves slightly better scores. We also show that our sim- ple trained baselines (logistic regression and near- est neighbor) also achieve relatively low scores, indicating that a simple, shallow model will not work on this challenging dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baseline Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discriminative Model Results</head><p>Next, we present the recall@k retrieval perfor- mance of our various discriminative models in <ref type="table">Ta</ref>   ble 3: dual encoder (chat context only), dual en- coder (video context only), triple encoder, and TriDAF model with self-attention. Our dual en- coder models are significantly better than random choice and all our simple baselines above, and further show that they have complementary in- formation because using both of them together (in 'Triple Encoder') improves the overall perfor- mance of the model. Finally, we show that our novel TriDAF model with self-attention performs significantly better than the triple encoder model. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Generative Model Results</head><p>Next, we evaluate the performance of our gener- ative models with both retrieval-based recall@k scores and phrase matching-based metrics as dis- cussed in Sec. 5 (as well as human evaluation). We first discuss the retrieval-based recall@k re- sults in <ref type="table">Table 3</ref>. Starting with a simple sequence- to-sequence attention model with video only, chat only, and both video and chat encoders, the re- call@k scores are better than all the simple base- lines. Moreover, using both video+chat context is again better than using only one context modal- ity. Finally, we show that the addition of the bidi- rectional attention flow mechanism improves the performance in all recall@k scores. 10 Note that generative model scores are lower than the dis- criminative models on retrieval recall@k metric, which is expected (see discussion in previous vi- sual dialogue work ( <ref type="bibr" target="#b10">Das et al., 2017)</ref>), because discriminative models can tune to the biases in the response candidate options, but generative mod- els are more useful for real-world tasks such as <ref type="bibr">9</ref> Statistical significance of p &lt; 0.01 for recall@1, based on the bootstrap test <ref type="bibr" target="#b41">(Noreen, 1989;</ref><ref type="bibr" target="#b15">Efron and Tibshirani, 1994)</ref>   generation of novel responses word-by-word from scratch in Siri/Alexa/Cortana style applications (whereas discriminative models can only rank the pre-given list of responses). We also evaluate our generative models with phrase-level matching metrics: METEOR and ROUGE-L, as shown in <ref type="table" target="#tab_3">Table 4</ref>. Again, our BiDAF model is stat. significantly better than non- BiDAF model on both METEOR (p &lt; 0.01) and ROUGE-L (p &lt; 0.02) metrics. Since dialogue systems can have several diverse, non-overlapping valid responses, we consider a multi-reference setup where all the utterances in the 10-sec re- sponse window are treated as valid responses. 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Human Evaluation of Models</head><p>Finally, we also perform human evaluation to compare our top two generative models, i.e., the video+chat seq2seq with attention and its exten- sion with BiDAF (Sec. 4.3), based on a 100-sized sample. We take the generated response from both these models, and randomly shuffle these pairs to anonymize model identity. We then ask two an- notators (for 50 task instances each) to score the responses of these two models based on relevance. Note that the human evaluators were familiar with Twitch FIFA-18 video games and also the Twitch's unique set of chat mannerisms and emotes. As shown in <ref type="table" target="#tab_4">Table 5</ref>, our BiDAF based generative model performs better than the non-BiDAF one, which is already quite a strong video+chat encoder model with attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ablations and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Negative Training Pairs</head><p>We also compare the effect of different negative training triples that we discussed in Sec. 5. <ref type="table" target="#tab_6">Ta- ble 6</ref>   training triple (with just a negative response) vs. three negative training triples (one with negative video context, one with negative chat context, and another with negative response), showing that us- ing the 3-negative examples setup is substantially better. <ref type="table">Table 7</ref> shows the performance comparison be- tween the classification loss and max-margin loss on our TriDAF with self-attention discriminative model (Sec. 4.2.2). We observe that max-margin loss performs better than the classification loss, which is intuitive because max-margin loss tries to differentiate between positive and negative train- ing example triples.  <ref type="table">Table 7</ref>: Ablation of classification vs. max-margin loss on our TriDAF discriminative model (on dev set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Discriminative Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Generative Loss Functions</head><p>For our best generative model (BiDAF), <ref type="table" target="#tab_10">Table 8</ref> shows that using a joint loss of cross-entropy and max-margin is better than just using only cross-entropy loss optimization (Sec. 4.3.1). Max- margin loss provides knowledge about the nega- tive samples for the generative model, hence im- proves the retrieval-based recall@k scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Attention Visualization and Examples</head><p>Finally, we show some interesting output exam- ples from both our discriminative and generative models as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>  visualizes that our models can learn some cor- rect attention alignments from the generated out- put response word to the appropriate (goal-related) video frames as well as chat context words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a new game-chat based video- context, many-speaker dialogue task and dataset. We also presented several baselines and state-of- the-art discriminative and generative models on this task. We hope that this testbed will be a good starting point to encourage future work on the challenging video-context dialogue paradigm. In future work, we plan to investigate the effects of multiple users, i.e., the multi-party aspect of this dataset. We also plan to explore advanced video features such as activity recognition, person iden- tification, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We release all data, code, and models at: https:// github.com/ramakanth-pasunuru/video-dialogue S1: what an offside trap OMEGALUL S2: Lol that finish bro S3: suprised you didn't do the extra pass S4: @S10 a drunk bet? S5: @S11 thanks mate S6: could have passed one more S7: Pass that S1: record now! S8: !record S9: done a nother pass there</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sample example from our many-speaker, video-context dialogue dataset, based on live soccer game chat. The task is to predict the response (bottomright) using the video context (left) and the chat context (top-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample page of live broadcast of FIFA-18 game on twitch.tv with concurrent user chat.</figDesc><graphic url="image-6.png" coords="3,72.82,62.81,167.62,94.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Distribution of #utterances in chat context (w.r.t. the #training examples for each case).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of our 'triple encoder' discriminative model, with bidirectional-LSTM-RNN encoders for video, chat context, and response.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overview of our tridirectional attention flow (TriDAF) model with all pairwise modality attention modules, as well as self-attention on video context, chat context, and response as inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>]</head><label></label><figDesc>for the i th timestep of the video context. Finally, we add self-attention mechanism (Lin et al., 2017) across the concatenated hidden states of each of the three modules. 6 IfˆhIfˆ Ifˆh v i is the final concatenated vector of the video context at time step i, then the self- attention weights α s for this video context are the softmax of e s :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Overview of our generative model with bidirectional attention flow between video context and chat context during response generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Output retrieval (left) and generative (right) examples from TriDAF and BiDAF models, resp. Chat Context: xxuxx haha 19 is not bad brotha. i didnt even qualify lol feelbad || pogchamp || siiiii pogchamp || boooooooooooooo lul || you guys think i should get dembele or if alessandrini Response: comeback goal Figure 9: Attention visualization: generated word 'goal' in response is intuitively aligning to goal-related video frames (top-3-weight frames highlighted) and context words (top-10-weight words highlighted).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Twitch-FIFA dataset's chat statistics (lengths are defined in terms of number of words).</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>-</head><label></label><figDesc></figDesc><table>Models 
METEOR ROUGE-L 
MULTIPLE REFERENCES 
Seq2seq + Atten. (C) 
2.59 
8.44 
Seq2seq + Atten. (V) 
2.66 
8.34 
Seq2seq + Atten. (C+V) ⊗ 
3.03 
8.84 
⊗ + BiDAF (C+V) 
3.70 
9.82 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Performance of our generative models on phrase matching metrics.</head><label>4</label><figDesc></figDesc><table>Models 
Relevance 
Seq2seq + Atten. (C+V) wins 
41.0 % 
BiDAF wins 
34.0 % 
Non-distinguishable 
25.0 % 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Human evaluation comparing the baseline and 
BiDAF generative models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : Ablation (dev) of one vs. three negative exam- ples for TriDAF self-attention discriminative model.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>shows the comparison between one negative bloodtrail bloodtrail bloodtrail bloodtrail bloodtrail || yoooo || kappapride || xxuxx skillzzzz , favourite player you have used this year ? || pl3ad aa9love || are you playin with ksi ? ? kappa xxuxx || bought okocha cuz of you ant . first game 2 goals 3 assists ! game changer thank you m8 || play || ! pause || resume ||aids || where has all thr challenges gone aswell ? || did mat yet messi ? || hellllllllllllllllllllllllllllllllllllllllllo || put messi on get in behind if u can || chris is getting ronaldo and messi || no one wants jamies coctail sausage haha || free kick with messi Ground-truth: play it to messi he makes good runs Generated: get messi for the other team</head><label></label><figDesc></figDesc><table>twerkchoke 
twerkchoke twerkchoke || lul 

1) good pass jebaited 

2) shawn mendez kreygasm 
kreygasm 

3) can say that i am american 

4) ! camera 

5) can you notice me 

6) do you have a main squad 

7) otw nelson for 47k imma buy 
right now on xbox 

8) do * 

9) inceptionderp inceptionlove 

10) bpl is over priced 

chat is </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Ablation of cross-entropy loss vs. cross-
entropy+maxmargin loss for our BiDAF-based gener-
ative model (on dev set). 

</table></figure>

			<note place="foot" n="1"> We use non-overlapping windows because: (1) the utterances are non-uniformly distributed in time and hence if we have a shifting window, sometimes a particular data instance/chunk becomes very sparse and contains almost zero utterances; (2) we do not want overlap between response of one window with the context of the next window, so as to avoid the encoder already having seen the response (as part of context) that the decoder needs to generate for the other window. 2 Based on intuition that if multiple speakers are saying the same response in that 10-second window, then this response should be more meaningful/relevant w.r.t. chat context.</note>

			<note place="foot" n="6"> In our preliminary experiments, we found that adding self-attention is 0.92% better in recall@1 and faster than passing the hidden states through another layer of RNN, as done in Seo et al. (2017).</note>

			<note place="foot" n="7"> This relatively low human recall@1 performance is because this is a challenging, 10-way-discriminative evaluation, i.e., the choice comes w.r.t. 9 tricky negative examples along with just 1 positive example (hence chance-baseline is only 10%). Note that these negative examples are an artifact of specifically recall-based evaluation only, and will not affect the more important real-world task of response generation (for which our dataset&apos;s response quality is 90%, as shown in Table 1). Moreover, our dataset filtering (see Sec. 3.1) also &apos;suppresses&apos; simple baselines and makes the task even harder. 8 Note that the performance of this baseline is worse than the random choice baseline (recall@1:10%, recall@2:20%, recall@5:50%) because our dataset filtering process already suppresses frequent responses (see Sec. 3.1), in order to provide a challenging dataset for the community.</note>

			<note place="foot" n="11"> Liu et al. (2016b) discussed that BLEU and most phrase matching metrics are not good for evaluating dialogue systems. Also, generative models have very low phrasematching metric scores because the generated response can be valid but still very different from the ground truth reference (Lowe et al., 2015; Liu et al., 2016b; Li et al., 2016). We present results for the relatively better metrics like paraphrase-enabled METEOR for completeness, but still focus on retrieval recall@k and human evaluation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful comments.</p><p>This work was supported by DARPA YFA17-D17AP00022, ARO-YIP Award W911NF-18-1-0336, Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>Sünderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards the understanding of gaming audiences by modeling twitch emotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa-Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Noisy Usergenerated Text</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Models of natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="9977" to="9982" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Embodied conversation: integrating face and gesture into automatic spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for spoken and text dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Natural Language Processing</title>
		<editor>Li Deng and Yang Liu</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Resolving referring expressions in conversational dialogs for natural user interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaleh</forename><surname>Feizollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkanitur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2094" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A universal model for flexible item selection in conversational dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaleh</forename><surname>Feizollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkanitur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="361" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to sportscast: a test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effects of voice-based synthetic assistant on performance of emergency care provider in training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Damacharla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parashar</forename><surname>Dhakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stumbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Javaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devabhaktuni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Artificial Intelligence in Education</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<title level="m">Visual dialog</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spoken language understanding: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato De</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition &amp; Understanding, 2007. ASRU. IEEE Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gemini: A natural language system for spoken-language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dowding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Mark</forename><surname>Gawron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Appelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Cherny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User modeling for spoken dialogue system evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding, 1997. Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">Gurman</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Guhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Oberlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM/IEEE international conference on Human robot interaction</title>
		<meeting>the 3rd ACM/IEEE international conference on Human robot interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video highlight prediction using audience chat reactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">User simulation for spoken dialogue systems: Learning and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallirroi</forename><surname>Georgila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03316</idno>
		<title level="m">Iqa: Visual question answering in interactive environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic similarity applied to spoken dialogue summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics, page 764. Association for Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics, page 764. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dialog state tracking with attention-based sequenceto-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Koji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="552" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cobot in lambdamoo: A social statistics agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Lee</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Kormann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatiotemporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2680" to="2688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The icsi meeting corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Janin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gelbart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nelson</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Peskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<editor>I-I. IEEE</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;03)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Match: An architecture for multimodal dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunaranjan</forename><surname>Vasireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ehlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Whittaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetam</forename><surname>Maloor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="376" to="383" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Task learning through visual demonstration and situated dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop: Symbiotic Cognitive Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09457</idno>
		<title level="m">Lstm based conversation models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Imagegrounded conversations: Multimodal context for natural question and response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanderwende</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08251</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03777</idno>
		<title level="m">Neural belief tracker: Data-driven dialogue state tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">An intelligent personal assistant for task and time management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melinda</forename><surname>Gervasio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Pfeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Tambe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">47</biblScope>
			<pubPlace>AI Magazine</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video highlights detection and summarization with lag-calibration based on concept-emotion mapping of crowdsourced time-sync comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaomei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Workshop on New Frontiers in Summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Natural language generation in dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Human language technology research</title>
		<meeting>the first international conference on Human language technology research</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A corpus collection and annotation framework for learning multimodal clarification strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Kruijff-Korbayová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th SIGdial Workshop on DISCOURSE and DIALOGUE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning effective multimodal dialogue strategies from wizardof-oz data: Bootstrapping and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lemon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="638" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An overview of end-to-end language understanding and dialog management for personal digital assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Robichaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">Zia</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Taking cscw seriously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kjeld</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Bannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Supported Cooperative Work</title>
		<imprint>
			<biblScope unit="page" from="7" to="40" />
			<date type="published" when="1992" />
			<publisher>CSCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Interactive reinforcement learning for taskoriented dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning for Action and Interaction Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reinforcement learning for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn A</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="956" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06714</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On-line active reward learning for policy optimisation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Deep Learning Workshop</title>
		<meeting>ICML Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Advances in automatic meeting record creation and access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schaaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;01)</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Elizaa computer program for the study of natural language communication between man and machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01745</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">CHALET: Cornell house agent learning environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bennnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07357</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
