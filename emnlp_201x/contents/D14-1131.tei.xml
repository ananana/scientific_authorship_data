<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exact Decoding for Phrase-Based Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
							<email>Marc.Dymetman@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Ac</forename><surname>Aziz@sheffield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Uk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Ac</forename><surname>Specia@sheffield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Xerox Research Centre Europe, Grenoble</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country>UK, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exact Decoding for Phrase-Based Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1237" to="1249"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model. We replace this intractable intersection by a tractable relaxation which incorporates a low-order up-perbound on the language model. Exact optimisation is achieved through a coarse-to-fine strategy with connections to adap-tive rejection sampling. We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets. This is the first work to tractably tackle exact opti-misation with language models of orders higher than 3.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Statistical Machine Translation (SMT), the task of producing a translation for an input string x = x 1 , x 2 , . . . , x I is typically associated with find- ing the best derivation d * compatible with the in- put under a linear model. In this view, a derivation is a structured output that represents a sequence of steps that covers the input producing a translation. Equation 1 illustrates this decoding process.</p><formula xml:id="formula_0">d * = argmax d∈D(x) f (d)<label>(1)</label></formula><p>The set D(x) is the space of all derivations com- patible with x and supported by a model of trans- lational equivalences <ref type="bibr" target="#b33">(Lopez, 2008)</ref>. The func- tion f (d) = Λ · H(d) is a linear parameteri- sation of the model <ref type="bibr" target="#b36">(Och, 2003)</ref>. It assigns a real-valued score (or weight) to every derivation d ∈ D(x), where Λ ∈ R m assigns a relative importance to different aspects of the derivation independently captured by m feature functions</p><formula xml:id="formula_1">H(d) = H 1 (d), . . . , H m (d) ∈ R m .</formula><p>The fully parameterised model can be seen as a discrete weighted set such that feature func- tions factorise over the steps in a derivation. That is, H k (d) = e∈d h k (e), where h k is a (local) feature function that assesses steps independently and d = e 1 , e 2 , . . . , e l is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·h 1 (e), h 2 (e), . . . , h m (e). The set D is typically finite, however, it contains a very large number of structures -exponential (or even factorial, see §2) with the size of x -making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable ( <ref type="bibr" target="#b41">Tillmann et al., 1997;</ref><ref type="bibr" target="#b35">Och et al., 2001</ref>), thus it is common to resort to heuristic techniques in order to find an approxi- mation to d * ( <ref type="bibr" target="#b28">Koehn et al., 2003;</ref><ref type="bibr" target="#b9">Chiang, 2007)</ref>.</p><p>Evaluation exercises indicate that approximate search algorithms work well in practice ( <ref type="bibr" target="#b3">Bojar et al., 2013</ref>). The most popular algorithms pro- vide solutions with unbounded error, thus pre- cisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artifi- cially small datasets ( <ref type="bibr" target="#b16">Germann et al., 2001;</ref><ref type="bibr" target="#b24">Iglesias et al., 2009;</ref><ref type="bibr" target="#b1">Aziz et al., 2013</ref>). Other work has employed less common approximations to the model reducing its search space complexity ( <ref type="bibr" target="#b31">Kumar et al., 2006;</ref><ref type="bibr" target="#b6">Chang and Collins, 2011;</ref><ref type="bibr" target="#b40">Rush and Collins, 2011</ref>). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models.</p><p>We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy ( . In a nutshell, we re- lax the decoding problem with respect to the Lan- guage Model (LM) component. This coarse view is incrementally refined based on evidence col-lected via maximisation. A refinement increases the complexity of the model only slightly, hence dynamic programming remains feasible through- out the search until convergence. We test our de- coding strategy with realistic models using stan- dard data sets. We also contribute with optimum derivations which can be used to assess future im- provements to approximate decoders. In the re- maining sections we present the general model ( §2), survey contributions to exact optimisation ( §3), formalise our novel approach ( §4), present experiments ( §5) and conclude ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Phrase-based SMT</head><p>In phrase-based SMT ( <ref type="bibr" target="#b28">Koehn et al., 2003)</ref>, the building blocks of translation are pairs of phrases (or biphrases). A translation derivation d is an ordered sequence of non-overlapping biphrases which covers the input text in arbitrary order gen- erating the output from left to right. 1</p><formula xml:id="formula_2">f (d) = ψ(y) + l i=1 φ(e i ) + l−1 i=1 δ(e i , e i−1 ) (2)</formula><p>Equation 2 illustrates a standard phrase-based model ( <ref type="bibr" target="#b28">Koehn et al., 2003)</ref>: ψ is a weighted tar- get n-gram LM component, where y is the yield of d; φ is a linear combination of features that decompose over phrase pairs directly (e.g. back- ward and forward translation probabilities, lexi- cal smoothing, and word and phrase penalties); and δ is an unlexicalised penalty on the num- ber of skipped input words between two adjacent biphrases. The weighted logic program in <ref type="figure">Figure  1</ref> specifies the fully parameterised weighted set of solutions, which we denote D(x), f (d). <ref type="bibr">2</ref> A weighted logic program starts from its ax- ioms and follows exhaustively deducing new items by combination of existing ones and no deduction happens twice. In <ref type="figure">Figure 1</ref>, a nonteminal item summarises partial derivation (or hypotheses). It is denoted by <ref type="bibr">[C, r, γ]</ref> (also known as carry), where: C is a coverage vector, necessary to impose the non-overlapping constraint; r is the rightmost po- sition most recently covered, necessary for the computation of δ; and γ is the last n − 1 words 1 Preventing phrases from overlapping requires an expo- nential number of constraints (the powerset of x) rendering the problem NP-complete <ref type="bibr" target="#b27">(Knight, 1999)</ref>. <ref type="bibr">2</ref> Weighted logics have been extensively used to describe weighted sets <ref type="bibr" target="#b34">(Lopez, 2009)</ref>, operations over weighted sets <ref type="bibr" target="#b9">(Chiang, 2007;</ref><ref type="bibr" target="#b13">Dyer and Resnik, 2010)</ref>, and a variety of dy- namic programming algorithms <ref type="bibr" target="#b10">(Cohen et al., 2008)</ref>.</p><formula xml:id="formula_3">ITEM {0, 1} I , [0, I + 1], ∆ n−1 GOAL 1 I , I + 1, EOS AXIOM BOS → BOS [0 I , 0, BOS] : ψ(BOS) EXPAND C, r, y j−1 j−n+1 x i i φr − − → y j j C , i , y j j −n+2 : w i k=i c k = ¯ 0 where c k = c k if k &lt; i or k &gt; i else ¯ 1 w = φr ⊗ δ(r, i) ⊗ ψ(y j j |y j−1 j−n+1 ) ACCEPT 1 I , r, γ [1 I , I + 1, EOS] : δ(r, I + 1) ⊗ ψ(EOS|γ)</formula><p>r ≤ I <ref type="figure">Figure 1</ref>: Specification for the weighted set of translation derivations in phrase-based SMT with unconstrained reordering.</p><p>in the yield, necessary for the LM component. The program expands partial derivations by concatena- tion with a translation rule</p><formula xml:id="formula_4">x i i φr − − → y j j</formula><p>, that is, an instantiated biphrase which covers the span x i i and yields y j j with weight φ r . The side condition im- poses the non-overlapping constraint (c k is the kth bit in C). The antecedents are used to compute the weight of the deduction, and the carry is updated in the consequent (item below the horizontal line). Finally, the rule ACCEPT incorporates the end-of- sentence boundary to complete items. <ref type="bibr">3</ref> It is perhaps illustrative to understand the set of weighted translation derivations as the intersection between two components. One that is only locally parameterised and contains all translation deriva- tions (a translation lattice or forest), and one that re-ranks the first as a function of the interactions between translation steps. The model of transla- tional equivalences parameterised only with φ is an instance of the former. An n-gram LM compo- nent is an instance of the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hypergraphs</head><p>A backward-hypergraph, or simply hypergraph, is a generalisation of a graph where edges have multiple origins and one destination ( <ref type="bibr" target="#b15">Gallo et al., 1993)</ref>. They can represent both finite-state and context-free weighted sets and they have been widely used in SMT ( <ref type="bibr" target="#b23">Huang and Chiang, 2007)</ref>. A hypergraph is defined by a set of nodes (or ver-tices) V and a weighted set of edges E, w. An edge e connects a sequence of nodes in its tail t[e] ∈ V * under a head node h[e] ∈ V and has weight w(e). A node v is a terminal node if it has no incoming edges, otherwise it is a nontermi- nal node. The node that has no outgoing edges, is called root, with no loss of generality we can assume hypergraphs to have a single root node.</p><p>Hypergraphs can be seen as instantiated logic programs. In this view, an item is a template for the creation of nodes, and a weighted deduc- tion rule is a template for edges. The tail of an edge is the sequence of nodes associated with the antecedents, and the head is the node associ- ated with the consequent. Even though the space of weighted derivations in phrase-based SMT is finite-state, using a hypergraph as opposed to a finite-state automaton makes it natural to encode multi-word phrases using tails. We opt for rep- resenting the target side of the biphrase as a se- quence of terminals nodes, each of which repre- sents a target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Beam filling algorithms</head><p>Beam search ( <ref type="bibr" target="#b28">Koehn et al., 2003</ref>) and cube prun- ing <ref type="bibr" target="#b9">(Chiang, 2007)</ref> are examples of state-of-the-art approximate search algorithms. They approximate the intersection between the translation forest and the language model by expanding a limited beam of hypotheses from each nonterminal node. Hy- potheses are organised in priority queues accord- ing to common traits and a fast-to-compute heuris- tic view of outside weights (cheapest way to com- plete a hypothesis) puts them to compete at a fairer level. Beam search exhausts a node's possible ex- pansions, scores them, and discards all but the k highest-scoring ones. This process is wasteful in that k is typically much smaller than the number of possible expansions. Cube pruning employs a pri- ority queue at beam filling and computes k high- scoring expansions directly in near best-first order. The parameter k is known as beam size and it con- trols the time-accuracy trade-off of the algorithm.</p><p>Heafield et al. (2013a) move away from us- ing the language model as a black-box and build a more involved beam filling algorithm. Even though they target approximate search, some of their ideas have interesting connections to ours (see §4). They group hypotheses that share partial language model state ( <ref type="bibr" target="#b32">Li and Khudanpur, 2008)</ref> reasoning over multiple hypotheses at once. They fill a beam in best-first order by iteratively vis- iting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is parti- tioned and the parts are pushed back to the queue. More recently, <ref type="bibr" target="#b22">Heafield et al. (2014)</ref> applied their beam filling algorithm to phrase-based decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exact optimisation</head><p>Exact optimisation for monotone translation has been done using A * search ( <ref type="bibr" target="#b41">Tillmann et al., 1997</ref>) and finite-state operations ( <ref type="bibr" target="#b31">Kumar et al., 2006</ref>). <ref type="bibr" target="#b35">Och et al. (2001)</ref> design near-admissible heuris- tics for A * and decode very short sentences (6- 14 words) for a word-based model <ref type="bibr" target="#b4">(Brown et al., 1993</ref>) with a maximum distortion strategy (d = 3).</p><p>Zaslavskiy et al. <ref type="formula">(2009)</ref> frame phrase-based de- coding as an instance of a generalised Travel- ling Salesman Problem (TSP) and rely on ro- bust solvers to perform decoding. In this view, a salesman graph encodes the translation options, with each node representing a biphrase. Non- overlapping constraints are imposed by the TSP solver, rather than encoded directly in the sales- man graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large. <ref type="bibr">4</ref> Chang and Collins (2011) relax phrase-based models w.r.t. the non-overlapping constraints, which are replaced by soft penalties through La- grangian multipliers, and intersect the LM com- ponent exhaustively. They do employ a maximum distortion limit (d = 4), thus the problem they tackle is no longer NP-complete. Rush and Collins (2011) relax a hierarchical phrase-based model <ref type="bibr" target="#b8">(Chiang, 2005)</ref>  <ref type="bibr">5</ref> w.r.t. the LM component. The translation forest and the language model trade their weights (through Lagrangian multipliers) so as to ensure agreement on what each component believes to be the maximum. In both approaches, when the dual converges to a compliant solution, the solution is guaranteed to be optimal. Other-wise, a subset of the constraints is explicitly added and the dual optimisation is repeated. They handle sentences above average length, however, resort- ing to compact rulesets (10 translation options per input segment) and using only 3-gram LMs.</p><p>In the context of hierarchical models, <ref type="bibr" target="#b1">Aziz et al. (2013)</ref> work with unpruned forests using up- perbounds. Their approach is the closest to ours. They also employ a coarse-to-fine strategy with the OS * framework ( , and investigate unbiased sampling in addition to op- timisation. However, they start from a coarser upperbound with unigram probabilities, and their refinement strategies are based on exhaustive in- tersections with small n-gram matching automata. These refinements make forests grow unmanage- able too quickly. Because of that, they only deal with very short sentences (up to 10 words) and even then decoding is very slow. We design bet- ter upperbounds and a more efficient refinement strategy. Moreover, we decode long sentences us- ing language models of order 3 to 5. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exact optimisation with OS *</head><p>Dymetman et al. <ref type="formula" target="#formula_0">(2012)</ref> introduced OS * , a unified view of optimisation and sampling which can be seen as a cross between adaptive rejection sam- pling ( <ref type="bibr" target="#b39">Robert and Casella, 2004</ref>) and A * optimisa- tion ( <ref type="bibr" target="#b19">Hart et al., 1968)</ref>. In this framework, a com- plex goal distribution is upperbounded by a sim- pler proposal distribution for which optimisation (and sampling) is feasible. This proposal is incre- mentally refined to be closer to the goal until the maximum is found (or until the sampling perfor- mance exceeds a certain level). <ref type="figure" target="#fig_0">Figure 2</ref> illustrates exact optimisation with OS * . Suppose f is a complex target goal distribution, such that we cannot optimise f , but we can as-</p><formula xml:id="formula_5">sess f (d) for a given d. Let g (0) be an upper- bound to f , i.e., g (0) (d) ≥ f (d) for all d ∈ D(x).</formula><p>Moreover, suppose that g (0) is simple enough to be optimised efficiently. The algorithm proceeds by solving d 0 = argmax d g (0) (d) and comput- ing the quantity r 0 = f (d0) /g (0) (d0). If r 0 were sufficiently close to 1, then g (0) (d 0 ) would be sufficiently close to f (d 0 ) and we would have found the optimum. However, in the illustration</p><formula xml:id="formula_6">g (0) (d 0 ) f (d 0 )</formula><p>, thus r 0 1. At this point the algorithm has concrete evidence to motivate a refinement of g <ref type="formula">(0)</ref> that can lower its maximum, bringing it closer to f * = max d f (d) at the cost of some small increase in complexity. The re- fined proposal must remain an upperbound to f . To continue with the illustration, suppose g <ref type="formula" target="#formula_0">(1)</ref> is obtained. The process is repeated until eventually</p><formula xml:id="formula_7">g (t) (d t ) = f (d t ), where d t = argmax d g (t) (d),</formula><p>for some finite t. At which point d t is the opti- mum derivation d * from f and the sequence of upperbounds provides a proof of optimality. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model</head><p>We work with phrase-based models in a standard parameterisation (Equation 2). However, to avoid having to deal with NP-completeness, we con- strain reordering to happen only within a limited window given by a notion of distortion limit. We require that the last source word covered by any biphrase must be within d words from the leftmost uncovered source position <ref type="bibr" target="#b34">(Lopez, 2009)</ref>. This is a widely used strategy and it is in use in the Moses toolkit ( <ref type="bibr" target="#b29">Koehn et al., 2007)</ref>. <ref type="bibr">8</ref> Nevertheless, the problem of finding the best derivation under the model remains impractica- ble due to nonlocal parameterisation (namely, the n-gram LM component). The weighted set D(x), f (d), which represents the objective, is a complex hypergraph which we cannot afford to construct. We propose to construct instead a simpler hypergraph for which optimisation by dy- namic programming is feasible. This proxy rep- resents the weighted set</p><formula xml:id="formula_8">D(x), g (0) (d)</formula><p>, where <ref type="bibr">x)</ref>. Note that this proposal contains exactly the same translation options as in the original decoding problem. The simplification happens only with respect to the pa- rameterisation. Instead of intersecting the com- plete n-gram LM distribution explicitly, we im- plicitly intersect a simpler upperbound view of it, where by simpler we mean lower-order.</p><formula xml:id="formula_9">g (0) (d) ≥ f (d) for every d ∈ D(</formula><formula xml:id="formula_10">g (0) (d) = l i=1 ω(y[ei]) + l i=1 φ(ei) + l−1 i=1 δ(ei, ei−1) (3)</formula><p>Equation 3 shows the model we use as a proxy to perform exact optimisation over f . In compar- ison to Equation 2, the term</p><formula xml:id="formula_11">l i=1 ω(y[ei]) replaces ψ(y) = λ ψ p LM (y)</formula><p>. While ψ weights the yield y taking into account all n-grams (including those crossing the boundaries of phrases), ω weights edges in isolation. Particularly, ω(y[e i ]) = λ ψ q LM (y[e i ]), where y[e i ] returns the sequence of target words (a target phrase) associated with the edge, and q LM (·) is an upperbound on the true LM probability p LM (·) (see §4.3). It is obvious from Equation 3 that our proxy model is much simpler than the original -the only form of nonlocal pa- rameterisation left is the distortion penalty, which is simple enough to represent exactly.</p><p>The program in <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the con- struction of  A standard ARPA table T stores entries Z, Z.p, Z.b, where Z is an n-gram equal to the concatenation Pz of a prefix P with a word z, Z.p is the conditional probability p(z|P), and Z.b is a so-called "backoff" weight associated with Z.</p><formula xml:id="formula_12">D(x), g (0) (d) . A nonterminal item</formula><formula xml:id="formula_13">l = l + α 1 (C) + 1, ITEM [1, I + 1], {0, 1} d−1 , [0, I + 1] GOAL [I, ∅, I + 1] AXIOMS BOS → BOS [1, 0 d−1 , 0] : ω(BOS) ADJACENT [l, C, r] x i i φr − − → y j j [l , C , i ] : φr ⊗ δ(r, i ) ⊗ ω(y j j ) i = l i −l k=i−l c k = ¯ 0 where l = l + α1(C) + 1 C α1(C) + 1 NON-ADJACENT [l, C, r] x i i φr − − → y j j [l, C , i ] : φr ⊗ δ(r, i ) ⊗ ω(y j j ) i &gt; l i −l k=i−l c k = ¯ 0 |r − i + 1| ≤ d |i − l + 1| ≤ d where c k = c k if k &lt; i − l or k &gt; i − l else ¯ 1 ACCEPT [I + 1, C, r] [I + 1, ∅, I + 1] : δ(r, I + 1) ⊗ ω(EOS) r ≤ I</formula><p>The conditional probability of an arbitrary n-gram p(z|P), whether listed or not, can then be recov- ered from T by the simple recursive procedure shown in Equation 4, where tail deletes the first word of the string P.</p><formula xml:id="formula_14">p(z|P) =    p(z| tail(P))</formula><p>Pz ∈ T and P ∈ T p(z| tail(P)) × P.b</p><p>Pz ∈ T and P ∈ T Pz.p Pz ∈ T</p><p>The optimistic version (or "max-backoff") q of p is defined as q(z|P) ≡ max H p(z|HP), where H varies over all possible contexts extending the prefix P to the left. The Max-ARPA table allows to compute q(z|P) for arbitrary values of z and P. It is constructed on the basis of the ARPA table T by adding two columns to T : a column Z.q that stores the value q(z|P) and a column Z.m that stores an optimistic version of the backoff weight.</p><p>These columns are computed offline in two passes by first sorting T in descending order of n-gram length. <ref type="bibr">12</ref> In the first pass (Algorithm 1), we compute for every entry in the table an opti- mistic backoff weight m. In the second pass (Algo- rithm 2), we compute for every entry an optimistic conditional probability q by maximising over 1- word history extensions (whose .q fields are al- ready known due to the sorting of T ).</p><p>The following Theorem holds (see proof be- low): For an arbitrary n-gram Z = Pz, the prob- ability q(z|P) can be recovered through the proce- dure shown in Equation 5.</p><formula xml:id="formula_16">q(z|P) =    p(z|P)</formula><p>Pz ∈ T and P ∈ T p(z|P) × P.m Pz ∈ T and P ∈ T Pz.q Pz ∈ T</p><p>Note that, if Z is listed in the table, we return its upperbound probability q directly. When the n- gram is unknown, but its prefix is known, we take into account the optimistic backoff weight m of the prefix. On the other hand, if both the n-gram and its prefix are unknown, then no additional context could change the score of the n-gram, in which case q(z|P) = p(z|P).</p><p>In the sequel, we will need the following defini- tions. Suppose α = y J I is a substring of y = y M 1 .</p><p>12 If an n-gram is listed in T , then all its substrings must also be listed. Certain pruning strategies may corrupt this property, in which case we make missing substrings explicit.</p><formula xml:id="formula_18">Then p LM (α) ≡ J k=I p(y k |y k−1 1</formula><p>) is the contribu- tion of α to the true LM score of y. We then ob- tain an upperbound q LM (α) to this contribution by defining q LM (α) ≡ q(y I |) J k=I+1 q(y k |y k−1 I ).</p><p>Proof of Theorem. Let us first suppose that the length of P is strictly larger than the order n of the language model. Then for any H, p(z|HP) = p(z|P); this is be- cause HP / ∈ T and P / ∈ T , along with all intermedi- ary strings, hence, by (4), p(z|HP) = p(z| tail(HP)) = p(z| tail(tail(HP))) = . . . = p(z|P). Hence q(z|P) = p(z|P), and, because Pz / ∈ T and P / ∈ T , the theorem is satisfied in this case.</p><p>Having established the theorem for |P| &gt; n, we now assume that it is true for |P| &gt; m and prove by induction that it is true for |P| = m. We use the fact that, by the definition of q, we have q(z|P) = maxx∈∆ q(z|xP). We have three cases to consider. First, suppose that Pz / ∈ T and P / ∈ T . Then xPz / ∈ T and xP / ∈ T , hence by induction q(z|xP) = p(z|xP) = p(z|P) for any x, therefore q(z|P) = p(z|P). We have thus proven the first case. Second, suppose that Pz / ∈ T and P ∈ T . Then, for any x, we have xPz / ∈ T , and:</p><formula xml:id="formula_19">q(z|P) = max x∈∆ q(z|xP) = max( max x∈∆, xP / ∈T q(z|xP), max x∈∆, xP∈T q(z|xP)).</formula><p>For xP / ∈ T , by induction, q(z|xP) = p(z|xP) = p(z|P), and therefore max x∈∆, xP / ∈T q(z|xP) = p(z|P). For xPz / ∈ T, xP / ∈ T , we have q(z|xP) = p(z|xP) = p(z|P) = Pz.p, where the last equality is due to the fact that Pz ∈ T . For xPz / ∈ T, xP ∈ T , we have q(z|xP) = p(z|xP) × xP.m = p(z|P) × xP.b × xP.m = Pz.p × xP.b × xP.m. For xPz ∈ T , we have q(z|xP) = xPz.q. Overall, we thus have:</p><formula xml:id="formula_20">q(z|P) = max( Pz.p, max x∈∆, xPz / ∈T, xP∈T Pz.p × xP.b × xP.m, max x∈∆, xPz∈T</formula><p>xPz.q ).</p><p>Note that xPz ∈ T ⇒ xP ∈ T , and then one can check that Algorithm 2 exactly computes Pz.q as this maximum over three maxima, hence Pz.q = q(z|P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Max-ARPA: first pass</head><p>1: for Z ∈ T do 2:</p><formula xml:id="formula_21">Z.m ← 1 3: for x ∈ ∆ s.t xZ ∈ T do 4: Z.m ← max(Z.m, xZ.b × xZ.m) 5:</formula><p>end for 6: end for Algorithm 2 Max-ARPA: second pass 1: for Z = Pz ∈ T do 2:</p><p>Pz.q ← Pz.p 3:</p><p>for x ∈ ∆ s.t xP ∈ T do 4:</p><p>if xPz ∈ T then 5:</p><p>Pz.q ← max(Pz.q, xPz.q) 6: else 7:</p><p>Pz.q ← max(Pz.q, Pz.p × xP.b × xP.m) 8:</p><p>end if 9:</p><p>end for 10: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Search</head><p>The search for the true optimum derivation is il- lustrated in Algorithm 3. The algorithm takes as input the initial proposal distribution g (0) (d) (see §4.2, <ref type="figure" target="#fig_2">Figure 3</ref>) and a maximum error (which we set to a small constant 0.001 rather than zero, to avoid problems with floating point precision). In line 3 we find the optimum derivation d in g <ref type="bibr">(0)</ref> (see §4.5). The variable g * stores the maximum score w.r.t. the current proposal, while the vari- able f * stores the maximum score observed thus far w.r.t. the true model (note that in line 5 we as- sess the true score of d). In line 6 we start a loop that runs until the error falls below . This error is the difference (in log-domain) between the proxy maximum g * and the best true score observed thus far f * . 13 In line 7, we refine the current proposal using evidence from d (see §4.6). In line 9, we update the maximum derivation searching through the refined proposal. In line 11, we keep track of the best score so far according to the true model, in order to compute the updated gap in line 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Dynamic Programming</head><p>Finding the best derivation in a proposal hyper- graph is straightforward with standard dynamic programming. We can compute inside weights in the max-times semiring in time proportional <ref type="bibr">13</ref> Because g (t) upperbounds f everywhere, in optimisation we have a guarantee that the maximum of f must lie in the interval [f * , g * ) (see <ref type="figure" target="#fig_0">Figure 2</ref>) and the quantity g * − f * is an upperbound on the error that we incur if we early-stop the search at any given time t. This bound provides a principled criterion in trading accuracy for performance (a direction that we leave for future work). Note that most algorithms for ap- proximate search produce solutions with unbounded error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Exact decoding</head><p>1: function OPTIMISE(g (0) , ) 2:</p><formula xml:id="formula_22">t ← 0 step 3: d ← argmax d g (t) (d) 4: g * ← g (t) (d) 5: f * ← f (d) 6:</formula><p>while (q * − f * ≥ ) do is the maximum error 7:</p><p>g (t+1) ← refine(g (t) , d) update proposal 8:</p><formula xml:id="formula_23">t ← t + 1 9: d ← argmax d g (t) (d) update argmax 10: g * ← g (t) (d) 11: f * ← max(f * , f (d))</formula><p>update "best so far" 12:</p><p>end while 13:</p><p>return g (t) , d 14: end function to O(|V | + |E|) <ref type="bibr" target="#b17">(Goodman, 1999)</ref>. Once inside weights have been computed, finding the Viterbi- derivation starting from the root is straightforward. A simple, though important, optimisation con- cerns the computation of inside weights. The in- side algorithm <ref type="bibr" target="#b2">(Baker, 1979</ref>) requires a bottom-up traverse of the nodes in V . To do that, we topolog- ically sort the nodes in V at time t = 0 and main- tain a sorted list of nodes as we refine g throughout the search -thus avoiding having to recompute the partial ordering of the nodes at every iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Refinement</head><formula xml:id="formula_24">If a derivation d = argmax d g (t) (d) is such that g (t) (d) f (d)</formula><p>, there must be in d at least one n- gram whose upperbound LM weight is far above its true LM weight. We then lower g (t) locally by refining only nonterminal nodes that participate in d. Nonterminal nodes are refined by having their LM states extended one word at a time. <ref type="bibr">14</ref> For an illustration, assume we are perform- ing optimisation with a bigram LM. Suppose that in the first iteration a derivation</p><formula xml:id="formula_25">d 0 = argmax d g (0) (d) is obtained. Now consider an edge in d 0 [l, C, r, ] αy 1 w − → [l 0 , C 0 , r 0 , ]</formula><p>where an empty LM state is made explicit (with an empty string ) and αy 1 represents a target phrase. We refine the edge's head [l 0 , C 0 , r 0 , ] by creating a node based on it, however, with an extended LM state, i.e., [l 0 , C 0 , r 0 , y 1 ]. This motivates a split of the set of incoming edges to the original node, such that, if the target projection of an incoming edge ends in y 1 , that edge is reconnected to the new node as below.</p><p>[l, C, r, ] αy 1</p><formula xml:id="formula_26">w − → [l 0 , C 0 , r 0 , y 1 ]</formula><p>The outgoing edges from the new node are reweighted copies of those leaving the original node. That is, outgoing edges such as</p><formula xml:id="formula_27">[l 0 , C 0 , r 0 , ] y 2 β w − → l , C , r , γ</formula><p>motivate edges such as</p><formula xml:id="formula_28">[l 0 , C 0 , r 0 , y 1 ] y 2 β w⊗w −−−→ l , C , r , γ</formula><p>where w = λ ψ q LM (y1y2) /q LM (y2) is a change in LM probability due to an extended context. <ref type="figure" target="#fig_4">Figure 4</ref> is the logic program that constructs the refined hypergraph in the general case. In com- parison to <ref type="figure" target="#fig_2">Figure 3</ref>, items are now extended to store an LM state. The input is the original hy- pergraph G = V, E and a node v 0 ∈ V to be refined by left-extending its LM state γ 0 with the word y. In the program, uσ</p><formula xml:id="formula_29">w − → v</formula><p>with u, v ∈ V and σ ∈ ∆ * represents an edge in E. An item [l, C, r, γ] v (annotated with a state v ∈ V ) rep- resents a node (in the refined hypergraph) whose signature is equivalent to v (in the input hyper- graph). We start with AXIOMS by copying the nodes in G. In COPY, edges from G are copied unless they are headed by v 0 and their target pro- jections end in yγ 0 (the extended context). Such edges are processed by REFINE, which instead of copying them, creates new ones headed by a re- fined version of v 0 . Finally, REWEIGHT contin- ues from the refined node with reweighted copies of the edges leaving v 0 . The weight update repre- sents a change in LM probability (w.r.t. the upper- bound distribution) due to an extended context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We used the dataset made available by the Work- shop on Statistical Machine Translation (WMT) ( <ref type="bibr" target="#b3">Bojar et al., 2013</ref>) to train a German-English phrase-based system using the Moses toolkit ( <ref type="bibr" target="#b29">Koehn et al., 2007</ref>) in a standard setup. For phrase extraction, we used both Europarl ( <ref type="bibr" target="#b30">Koehn, 2005)</ref> and News Commentaries (NC) totalling about 2.2M sentences. <ref type="bibr">15</ref> For language modelling, in addition to the monolingual parts of Europarl <ref type="bibr">15</ref> Pre-processing: tokenisation, truecasing and automatic compound-splitting (German only). Following <ref type="bibr" target="#b12">Durrani et al. (2013)</ref>, we set the maximum phrase length to 5. and NC, we added News-2013 totalling about 25M sentences. We performed language model interpo- lation and batch-mira tuning (Cherry and Foster, 2012) using newstest2010 (2,849 sentence pairs). For tuning we used cube pruning with a large beam size (k = 5000) and a distortion limit d = 4. Un- pruned language models were trained using lmplz <ref type="bibr" target="#b21">(Heafield et al., 2013b</ref>) which employs modified Kneser-Ney smoothing <ref type="bibr" target="#b26">(Kneser and Ney, 1995)</ref>. We report results on newstest2012.</p><formula xml:id="formula_30">INPUT G = V, E v0 = [l0, C0, r0, γ0] ∈ V where γ0 ∈ ∆ * y ∈ ∆ ITEM [l, C, r, γ ∈ ∆ * ] AXIOMS [l, C, r, γ] v v ∈ V COPY [l, C, r, α] u uβ w − → v [l , C , r , α ] v : w v = v0 ∨ αβ = σyγ0 α, α , β, σ ∈ ∆ * REFINE [l, C, R, α] u uβ w − → v0 [l0, C0, r0, yγ0] : w αβ = σyγ0 α, β, σ ∈ ∆ * REWEIGHT [l0, C0, r0, yγ0] v0σ w − → v [l, C, r, γ] v : w ⊗ w σ, γ ∈ ∆ * where w = λ ψ q LM (yγ0) q LM (γ0)</formula><p>Our exact decoder produces optimal translation derivations for all the 3,003 sentences in the test set. <ref type="table">Table 1</ref> summarises the performance of our novel decoder for language models of order n = 3 to n = 5. For 3-gram LMs we also varied the dis- tortion limit d (from 4 to 6). We report the average time (in seconds) to build the initial proposal, the total run time of the algorithm, the number of it- erations N before convergence, and the size of the hypergraph in the end of the search (in thousands of nodes and thousands of edges). <ref type="bibr">16</ref> n d build (s) total (s) N |V | |E| 3 4</p><p>1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.5 21 190 2.5 159 3 5 3.5 55 303 4.4 343 3 6 10 162 484 8 725 4 4 1.5 50 350 4 288 5 4</head><p>1.5 106 555 6.1 450 <ref type="table">Table 1</ref>: Performance of the exact decoder in terms of: time to build g (0) , total decoding time in- cluding build, number of iterations (N), and num- ber of nodes and edges (in thousands) at the end of the search.</p><p>It is insightful to understand how different as- pects of the initial proposal impact on perfor- mance. Increasing the translation option limit (tol) leads to g (0) having more edges (this dependency is linear with tol). In this case, the number of nodes is only minimally affected -due to the pos- sibility of a few new segmentations. The maxi- mum phrase length (mpl) introduces in g (0) more configurations of reordering constraints ([l, C] in <ref type="figure" target="#fig_2">Figure 3</ref>). However, not many more, due to C being limited by the distortion limit d. In prac- tice, we observe little impact on time performance. Increasing d introduces many more permutations of the input leading to exponentially many more nodes and edges. Increasing the order n of the LM has no impact on g (0) and its impact on the overall search is expressed in terms of a higher number of nodes being locally intersected.</p><p>An increased hypergraph, be it due to addi- tional nodes or additional edges, necessarily leads to slower iterations because at each iteration we must compute inside weights in time O(|V |+|E|). The number of nodes has the larger impact on the number of iterations. OS * is very efficient in ig- noring hypotheses (edges) that cannot compete for an optimum. For instance, we observe that run- ning time depends linearly on tol only through the computation of inside weights, while the number of iterations is only minimally affected. <ref type="bibr">17</ref> An in- |E0| = 178 with d = 6. Observe the exponential depen- dency on distortion limit, which also leads to exponentially longer running times. <ref type="bibr">17</ref> It is possible to reduce the size of the hypergraph throughout the search using the upperbound on the search error g * − f * to prune hypotheses that surely do not stand a chance of competing for the optimum ( <ref type="bibr" target="#b18">Graehl, 2005</ref>). An- other direction is to group edges connecting the same nonter- minal nodes into one partial edge ( <ref type="bibr" target="#b20">Heafield et al., 2013a</ref>) - this is particularly convenient due to our method only visiting the 1-best derivation from g(d) at each iteration. n Nodes at level m LM states at level m 0 1 2 3 4 1 2 3 4 3 0.4 1.2 0.5 - -113 263 - - 4 0.4 <ref type="table">Table 2</ref>: Average number of nodes (in thousands) whose LM state encode an m-gram, and average number of unique LM states of order m in the fi- nal hypergraph for different n-gram LMs (d = 4 everywhere).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6">1.4 0.3 -132 544 212 - 5 0.4 2.1 2.4 0.7 0.1 142 790 479 103</head><p>creased LM order, for a fixed distortion limit, im- pacts much more on the number of iterations than on the average running time of a single iteration. Fixing d = 4, the average time per iteration is 0.1 (n = 3), 0.13 (n = 4) and 0.18 (n = 5). Fixing a 3-gram LM, we observe 0.1 (d = 4), 0.17 (d = 5) and 0.31 (d = 6). Note the exponential growth of the latter, due to a proposal encoding exponen- tially many more permutations. <ref type="table">Table 2</ref> shows the average degree of refine- ment of the nodes in the final proposal. Nodes are shown by level of refinement, where m indi- cates that they store m words in their carry. The table also shows the number of unique m-grams ever incorporated to the proposal. This table il- lustrates well how our decoding algorithm moves from a coarse upperbound where every node stores an empty string to a variable-order representation which is sufficient to prove an optimum derivation.</p><p>In our approach a complete derivation is opti- mised from the proxy model at each iteration. We observe that over 99% of these derivations project onto distinct strings. In addition, while the opti- mum solution may be found early in the search, a certificate of optimality requires refining the proxy until convergence (see §4.1). It turns out that most of the solutions are first encountered as late as in the last 6-10% of the iterations.</p><p>We use the optimum derivations obtained with our exact decoder to measure the number of search errors made by beam search and cube pruning with increasing beam sizes (see <ref type="table" target="#tab_0">Table 3</ref>). Beam search reaches optimum derivations with beam sizes k ≥ 500 for all language models tested. Cube prun- ing, on the other hand, still makes mistakes at k = 1000. <ref type="table" target="#tab_1">Table 4</ref> shows translation quality achieved with different beam sizes for cube prun- ing and compares it to exact decoding. Note that for k ≥ 10 4 cube pruning converges to optimum  <ref type="table" target="#tab_0">Cube pruning  3  4  5  3  4  5  10  938 1294 1475 2168 2347 2377  10 2  19  60  112  613  999 1126  10 3  0  0  0  29  102  167  10 4</ref> 0 0 0 0 4 7  derivations in the vast majority of the cases (100% with a 3-gram LM) and translation quality in terms of BLEU is no different from OS * . However, with k &lt; 10 4 both model scores and translation quality can be improved. <ref type="figure">Figure 5</ref> shows a finer view on search errors as a function of beam size for LMs of order 3 to 5 (fixed d = 4). In <ref type="figure" target="#fig_6">Figure 6</ref>, we fix a 3-gram LM and vary the distortion limit (from 4 to 6). Dotted lines correspond to beam search and dashed lines correspond to cube pruning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented an approach to decoding with unpruned hypergraphs using upperbounds on the language model distribution. The algorithm is an instance of a coarse-to-fine strategy with connec- tions to A * and adaptive rejection sampling known as OS * . We have tested our search algorithm us- ing state-of-the-art phrase-based models employ- ing robust language models. Our algorithm is able to decode all sentences of a standard test set in manageable time consuming very little memory.</p><p>We have performed an analysis of search errors made by beam search and cube pruning and found that both algorithms perform remarkably well for phrase-based decoding. In the case of cube prun- ing, we show that model score and translation  quality can be improved for beams k &lt; 10, 000.</p><p>There are a number of directions that we intend to investigate to speed up our decoder, such as: <ref type="formula" target="#formula_0">(1)</ref> error-safe pruning based on search error bounds; (2) use of reinforcement learning to guide the de- coder in choosing which n-gram contexts to ex- tend; and (3) grouping edges into partial edges, effectively reducing the size of the hypergraph and ultimately computing inside weights in less time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sequence of incrementally refined upperbound proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[l,</head><label></label><figDesc>C, r] stores: the leftmost uncovered position l and a truncated coverage vector C (together they track d input positions); and the rightmost position r most recently translated (necessary for the com- putation of the distortion penalty). Observe how nonterminal items do not store the LM state. 9 The rule ADJACENT expands derivations by concate- nation with a biphrase x i i → y j j starting at the leftmost uncovered position i = l. That causes the coverage window to move ahead to the next leftmost uncovered position:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Specification of the initial proposal hypergraph. This program allows the same reorderings as (Lopez, 2009) (see logic WLd), however, it does not store LM state information and it uses the upperbound LM distribution ω(·).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For xP ∈ T , we have q(z|xP) = p(z|xP) × xP.m = p(z|P) × xP.b × xP.m. Thus, we have: max x∈∆, xP∈T q(z|xP) = p(z|P)× max x∈∆, xP∈T xP.b×xP.m. But now, because of lines 3 and 4 of Algorithm 1, P.m = maxx∈∆, xP∈T xP.b × xP.m, hence maxx∈∆, xP∈T q(z|xP) = p(z|P) × P.m. Therefore, q(z|P) = max(p(z|P), p(z|P)×P.m) = p(z|P)×P.m, where we have used the fact that P.m ≥ 1 due to line 1 of Algorithm 1. We have thus proven the second case. Finally, suppose that Pz ∈ T . Then, again, q(z|P) = max x∈∆ q(z|xP) = max( max x∈∆, xPz / ∈T, xP / ∈T q(z|xP), max x∈∆, xPz / ∈T, xP∈T q(z|xP), max x∈∆, xPz∈T q(z|xP) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = V, E, a node v 0 ∈ V singly identified by its carry [l 0 , C 0 , r 0 , γ 0 ] and a left-extension y for its LM context γ 0. The program copies most of the edges uσ w − → v ∈ E. If a derivation goes through v 0 and the string under v 0 ends in yγ 0 , the program refines and reweights it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Search errors made by beam search and cube pruning as a function of the distortion limit (decoding with a 3-gram LM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Beam search and cube pruning search er-
rors (out of 3,003 test samples) by beam size using 
LMs of order 3 to 5 (d = 4). 

order 
3 
4 
5 
k 
d = 4 d = 5 d = 6 d = 4 d = 4 
10 
20.47 20.13 19.97 20.71 20.69 
10 2 
21.14 21.18 21.08 21.73 21.76 
10 3 
21.27 21.34 21.32 21.89 21.91 
10 4 
21.29 21.37 21.37 21.92 21.93 
OS  *  
21.29 21.37 21.37 21.92 21.93 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Translation quality in terms of BLEU as 
a function of beam size in cube pruning with lan-
guage models of order 3 to 5. The bottom row 
shows BLEU for our exact decoder. 

</table></figure>

			<note place="foot" n="3"> Figure 1 can be seen as a specification for a weighted acyclic finite-state automaton whose states are indexed by [l, C, r] and transitions are labelled with biphrases. However, for generality of representation, we opt for using acyclic hypergraphs instead of automata (see §2.1).</note>

			<note place="foot" n="4"> Exact decoding had been similarly addressed with Integer Linear Programming (ILP) in the context of word-based models for very short sentences using a 2-gram LM (Germann et al., 2001). Riedel and Clarke (2009) revisit that formulation and employ a cutting-plane algorithm (Dantzig et al., 1954) reaching 30 words. 5 In hierarchical translation, reordering is governed by a synchronous context-free grammar and the underlying problem is no longer NP-complete. Exact decoding remains infeasible because the intersection between the translation forest and the target LM is prohibitively slow.</note>

			<note place="foot" n="6"> The intuition that a full intersection is wasteful is also present in (Petrov et al., 2008) in the context of approximate search. They start from a coarse distribution based on automatic word clustering which is refined in multiple passes. At each pass, hypotheses are pruned a posteriori on the basis of their marginal probabilities, and word clusters are further split. We work with upperbounds, rather than word clusters, with unpruned distributions, and perform exact optimisation. f g (0) d 0 D(x)</note>

			<note place="foot" n="7"> If d is a maximum from g and g(d) = f (d), then it is easy to show by contradiction that d is the actual maximum from f : if there existed d such that f (d ) &gt; f (d), then it follows that g(d ) ≥ f (d ) &gt; f (d) = g(d), and hence d would not be a maximum for g. 8 A distortion limit characterises a form of pruning that acts directly in the generative capacity of the model leading to induction errors (Auli et al., 2009). Limiting reordering like that lowers complexity to a polynomial function of I and an exponential function of the distortion limit.</note>

			<note place="foot" n="9"> Drawing a parallel to (Heafield et al., 2013a), a nonterminal node in our hypergraph groups derivations while exposing only an empty LM state.</note>

			<note place="foot" n="10"> This constraint prevents items from becoming dead-ends where incomplete derivations require a reordering step larger than d. This is known to prevent many search errors in beam search (Chang and Collins, 2011). 11 Unlike Aziz et al. (2013), rather than unigrams only, we score all n-grams within a translation rule (including incomplete ones).</note>

			<note place="foot" n="14"> The refinement operation is a special case of a general finite-state intersection. However, keeping its effect local to derivations going through a specific node is non-trivial using the general mechanism and justifies a tailored operation.</note>

			<note place="foot" n="16"> The size of the initial proposal does not depend on LM order, but rather on distortion limit (see Figure 3): on average (in thousands) |V0| = 0.6 and |E0| = 27 with d = 4, |V0| = 1.3 and |E0| = 70 with d = 5, and |V0| = 2.5 and</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work of Wilker Aziz and Lucia Specia was supported by EPSRC (grant EP/K024272/1).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A systematic analysis of translation model search spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT &apos;09</title>
		<meeting>the Fourth Workshop on Statistical Machine Translation, StatMT &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Investigations in exact inference for hierarchical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="472" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trainable grammars for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spring Conference of the</title>
		<meeting>the Spring Conference of the<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Acoustical Society of America</publisher>
			<date type="published" when="1979-06" />
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exact sampling and decoding in high-order hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exact decoding of phrase-based translation models through Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic programming algorithms as products of weighted logic programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Logic Programming</title>
		<editor>Maria Garcia de la Banda and Enrico Pontelli</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">5366</biblScope>
			<biblScope unit="page" from="114" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Solution of a large-scale traveling-salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="393" to="410" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s machine translation systems for European language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="114" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-free reordering, finite-state translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization and sampling for NLP from a unified viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Optimization Techniques for Human Language Technology</title>
		<meeting>the First International Workshop on Optimization Techniques for Human Language Technology<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="79" to="94" />
		</imprint>
	</monogr>
	<note>The COLING 2012 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Directed hypergraphs and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giustino</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Pallottino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="177" to="201" />
			<date type="published" when="1993-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast decoding and optimal decoding for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="228" to="235" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semiring parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="605" />
			<date type="published" when="1999-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relatively useless pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Graehl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USC Information Sciences Institute</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A formal basis for the heuristic determination of minimum cost paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Raphael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Systems Science And Cybernetics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="107" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grouping language model boundary words 1247 to speed k-best extraction from hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="958" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster Phrase-Based decoding by refining feature state</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rule filtering by pattern for efficient hierarchical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">R</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)<address><addrLine>Athens, Greece, March</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Series in Artificial Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoding complexity in wordreplacement translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="607" to="615" />
			<date type="published" when="1999-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit</title>
		<meeting>Machine Translation Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A weighted finite state transducer translation template model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="75" />
			<date type="published" when="2006-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A scalable decoder for parsing-based machine translation with equivalent language model state maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Syntax and Structure in Statistical Translation, SSST &apos;08</title>
		<meeting>the Second Workshop on Syntax and Structure in Statistical Translation, SSST &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Translation as weighted deduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="532" to="540" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An efficient A* search algorithm for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Data-driven methods in machine translation</title>
		<meeting>the workshop on Data-driven methods in machine translation<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>DMMT &apos;01. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>of ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coarse-to-fine syntactic machine translation using language projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Revisiting optimal decoding for machine translation IBM model 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short &apos;09</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Monte Carlo Statistical Methods (Springer Texts in Statistics)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer-Verlag New York, Inc</publisher>
			<pubPlace>Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exact decoding of syntactic translation models through Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="72" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A DP based search using monotone alignments in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, EACL &apos;97</title>
		<meeting>the eighth conference on European chapter of the Association for Computational Linguistics, EACL &apos;97<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Phrase-based statistical machine translation as a traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Zaslavskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
	<note>ACL &apos;09. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
