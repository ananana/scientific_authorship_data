<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Autoregressive Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Autoregressive Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="479" to="488"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>479</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing approaches to neural machine translation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decoding long sequences. In this paper, we propose a novel model for fast sequence generation-the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT&apos;14 English-German translation, the SAT achieves 5.58× speedup while maintaining 88% translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have been successfully applied to a variety of tasks, including machine transla- tion. The encoder-decoder architecture is the cen- tral idea of neural machine translation (NMT). The encoder first encodes a source-side sentence x = x 1 . . . x m into hidden states and then the decoder generates the target-side sentence y = y 1 . . . y n from the hidden states according to an autoregres- sive model p(y t |y 1 . . . y t−1 , x)</p><p>Recurrent neural networks (RNNs) are inherently good at processing sequential data. Sutskever * Part of this work was done when the author was at In- stitute of Automation, Chinese Academy of Sciences.  The sequential property of RNNs leads to its wide application in language processing. How- ever, the property also hinders its parallelizability thus RNNs are slow to execute on modern hard- ware optimized for parallel execution. As a result, a number of more parallelizable sequence models were proposed such as <ref type="bibr">ConvS2S (Gehring et al., 2017</ref>) and the Transformer ( <ref type="bibr" target="#b23">Vaswani et al., 2017</ref>). These models avoid the dependencies between dif-ferent positions in each layer thus can be trained much faster than RNN based models. When infer- ence, however, these models are still slow because of the autoregressive property.</p><p>A recent work ( <ref type="bibr" target="#b7">Gu et al., 2017</ref>) proposed a non-autoregressive NMT model that generates all target-side words in parallel. While the paral- lelizability is greatly improved, the translation quality encounter much decrease. In this paper, we propose the semi-autoregressive Transformer (SAT) for faster sequence generation. Unlike <ref type="bibr" target="#b7">Gu et al. (2017)</ref>, the SAT is semi-autoregressive, which means it keeps the autoregressive property in global but relieves in local. As the result, the SAT can produce multiple successive words in parallel at each time step. <ref type="figure" target="#fig_0">Figure 1</ref> gives an il- lustration of the different levels of autoregressive properties.</p><p>Experiments conducted on English-German and Chinese-English translation show that com- pared with non-autoregressive methods, the SAT achieves a better balance between translation qual- ity and decoding speed. On WMT'14 English- German translation, the proposed SAT is 5.58× faster than the Transformer while maintaining 88% of translation quality. Besides, when produc- ing two words at each time step, the SAT is almost lossless.</p><p>It is worth noting that although we apply the SAT to machine translation, it is not designed specifically for translation as <ref type="bibr" target="#b7">Gu et al. (2017)</ref>; <ref type="bibr" target="#b15">Lee et al. (2018)</ref>. The SAT can also be applied to any other sequence generation task, such as summary generation and image caption generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Almost all state-of-the-art NMT models are au- toregressive ( <ref type="bibr" target="#b24">Wu et al., 2016;</ref><ref type="bibr" target="#b6">Gehring et al., 2017;</ref><ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>, meaning that the model gen- erates words one by one and is not friendly to mod- ern hardware optimized for parallel execution. A recent work ( <ref type="bibr" target="#b7">Gu et al., 2017</ref>) attempts to acceler- ate generation by introducing a non-autoregressive model. Based on the Transformer ( <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>, they made lots of modifications. The most significant modification is that they avoid feeding the previously generated target words to the de- coder, but instead feeding the source words, to pre- dict the next target word. They also introduced a set of latent variables to model the fertilities of source words to tackle the multimodality problem in translation. <ref type="bibr" target="#b15">Lee et al. (2018)</ref> proposed another non-autoregressive sequence model based on iter- ative refinement. The model can be viewed as both a latent variable model and a conditional denoising autoencoder. They also proposed a learning algo- rithm that is hybrid of lower-bound maximization and reconstruction error minimization.</p><p>The most relevant to our proposed semi- autoregressive model is ( <ref type="bibr" target="#b11">Kaiser et al., 2018)</ref>. They first autoencode the target sequence into a shorter sequence of discrete latent variables, which at in- ference time is generated autoregressively, and fi- nally decode the output sequence from this shorter latent sequence in parallel. What we have in com- mon with their idea is that we have not entirely abandoned autoregressive, but rather shortened the autoregressive path.</p><p>A related study on realistic speech synthesis is the parallel WaveNet ( <ref type="bibr" target="#b17">Oord et al., 2017</ref>). The pa- per introduced probability density distillation, a new method for training a parallel feed-forward network from a trained WaveNet (Van Den Oord et al., 2016) with no significant difference in qual- ity.</p><p>There are also some work share a somehow simillar idea with our work: character-level NMT ( <ref type="bibr" target="#b5">Chung et al., 2016;</ref><ref type="bibr" target="#b14">Lee et al., 2016</ref>) and chunk- based NMT ( <ref type="bibr" target="#b10">Ishiwatari et al., 2017)</ref>. Unlike the SAT, these models are not able to produce multiple tokens (characters or words) each time step. <ref type="bibr" target="#b16">Oda et al. (2017)</ref> proposed a bit- level decoder, where a word is represented by a binary code and each bit of the code can be pre- dicted in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Transformer</head><p>Since our proposed model is built upon the Trans- former ( <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>, we will briefly in- troduce the Transformer. The Transformer uses an encoder-decoder architecture. We describe the en- coder and decoder below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Encoder</head><p>From the source tokens, learned embeddings of di- mension d model are generated which are then mod- ified by an additive positional encoding. The po- sitional encoding is necessary since the network does not leverage the order of the sequence by re- currence or convolution. The authors use additive encoding which is defined as:</p><formula xml:id="formula_0">P E(pos, 2i) = sin(pos/10000 2i/d model ) P E(pos, 2i + 1) = cos(pos/10000 2i/d model )</formula><p>where pos is the position of a word in the sen- tence and i is the dimension. The authors chose this function because they hypothesized it would allow the model to learn to attend by relative po- sitions easily. The encoded word embeddings are then used as input to the encoder which consists of N blocks each containing two layers: (1) a multi- head attention layer, and (2) a position-wise feed- forward layer.</p><p>Multi-head attention builds upon scaled dot- product attention, which operates on a query Q, key K and value V:</p><formula xml:id="formula_1">Attention(Q, K, V ) = sof tmax( QK T √ d k )V</formula><p>where d k is the dimension of the key. The au- thors scale the dot product by 1/ √ d k to avoid the inputs to softmax function growing too large in magnitude. Multi-head attention computes h dif- ferent queries, keys and values with h linear pro- jections, computes scaled dot-product attention for each query, key and value, concatenates the re- sults, and projects the concatenation with another linear projection:</p><formula xml:id="formula_2">H i = Attention(QW Q i , KW K i , V W V i ) M ultiHead(Q, K, V ) = Concat(H 1 , . . . H h ) in which W Q i , W K i ∈ R d model ×d k and W V i ∈ R d model ×dv . The attention mechanism in the en- coder performs attention over itself (Q = K = V ), so it is also called self-attention.</formula><p>The second component in each encoder block is a position-wise feed-forward layer defined as:</p><formula xml:id="formula_3">F F N (x) = max(0, xW 1 + b 1 )W 2 + b 2 where W 1 ∈ R d model ×d f f , W 2 ∈ R d f f ×d model , b 1 ∈ R d f f , b 2 ∈ R d model .</formula><p>For more stable and faster convergence, resid- ual connection <ref type="bibr" target="#b8">(He et al., 2016</ref>) is applied to each layer, followed by layer normalization ( <ref type="bibr">Ba et al., 2016)</ref>. For regularization, dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014</ref>) are applied before residual connec- tions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Decoder</head><p>The decoder is similar with the encoder and is also composed by N blocks. In addition to the two layers in each encoder block, the decoder inserts a third layer, which performs multi-head attention over the output of the encoder. It is worth noting that, different from the en- coder, the self-attention layer in the decoder must be masked with a causal mask, which is a lower triangular matrix, to ensure that the prediction for position i can depend only on the known outputs at positions less than i during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Group-Level Chain Rule</head><p>Standard NMT models usually factorize the joint probability of a word sequence y 1 . . . y n according to the word-level chain rule</p><formula xml:id="formula_4">p(y 1 . . . y n |x) = n ∏ t=1 p(y t |y 1 . . . y t−1 , x)</formula><p>resulting in decoding each word depending on all previous decoding results, thus hindering the par- allelizability. In the SAT, we extend the standard word-level chain rule to the group-level chain rule.</p><p>We first divide the word sequence y 1 . . . y n into consecutive groups</p><formula xml:id="formula_5">G 1 , G 2 , . . . , G [(n−1)/K]+1 = y 1 . . . y K , y K+1 . . . y 2K , . . . , y [(n−1)/K]×K+1 . . . y n</formula><p>where [·] denotes floor operation, K is the group size, and also the indicator of parallelizability. The larger the K, the higher the parallelizability. Ex- cept for the last group, all groups must contain K words. Then comes the group-level chain rule</p><formula xml:id="formula_6">p(y 1 . . . y n |x) = [(n−1)/K]+1 ∏ t=1 p(G t |G 1 . . . G t−1 , x)</formula><p>This group-level chain rule avoids the depen- dencies between consecutive words if they are in the same group. With group-level chain rule, the model no longer produce words one by one as the Transformer, but rather group by group. In next subsections, we will show how to implement the model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long-Distance Prediction</head><p>In autoregressive models, to predict y t , the model should be fed with the previous word y t−1 . We refer it as short-distance prediction. In the SAT, however, we feed y t−K to predict y t , to which we refer as long-distance prediction. At the be- ginning of decoding, we feed the model with K special symbols &lt;s&gt; to predict y 1 . . . y K in paral- lel. Then y 1 . . . y K are fed to the model to predict y K+1 . . . y 2K in parallel. This process will con- tinue until a terminator &lt;/s&gt; is generated. <ref type="figure" target="#fig_2">Fig- ure 3</ref> gives illustrations for both short and long- distance prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relaxed Causal Mask</head><p>In the Transformer decoder, the causal mask is a lower triangular matrix, which strictly prevents earlier decoding steps from peeping information from later steps. We denote it as strict causal mask. However, in the SAT decoder, strict causal mask is not a good choice. As described in the previous subsection, in long-distance prediction, the model predicts y K+1 by feeding with y 1 . With strict causal mask, the model can only access to y 1 when predict y K+1 , which is not reasonable since y 1 . . . y K are already produced. It is better to al- low the model to access to y 1 . . . y K rather than only y 1 when predict y K+1 . Therefore, we use a coarse-grained lower trian- gular matrix as the causal mask that allows peep- ing later information in the same group. We re- fer to it as relaxed causal mask. Given the tar- get length n and the group size K, relaxed causal mask M ∈ R n×n and its elements are defined be- low:</p><formula xml:id="formula_7">M [i][j] = { 1 if j &lt; ([(i − 1)/K] + 1) × K 0 other</formula><p>For a more intuitive understanding, <ref type="figure">Figure 4</ref> gives a comparison between strict and relaxed causal mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The SAT</head><p>Using group-level chain rule instead of word- level chain rule, long-distance prediction instead of short-distance prediction, and relaxed causal <ref type="figure">Figure 4</ref>: Strict causal mask (left) and relaxed causal mask (right) when the target length n = 6 and the group size K = 2. We mark their differences in bold.</p><formula xml:id="formula_8">        1 0 0 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</head><formula xml:id="formula_9">               </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">0 0 0 0 0 0 0 0 0 0 0 0 1</head><formula xml:id="formula_10">       </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Complexity Acceleration Transformer <ref type="table">Table 1</ref>: Theoretical complexity and acceleration of the SAT. a denotes the time consumed on the decoder net- work (calculating a distribution over the target vocabu- lary) each time step and b denotes the time consumed on search (searching for top scores, expanding nodes and pruning). In practice, a is usually much larger than b since the network is deep.</p><formula xml:id="formula_11">N (a + b) 1 SAT (beam search) N K a + N b K( a+b a+Kb ) SAT (greedy search) N K (a + b) K</formula><p>mask instead of strict causal mask, we success- fully extended the Transformer to the SAT. The Transformer can be viewed as a special case of the SAT, when the group size K = 1. The non- autoregressive Transformer (NAT) described in <ref type="bibr" target="#b7">Gu et al. (2017)</ref> can also be viewed as a special case of the SAT, when the group size K is not less than maximum target length. <ref type="table">Table 1</ref> gives the theoretical complexity and ac- celeration of the model. We list two search strate- gies separately: beam search and greedy search. Beam search is the most prevailing search strategy. However, it requires the decoder states to be up- dated once every word is generated, thus hinders the decoding parallelizability. When decode with greedy search, there is no such concern, therefore the parallelizability of the SAT can be maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate the proposed SAT on English-German and Chinese-English translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets For English-German translation, we choose the corpora provided by WMT 2014 <ref type="bibr" target="#b3">(Bojar et al., 2014</ref>). We use the newstest2013 dataset for development, and the newstest2014 dataset for test. For Chinese-English translation, the corpora Sentence Number</p><p>Vocab Size Source <ref type="table" target="#tab_3">Target  EN-DE  4.5M  36K  36K  ZH-EN</ref> 1.8M 9K 34K we use is extracted from LDC 1 . We chose the NIST02 dataset for development, and the NIST03, NIST04 and NIST05 datasets for test. For En- glish and German, we tokenized and segmented them into subword symbols using byte-pair encod- ing (BPE) ( <ref type="bibr" target="#b19">Sennrich et al., 2015</ref>) to restrict the vo- cabulary size. As for Chinese, we segmented sen- tences into characters. For English-German trans- lation, we use a shared source and target vocabu- lary. <ref type="table" target="#tab_1">Table 2</ref> summaries the two corpora.</p><p>Baseline We use the base Transformer model described in <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref> as the base- line, where d model = 512 and N = 6. In addition, for comparison, we also prepared a lighter Transformer model, in which two en- coder/decoder blocks are used (N = 2), and other hyper-parameters remain the same.</p><p>Hyperparameters Unless otherwise specified, all hyperparameters are inherited from the base Transformer model. We try three different settings of the group size K: K = 2, K = 4, and K = 6. For English-German translation, we share the same weight matrix between the source and tar- get embedding layers and the pre-softmax linear layer. For Chinese-English translation, we only share weights of the target embedding layer and the pre-softmax linear layer.  autoregressive Transformer network. This method consists of three steps: (1) train an autoregressive Transformer network (the teacher), (2) run beam search over the training set with this model and (3) train the SAT (the student) on this new created corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search Strategies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>Since the SAT and the Trans- former have only slight differences in their ar- chitecture (see <ref type="figure" target="#fig_1">Figure 2</ref>), in order to accelerate convergence, we use a pre-trained Transformer model to initialize some parameters in the SAT. These parameters include all parameters in the en- coder, source and target word embeddings, and pre-softmax weights. Other parameters are initial- ized randomly. In addition to accelerating conver- gence, we find this method also slightly improves the translation quality.</p><p>Training Same as <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>, we train the SAT by minimize cross-entropy with la- bel smoothing. The optimizer we use is Adam ( <ref type="bibr" target="#b13">Kingma and Ba, 2015</ref>) with β 1 = 0.9, β 2 = 0.98 and ε = 10 −9 . We change the learning rate during training using the learning rate funtion described in <ref type="bibr" target="#b23">Vaswani et al. (2017)</ref>. All models are trained for 10K steps on 8 NVIDIA TITAN Xp with each minibatch consisting of about 30k tokens. For evaluation, we average last five checkpoints saved with an interval of 1000 training steps.</p><p>Evaluation Metrics We evaluate the translation quality of the model using BLEU score ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>).</p><p>Implementation We implement the proposed SAT with TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2016)</ref>. The code and resources needed for reproducing the re- sults are released at https://github.com/ chqiwang/sa-nmt. <ref type="table" target="#tab_3">Table 3</ref> summaries results of English-German translation. According to the results, the trans- lation quality of the SAT gradually decreases as K increases, which is consistent with intuition. When K = 2, the SAT decodes 1.51× faster than the Transformer and is almost lossless in transla- tion quality (only drops 0.21 BLEU score). With K = 6, the SAT can achieve 2.98× speedup while the performance degeneration is only 8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on English-German</head><p>When using greedy search, the acceleration be- comes much more significant. When K = 6, the decoding speed of the SAT can reach about 5.58× of the Transformer while maintaining 88% Model b=1 b=16 b=32 b=64 Transformer 346ms 58ms 53ms 56ms SAT, K=2 229ms 38ms 32ms 32ms SAT, K=4 149ms 24ms 21ms 20ms SAT, K=6 116ms 20ms 17ms 16ms  of translation quality. Comparing with <ref type="bibr" target="#b7">Gu et al. (2017)</ref>; <ref type="bibr" target="#b11">Kaiser et al. (2018)</ref>; <ref type="bibr" target="#b15">Lee et al. (2018)</ref>, the SAT achieves a better balance between transla- tion quality and decoding speed. Compared to the lighter Transformer (N = 2), with K = 4, the SAT achieves a higher speedup with significantly better translation quality.</p><p>In a real production environment, it is often not to decode sentences one by one, but batch by batch. To investigate whether the SAT can accel- erate decoding when decoding in batches, we test the decoding latency under different batch size set- tings. As shown in <ref type="table" target="#tab_4">Table 4</ref>, the SAT significantly accelerates decoding even with a large batch size.</p><p>It is also good to know if the SAT can still accel- erate decoding on CPU device that does not sup- port parallel execution as well as GPU. Results in <ref type="table" target="#tab_5">Table 5</ref> show that even on CPU device, the SAT can still accelerate decoding significantly. <ref type="table">Table 6</ref> summaries results on Chinese-English translation. With K = 2, the SAT decodes 1.69× while maintaining 97% of the translation quality. In an extreme setting where K = 6 and beam size = 1, the SAT can achieve 6.41× speedup while maintaining 83% of the translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Chinese-English</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>Effects of Knowledge Distillation As shown in <ref type="figure">Figure 5</ref>, sequence-level knowledge distillation is very effective for training the SAT. For larger K, the effect is more significant. This phenomenon is echoing with observations by <ref type="bibr" target="#b7">Gu et al. (2017)</ref>; <ref type="bibr" target="#b17">Oord et al. (2017)</ref>; <ref type="bibr" target="#b15">Lee et al. (2018)</ref>. In addition, we tried word-level knowledge distillation <ref type="bibr" target="#b12">(Kim and Rush, 2016</ref>) but only a slight improvement was observed.</p><p>Position-Wise Cross-Entropy In <ref type="figure">Figure 6</ref>, we plot position-wise cross-entropy for various mod- els. To compare with the baseline model, the results in the figure are from models trained on the original corpora, i.e., without knowledge distillation. As shown in the figure, position- wise cross-entropy has an apparent periodicity with a period of K. For positions in the same group, the position-wise cross-entropy increase monotonously, which indicates that the long- distance dependencies are always more difficult to model than short ones. It suggests the key to fur- ther improve the SAT is to improve the ability of modeling long-distance dependencies.</p><p>Case Study  <ref type="table">Table 6</ref>: Results on Chinese-English translation. Latency is calculated on NIST02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Transformer the international football federation will severely punish the fraud on the football field SAT, k=2 fifa will severely punish the deception on the football field SAT, k=4 fifa a will severely punish the fraud on the football court SAT, k=6 fifa a will severely punish the fraud on the football football court Reference federation international football association to mete out severe punishment for fraud on the football field Source Transformer the largescale exhibition of campus culture will also be held during the meeting .</p><p>SAT, k=2 the largescale cultural cultural exhibition on campus will also be held during the meeting .</p><p>SAT, k=4 the campus campus exhibition will also be held during the meeting .</p><p>SAT, k=6 a largescale campus culture exhibition will also be held on the sidelines of the meeting .</p><p>Reference there will also be a large - scale campus culture show during the conference .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Transformer this is the second time mr koizumi has visited the yasukuni shrine since he came to power .</p><p>SAT, k=2 this is the second time that mr koizumi has visited the yasukuni shrine since he took office .</p><p>SAT, k=4 this is the second time that koizumi has visited the yasukuni shrine since he came into power .</p><p>SAT, k=6 this is the second visit to the yasukuni shrine since mr koizumi came office power .</p><p>Reference this is the second time that junichiro koizumi has paid a visit to the yasukuni shrine since he became prime minister . erate fluent sentences. As reported by <ref type="bibr" target="#b7">Gu et al. (2017)</ref>, instances of repeated words or phrases are most prevalent in their non-autoregressive model. In the SAT, this is also the case. This suggests that we may be able to improve the translation quality of the SAT by reducing the similarity of the output distribution of adjacent positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have introduced a novel model for faster sequence generation based on the Trans- former ( <ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>, which we refer to as the semi-autoregressive Transformer (SAT). Com- bining the original Transformer with group-level chain rule, long-distance prediction and relaxed causal mask, the SAT can produce multiple con- secutive words at each time step, thus speedup de- coding significantly. We conducted experiments on English-German and Chinese-English transla- tion. Compared with previously proposed non- autoregressive models ( <ref type="bibr" target="#b7">Gu et al., 2017;</ref><ref type="bibr" target="#b15">Lee et al., 2018;</ref><ref type="bibr" target="#b11">Kaiser et al., 2018)</ref>, the SAT achieves a bet- ter balance between translation quality and decod- ing speed. On WMT'14 English-German transla- tion, the SAT achieves 5.58× speedup while main- taining 88% translation quality, significantly bet-ter than previous methods. When producing two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score).</p><p>In the future, we plan to investigate better meth- ods for training the SAT to further shrink the per- formance gap between the SAT and the Trans- former. Specifically, we believe that the following two directions are worth study. First, use object function beyond maximum likelihood to improve the modeling of long-distance dependencies. Sec- ond, explore new method for knowledge distilla- tion. We also plan to extend the SAT to allow the use of different group sizes K at different posi- tions, instead of using a fixed value.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The different levels of autoregressive properties. Lines with arrow indicate dependencies. We mark the longest dependency path with bold red lines. The length of the longest dependency path decreases as we relieve the autoregressive property. An extreme case is non-autoregressive, where there is no dependency at all.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of the Transformer, also of the SAT, where the red dashed boxes point out the different parts of these two models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Short-distance prediction (top) and longdistance prediction (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Performance of the SAT with and without sequence-level knowledge distillation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>y 1 y 2 y 3 y 4 y 5 Autoregressive y 6 y 1 y 2 y 3 y 4 y 5 Semi-Autoregressive y 6 y 1 y 2 y 3 y 4 y 5</head><label></label><figDesc></figDesc><table>Non-Autoregressive 

y 6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Summary of the two corpora.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Even on the same platform, implementation and hardware may not exactly be the same. Therefore, it is not fair to directly compare BLEU and latency. A fairer way is to compare performance degradation and speedup, which are calculated based on their own baseline.</figDesc><table>Results on English-German translation. Latency is calculated on a single NVIDIA TITAN Xp without 
batching. For comparison, we also list results reported by Gu et al. (2017); Kaiser et al. (2018); Lee et al. (2018). 
Note that Gu et al. (2017); Lee et al. (2018) used PyTorch as their platform, but we and Kaiser et al. (2018) used 
TensorFlow. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Time needed to decode one sentence under 
various batch size settings. A single NVIDIA TIAN 
Xp is used in this test. 

Model 
K=1 
K=2 
K=4 
K=6 
Latency 1384ms 607ms 502ms 372ms 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Time needed to decode one sentence on CPU device. Sentences are decoded one by one without batching. K=1 denotes the Transformer.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 lists</head><label>7</label><figDesc></figDesc><table>three sample Chinese-
English translations from the development set. As 
shown in the table, even when producing K = 6 
words at each time step, the model can still gen-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Three sample Chinese-English translations by the SAT and the Transformer. We mark repeated words or 
phrases by red font and underline. 

</table></figure>

			<note place="foot" n="4"> The Semi-Autoregressive Transformer We propose a novel NMT model-the SemiAutoregressive Transformer (SAT)-that can produce multiple successive words in parallel. As shown in Figure 2, the architecture of the SAT is almost the same as the Transformer, except some modifications in the decoder.</note>

			<note place="foot" n="1"> The corpora include LDC2002E18, LDC2003E14, LDC2004T08 and LDC2005T0.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. We also thank Wenfu Wang, Hao Wang for helpful discussion and Lin-hao Dong, Jinghao Niu for their help in paper writ-ting.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint/>
	</monogr>
<note type="report_type">ton. 2016. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Nonautoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chunk-based decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shonosuke</forename><surname>Ishiwatari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingtao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijia</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1901" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast decoding in sequence models using discrete latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Pamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03382</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequencelevel knowledge distillation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>international conference on learning representations</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03017</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06901</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural machine translation via binary code prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06918</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stimberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10433</idno>
		<title level="m">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Chunk-based biscale decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01452</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
