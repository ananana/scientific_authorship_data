<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Script Induction as Language Modeling Center for Language and Speech Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology Center of Excellence</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Script Induction as Language Modeling Center for Language and Speech Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The narrative cloze is an evaluation metric commonly used for work on automatic script induction. While prior work in this area has focused on count-based methods from distributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task. By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although the concept of scripts in artificial intelli- gence dates back to the 1970s ( <ref type="bibr" target="#b22">Schank and Abelson, 1977)</ref>, interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, <ref type="bibr" target="#b0">Chambers and Jurafsky (2008)</ref>, treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works <ref type="bibr" target="#b1">(Chambers and Jurafsky, 2009;</ref><ref type="bibr" target="#b11">Jans et al., 2012;</ref><ref type="bibr" target="#b19">Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b21">Rudinger et al., 2015</ref>) employ and ex- tend <ref type="bibr" target="#b0">Chambers and Jurafsky (2008)</ref>'s methods for learning narrative chains, each using the narrative cloze to evaluate their work. <ref type="bibr">1</ref> In this paper, we take the position that the nar- rative cloze test, which has been treated predom-inantly as a method for evaluating script knowl- edge, is more productively thought of simply as a language modeling task. <ref type="bibr">2</ref> To support this claim, we demonstrate a marked improvement over pre- vious methods on this task using a powerful dis- criminative language model -the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: ei- ther discriminative language models are a more effective technique for script induction than pre- vious methods, or the narrative cloze test is not a suitable evaluation for this task. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition</head><p>Following the definitions of Chambers and Juraf- sky (2008), a narrative chain is "a partially or- dered set of narrative events that share a common actor," where a narrative event is "a tuple of an event (most simply a verb) and its participants, represented as typed dependencies." <ref type="bibr" target="#b6">(De Marneffe et al., 2006</ref>) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an example, consider the following narrative:</p><p>John studied for the exam and aced it. His teacher congratulated him.</p><p>With John as protagonist, we have a se- quence of three narrative events: (study, nsubj), (ace, nsubj), and (congratulate, dobj).</p><p>In the narrative cloze test, a sequence of nar- rative events (like the example provided here) is extracted automatically from a document, and one narrative event is removed; the task is to predict the missing event.</p><p>Data Each of the models discussed in the fol- lowing section are trained and tested on chains of narrative events extracted from stories in the New York Times portion of the Gigaword corpus ( <ref type="bibr" target="#b9">Graff et al., 2003</ref>) with Concrete annotations <ref type="bibr" target="#b7">(Ferraro et al., 2014</ref>). Training is on the entirety of the 1994- 2006 portion <ref type="bibr">(16,</ref><ref type="bibr">688,</ref><ref type="bibr">422</ref> chains with 58,515,838 narrative events); development is a subset of the 2007-2008 portion (10,000 chains with 35,109 events); and test is a subset of the 2009-2010 por- tion (5,000 chains with 17,836 events). All ex- tracted chains are of length two or greater.</p><p>Chain Extraction To extract chains of narra- tive events for training and testing, we rely on the (automatically-generated) coreference chains present in Concretely Annotated Gigaword. Each narrative event in an extracted chain is derived from a single mention in the corresponding coref- erence chain, i.e., it consists of the verb and syn- tactic dependency (nsubj or dobj) that governs the head of the mention, if such a dependency ex- ists. Overlapping mentions within a coreference chain are collapsed to a single mention to avoid redundant extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>In this section we present each of the models we train for the narrative cloze evaluation. In a sin- gle narrative cloze test, a sequence of narrative events, (e 1 , · · · , e L ), with an insertion point, k, for the missing event is provided. Given a fixed vocabulary of narrative events, V, a candidate se- quence is generated for each vocabulary item by inserting that item into the sequence at index k. Each model generates a score for the candidate se- quences, yielding a ranking over the vocabulary items. The rank assigned to the actual missing vo- cabulary item is the score the model receives on that cloze test. In this case, we set V to include all narrative events, e, that occur at least ten times in training, yielding a vocabulary size of 12,452. All out-of-vocabulary events are converted to (and scored as) the symbol UNK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Count-based Methods</head><p>Unigram Baseline (UNI) A simple but strong baseline introduced by <ref type="bibr" target="#b19">Pichotta and Mooney (2014)</ref> for this task is the unigram model: can- didates are ranked by their observed frequency in training, without regard to context.</p><p>Unordered PMI (UOP) The original model for this task, proposed by <ref type="bibr" target="#b0">Chambers and Jurafsky (2008)</ref>, is based on the pointwise mutual informa- tion (PMI) between events.</p><formula xml:id="formula_0">pmi(e 1 , e 2 ) ∝ log C(e 1 , e 2 ) C(e 1 , * )C( * , e 2 )<label>(1)</label></formula><p>Here, C(e 1 , e 2 ) is the number of times e 1 and e 2 occur in the same narrative event sequence, i.e., the number of times they "had a coreferring entity filling the values of [their] dependencies," and the ordering of e 1 and e 2 is not considered. In our implementation, individual counts are defined as follows:</p><formula xml:id="formula_1">C(e, * ) := e ∈V C(e, e )<label>(2)</label></formula><p>This model selects the best candidate event in a given cloze test according to the following score:</p><formula xml:id="formula_2">ˆ e = arg max e∈V L i=1 pmi(e, e i )<label>(3)</label></formula><p>We tune this model with an option to apply a mod- ified version of discounting for PMI from <ref type="bibr" target="#b16">Pantel and Ravichandran (2004)</ref>.</p><p>Ordered PMI (OP) This model is a slight vari- ation on Unordered PMI introduced by <ref type="bibr" target="#b11">Jans et al. (2012)</ref>. The only distinction is that C(e 1 , e 2 ) is treated as an asymmetric count, sensitive to the or- der in which e 1 and e 2 occur within a chain.</p><p>Bigram Probability (BG) Another variant intro- duced by <ref type="bibr" target="#b11">Jans et al. (2012)</ref>, the "bigram proba- bility" model uses conditional probabilities rather than PMI to compute scores. In a cloze test, this model selects the following event:</p><formula xml:id="formula_3">ˆ e = arg max e∈V k i=1 p(e|e i ) L i=k+1 p(e i |e) (4)</formula><p>where p(e 2 |e 1 ) = C(e 1 ,e 2 ) C(e 1 , * ) and C(e 1 , e 2 ) is asym- metric. We tune this model with an option to per- form absolute discounting. Note that this model is not a bigram model in the typical language mod- eling sense. <ref type="table">UNI  UOP  OP  BG  LBL2  LBL4  Tests  2  490  1887  2363  1613  369  371  5668  3  452  1271  1752  1009  330  334  2793  4  323  806  1027  502  229  232  1616  5  364  735  937  442  254  243  1330  6  347  666  891  483  257  249  942  7  330  629  838  468  241  237  630  8  259  466  510  278  208  201  512  9  299  610  639  348  198  195  396  10+  331  472  397  277  240  229  3949  ALL  400  1115  1382  868</ref>  Skip N-gram We tune the previous three models (UOP, OP, and BG) with the skip n-gram counting methods introduced by <ref type="bibr" target="#b11">Jans et al. (2012)</ref> for this task, varying the ways in which the counts, C(e 1 , e 2 ), are collected. Using skip-n counting, C(e 1 , e 2 ) is incremented every time e 1 and e 2 co-occur within a window of size n. We experiment with skip-0 (consecutive events only), skip-3 (window size 3), and skip-all (entire chain length) settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Len</head><p>For each of the four narrative cloze scoring metrics we report on (average rank, mean re- ciprocal rank, recall at 10, and recall at 50), we tune the Unordered PMI, Ordered PMI, and Bigram Probability models over the following parameter space: {skip-0, skip-3, skip-all} × {discount, no-discount} × {T=4, T=10, T=20}, where T is a pairwise count threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Discriminative Method</head><p>Log-Bilinear Language Model (LBL) The Log-Bilinear language model is a language model that was introduced by <ref type="bibr" target="#b14">Mnih and Hinton (2007)</ref>. Like other language models, the LBL produces a probability distribution over the next possible word given a sequence of N previously observed words. N is a hyper-parameter that determines the size of the context used for computing the prob- abilities. While many variants of the LBL have been proposed since its introduction, we use the simple variant described below.</p><p>Formally, we associate one context vector c e ∈ R d , one bias parameter b e ∈ R, and one tar- get vector t e ∈ R d to each narrative event e ∈ V ∪ { UNK, BOS, EOS }. V is the vocab- ulary of events and BOS, EOS, and UNK are the beginning-of-sequence, end-of-sequence, and out- of-vocabulary symbols, respectively. The proba- bility of an event e that appears after a sequence s = [s 1 , s 2 , . . . , s N ] of context words is defined as:</p><formula xml:id="formula_4">p(e|s) = exp(t e ˆ t s + b e ) e ∈V∪{ UNK, EOS } exp(t e ˆ t s + b e ) wherê t s = N j=1 m j c s j</formula><p>The operator performs element-wise multiplica- tion of two vectors. The parameters that are opti- mized during training are m j ∀j ∈ [1, . . . , N ] and c e , t e ∀e ∈ V ∪ { UNK, BOS, EOS }. To calcu- late the log-probability of a sequence of narrative events E = (e 1 , . . . , e L ) we compute:</p><formula xml:id="formula_5">l(S) = n i=1 log(p(e i |f E (e i ))) + log(p(EOS|f E (EOS)))<label>(5)</label></formula><p>Here f E is a function that returns the sequence of N words that precede the event e i in the se- quence E made by prepending N BOS tokens and appending a single EOS token to E. The LBL models are trained by minimizing the objective described in Equation 5 for all the sequences in the training corpus. We used the OxLM toolkit ( <ref type="bibr" target="#b18">Paul et al., 2014</ref>) which internally uses Noise-Contrastive Estimation (NCE) <ref type="bibr" target="#b10">(Gutmann and Hyvärinen, 2010)</ref> and processor paral- lelization for speeding up the training. For this task, we train LBL models with N = 2 (LBL2) and N = 4 (LBL4). In our experiments, increas- ing context size to N = 6 did not significantly improve (or degrade) performance. <ref type="table">Table 1</ref> shows the results of 17,836 narrative cloze tests (derived from 5,000 held-out test chains), with results bucketed by chain length. Perfor- mance is reported on four metrics: average rank, mean reciprocal rank, recall at 10, and recall at 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>For each of the four metrics, the best overall performance is achieved by one of the two LBL models (context size 2 or 4); the LBL models also achieve the best performance on every chain length. Not only are the gains achieved by the discriminative LBL consistent across metrics and chain length, they are large. For average rank, the LBL achieves a 27.0% relative improvement over the best non-discriminative model; for mean re- ciprocal rank, a 19.9% improvement; for recall at 10, a 22.8% improvement; and for recall at 50, a 17.6% improvement. (See <ref type="figure" target="#fig_0">Figure 1.</ref>) Further- more, note that both PMI models and the Bigram model have been individually tuned for each met- ric, while the LBL models have not. (The two LBL models are tuned only for overall perplexity on the development set.) All models trend toward improved performance on longer chains. Because the unigram model also improves with chain length, it appears that longer chains contain more frequent events and are thus easier to predict. However, LBL performance is also likely improving on longer chains because of additional contextual information, as is evident from LBL4's slight relative gains over LBL2 on longer chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Pointwise mutual information and other related count-based techniques have been used widely to identify semantically similar words <ref type="bibr" target="#b5">(Church and Hanks, 1990;</ref><ref type="bibr" target="#b12">Lin and Pantel, 2001;</ref><ref type="bibr">Tur-ney and Pantel, 2010)</ref>, so it is natural that these techniques have also been applied to the task of script induction. Qualitatively, PMI often identifies intuitively compelling matches; among the top 15 events to share a high PMI with (eat, nsubj) under the Unordered PMI model, for example, we find events such as (overeat, nsubj), (taste, nsubj), (smell, nsubj), (cook, nsubj), and (serve, dobj). When evaluated by the narra- tive cloze test, however, these count-based meth- ods are overshadowed by the performance of a general-purpose discriminative language model.</p><p>Our decision to attempt this task with the Log- Bilinear model was motivated by the simple ob- servation that the narrative cloze test is, in reality, a language modeling task. Does the LBL's suc- cess on this task mean that work in script induc- tion should abandon traditional count-based meth- ods for discriminative language modeling tech- niques? Or does it mean that an alternative eval- uation metric is required to measure script knowl- edge? While we believe our results are sufficient to conclude that one of these alternatives is the case, we leave the task of determining which to future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Narrative cloze results over all chain lengths. Unigram Model (UNI), Unordered PMI Model (UOP), Ordered PMI Model (OP), Bigram Probability Model (BG), Log-Bilinear Model with context size 2 or 4 (LBL2, LBL4). Average Rank (avgrnk), Mean Reciprocal Rank (mrr), % Recall at 10 (rec10), % Recall at 50 (rec50).</figDesc></figure>

			<note place="foot" n="1"> A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010)</note>

			<note place="foot" n="2"> Manshadi et al. (2008) also take a language modeling approach to event prediction, although their experiments are not directly comparable. 3 We note that, whether the narrative cloze was originally intended as a rigorous evaluation of script induction techniques or merely a preliminary metric, we are motivated by the observation that this evaluation has nonetheless become a standard metric for this task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Paul Allen In-stitute for Artificial Intelligence (Acquisition and Use of Paraphrases in a Knowledge-Rich Setting), a National Science Foundation Graduate Research Fellowship (Grant No. DGE-1232825), the Johns Hopkins HLTCOE, and DARPA DEFT (FA8750-13-2-001, Large Scale Paraphrasing for Natural Language Understanding). We would also like to thank three anonymous reviewers for their feed-back. Any opinions expressed in this work are those of the authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Columbus, Ohio. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL-08: HLT</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative schemas and their participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event schema induction with a probabilistic entity-driven model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Probabilistic domain modelling with contextualized distributional semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic frame induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie Chi Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="837" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Concretely Annotated Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Workshop on Automated Knowledge Base Construction (AKBC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A hierarchical bayesian model for unsupervised induction of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>English gigaword. Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Skip n-grams and ranking functions for predicting script events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Jans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dirt-discovery of inference rules from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a probabilistic model of event sequences from internet weblog stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Manshadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inducing neural models of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automatically labeling semantic classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Main Proceedings</title>
		<editor>Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL</editor>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Oxlm: A neural language modelling framework for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baltescu</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blunsom</forename><surname>Phil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoang</forename><surname>Hieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical script learning with multi-argument events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning script knowledge with web experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict script events from domainspecific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical and Computational Semantics (* SEM 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scripts, plans, goals and understanding: An inquiry into human knowledge structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Abelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lawrence Erlbaum Associates</title>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
