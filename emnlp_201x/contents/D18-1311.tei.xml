<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>MOE</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>MOE</addrLine>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sogou Technology Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2857" to="2863"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2857</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Pre-trained word embeddings and language model have been shown useful in a lot of tasks. However, both of them cannot directly capture word connections in a sentence, which is important for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly capture word connections from unla-beled data by a word ordering model with self-attention mechanism. Experiments show that these implicit word connections do improve our parsing model. Furthermore, by combining with a pre-trained language model, our model gets state-of-the-art performance on the English PTB dataset, achieving 96.35% UAS and 95.25% LAS.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is a fundamental task for lan- guage processing which aims to establish syntac- tic relations between words in a sentence. Graph- based models <ref type="bibr" target="#b14">(McDonald et al., 2005;</ref><ref type="bibr" target="#b15">McDonald and Pereira, 2006;</ref><ref type="bibr">Carreras, 2007;</ref><ref type="bibr" target="#b10">Koo and Collins, 2010)</ref> and transition-based models <ref type="bibr" target="#b17">(Nivre, 2008;</ref><ref type="bibr" target="#b28">Zhang and Nivre, 2011</ref>) are the most suc- cessful solutions to the challenge.</p><p>Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models <ref type="bibr">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b18">Pei et al., 2015;</ref>) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) are then ap- plied to dependency parsing ( <ref type="bibr" target="#b5">Dyer et al., 2015;</ref><ref type="bibr" target="#b3">Cross and Huang, 2016;</ref><ref type="bibr" target="#b26">Wang and Chang, 2016;</ref><ref type="bibr" target="#b9">Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b4">Dozat and Manning, 2016</ref>) due to its ability to capture contextual information. Generative neural network models <ref type="bibr" target="#b6">(Dyer et al., 2016;</ref><ref type="bibr" target="#b22">Smith et al., 2017;</ref><ref type="bibr">Choe and Charniak, 2016</ref>) also show promising parsing per- formance. Different from Machine Translation task where massive sets of labeled data could be easily ob- tained, parsing performance is limited by the rel- atively small size of available treebank. <ref type="bibr" target="#b25">Vinyals et al. (2015)</ref> and  adopt an ap- proach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existing parsers and selecting only the sentences for which the two parsers produced the same trees. However, the trees produced this way have noise 1 and tend to be short sentences, since it is easier for different parsers to get consistent results.</p><p>Pre-trained neural networks are another meth- ods to take advantage of unlabeled data. Pre- trained word embeddings ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref> and language model <ref type="bibr" target="#b8">(Józefowicz et al., 2016;</ref><ref type="bibr" target="#b19">Peters et al., 2017</ref><ref type="bibr" target="#b20">Peters et al., , 2018</ref>) have been shown useful in modelling NLP tasks since word embeddings could capture word semantic information and lan- guage model could capture contextual information at the sentence level. However, connections be- tween words in the sentence cannot be directly captured by word embeddings or language model, which are crucial for dependency parsing given its goal is to establish dependency relations be- tween words. In this paper, we propose to im- plicitly model word connections by a word or- dering model. The purpose of word ordering model is to generate a well-formed sentence given a bag of words. We human could make sentences easily from unordered words since we have syn- tactic knowledge, thus a model generating well- formed sentences from the bag of words encodes syntactic information. In addition, word order- ing task allows us to use self-attention mechanism to model connections between words in the sen- tence. Different from the tri-training approach, our approach takes advantage of implicit word con- nections learned by self-attended word ordering model in an unsupervised way.</p><p>Experiments show that pre-trained word or- dering model significantly improves our depen- dency parsing model. Ablation tests also show self-attention mechanism is critical. Moreover, by combining word ordering model and lan- guage model, our graph-based dependency pars- ing model achieves SOTA performance on the En- glish Penn Treebank ( <ref type="bibr" target="#b12">Marcus et al., 1993</ref>) with 96.35% UAS and 95.25% LAS.  <ref type="figure">Figure 1</ref>: Overview of our word ordering model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Word Ordering Model</head><p>The target of word ordering is to generate a well- formed sentence given a bag of words. To cap- ture word connections implicated in the sentence, an LSTM-based word ordering model with self- attention is proposed. Self-attention mechanism effectively decides which words in the word bag are more important in generating the next word. It improves the ability of our model to capture word connections. As illustrated in <ref type="figure">Figure 1</ref>, the pro- posed word ordering model consists of two layers:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder Layer</head><p>Given a bag of words w 1 , w 2 , ..., w n , we encode each word by a character-level BiLSTM (c wo w 1:n ), which could reduce the parameters used in our model compared with word embeddings. For the input word of current time-step (w i ), a self- attention layer is utilized to align the word with its related words, producing its self-attended vector (sa wo w i ) as following:</p><formula xml:id="formula_0">s i j = v T ReLU(W sa [c wo w i ; c wo w j ])<label>(1)</label></formula><formula xml:id="formula_1">a i t = exp(s i t )/Σ n j=1 exp(s i j )<label>(2)</label></formula><p>sa wo</p><formula xml:id="formula_2">w i = Σ n j=1 a i j c wo w j (3)</formula><p>The scores s i 1:n in self-attention explicitly repre- sent the connections between words.</p><p>We then concatenate the character-level word embedding (c wo w i ) and its self-attended vector (sa wo w i ):</p><formula xml:id="formula_3">x wo i = [c wo w i ; sa wo w i ]<label>(4)</label></formula><p>x wo i is fed into the decoder layer to generate the next word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decoder Layer</head><p>Given the current input vector (x wo i ), which con- tains current word information and weighted infor- mation of related words, a forward LSTM is used to generate the next word. We initialize the for- ward LSTM with an average of the input word em- beddings (c wo w i:n ). A virtual token BOS is added as the input of the first LSTM time-step:</p><formula xml:id="formula_4">− → h wo i = LSTM(x wo i , − → h wo i−1 )<label>(5)</label></formula><p>At each time-step, the hidden state − → h wo i is utilized to predict the next word. Due to the output vocab- ulary is limited in the bag of words, we just com- pute scores for the given words (w 1:n ):</p><formula xml:id="formula_5">so i j = v T ReLU(W o [ − → h wo i ; cd wo w j ])+ − → h wo i T M wd wo w j (6)</formula><p>To reduce the parameters, each output word is represented by a character-level BiLSTM embed- ding (cd wo w j ) and a low-dimensional word embed- ding 2 (wd wo w j ). M is a matrix projecting a low- dimensional embedding back up to the dimension- ality of LSTM hidden states. The scores so i 1:n are then normalized with Softmax, and the word with max probability is chosen as the next token.</p><p>The word ordering model could be trained eas- ily in an unsupervised manner. Given a large set of unlabeled sentences, we can just ignore the word order of sentence and train the model to gener- ate the corresponding well-formed sentence in the training set. To be specific, we minimize the sum of negative log probabilities of the ground truth words on the unlabeled data set. Different from language model, the choice for each decoder step is limited in the bag of words. Moreover, self- attention can be introduced into the word order- ing model since we have known the bag of words, which could capture the dependency connections between words. We also pre-train a backward word ordering model to generate sentences in re- verse order. The forward and backward models share character-level BiLSTM embeddings, self- attention layer, and Softmax layer.</p><p>Different from previous word ordering mod- els ( <ref type="bibr" target="#b11">Liu et al., 2015;</ref><ref type="bibr" target="#b21">Schmaltz et al., 2016)</ref>, self- attention mechanism is introduced into our model to capture word connections. Moreover, our more important goal is to implicitly utilize large-scale unlabeled data to help dependency parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Graph-based Parsing Model</head><p>We implement an LSTM-based neural network model as our graph-based dependency parsing baseline, which is similar to <ref type="bibr" target="#b9">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b26">Wang and Chang, 2016</ref>). As shown in the <ref type="figure" target="#fig_0">Figure 2,</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Layer</head><p>Given a n-words input sentence s with words w 1 , w 2 , ..., w n and its POS tags p 1 , p 2 , ..., p n . The input layer creates a sequence of input vectors x 1:n in which each x i is a concatenation of its word embedding (e w i ), POS tag embedding (e p i ), character-level BiLSTM embedding (c w i ), and word ordering model pre-trained vector (wo w i ):</p><formula xml:id="formula_6">x i = [e w i ; e p i ; c w i ; wo w i ]<label>(7)</label></formula><p>To get the word ordering model pre-trained vec- tor (wo w i ), the sentence s is fed into the pre- trained word ordering model. Following <ref type="bibr" target="#b20">Peters et al. (2018)</ref></p><note type="other">, we then combine the input vector (x wo i = [c wo w i ; sa wo w i ]) and L-layer BiLSTM vectors (h wo</note><formula xml:id="formula_7">i,j =[ − → h wo i,j ; ← − h wo i,j ] | j=1, 2, .</formula><p>.., L) by a Softmax- normalized weight (W woc ) and a scalar parameter (γ):</p><formula xml:id="formula_8">wo w i = γ(W woc 0 x wo i +Σ L j=1 W woc j h wo i,j )<label>(8)</label></formula><p>The parameters of word ordering model are fixed during the training of parsing model. However, the weight and scalar parameters are tuned to bet- ter adapt to it. The combined output wo w i contains word connections from self-attention, word infor- mation from character-level embedding and con- textual information from LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder &amp; Output Layer</head><p>To introduce more contextual information, we en- code each input element by deep BiLSTMs:</p><formula xml:id="formula_9">v i = BiLSTM(x 1:n , i)<label>(9)</label></formula><p>Two different highway networks ( <ref type="bibr" target="#b23">Srivastava et al., 2015)</ref> are then used to encode head word repre- sentations (v head 1:n ) and dependent word represen- tations (v dep 1:n ). For a head-dependent dependency pair (w h , w d ), the dependency arc and label score are computed by two MLP networks:</p><formula xml:id="formula_10">i h,d = [v head h ; v dep d ; v head h v dep d ] (10) s arc h,d = W arc 1 ReLU(W arc 2 i h,d )<label>(11)</label></formula><formula xml:id="formula_11">s label h,d = W label 1 ReLU(W label 2 i h,d )<label>(12)</label></formula><p>We use the Max-Margin criterion to train our parsing model, which is the same as <ref type="bibr" target="#b9">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b26">Wang and Chang, 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on the English Penn Treebank and the CoNLL 09 English dataset. For   <ref type="bibr" target="#b24">Toutanova et al., 2003</ref>) is used for assigning POS tags. Fol- lowing previous work, UAS (unlabeled attach- ment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation. For the CoNLL 09 English dataset, we follow the standard practice and include all punctuation in the evalua- tion. We pre-train our word ordering model on the 1 billion word benchmark ( <ref type="bibr">Chelba et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The graph-based dependency parsing model and word ordering model are optimized with Adam with an initial learning rate of 2e −3 . The β 1 and β 2 used in Adam are 0.9 and 0.999 respectively.</p><p>The following hyper-parameters are used in all graph-based dependency parsing models: word embedding size = 300, POS tag embedding size = 32, character embedding size = 50, word-level LSTM hidden vector size = 200, word-level BiL- STM layer number = 3, character-level LSTM hidden vector size = 50, character-level BiLSTM layer number = 2, batch size = 32. We also apply dropout for the input and each layer with dropout rate of 0.3. We use pre-trained case- sensitive GloVe embeddings 4 to initialize word embeddings. These word embeddings are fine tuned with the graph-based dependency parsing model. The parameters of pre-trained word or- dering model are fixed during the training of de- pendency parsing model. For deep BiLSTM, we concatenate the outputs of each layer as its final outputs.</p><p>For our word ordering model: input character- level LSTM hidden vector size = 512, input character-level BiLSTM layer number = 1, word- level LSTM hidden vector size = 1024, word-level LSTM layer number = 2, output character-level LSTM hidden vector size = 512, output character- level BiLSTM layer number = 1, output low- dimensional word embedding size = 64, batch size = 32, dropout for the input and each layer = 0.5. <ref type="table" target="#tab_2">Table 1</ref> shows the performance of our model and previous work on two English benchmarks. Our model achieves promising results on both datasets. Two sets of experiments are provided to show the effectiveness of pre-trained word ordering model. Although our baseline system is similar to <ref type="bibr" target="#b9">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b26">Wang and Chang, 2016</ref>) but with subtle differences in architecture, the baseline could perform much better to our surprise and thus constitutes a very strong base- line. Compared with this baseline, introducing the pre-trained word ordering model achieves a significant improvement (almost 0.6% UAS gains for both datasets, p &lt; 0.001). To further show the effectiveness of word ordering model, we also implement an even stronger baseline with pre- trained language model <ref type="bibr">5</ref> . Compared with this much stronger baseline, incorporating pre-trained word ordering model still achieves a significant improvement (0.3% UAS gains for both datasets, p &lt; 0.01). We attribute the improvement to the ability of word ordering model to capture word connections, which cannot be directly captured by language model. Moreover, by combining with a pre-trained language model, our model outper- forms current SOTA model from 95.9% UAS to 96.35% UAS on the PTB dataset. The introduction data/glove.840B.300d.zip.  of POS tag features could contribute about 0.2% improvement in our experiments. The word order- ing model could be more helpful without POS tag features and seem to compensate for the lack of POS tag features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results &amp; Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UAS / LAS</head><p>To show the importance of self-attention mech- anism, we do ablation tests on the models with pre-trained word ordering model vectors. We re- move self-attention vectors by replacing it with the character-level representations. As shown in table 2, self-attention further improves depen- dency parsing. Word connections modeled by self- attention are important for dependency parsing. <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of word connec- tions learned by the model, where we use the solid line to indicate the word connections learned by the word-ordering model and dashed line to the expected dependencies. We can see meaningful overlap could be observed in the example. The percentage of overlap between connections and dependency arcs is over 40% for the sentences less than 10 words. The differences between connec- tions and dependency arcs are because that our word ordering model trained without any super- vised dependency information. The connections are actually built to increase the likelihood.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose to implicitly capture word connections from large-scale unlabeled data by a word ordering model with self-attention. Ex- periments show these features are helpful for de- pendency parsing. Moreover, with the help of word ordering model and language model, our model achieves SOTA results on the PTB dataset.</p><p>As for future work, we are testing on languages other than English. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of neural graph-based dependency parsing model. WO represents pre-trained vectors from word ordering model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Method</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of self-attention. The solid line denotes the mostly attended word when generating the next word, dashed line denotes the correct dependency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>it consists of three layers:</head><label></label><figDesc></figDesc><table>Input vectors 

He 
… 

LSTM 
LSTM 

likes 
eating 

LSTM 

H 
D 
H 
D 
H 
D 

(í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí± , í µí±í µí±í µí±í µí±í µí± í µí±í µí±í µí±í µí±í µí± ) 

Concat 

Highway 
networks 
for head&amp;dep 

Dependent representations 

Encoder layer 

Output scores 

Head representations 

Concat 
Concat 

Word 
Char WO 
POS 
Word 
Char WO 
POS 
Word 
Char WO 
POS 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Results on the English PTB dataset and CoNLL 09 English dataset. WO represents the pre- trained word ordering model. LM represents the pre- trained language model from Peters et al. (2018). PTB dataset, we follow the standard splits. Us- ing section 2-21 for training, section 22 as devel- opment set and 23 as test set. The treebank is converted to Stanford Basic Dependencies (Marn- effe et al., 2006) by version 3.3.0 3 of the Stan- ford parser. The Stanford POS Tagger (</figDesc><table>PTB 
CONLL 09 
UAS / LAS 
UAS / LAS 
Bohnet2012 
-/ -
92.87 / 90.60 
Weiss2015 
94.26 / 92.41 
-/ -
Alberti2015 
94.23 / 92.36 
92.7 / 90.56 
Kiperwasser2016 
93.9 / 91.9 
-/ -
Wang2016 
94.08 / 91.82 
-/ -
Andor2016 
94.61 / 92.79 93.22 / 91.23 
Dozat2016 
95.74 / 94.08 95.21 / 93.20 
Smith2017 
95.8 / 94.6 
-/ -
Choe2016 
95.9 / 94.1 
-/ -
Baseline 
95.12 / 93.98 93.61 / 91.70 
Baseline+WO 
95.66 / 94.54 94.21 / 92.27 
Baseline+LM 
96.05 / 94.88 94.50 / 92.70 
Baseline+LM&amp;WO 96.35 / 95.25 94.83 / 93.05 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Ablation tests of self-attention mechanism on the PTB dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> Character-level representations lack the capacity to differentiate between words that have very different meanings but that are spelled similarly. Low-dimensional word embeddings are added to improve the ability.</note>

			<note place="foot" n="3"> http://nlp.stanford.edu/software/ lex-parser.shtml 4 Downloaded from http://nlp.stanford.edu/</note>

			<note place="foot" n="5"> We use the pre-trained language model provided by Peters et al. (2018), which can be downloaded at http:// allennlp.org/elmo. The pre-trained language model vectors are added in the input layer, which are included in the same way as word ordering model. Peters et al. (2018) found the pre-trained language model works extremely well in six NLP tasks including QA, SRL, and others, we confirm its effectiveness in parsing task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank all the anonymous reviewers for their helpful comments. This work is supported by Na-tional Natural Science Foundation of China under Grant No.61876004 and No.61751201. The cor-responding author of this paper is Baobao Chang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing and tagging with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1354" to="1359" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno>Au- gust 7-12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Language Processing</title>
		<meeting><address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental parsing with minimal features using bi-directional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transition-based syntactic linearization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
			<pubPlace>Lrec</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<editor>EACL. Citeseer</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An effective neural network model for graph-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/1802.05365</idno>
		<title level="m">Deep contextualized word representations. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word ordering without syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Schmaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="2319" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain; Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<imprint>
			<publisher>Quebec</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
