<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Macro Grammars and Holistic Triggering for Efficient Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Macro Grammars and Holistic Triggering for Efficient Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1214" to="1223"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To learn a semantic parser from denota-tions, a learning algorithm must search over a combinatorially large space of logical forms for ones consistent with the annotated denotations. We propose a new online learning algorithm that searches faster as training progresses. The two key ideas are using macro grammars to cache the abstract patterns of useful logical forms found thus far, and holistic triggering to efficiently retrieve the most relevant patterns based on sentence similarity. On the WIKITABLEQUESTIONS dataset, we first expand the search space of an existing model to improve the state-of-the-art accuracy from 38.7% to 42.7%, and then use macro grammars and holistic triggering to achieve an 11x speedup and an accuracy of 43.7%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the task of learning a semantic parser for question answering from question- answer pairs <ref type="bibr" target="#b5">(Clarke et al., 2010;</ref><ref type="bibr" target="#b24">Liang et al., 2011;</ref><ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr" target="#b27">Pasupat and Liang, 2015)</ref>. To train such a parser, the learning algorithm must somehow search for consistent logical forms (i.e., logical forms that execute to the correct answer denota- tion). Typically, the search space is defined by a compositional grammar over logical forms (e.g., a context-free grammar), which we will refer to as the base grammar.</p><p>To cover logical forms that answer complex questions, the base grammar must be quite general and compositional, leading to a huge search space that contains many useless logical forms. For ex- ample, the parser of <ref type="bibr" target="#b27">Pasupat and Liang (2015)</ref>   <ref type="table">Table 1</ref>: A knowledge base for the question x = "Who ranked right after Turkey?". The target de- notation is y = {Sweden}.</p><p>Wikipedia table questions (with beam size 100) generates and featurizes an average of 8,400 par- tial logical forms per example. Searching for con- sistent logical forms is thus a major computational bottleneck.</p><p>In this paper, we propose macro grammars to bias the search towards structurally sensible logi- cal forms. To illustrate the key idea, suppose we managed to parse the utterance "Who ranked right after Turkey?" in the context of <ref type="table">Table 1</ref> into the following consistent logical form (in lambda DCS) (Section 2.1):</p><p>R <ref type="bibr">[Nation]</ref>.R <ref type="bibr">[Next]</ref>.Nation.Turkey, which identifies the cell under the Nation column in the row after Turkey. From this logical form, we can abstract out all relations and entities to pro- duce the following macro:</p><formula xml:id="formula_0">R[{Rel#1}]</formula><p>.R <ref type="bibr">[Next]</ref>.{Rel#1}.{Ent#2}, which represents the abstract computation: "iden- tify the cell under the {Rel#1} column in the row after {Ent#2}." More generally, macros capture the overall shape of computations in a way that generalizes across different utterances and knowl- edge bases. Given the consistent logical forms of utterances parsed so far, we extract a set of macro rules. The resulting macro grammar consisting of these rules generates only logical forms conform- ing to these macros, which is a much smaller and higher precision set compared to the base gram- mar.</p><p>Though the space of logical forms defined by the macro grammar is smaller, it is still expensive to parse with them as the number of macro rules grows with the number of training examples. To address this, we introduce holistic triggering: for a new utterance, we find the K most similar utter- ances and only use the macro rules induced from any of their consistent logical forms. Parsing now becomes efficient as only a small subset of macro rules are triggered for any utterance. Holistic trig- gering can be contrasted with the norm in semantic parsing, in which logical forms are either triggered by specific phrases (anchored) or can be triggered in any context (floating).</p><p>Based on the two ideas above, we propose an online algorithm for jointly inducing a macro grammar and learning the parameters of a se- mantic parser. For each training example, the algorithm first attempts to find consistent logi- cal forms using holistic triggering on the current macro grammar. If it succeeds, the algorithm uses the consistent logical forms found to update model parameters. Otherwise, it applies the base gram- mar for a more exhaustive search to enrich the macro grammar. At test time, we only use the learned macro grammar.</p><p>We evaluate our approach on the WIKITABLE- QUESTIONS dataset <ref type="bibr" target="#b27">(Pasupat and Liang, 2015)</ref>, which features a semantic parsing task with open- domain knowledge bases and complex questions. We first extend the model in <ref type="bibr" target="#b27">Pasupat and Liang (2015)</ref> to achieve a new state-of-the-art test ac- curacy of 42.7%, representing a 10% relative im- provement over the best reported result ( <ref type="bibr" target="#b11">Haug et al., 2017)</ref>. We then show that training with macro grammars yields an 11x speedup compared to training with only the base grammar. At test time, using the learned macro grammar achieves a slightly better accuracy of 43.7% with a 16x run time speedup over using the base grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We base our exposition on the task of question an- swering on a knowledge base. Given a natural lan- guage utterance x, a semantic parser maps the ut- terance to a logical form z. The logical form is executed on a knowledge base w to produce deno- tation z w . The goal is to train a semantic parser from a training set of utterance-denotation pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge base and logical forms</head><p>A knowledge base refers to a collection of enti- ties and relations. For the running example "Who ranked right after Turkey?", we use <ref type="table">Table 1 from  Wikipedia as the knowledge base. Table cells</ref> (e.g., Turkey) and rows (e.g., r 3 = the 3rd row) are treated as entities. Relations connect enti- ties: for example, the relation Nation maps r 3 to Turkey, and a special relation Next maps r 3 to r 4 .</p><p>A logical form z is a small program that can be executed on the knowledge base. We use lambda DCS <ref type="bibr" target="#b23">(Liang, 2013)</ref> as the language of logical forms. The smallest units of lambda DCS are en- tities (e.g., Turkey) and relations (e.g., Nation). Larger logical forms are composed from smaller ones, and the denotation of the new logical form can be computed from denotations of its con- stituents. For example, applying the join operation on Nation and Turkey gives Nation.Turkey, whose denotation is Nation.Turkey w = {r 3 }, which corresponds to the 3rd row of the table. The partial logical form Nation.Turkey can then be used to construct a larger logical form:</p><formula xml:id="formula_1">z = R[Nation].R[Next].Nation.Turkey, (1)</formula><p>where R <ref type="bibr">[·]</ref> represents the reverse of a relation. The denotation of the logical form z with respect to the knowledge base w is equal to z w = {Sweden}. See Liang (2013) for more details about the semantics of lambda DCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Grammar rules</head><p>The space of logical forms is defined recursively by grammar rules. In this setting, each constructed logical form belongs to a category (e.g., Entity, Rel, Set), with a special category Root for com- plete logical forms. A rule specifies the categories of the arguments, category of the resulting logi- cal form, and how the logical form is constructed from the arguments. For instance, the rule</p><formula xml:id="formula_2">Rel[z 1 ] + Set[z 2 ] → Set[z 1 .z 2 ]<label>(2)</label></formula><p>specifies that a partial logical form z 1 of category Rel and z 2 of category Set can be combined into z 1 .z 2 of category Set. With this rule, we can construct Nation.Turkey if we have constructed Nation of type Rel and Turkey of type Set.</p><p>We consider the rules used by Pasupat and Liang (2015) for their floating parser. <ref type="bibr">1</ref> The rules</p><formula xml:id="formula_3">Root[z1] R[Nation].R[Next].Nation.Turkey Set[R[z1].z2] R[Nation].R[Next].Nation.Turkey Set[R[Next].z1] R[Next].Nation.Turkey Set[z1.z2] Nation.Turkey Set[z1] Turkey Ent[Turkey] Turkey Rel[Nation] Nation Rel[Nation] Nation ∅ → ∅ → "Turkey" → (a) Derivation tree (zi represents the ith child) Root[z1] Set[R[z1].z2] Set[R[Next].z1] Set[z1.z2] Set[z1] Ent Rel (b) Macro Root[z1] M2 Sub-macro M3 Set[z1] Ent Sub-macro M1 Set[R[z1].z2] Set[R[Next].z1] Set[z1.z2]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M1 Rel</head><p>Sub-macro M2 (c) Atomic sub-macros <ref type="figure">Figure 1</ref>: From the derivation tree (a), we extract a macro (b), which can be further decomposed into atomic sub-macros (c). Each sub-macro is con- verted into a macro rule.</p><p>are divided into compositional rules and terminal rules. Rule (2) above is an example of a compo- sitional rule, which combines one or more partial logical forms together. A terminal rule has one of the following forms:</p><formula xml:id="formula_4">T okenSpan[span] → c[f (span)] (3) ∅ → c[f (∅)]<label>(4)</label></formula><p>where c is a category. A rule with the form (3) con- verts an utterance token span (e.g., "Turkey") into a partial logical form (e.g., Turkey). A rule with the form (4) generates a partial logical form with- out any trigger. This allows us to generate logical predicates that do not correspond to any part of the utterance (e.g., Nation). A complete logical form is generated by recur- sively applying rules. We can represent the deriva- tion process by a derivation tree such as in <ref type="figure">Fig-  ure 1a</ref>. Every node of the derivation tree corre- sponds to one rule. The leaf nodes correspond to terminal rules, and the intermediate nodes corre- spond to compositional rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning a semantic parser</head><p>Parameters of the semantic parser are learned from training data {(x i , w i , y i )} n i=1 . Given a training example with an utterance x, a knowledge base w, and a target denotation y, the learning algo- rithm constructs a set of candidate logical forms indicated by Z. It then extracts a feature vector φ(x, w, z) for each z ∈ Z, and defines a log-linear distribution over the candidates z:</p><formula xml:id="formula_5">p θ (z | x, w) ∝ exp(θ φ(x, w, z)),<label>(5)</label></formula><p>where θ is a parameter vector. The straightfor- ward way to construct Z is to enumerate all possi- ble logical forms induced by the grammar. When the search space is prohibitively large, it is a com- mon practice to use beam search. More precisely, the algorithm constructs partial logical forms re- cursively by the rules, but for each category and each search depth, it keeps only the B highest- scoring logical forms according to the model prob- ability (5). During training, the parameter θ is learned by maximizing the regularized log-likelihood of the correct denotations:</p><formula xml:id="formula_6">J(θ) = 1 n n i=1 log p θ (y i | x i , w i ) − λθ 1 , (6)</formula><p>where the probability p θ (y i | x i , w i ) marginalizes over the space of candidate logical forms:</p><formula xml:id="formula_7">p θ (y i | x i , w i ) = z∈Z i :zw i =y i p θ (z | x i , w i ).</formula><p>The objective is optimized using AdaGrad ( <ref type="bibr" target="#b8">Duchi et al., 2010)</ref>. At test time, the algorithm selects a logical form z ∈ Z with the highest model prob- ability (5), and then executes it on the knowledge base w to predict the denotation z w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning a macro grammar</head><p>The base grammar usually defines a large search space containing many irrelevant logical forms. For example, the grammar in Pasupat and Liang (2015) can generate long chains of join opera- tions (e.g., R <ref type="bibr">[Silver]</ref>.Rank.R <ref type="bibr">[Gold]</ref>.Bronze.2) that rarely express meaningful computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Processing a training example</head><p>Data: example (x, w, y), macro grammar, base grammar with terminal rules T 1 Select a set R of macro rules (Section 3.4); 2 Generate a set Z of candidate logical forms from rules R ∪ T (Section 2.3);</p><p>3 if Z contains consistent logical forms then 4</p><p>Update model parameters (Section 3.5);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">else 6</head><p>Apply the base grammar to search for a consistent logical form (Section 2.3);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7</head><p>Augment the macro grammar (Section 3.6); 8 end <ref type="bibr">9</ref> Associate utterance x with the highest- scoring consistent logical form found;</p><p>The main contribution of this paper is a new al- gorithm to speed up the search based on previous searches. At a high-level, we incrementally build a macro grammar which encodes useful logical form macros discovered during training. Algo- rithm 1 describes how our learning algorithm pro- cesses each training example. It first tries to use an appropriate subset of rules in the macro grammar to search for logical forms. If the search succeeds, then the semantic parser parameters are updated as usual. Otherwise, it falls back to the base gram- mar, and then add new rules to the macro grammar based on the consistent logical form found. Only the macro grammar is used at test time.</p><p>We first describe macro rules and how they are generated from a consistent logical form. Then we explain the steps of the training algorithm in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Logical form macros</head><p>A macro characterizes an abstract logical form structure. We define the macro for any given log- ical form z by transforming its derivation tree as illustrated in <ref type="figure">Figure 1b</ref>. First, for each terminal rule (leaf node), we substitute the rule by a place- holder, and name it with the category on the right- hand side of the rule. Then we merge leaf nodes that represent the same partial logical form. For example, the logical form (1) uses the relation Nation twice, so in <ref type="figure">Figure 1b</ref>, we merge the two leaf nodes to impose such a constraint.</p><p>While the resulting macro may not be tree-like, we call each node root or leaf if it is a root node or a leaf node of the associated derivation tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing macro rules from macros</head><p>For any given macro M , we can construct a set of macro rules that, when combined with termi- nal rules from the base grammar, generates exactly the logical forms that satisfy the macro M . The straightforward approach is to associate a unique rule with each macro: assuming that its k leaf nodes contain categories c 1 , . . . , c k , we can define a rule:</p><formula xml:id="formula_8">c 1 [z 1 ] + · · · + c k [z k ] → Root[f (z 1 , . . . , z k )], (7)</formula><p>where f substitutes z 1 , . . . , z k into the corre- sponding leaf nodes of macro M . For example, the rule for the macro in <ref type="figure">Figure 1b</ref> is</p><formula xml:id="formula_9">Rel[z 1 ]+Ent[z 2 ] → Root[R[z 1 ].R[Next].z 1 .z 2 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decomposed macro rules</head><p>Defining a unique rule for each macro is computa- tionally suboptimal since the common structures shared among macros are not being exploited. For example, while max(R <ref type="bibr">[Rank]</ref>.Gold.Num.2) and R <ref type="bibr">[Nation]</ref>.argmin(Gold.Num.2, Index) be- long to different macros, the partial logical form Gold.Num.2 is shared, and we wish to avoid gen- erating and featurizing it more than once.</p><p>In order to reuse such shared parts, we de- compose macros into sub-macros and define rules based on them. A subgraph M of M is a sub- macro if (1) M contains at least one non-leaf node; and (2) M connects to the rest of the macro M \M only through one node (the root of M ). A macro M is called atomic if the only sub-macro of M is itself.</p><p>Given a non-atomic macro M , we can find an atomic sub-macro M of M . For example, from <ref type="figure">Figure 1b</ref>, we first find sub-macro M = M 1 . We detach M from M and define a macro rule:</p><formula xml:id="formula_10">c 1 [z 1 ] + · · · + c k [z k ] → c out [f (z 1 , . . . , z k )],<label>(8)</label></formula><p>where c 1 , . . . , c k are categories of the leaf nodes of M , and f substitutes z 1 , . . . , z k into the sub- macro M . The category c out is computed by serializing M as a string; this way, if the sub- macro M appears in a different macro, the cat- egory name will be shared. Next, we substitute the subgraph M in M by a placeholder node with name c out . The procedure is repeated on the new graph until the remaining macro is atomic. Finally, we define a single rule for the atomic macro. The macro grammar uses the decomposed macro rules in replacement of Rule <ref type="formula">(7)</ref>.</p><p>For example, the macro in <ref type="figure">Figure 1b</ref> is decom- posed into three macro rules:</p><formula xml:id="formula_11">Ent[z 1 ] → M 1 [z 1 ], Rel[z 1 ] + M 1 [z 2 ] → M 2 [R[z 1 ].R[Next].z 1 .z 2 ], M 2 [z 1 ] → Root[z 1 ].</formula><p>These correspond to the three atomic sub-macros M 1 , M 2 and M 3 in <ref type="figure">Figure 1c</ref>. The first and the second macro rules can be reused by other macros.</p><p>Having defined macro rules, we now describe how Algorithm 1 uses and updates the macro grammar when processing each training example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Triggering macro rules</head><p>Throughout training, we keep track of a set S of training utterances that have been associated with a consistent logical form. (The set S is updated by Step 9 of Algorithm 1.) Then, given a train- ing utterance x, we compute its K-nearest neigh- bor utterances in S, and select all macro rules that were extracted from their associated logical forms. These macro rules are used to parse utterance x.</p><p>We use token-level Levenshtein distance as the distance metric for computing nearest neigh- bors. More precisely, every utterance is writ- ten as a sequence of lemmatized tokens x = (x (1) , . . . , x (m) ). After removing all determiners and infrequent nouns that appear in less than 2% of the training utterances, the distance between two utterances x and x is defined as the Levenshtein distance between the two sequences. When com- puting the distance, we treat each word token as an atomic element. For example, the distance be- tween "highest score" and "best score" is 1. De- spite its simplicity, the Levenshtein distance does a good job in capturing the structural similarity between utterances. <ref type="table">Table 2</ref> shows that nearest neighbor utterances often map to consistent logi- cal forms with the same macro.</p><p>In order to compute the nearest neighbors effi- ciently, we pre-compute a sorted list of K max = 100 nearest neighbors for every utterance before training starts. During training, calculating the in- tersection of this sorted list with the set S gives the nearest neighbors required. For our experiments, the preprocessing time is negligible compared to the overall training time (less than 3%), but if com- puting nearest neighbors is expensive, then paral- Who ranked right after Turkey? Who took office right after Uriah Forrest? How many more passengers flew to Los Angeles than to Saskatoon in 2013? How many more Hungarians live in the Serbian Banat region than Romanians in 1910? Which is deeper, Lake Tuz or Lake Palas Tuzla? Which peak is higher, Mont Blanc or Monte Rosa? <ref type="table">Table 2</ref>: Examples of nearest neighbor utterances in the WIKITABLEQUESTIONS dataset. lelization or approximate algorithms (e.g., Indyk, 2004) could be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Updating model parameters</head><p>Having computed the triggered macro rules R, we combine them with the terminal rules T from the base grammar (e.g., for building Ent and Rel) to create a per-example grammar R ∪ T for the ut- terance x. We use this grammar to generate logi- cal forms using standard beam search. We follow Section 2.3 to generate a set of candidate logical forms Z and update model parameters.</p><p>However, we deviate from Section 2.3 in one way. Given a set Z of candidate logical forms for some training example (x i , w i , y i ), we pick the logical form z + i with the highest model probability among consistent logical forms, and pick z − i with the highest model probability among inconsistent logical forms, then perform a gradient update on the objective function:</p><formula xml:id="formula_12">J(θ) = 1 n n i=1 log p + i p + i + p − i − λθ 1 ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_13">p + i = p θ (z + i | x i , w i ) p − i = p θ (z − i | x i , w i ).</formula><p>Compared to (6), this objective function only con- siders the top consistent and inconsistent logical forms for each example instead of all candidate logical forms. Empirically, we found that opti- mizing (9) gives a 2% gain in prediction accuracy compared to optimizing (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Updating the macro grammar</head><p>If the triggered macro rules fail to find a consis- tent logical form, we fall back to performing a beam search on the base grammar. For efficiency, we stop the search either when a consistent logical form is found, or when the total number of gener- ated logical forms exceeds a threshold T . The two stopping criteria prevent the search algorithm from spending too much time on a complex example. We might miss consistent logical forms on such examples, but because the base grammar is only used for generating macro rules, not for updat- ing model parameters, we might be able to induce the same macro rules from other examples. For instance, if an example has an uttereance phrase that matches too many knowledge base entries, it would be more efficient to skip the example; the macro that would have been extracted from this example can be extracted from less ambiguous ex- amples with the same question type. Such omis- sions are not completely disastrous, and can speed up training significantly. When the algorithm succeeds in finding a con- sistent logical form z using the base grammar, we derive its macro M following Section 3.1, then construct macro rules following Section 3.3. These macro rules are added to the macro gram- mar. We also associate the utterance x with the consistent logical form z, so that the macro rules that generate z can be triggered by other examples. Parameters of the semantic parser are not updated in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Prediction</head><p>At test time, we follow Steps 1-2 of Algorithm 1 to generate a set Z of candidate logical forms from the triggered macro rules, and then output the highest-scoring logical form in Z. Since the base grammar is never used at test time, prediction is generally faster than training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We report experiments on the WIKITABLEQUES- TIONS dataset <ref type="bibr" target="#b27">(Pasupat and Liang, 2015)</ref>. Our al- gorithm is compared with the parser trained only with the base grammar, the floating parser of Pa- supat and Liang (2015) (PL15), the Neural Pro- grammer parser <ref type="bibr" target="#b25">(Neelakantan et al., 2016</ref>) and the Neural Multi-Step Reasoning parser ( <ref type="bibr" target="#b11">Haug et al., 2017</ref>). Our algorithm not only outperforms the others, but also achieves an order-of-magnitude speedup over the parser trained with the base grammar and the parser in PL15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The dataset contains 22,033 complex questions on 2,108 Wikipedia tables. Each question comes with a table, and the tables during evaluation are dis- "Which driver appears the most?" argmax(R <ref type="bibr">[Driver]</ref>.Type.Row, R[λx.count(Driver.x)]) "What language was spoken more during the Olympic oath, English or French?"</p><formula xml:id="formula_14">argmax(English French, R[λx.count(Language.x)])</formula><p>"Who is taller, Rose or Tim?"</p><p>argmax(Rose Tim, R[λx.R <ref type="bibr">[Num]</ref>.R <ref type="bibr">[Height]</ref>.Name.x)]) We use the same features and logical form prun- ing strategies as PL15, but generalize their base grammar. To control the search space, the actual system in PL15 restricts the superlative operators argmax and argmin to be applied only on the set of table rows. We allow these operators to be ap- plied on the set of tables cells as well, so that the grammar captures certain logical forms that are not covered by PL15 (see <ref type="table" target="#tab_1">Table 3</ref>). Additionally, for terminal rule (3), we allow f (span) to pro- duce entities that approximately match the token span in addition to exact matches. For example, the phrase "Greenville" can trigger both entities Greenville Ohio and Greensville.</p><p>We chose hyperparameters using the first train- dev split. The beam size B of beam search is cho- sen to be B = 100. The K-nearest neighbor pa- rameter is chosen as K = 40. Like PL15, our algorithm takes 3 passes over the dataset for train- ing. The maximum number of logical forms gen- erated in step 6 of Algorithm 1 is set to T = 5,000 for the first pass. For subsequent passes, we set T = 0 (i.e., never fall back to the base grammar) so that we stop augmenting the macro grammar. During the first pass, Algorithm 1 falls back to the base grammar on roughly 30% of the training ex- amples.</p><p>For training the baseline parser that only relies on the base grammar, we use the same beam size B = 100, and take 3 passes over the dataset for training. There is no maximum constraint on the Dev Test <ref type="bibr" target="#b27">Pasupat and Liang (2015)</ref> 37.0% 37.1% <ref type="bibr" target="#b25">Neelakantan et al. (2016)</ref> 37.5% 37.7% <ref type="bibr" target="#b11">Haug et al. (2017)</ref> - 38.7% This paper: base grammar 40.6% 42.7% This paper: macro grammar 40.4% 43.7% <ref type="table">Table 4</ref>: Results on WIKITABLEQUESTIONS.</p><p>number of logical forms that can be generated for each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Coverage of the macro grammar</head><p>With the base grammar, our parser generates 13,700 partial logical forms on average for each training example, and hits consistent logical forms on 81.0% of the training examples. With the macro rules from holistic triggering, these num- bers become 1,300 and 75.6%. The macro rules generate much fewer partial logical forms, but at the cost of slightly lower coverage.</p><p>However, these coverage numbers are com- puted based on finding any logical form that ex- ecutes to the correct denotation. This includes spurious logical forms, which do not reflect the semantics of the question but are coincidentally consistent with the correct denotation. (For exam- ple, the question "Who got the same number of sil- vers as France?" on <ref type="table">Table 1</ref> might be spuriously parsed as R <ref type="bibr">[Nation]</ref>.R <ref type="bibr">[Next]</ref>.Nation.France, which represents the nation listed after France.) To evaluate the "true" coverage, we sample 300 training examples and manually label their logi- cal forms. We find that on 48.7% of these exam- ples, the top consistent logical form produced by the base grammar is semantically correct. For the macro grammar, this ratio is also 48.7%, meaning that the macro grammar's effective coverage is as good as the base grammar.</p><p>The macro grammar extracts 123 macros in to- tal. Among the 75.6% examples that were covered by the macro grammar, the top 34 macros cover 90% of consistent logical forms. By examining the top 34 macros, we discover explicit semantic meanings for 29 of them, which are described in detail in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Accuracy and speedup</head><p>We report prediction accuracies in <ref type="table">Table 4</ref>. With a more general base grammar (additional superla- tives and approximate matching), and by optimiz- Time (ms/ex) Acc.</p><p>Train <ref type="table" target="#tab_1">Pred  PL15  37.0%  619  645  Ours: base grammar  40.6% 1,117 1,150  Ours: macro grammar  40.4%  99  70  no holistic triggering  40.1%  361  369  no macro decomposition 40.3%</ref> 177 159 ing the objective function (9), our base parser out- performs PL15 (42.7% vs 37.1%). Learning a macro grammar slightly improves the accuracy to 43.7% on the test set. On the three train-dev splits, the averaged accuracy achieved by the base gram- mar and the macro grammar are close (40.6% vs 40.4%).</p><p>In <ref type="table" target="#tab_2">Table 5</ref>, we compare the training and predic- tion time of PL15 as well as our parsers. For a fair comparison, we trained all parsers using the SEMPRE toolkit <ref type="bibr" target="#b1">(Berant et al., 2013</ref>) on a ma- chine with Xeon 2.6GHz CPU and 128GB mem- ory without parallelization. The time for con- structing the macro grammar is included as part of the training time. <ref type="table" target="#tab_2">Table 5</ref> shows that our parser with the base grammar is more expensive to train than PL15. However, training with the macro grammar is substantially more efficient than train- ing with only the base grammar-it achieves 11x speedup for training and 16x speedup for test time prediction.</p><p>We run two ablations of our algorithm to evalu- ate the utility of holistic triggering and macro de- composition. The first ablation triggers all macro rules for parsing every utterance without holistic triggering, while the second ablation constructs Rule (7) for every macro without decomposing it into smaller rules. <ref type="table" target="#tab_2">Table 5</ref> shows that both vari- ants result in decreased efficiency. This is be- cause holistic triggering effectively prunes irrele- vant macro rules, while macro decomposition is important for efficient beam search and featuriza- tion. curacy of the macro grammar is robust to varying beam sizes as long as B ≥ 25. <ref type="figure">Figure 2b</ref> shows the influence of the neighbor size K. A smaller neighborhood triggers fewer macro rules, leading to faster computation. The accuracy peaks at K = 40 then decreases slightly for large K. We conjecture that the smaller num- ber of neighbors acts as a regularizer. <ref type="figure">Figure 2c</ref> reports an experiment where we limit the number of fallback calls to the base grammar to m. After the limit is reached, subsequent train- ing examples that require fallback calls are simply skipped. This limit means that the macro gram- mar will get augmented at most m times during training. We find that for small m, the prediction accuracy grows with m, implying that building a richer macro grammar improves the accuracy. For larger m, however, the accuracies hardly change. According to the plot, a competitive macro gram- mar can be built by calling the base grammar on less than 15% of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Influence of hyperparameters</head><p>Based on <ref type="figure">Figure 2</ref>, we can trade accuracy for speed by choosing smaller values of (B, K, m). With B = 50, K = 40 and m = 2000, the macro grammar achieves a slightly lower averaged devel- opment accuracy (40.2% rather than 40.4%), but with an increased speedup of 15x (versus 11x) for training and 20x (versus 16x) for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work and discussion</head><p>A traditional semantic parser maps natural lan- guage phrases into partial logical forms and composes these partial logical forms into com- plete logical forms. Parsers define composi- tion based on a grammar formalism such as Combinatory Categorial Grammar (CCG) <ref type="bibr" target="#b31">(Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b21">Kwiatkowski et al., 2011</ref><ref type="bibr" target="#b19">Kwiatkowski et al., , 2013</ref><ref type="bibr" target="#b18">Kushman and Barzilay, 2013;</ref><ref type="bibr" target="#b17">Krishnamurthy and Kollar, 2013)</ref>, Synchronous CFG ( <ref type="bibr" target="#b30">Wong and Mooney, 2007)</ref>, and CFG ( <ref type="bibr" target="#b15">Kate and Mooney, 2006;</ref><ref type="bibr" target="#b4">Chen and Mooney, 2011;</ref><ref type="bibr" target="#b1">Berant et al., 2013;</ref><ref type="bibr" target="#b6">Desai et al., 2016)</ref>, while others use the syntactic structure of the utterance to guide composition ( <ref type="bibr" target="#b28">Poon and Domingos, 2009;</ref><ref type="bibr" target="#b29">Reddy et al., 2016)</ref>. Recent neural semantic parsers al- low any sequence of logical tokens to be generated ( <ref type="bibr" target="#b7">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b13">Jia and Liang, 2016;</ref><ref type="bibr">Kocisk´ycisk´y et al., 2016;</ref><ref type="bibr" target="#b25">Neelakantan et al., 2016;</ref><ref type="bibr" target="#b10">Guu et al., 2017)</ref>. The flexibility of these composition methods allows arbitrary logi- cal forms to be generated, but at the cost of a vastly increased search space.</p><p>Whether we have annotated logical forms or not has dramatic implications on what type of ap- proach will work. When logical forms are avail- able, one can perform grammar induction to mine grammar rules without search ( <ref type="bibr" target="#b20">Kwiatkowski et al., 2010)</ref>. When only annotated denotations are avail- able, as in our setting, one must use a base gram- mar to define the output space of logical forms. Usually these base grammars come with many re- strictions to guard against combinatorial explosion <ref type="bibr" target="#b27">(Pasupat and Liang, 2015)</ref>.</p><p>Previous work on higher-order unification for lexicon induction ( <ref type="bibr" target="#b20">Kwiatkowski et al., 2010</ref>) us- ing factored lexicons ( <ref type="bibr" target="#b21">Kwiatkowski et al., 2011</ref>) also learns logical form macros with an online al- gorithm. The result is a lexicon where each entry contains a logical form template and a set of possi- ble phrases for triggering the template. In contrast, we have avoided binding grammar rules to particu- lar phrases in order to handle lexical variations. In- stead, we use a more flexible mechanism-holistic triggering-to determine which rules to fire. This allows us to generate logical forms for utterances containing unseen lexical paraphrases or where the triggering is spread throughout the sentence. For example, the question "Who is X, John or Y" can still trigger the correct macro extracted from the last example in <ref type="table" target="#tab_1">Table 3</ref> even when X and Y are unknown words.</p><p>Our macro grammars bears some resemblance to adaptor grammars <ref type="bibr" target="#b14">(Johnson et al., 2006</ref>) and fragment grammars (O'Donnell, 2011), which are also based on the idea of caching useful chunks of outputs. These generative approaches aim to solve the modeling problem of assigning higher proba- bility mass to outputs that use reoccurring parts. In contrast, our learning algorithm uses caching as a way to constrain the search space for computa- tional efficiency; the probabilities of the candidate outputs are assigned by a separate discriminative model. That said, the use of macro grammars does have a small positive modeling contribution, as it increases test accuracy from 42.7% to 43.7%.</p><p>An orthogonal approach for improving search efficiency is to adaptively choose which part of the search space to explore. For example, <ref type="bibr" target="#b3">Berant and Liang (2015)</ref> uses imitation learning to strate- gically search for logical forms. Our holistic trig- gering method, which selects macro rules based on the similarity of input utterances, is related to the use of paraphrases <ref type="bibr" target="#b2">(Berant and Liang, 2014;</ref><ref type="bibr" target="#b9">Fader et al., 2013</ref>) or string kernels ( <ref type="bibr" target="#b15">Kate and Mooney, 2006</ref>) to train semantic parsers. While the input similarity measure is critical for scoring logical forms in these previous works, we use the measure only to retrieve candidate rules, while scoring is done by a separate model. The retrieval bar means that our similarity metric can be quite crude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Summary</head><p>We have presented a method for speeding up se- mantic parsing via macro grammars. The main source of efficiency is the decreased size of the logical form space. By performing beam search on a few macro rules associated with the K- nearest neighbor utterances via holistic triggering, we have restricted the search space to semanti- cally relevant logical forms. At the same time, we still maintain coverage over the base logical form space by occasionally falling back to the base grammar and using the consistent logical forms found to enrich the macro grammar. The higher ef- ficiency allows us expand the base grammar with- out having to worry much about speed: our model achieves a state-of-the-art accuracy while also en- joying an order magnitude speedup.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 2a shows that for all beam sizes, training with the macro grammar is more efficient than training with the base grammar, and the speedup rate grows with the beam size. The test time ac</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>on</head><label></label><figDesc></figDesc><table>Rank Nation Gold Silver Bronze 
r1 : 
1 
France 
3 
1 
1 
r2 : 
2 
Ukraine 
2 
1 
2 
r3 : 
3 
Turkey 
2 
0 
1 
r4 : 
4 
Sweden 
2 
0 
0 
r5 : 
5 
Iran 
1 
2 
1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Several example logical forms our gram-
mar can generate that are not covered by PL15. 

joint from the ones during training. The train-
ing and test sets contain 14,152 and 4,344 exam-
ples respectively. 2 Following PL15, the develop-
ment accuracy is averaged over the first three 80-
20 training data splits given in the dataset package. 
The test accuracy is reported on the train-test data 
split. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison and ablation study: the 
columns report averaged prediction accuracy, 
training time, and prediction time (milliseconds 
per example) on the three train-dev splits. 

</table></figure>

			<note place="foot" n="1"> Their grammar and our implementation use more finegrained categories (Atomic, V alues, Records) instead of Set. We use the coarser category here for simplicity.</note>

			<note place="foot" n="2"> The remaining 3,537 examples were not included in the original data split.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UW SPF: The University of Washington semantic parsing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.3011</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imitation learning of agenda-based semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="859" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Program synthesis using natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hingorani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karkare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="345" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grnarova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06589</idno>
		<title level="m">Neural multi-step reasoning for question answering on semi-structured tables</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor under edit distance via product metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Discrete Algorithms (SODA)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="646" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using stringkernels for learning semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="913" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1078" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using semantic unification to generate regular expressions from natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="826" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inducing probabilistic CCG grammars from logical form with higher-order unification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1223" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D F N</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4408</idno>
		<title level="m">Lambda dependency-based compositional semantics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Productivity and Reuse in Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>O&amp;apos;donnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="127" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="960" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
