<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reordering Grammar Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><forename type="middle">Stanojevi´c</forename><surname>Stanojevi´c</surname></persName>
							<email>m.stanojevic@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
							<email>k.simaan@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reordering Grammar Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel approach for unsu-pervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation. Unlike previous approaches , we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora. Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation. Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering , and over two known preordering baselines for English-Japanese.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Preordering ( <ref type="bibr" target="#b7">Collins et al., 2005</ref>) aims at permut- ing the words of a source sentence s into a new order´sorder´order´s, hopefully close to a plausible target word order. Preordering is often used to bridge long dis- tance reorderings (e.g., in Japanese-or German- English), before applying phrase-based models ( <ref type="bibr" target="#b19">Koehn et al., 2007)</ref>. Preordering is often bro- ken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolin- gual syntactic trees and focus on finding a trans- duction function of the sibling subtrees under the nodes ( <ref type="bibr" target="#b22">Lerner and Petrov, 2013;</ref><ref type="bibr" target="#b39">Xia and Mccord, 2004</ref>). The (direct correspondence) assumption underlying this approach is that permuting the sib- lings of nodes in a source syntactic tree can pro- duce a plausible target order. An alternative ap- proach creates reordering rules manually and then learns the right structure for applying these rules <ref type="bibr" target="#b17">(Katz-Brown et al., 2011</ref>). Others attempt learn- ing the transduction structure and the transduction function in two separate, consecutive steps <ref type="bibr" target="#b10">(DeNero and Uszkoreit, 2011</ref>). Here we address the challenge of learning both the trees and the trans- duction functions jointly, in one fell swoop, from word-aligned parallel corpora.</p><p>Learning both trees and transductions jointly raises two questions. How to obtain suitable trees for the source sentence and how to learn a distri- bution over random variables specifically aimed at reordering in a hierarchical model? In this work we solve both challenges by using the fac- torizations of permutations into Permutation Trees (PETs) ( <ref type="bibr" target="#b40">Zhang and Gildea, 2007)</ref>. As we ex- plain next, PETs can be crucial for exposing the hierarchical reordering patterns found in word- alignments.</p><p>We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permuta- tion (1:1 and onto) on the source side. Every per- mutation can be factorized into a forest of PETs (over the source sentences) which we use as a la- tent treebank for training a Probabilistic Context- Free Grammar (PCFG) tailor made for preorder- ing as we explain next. <ref type="figure">Figure 1</ref> shows two alternative PETs for the same permutation over minimal phrases. The nodes have labels (like P 3142) which stand for lo- cal permutations (called prime permutation) over the child nodes; for example, the root label P 3142 stands for prime permutation 3, 1, 4, 2, which says that the first child of the root becomes 3 rd on the target side, the second becomes 1 st , the third becomes 4 th and the fourth becomes 2 nd . The prime permutations are non-factorizable permuta- tions like 1, 2, 2, 1 and 2, 4, 1, 3.</p><p>We think PETs are suitable for learning pre- ordering for two reasons. Firstly, PETs specify ex- actly the phrase pairs defined by the permutation. Secondly, every permutation is factorizable into prime permutations only <ref type="bibr" target="#b0">(Albert and Atkinson, 2005</ref>). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering.</p><p>For learning preordering, we first extract an ini- tial PCFG from the latent treebank of PETs over the source sentences only. We initialize the non- terminal set of this PCFG to the prime permuta- tions decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing <ref type="bibr" target="#b24">(Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b29">Prescher, 2005;</ref><ref type="bibr" target="#b28">Petrov et al., 2006;</ref><ref type="bibr" target="#b32">Saluja et al., 2014</ref>). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s).</p><p>Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permu- tations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version´sversion´version´s of s. In this sense, our latent splits are dedicated to reordering.</p><p>We face two technical difficulties alien to work on latent PCFGs in treebank parsing. Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank. 1 And secondly, after we parse a source string s, we are interested in´sin´in´s, the per- muted version of s, not in the best derivation/PET. Exact computation is a known NP-Complete prob- lem <ref type="bibr" target="#b33">(Sima'an, 2002</ref>). We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations ( <ref type="bibr" target="#b1">Birch and Osborne, 2011;</ref><ref type="bibr" target="#b15">Isozaki et al., 2010a)</ref>.</p><p>In summary, this paper contributes:</p><p>• A novel latent hierarchical source reordering model working over all derivations of PETs 1 All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure ( <ref type="bibr" target="#b40">Zhang and Gildea, 2007</ref>).</p><p>• A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations.</p><p>• A fast Minimum Bayes Risk decoding over Kendall τ reordering score for selecting´sselecting´selecting´s. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outper- forming two existing baselines for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PETs and the Hidden Treebank</head><p>We aim at learning a PCFG which we will use for parsing source sentences s into synchronous trees, from which we can obtain a reordered source ver- sion´ssion´sion´s. Since PCFGs are non-synchronous gram- mars, we will use the nonterminal labels to encode reordering transductions, i.e., this PCFG is implic- itly an SCFG. We can do this because s and´sand´and´s are over the same alphabet.</p><p>Here, we have access only to a word-aligned parallel corpus, not a treebank. The following steps summarize our approach for acquiring a la- tent treebank and how it is used for learning a Re- ordering PCFG:</p><p>1. Obtain a permutation over minimal phrases from every word-alignment. 2. Obtain a latent treebank of PETs by factoriz- ing the permutations. 3. Extract a PCFG from the PETs with initial nonterminals taken from the PETs. 4. Learn to split the initial nonterminals and es- timate rule probabilities. These steps are detailed in the next section, but we will start out with an intuitive exposition of PETs, the latent treebank and the Reordering Grammar. <ref type="figure">Figure 1</ref> shows examples of how PETs look like -see ( <ref type="bibr" target="#b40">Zhang and Gildea, 2007</ref>) for algorith- mic details. Here we label the nodes with nonter- minals which stand for prime permutations from the operators on the PETs. For example, non- terminals P 12, P 21 and P 3142 correspond re- spectively to reordering transducers 1, 2, 2, 1 and 3, 1, 4, 2. A prime permutation on a source node µ is a transduction dictating how the chil- dren of µ are reordered at the target side, e.g., P 21 inverts the child order. We must stress that any similarity with ITG ( <ref type="bibr" target="#b38">Wu, 1997</ref>) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations in PETs (P 12 and P 21). ITGs recognize only the binarizable permutations, which is a major restric- tion when used on the data: there are many non- binarizable permutations in actual data <ref type="bibr" target="#b37">(Wellington et al., 2006</ref>). In contrast, our PETs are ob- tained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus. In practice we limit them to maximum arity 5.</p><p>We can extract PCFG rules from the PETs, e.g., P 21 → P 12 P 2413. However, these rules are decorated with too coarse labels. A similar prob- lem was encountered in non-lexicalized monolin- gual parsing, and one solution was to lexicalize the productions <ref type="bibr" target="#b8">(Collins, 2003</ref>) using head words. But linguistic heads do not make sense for PETs, so we opt for the alternative approach ( <ref type="bibr" target="#b24">Matsuzaki et al., 2005</ref>), which splits the nonterminals and softly percolates the splits through the trees gradu- ally fitting them to the training data. Splitting has a shadow side, however, because it leads to com- binatorial explosion in grammar size.</p><p>Suppose for example node P 21 could split into P 21 1 and P 21 2 and similarly P 2413 splits into P 2413 1 and 2413 2 . This means that rule P 21 → P 12 P 2413 will form eight new rules:</p><formula xml:id="formula_0">P 21 1 → P 12 1 P 2413 1 P 21 1 → P 12 1 P 2413 2 P 21 1 → P 12 2 P 2413 1 P 21 1 → P 12 2 P 2413 2 P 21 2 → P 12 1 P 2413 1 P 21 2 → P 12 1 P 2413 2 P 21 2 → P 12 2 P 2413 1 P 21 2 → P 12 2 P 2413 2</formula><p>Should we want to split each nonterminal into 30 subcategories, then an n-ary rule will split into 30 n+1 new rules, which is prohibitively large. Here we use the "unary trick" as in <ref type="figure">Figure 2</ref>. The superscript on the nonterminals denotes the child position from left to right. For example P 21 2 1 means that this node is a second child, and the mother nonterminal label is P 21 1 . For the running example rule, this gives the following rules:</p><formula xml:id="formula_1">P 21 1 → P 21 1 1 P 21 2 1 P 21 2 → P 21 1 2 P 21 2 2 P 21 1 1 → P 12 1 P 21 2 1 → P 2413 1 P 21 1 1 → P 12 2 P 21 2 1 → P 2413 2 P 21 1 2 → P 12 1 P 21 2 2 → P 2413 1 P 21 1 2 → P 12 2 P 21 2 2 → P 2413 2</formula><p>The unary trick leads to substantial reduction in grammar size, e.g., for arity 5 rules and 30 splits we could have had 30 6 = 729000000 split-rules, but with the unary trick we only have 30+30 2 * 5 = 4530 split rules. The unary trick was used in early lexicalized parsing work <ref type="bibr" target="#b2">(Carroll and Rooth, 1998)</ref>. <ref type="bibr">2</ref> This split PCFG constitutes a latent PCFG because the splits cannot be read of a tree- bank. It must be learned from the latent treebank of PETs, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P12</head><p>Professor Chomsky , I would like to thank you Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken</p><formula xml:id="formula_2">P3142 P12 P21 P12 1 P12 2 P12 1 P12 2 P21 1 P21 2 P3142 1 P3142 2 P3142 3 P3142 4</formula><p>Figure 2: Permutation Tree with unary trick</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Details of Latent Reordering PCFG</head><p>Obtaining permutations Given a source sen- tence s and its alignment a to a target sentence t in the training corpus, we segment s, a, t into a sequence of minimal phrases s m (maximal se- quence) such that the reordering between these minimal phrases constitutes a permutation π m . We do not extract non-contiguous or non-minimal phrases because reordering them often involves complicated transductions which could hamper the performance of our learning algorithm. 3</p><p>Unaligned words Next we describe the use of the factorization of permutations into PET forests for training a PCFG model. But first we need to extend the PETs to allow for unaligned words. An unaligned word is joined with a neighboring phrase to the left or the right, depending on the source language properties (e.g., whether the lan- guage is head-initial or -final <ref type="bibr" target="#b6">(Chomsky, 1970)</ref>).</p><p>Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right. This modifies a PET by adding a new binary branching node µ (dominat- ing the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P 01 if the unaligned word joins to the right and P 10 if it joins to the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probability model</head><p>We decompose the permutation π m into a forest of permutation trees P EF (π m ) in O(n 3 ), follow- ing algorithms in ( <ref type="bibr" target="#b41">Zhang et al., 2008;</ref><ref type="bibr" target="#b40">Zhang and Gildea, 2007</ref>) with trivial modifications. Each PET ∆ ∈ P EF (π m ) is a different bracketing (differing in binary branching structure only). We consider the bracketing hidden in the latent tree- bank, and apply unsupervised learning to induce a distribution over possible bracketings. Our prob- ability model starts from the joint probability of a sequence of minimal phrases s m and a permuta- tion π m over it. This demands summing over all PETs ∆ in the forest P EF (π m ), and for every PET also over all its label splits, which are given by the grammar derivations d:</p><formula xml:id="formula_3">P (s m , π m ) = ∆∈P EF (πm) d∈∆ P (d, s m )<label>(1)</label></formula><p>The probability of a derivation d is a product of probabilities of all the rules r that build it:</p><formula xml:id="formula_4">P (s m , π m ) = ∆∈P EF (πm) d∈∆ r∈d P (r)<label>(2)</label></formula><p>3 Which differs from <ref type="bibr" target="#b30">(Quirk and Menezes, 2006</ref>).</p><p>As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Splits on Latent Treebank</head><p>For training the latent PCFG over the latent tree- bank, we resort to EM <ref type="bibr" target="#b9">(Dempster et al., 1977)</ref> which estimates PCFG rule probabilities to max- imize the likelihood of the parallel corpus in- stances. Computing expectations for EM is done efficiently using Inside-Outside ( <ref type="bibr" target="#b21">Lari and Young, 1990)</ref>. As in other state splitting models ( <ref type="bibr" target="#b24">Matsuzaki et al., 2005</ref>), after splitting the non- terminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry. We split the non-terminals only once as in ( <ref type="bibr" target="#b24">Matsuzaki et al., 2005</ref>) (unlike ( <ref type="bibr" target="#b28">Petrov et al., 2006</ref>)). For es- timating the distribution for unknown words we replace all words that appear ≤ 3 times with the "UNKNOWN" token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>We use CKY+ <ref type="bibr" target="#b3">(Chappelier and Rajman, 1998</ref>) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively´salternatively´alternatively´s) as in</p><formula xml:id="formula_5">argmax π∈Π ∆∈P EF (π) d∈∆ P (d, π m )</formula><p>from a lattice of permutations Π using a PCFG is NP-complete <ref type="bibr" target="#b33">(Sima'an, 2002</ref>). Existing tech- niques, like variational decoding or Minimum- Bayes Risk (MBR), used for minimizing loss over trees as in <ref type="bibr" target="#b27">(Petrov and Klein, 2007)</ref>, are not di- rectly applicable here. Hence, we opt for mini- mizing the risk of making an error under a loss function over permutations using the MBR deci- sion rule ( <ref type="bibr" target="#b20">Kumar and Byrne, 2004)</ref>:</p><formula xml:id="formula_6">ˆ π = argmin π πr Loss(π, π r )P (π r )<label>(3)</label></formula><p>The loss function we minimize is Kendall τ (Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (in- cluding gapped pairs) to the total number of pairs. We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss. We sample from the true distribution by sampling edges recursively using their inside probabilities. An empirical dis- tribution over permutations P (π) is given by the relative frequency of π in the sample. With large samples it is hard to efficiently com- pute expected Kendall τ loss for each sampled hypothesis. For sentence of length k and sam- ple of size n the complexity of a naive algorithm is O(n 2 </p><note type="other">k 2 ). Computing Kendall τ alone takes O(k 2 ). We use the fact that Kendall τ decom- poses as a linear function over all skip-bigrams b that could be built for any permutation of length k:</note><formula xml:id="formula_7">Kendall(π, π r ) = b 1 − δ(π, b) k(k−1) 2 δ(π r , b)<label>(4</label></formula><formula xml:id="formula_8">ˆ π = argmin π πr b 1 − δ(π, b) k(k−1) 2 δ(π r , b)P (π r )<label>(5)</label></formula><p>We can move the summation inside and reformu- late the expected Kendall τ loss as expectation over the skip-bigrams of the permutation. </p><p>= argmin</p><formula xml:id="formula_10">π b (1 − δ(π, b))E P (πr) δ(π r , b) (7) = argmax π b δ(π, b)E P (πr) δ(π r , b)<label>(8)</label></formula><p>This means we need to pass through the sampled list only twice: (1) to compute expectations over skip bigrams and (2) to compute expected loss of each sampled permutation. The time complexity is O(nk 2 ) which is quite fast in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments with three baselines:</p><p>• Baseline A: No preordering.</p><p>• Baseline B: Rule based preordering (Isozaki et al., 2010b), which first obtains an HPSG parse tree using Enju parser <ref type="bibr">4</ref> and after that swaps the children by moving the syntactic head to the final position to account for differ- ent head orientation in English and Japanese. <ref type="bibr">4</ref> http://www.nactem.ac.uk/enju/</p><p>• Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables. We used LADER in standard set- tings without any linguistic features (POS tags or syntactic trees).</p><p>And we test four variants of our model:</p><p>• RG left -only canonical left branching PET • RG right -only canonical right branching PET • RG ITG-forest -all PETs that are binary (ITG)</p><p>• RG PET-forest -all PETs.</p><p>We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task. For tuning we use all NTCIR-7 dev sets and for test- ing the test set from NTCIR-9 from both direc- tions. All used data was tokenized (English with Moses tokenizer and Japanese with KyTea 5 ) and filtered for sentences between 4 and 50 words. A subset of this data is used for training the Reorder- ing Grammar, obtained by filtering out sentences that have prime permutations of arity &gt; 5, and for the ITG version arity &gt; 2. Baseline C was trained on 600 sentences because training is prohibitively slow. <ref type="table">Table 1</ref>  The Reordering Grammar was trained for 10 it- erations of EM on train RG data. We use 30 splits for binary non-terminals and 3 for non-binary. Training on this dataset takes 2 days and parsing tuning and testing set without any pruning takes 11 and 18 hours respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Intrinsic evaluation</head><p>We test how well our model predicts gold reorder- ings before translation by training the alignment model using MGIZA++ 6 on the training corpus and using it to align the test corpus. Gold re- orderings for the test corpus are obtained by sort- ing words by their average target position and (unaligned words follow their right neighboring word). We use Kendall τ score for evaluation (note the difference with Section 3.3 where we de- fined it as a loss function). <ref type="table" target="#tab_2">Table 2</ref> shows that our models outperform all baselines on this task. The only strange result here is that rule-based preordering obtains a lower score than no preordering, which might be an ar- tifact of the Enju parser changing the tokenization of its input, so the Kendall τ of this system might not really reflect the real quality of the preorder- ing. All other systems use the same tokenization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Extrinsic evaluation in MT</head><p>The reordered output of all the mentioned base- lines and versions of our model are translated with phrase-based MT system ( <ref type="bibr" target="#b19">Koehn et al., 2007</ref>) (dis- tortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ´ s − t. The only exception is Base- line A which is trained on original s − t.</p><p>We use a 5-gram language model trained with KenLM 8 , tune 3 times with kb-mira <ref type="bibr" target="#b4">(Cherry and Foster, 2012)</ref> to account for tuner instability and evaluated using Multeval 9 for statistical signifi- cance on 3 metrics: BLEU ( <ref type="bibr" target="#b26">Papineni et al., 2002</ref>), METEOR <ref type="bibr" target="#b13">(Denkowski and Lavie, 2014</ref>) and TER ( <ref type="bibr" target="#b34">Snover et al., 2006</ref>). We additionally report RIBES score ( <ref type="bibr" target="#b15">Isozaki et al., 2010a</ref>) that concen- trates on word order more than other metrics.</p><p>Single or all PETs? In <ref type="table">Table 3</ref> we see that using all PETs during training makes a big im- pact on performance. Only the all PETs variants <ref type="bibr">7</ref> Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed´sguessed´guessed´s − t pairs, which are the word re-aligned and then used for training the back-end MT system ( <ref type="bibr" target="#b18">Khalilov and Sima'an, 2011)</ref>. We skip this, we take the risk of mismatch between the preordering and the back-end system, but this simplifies training and saves a good amount of training time.</p><p>8 http://kheafield.com/code/kenlm/ 9 https://github.com/jhclark/multeval  <ref type="table">Table 3</ref>: Comparison of different preordering models. Superscripts A, B and C signify if the sys- tem is significantly better (p &lt; 0.05) than the re- spective baseline or significantly worse (in which case it is a subscript). Significance tests were not computed for RIBES. Score is bold if the system is significantly better than all the baselines.</p><formula xml:id="formula_11">System BLEU ↑ METEOR ↑ TER ↓ RIBES ↑ A N o</formula><p>(RG ITG-forest and RG PET-forest ) significantly outper- form all baselines. If we are to choose a single PET per training instance, then learning RG from only left-branching PETs (the one usually cho- sen in other work, e.g. ( <ref type="bibr" target="#b32">Saluja et al., 2014)</ref>) per- forms slightly worse than the right-branching PET. This is possibly because English is mostly right- branching. So even though both PETs describe the same reordering, RG right captures reordering over English input better than RG left .</p><p>All PETs or binary only? RG PET-forest performs significantly better than RG ITG-forest (p &lt; 0.05).</p><p>Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET. Furthermore, having these operators during training might allow for better fit to the data.</p><p>How much reordering is resolved by the Reordering Grammar? Obviously, completely factorizing out the reordering from the transla- tion process is impossible because reordering de- pends to a certain degree on target lexical choice.</p><p>To quantify the contribution of Reordering Gram- mar, we tested decoding with different distortion limit values in the SMT system. We compare the phrase-based (PB) system with distance based cost function for reordering ( <ref type="bibr" target="#b19">Koehn et al., 2007</ref>) with and without preordering. <ref type="figure">Figure 3</ref> shows that Reordering Grammar gives substantial performance improvements at all distortion limits (both BLEU and RIBES). RG PET-forest is less sensitive to changes in decoder distortion limit than standard PBSMT. The perfor- <ref type="figure">Figure 3</ref>: Distortion effect on BLEU and RIBES mance of RG PET-forest varies only by 1.1 BLEU points while standard PBSMT by 4.3 BLEU points. Some local reordering in the decoder seems to help RG PET-forest but large distortion limits seem to degrade the preordering choice. This shows also that the improved performance of RG PET-forest is not only a result of efficiently ex- ploring the full space of permutations, but also a result of improved scoring of permutations.  Does the improvement remain for a decoder with MSD reordering model? We compare the RG PET-forest preordered model against a decoder that uses the strong MSD model <ref type="bibr" target="#b35">(Tillmann, 2004;</ref><ref type="bibr" target="#b19">Koehn et al., 2007</ref>). <ref type="table" target="#tab_5">Table 4</ref> shows that using Reordering Grammar as front-end to MSD re- ordering (full Moses) improves performance by 2.8 BLEU points. The improvement is confirmed by METEOR, TER and RIBES. Our preordering model and MSD are complementary -the Re- ordering Grammar captures long distance reorder- ing, while MSD possibly does better local reorder- ings, especially reorderings conditioned on the lexical part of translation units. Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference be- tween these systems as back-ends to Reordering Grammar (respectively BLEU 32.4 and 32.0) is far smaller (0.4 BLEU). This suggests that a ma- jor share of reorderings can be handled well by preordering without conditioning on target lexical choice. Furthermore, this shows that RG PET-forest preordering is not very sensitive to the decoder's reordering model.</p><formula xml:id="formula_12">System BLEU ↑ METEOR ↑ TER ↓ RIBES ↑ D P BM</formula><p>Comparison to a Hierarchical model (Hiero). Hierarchical preordering is not intended for a hi- erarchical model as Hiero <ref type="bibr" target="#b5">(Chiang, 2005</ref>). Yet, here we compare our preordering system (PB MSD+RG) to Hiero for completeness, while we should keep in mind that Hiero's reordering model has access to much richer training data. We will discuss these differences shortly. <ref type="table" target="#tab_5">Table 4</ref> shows that the difference in BLEU is not statistically significant, but there is more dif- ference in METEOR and TER. RIBES, which concentrates more on reordering, prefers Reorder- ing Grammar over Hiero. It is somewhat sur- prising that a preordering model combined with a phrase-based model succeeds to rival Hiero's per- formance on English-Japanese. Especially when looking at the differences between the two:</p><p>1. Reordering Grammar uses only minimal phrases, while Hiero uses composite (longer) phrases which encapsulate internal reorder- ings, but also non-contiguous phrases. 2. Hiero conditions its reordering on the lexical target side, whereas the Reordering Grammar does not (by definition). 3. Hiero uses a range of features, e.g., a lan- guage model, while Reordering Grammar is a mere generative PCFG. The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model. Which structure is learned? <ref type="figure" target="#fig_3">Figure 4</ref> shows an example PET output showing how our model learns: (1) that the article "the" has no equiva- lent in Japanese, (2) that verbs go after their ob- ject, (3) to use postpositions instead of preposi- tions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>The majority of work on preordering is based on syntactic parse trees, e.g., <ref type="bibr" target="#b22">(Lerner and Petrov, 2013;</ref><ref type="bibr" target="#b18">Khalilov and Sima'an, 2011;</ref><ref type="bibr" target="#b39">Xia and Mccord, 2004</ref>). Here we concentrate on work that has common aspects with this work. Neubig et  <ref type="bibr" target="#b36">and Eisner (2009)</ref> use ITG but do not train the grammar. They only use it to constrain the lo- cal search. DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it. In con- trast, here we learn both the structure and the re- ordering function simultaneously. Furthermore, at test time, our inference with MBR over a mea- sure of permutation (Kendall) allows exploiting both structure and reordering weights for infer- ence, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process -the parser forwards to the next stage the best parse. <ref type="bibr" target="#b14">Dyer and Resnik (2010)</ref> treat reordering as a la- tent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation. In their work they consider all permutations allowed by a given syntactic tree.</p><p>Saers et al (2012) induce synchronous gram- mar for translation by splitting the non-terminals, but unlike our approach they split generic non- terminals and not operators. Their most expres- sive grammar covers only binarizable permuta- tions. The decoder that uses this model does not try to sum over many derivations that have the same yield. They do not make independence as- sumption like our "unary trick" which is proba- bly the reason they do not split more than 8 times. They do not compare their results to any other SMT system and test on a very small dataset.</p><p>Saluja et al (2014) attempts inducing a refined Hiero grammar (latent synchronous CFG) from Normalized Decomposition Trees (NDT) ( <ref type="bibr" target="#b41">Zhang et al., 2008)</ref>. While there are similarities with the present work, there are major differences. On the similarity side, NDTs are decomposing align- ments in ways similar to PETs, and both Saluja's and our models refine the labels on the nodes of these decompositions. However, there are major differences between the two:</p><p>• Our model is completely monolingual and unlexicalized (does not condition its reorder- ing on the translation) in contrast with the La- tent SCFG used in ( <ref type="bibr" target="#b32">Saluja et al., 2014</ref>), • Our Latent PCFG label splits are defined as refinements of prime permutations, i.e., specifically designed for learning reordering, whereas <ref type="bibr" target="#b32">(Saluja et al., 2014</ref>) aims at learn- ing label splitting that helps predicting NDTs from source sentences, • Our model exploits all PETs and all deriva- tions, both during training (latent treebank) and during inferences. In ( <ref type="bibr" target="#b32">Saluja et al., 2014</ref>) only left branching NDT derivations are used for learning the model. • The training data used by <ref type="bibr" target="#b32">(Saluja et al., 2014)</ref> is about 60 times smaller in number of words than the data used here; the test set of ( <ref type="bibr" target="#b32">Saluja et al., 2014</ref>) also consists of far shorter sen- tences where reordering could be less crucial.</p><p>A related work with a similar intuition is presented in (Maillette de Buy <ref type="bibr">Wenniger and Sima'an, 2014)</ref>, where nodes of a tree structure similar to PETs are labeled with reordering patterns ob- tained by factorizing word alignments into Hierar- chical Alignment Trees. These patterns are used for labeling the standard Hiero grammar. Unlike this work, the labels extracted by (Maillette de Buy Wenniger and Sima'an, 2014) are clustered manually into less than a dozen labels without the possibility of fitting the labels to the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>51</head><p>We present a generative Reordering PCFG model learned from latent treebanks over PETs obtained by factorizing permutations over minimal phrase pairs. Our Reordering PCFG handles non-ITG reordering patterns (up to 5-ary branching) and it works with all PETs that factorize a permuta- tion (rather than a single PET). To the best of our knowledge this is the first time both extensions are shown to improve performance. The empiri- cal results on English-Japanese show that (1) when used for preordering, the Reordering PCFG helps particularly with relieving the phrase-based model from long range reorderings, (2) combined with a state-of-the-art phrase model, Reordering PCFG shows performance not too different from Hiero, supporting the common wisdom of factorizing long range reordering outside the decoder, (3) Re- ordering PCFG generates derivations that seem to coincide well with linguistically-motivated re- ordering patterns for English-Japanese. There are various direction we would like to explore, the most obvious of which are integrating the learned reordering with other feature functions in a dis- criminative setting, and extending the model to deal with non-contiguous minimal phrases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Possible Permutation Trees (PETs) for one sentence pair</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0. With this decom- position we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypoth- esis. Combining Equations 3 and 4 we get:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example parse of English sentence that predicts reordering for English-Japanese</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>shows the sizes of data used.</figDesc><table>corpus 
#sents 
#words #words 
source target 
train RGPET 
786k 21M 
-
train RGITG 
783k 21M 
-
train LADER 
600 
15k 
-
train translation 950k 25M 30M 
tune translation 
2k 
55K 
66K 
test translation 
3k 
78K 
93K 

Table 1: Data stats 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Reordering prediction</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Comparison to MSD and Hiero</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> After applying the unary trick, we add a constraint on splitting: all nonterminals on an n-ary branching rule must be split simultaneously.</note>

			<note place="foot" n="5"> http://www.phontron.com/kytea/ 6 http://www.kyloo.net/software/doku.php/mgiza:overview</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by STW grant nr. 12271 and NWO VICI grant nr. 277-89-002. We thank Wilker Aziz for comments on earlier version of the paper and discussions about MBR and sampling.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simple Permutations and Pattern Restricted Permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">D</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reordering Metrics for MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Valence Induction with a Head-Lexicalized PCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Third Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Third Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Generalized CYK Algorithm for Parsing Stochastic CFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Cédric</forename><surname>Chappelier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rajman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Tabulation in Parsing and Deduction</title>
		<meeting>the First Workshop on Tabulation in Parsing and Deduction</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="133" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch Tuning Strategies for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Hierarchical Phrase-Based Model for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Remarks on Nominalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in English Transformational Grammar</title>
		<editor>Roderick A. Jacobs and Peter S. Rosenbaum</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Ginn</publisher>
			<date type="published" when="1970" />
			<biblScope unit="page" from="184" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clause Restructuring for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kučerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Head-Driven Statistical Models for Natural Language Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inducing Sentence Structure from Parallel Corpora for Reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast Consensus Decoding over Translation Forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th</title>
		<meeting>the Joint Conference of the 47th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="575" />
		</imprint>
	</monogr>
	<note>ACL &apos;09</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Context-free Reordering, Finite-state Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Translation Quality for Distant Language Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Head Finalization: A Simple Reordering Rule for SOV Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT &apos;10</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="244" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training a Parser for Machine Translation Reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Katz-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ichikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Seno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ContextSensitive Syntactic Source-Reordering by Statistical Transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Khalilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimum Bayes-Risk Decoding for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</title>
		<meeting>the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Estimation of Stochastic Context-Free Grammars using the InsideOutside Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Source-Side Classifier Preordering for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilingual Markov Reordering Labels for Hierarchical SMT</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<editor>Gideon Maillette de Buy Wenniger and Khalil Sima&apos;an</editor>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with Latent Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inducing a Discriminative Parser to Optimize Machine Translation Reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting><address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="843" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved Inference for Unlexicalized Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Accurate, Compact, and Interpretable Tree Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-Bank Grammar for Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detlef</forename><surname>Prescher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2006. ACL/SIGPARSE</title>
		<meeting>HLT-NAACL 2006. ACL/SIGPARSE</meeting>
		<imprint>
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From Finite-State to Inversion Transductions: Toward Unsupervised Bilingual Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Saers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-08-15" />
			<biblScope unit="page" from="2325" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LatentVariable Synchronous CFGs for Hierarchical Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Complexity of Probabilistic Disambiguation. Grammars</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="151" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Unigram Orientation Model for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short &apos;04</title>
		<meeting>HLT-NAACL 2004: Short Papers, HLTNAACL-Short &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Linear Ordering Problems for Better Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Tromble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="1007" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Empirical lower bounds on the complexity of translational equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wellington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonjia</forename><surname>Waxmonsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Dan</forename><surname>Melamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2006</title>
		<meeting>ACL 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
	<note>September</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving a Statistical MT System with Automatically Learned Rewrite Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>COLING</publisher>
			<date type="published" when="2004-08" />
			<biblScope unit="page" from="508" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Factorization of Synchronous Context-Free Grammars in Linear Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Syntax and Structure in Statistical Translation (SSST)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08)</title>
		<meeting>the 22nd International Conference on Computational Linguistics (COLING-08)<address><addrLine>Manchester, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
