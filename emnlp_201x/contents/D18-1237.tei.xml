<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Back</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sathish</forename><surname>Indurthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihie</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
							<email>jchoo@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2131" to="2140"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2131</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In detail, our method has two novel aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory. Our proposed architecture is widely applicable to other models. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed method outperforms existing methods, especially for lengthy documents.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most of the human knowledge has been stored in the form of text. Reading comprehension (RC) to understand this knowledge is a major challenge that can vastly increase the range of knowledge available to the machines. Many neural network- based methods have been proposed, pushing per- formance close to a human level. Nonetheless, there still exists room to improve the performance especially in comprehending lengthy documents that involve complicated reasoning processes. We identify the main bottleneck as the lack of the long-term memory and its improper controlling mechanism.</p><p>Previously, several memory-augmenting meth- ods have been proposed to solve the long-term de- * To whom correspondence should be addressed. pendency problem. For example, in relatively sim- ple tasks such as bAbI tasks <ref type="bibr" target="#b24">(Weston et al., 2015)</ref>, <ref type="bibr" target="#b6">Graves et al. (2014</ref><ref type="bibr" target="#b7">Graves et al. ( , 2016</ref>; <ref type="bibr" target="#b8">Henaff et al. (2017)</ref> proposed methods that handle the external mem- ory to address long-term dependency. Inspired by these approaches, we develop a customized mem- ory controller along with an external memory aug- mentation ( <ref type="bibr" target="#b7">Graves et al., 2016</ref>) for complicated RC tasks. However, we found that the memory controller is susceptible to information distortion as neural networks become deeper, this distortion can hinder the performance.</p><p>To overcome this issue, we propose two novel strategies that improve the memory-handling ca- pability while mitigating the information distor- tion. We extend the memory controller with a residual connection to alleviate the information distortion occurring in it. We also expand the gated recurrent unit (GRU) ( <ref type="bibr" target="#b3">Cho et al., 2014</ref>) with a dense connection that conveys enriched features to the next layer containing the origi- nal as well as the transformed information. We conducted extensive experiments through several benchmark datasets such as TriviaQA, QUASAR- T, and SQuAD. The results show that the proposed model outperforms all the published results. We also integrated the proposed memory controller and the expanded GRU cell block with other ex- isting methods to ensure that our proposed compo- nents are widely applicable. The results show that our components consistently bring performance improvement across various state-of-the-art archi- tectures.</p><p>The main contributions of this work include the following: (1) We propose an extended memory controller module for RC tasks. (2) We propose a densely connected encoder block with self atten- tion to provide rich representation of given data, reducing information loss due to deep layers of the network. (3) We present the state-of-the-art results in lengthy-document RC tasks such as TriviaQA and QUASAR-T as well as relatively short docu- ment RC tasks such as SQuAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Method</head><p>This section presents two of our proposed compo- nents in detail, as depicted in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Memory Controller</head><p>Our first proposed component is an advanced ex- ternal memory controller module for solving RC tasks. We modified the recently proposed mem- ory controller ( <ref type="bibr" target="#b7">Graves et al., 2016</ref>) by using our new encoder block and layer-wise residual con- nections. These modifications enable the memory controller to reason over a lengthy document, lead- ing to the overall performance improvement.</p><p>This layer takes input as a sequence of vec- tor representations corresponding to individual to- kens, d t ∈ R l , where l is the given vector dimen- sion. For example, such input can be the output of the co-attention layer in Section 3. The operation of this layer is defined as</p><formula xml:id="formula_0">o t , i t = Controller(d t , M t−1 ).</formula><p>That is, at time step t, the controller generates an interface vector i t for read and write operations and an output vector o t based on the input vec- tor d t and the external memory content from the previous time step, M t−1 ∈ R p×q , where p is the memory size and q is the vector dimension of each memory.</p><p>Through this controller, we encode an input D = {d t } n t=1 to {x t } n t=1 by using the encoder block, i.e.,</p><formula xml:id="formula_1">{x t } n t=1 = EncoderBlock x (D) ∈ R n×k ,</formula><p>where k is the output dimension of the encoder block. In general, this block is implemented as a recurrent unit, e.g., <ref type="bibr">GRU (Cho et al., 2014</ref>). In our model, we replace it with our dense encoder block with self attention (DEBS), as will be discussed in Section 2.2.</p><p>To generate a memory-augmented vector z t , we concatenate x t with the vectors read from the pre- vious time step memory, M t−1 , i.e.,</p><formula xml:id="formula_2">z t = [x t ; m 1 t−1 ; · · · ; m s t−1 ] ∈ R k+sq ,</formula><p>where s represents the number of read heads in the memory interface. We then feed the vector z t to the bi-directional GRU (BiGRU) layer and obtain the output vector h m t as</p><formula xml:id="formula_3">h m t = BiGRU(z t , h m t−1 , h m t+1 ) ∈ R 2l .</formula><p>Afterwards, we generate output vector v t as the weighted sum of the BiGRU output and read vec- tors from the memory in the current step, i.e.,</p><formula xml:id="formula_4">v t = W h h m t + W m [m 1 t ; · · · ; m s t ] ∈ R 2l .</formula><p>Finally, we add a residual connection between the input d t and the output v t to mitigate any possible information distortion that can occur while access- ing the memory, resulting in a the output vector that can handle long-term dependency, i.e.,</p><formula xml:id="formula_5">o t = ReLU(W v v t + d t ) ∈ R l .</formula><p>For further details on how the interface vector works, we refer the readers to <ref type="bibr" target="#b7">Graves et al. (2016)</ref> as well as our supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dense Encoder Block with Self Attention</head><p>The second novel component we propose is a dense encoder block with self attention (DEBS), which further improves a GRU cell. Recently, <ref type="bibr" target="#b10">Huang et al. (2017a)</ref> proposed that adding a con- nection between each layer to the other layers in convolution networks can help to properly convey the information across multiple layers. Inspired by this, we add such dense connections that con- catenate the input to a particular layer to its out- put. We also add a self-attention module to this block, to properly address long-term dependency in a length document. In this manner, our en- coder block maintains the necessary information not only along the vertical direction (across layers) through dense connections but also along the hor- izontal direction (across time steps) through self attention.</p><p>DEBS takes the input vector sequence with its length as n and transforms each vector to an l- dimensional vector p t through the fully connected layer with ReLU as a nonlinear unit and generates a contextually encoded vector r t as</p><formula xml:id="formula_6">r t = BiGRU(p t , r t−1 , r t+1 ) ∈ R 2l .</formula><p>Then we concatenate each output vector r t to the projected input p t to obtain g t = [r t ; p t ] ∈ R 3l and pass it to the self-attention layer. The self- attention layer then calculates the similarity map S g ∈ R n×n using the tri-linear function as  <ref type="figure">Figure 1</ref>: Overview of our model (A) and dense encoder block with self attention (B).</p><formula xml:id="formula_7">s g ij = w a · g i + w b · g j + w f · (g i g j ),</formula><p>where i, j = 1, . . . , n. Finally, the self-attended representation Q = {q t } n t=1 is obtained by per- forming column-wise softmax on S g to get the at- tention matrix A g , which is further multiplied with G = {g t } n t=1 , i.e.,</p><formula xml:id="formula_8">Q = A g G ∈ R n×3l .</formula><p>The final output is obtained as the concatenation of outputs from the recurrent layer (BiGRU) and the self-attention layer, i.e., [r t ; q t ] ∈ R 5l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reading Comprehension Model with Proposed Components</head><p>We apply the proposed components to our model for RC tasks. As depicted in <ref type="figure">Figure 1</ref>, the model consists of three major layers: the co-attention layer, the memory controller, and the prediction layer. Given the embeddings of a question and a document, the co-attention layer generates query- aware contextual representations. The memory controller further refines these contextual repre- sentations using an external memory. Based on such representations, the prediction layer deter- mines the start and the end token indices that form the answer span. In addition, we replace all the en- coder block with DEBS in the three major layers.</p><p>Embedding. We incorporate both word-and character-level embedding methods to obtain the vector representation of each word in the input data. For word-level embedding e w , we utilize pre-trained, 300-dimensional embedding vectors from GloVe 6B ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>). The character-level word embedding e c is obtained as a 100-dimensional vector by first applying a con- volution layer with 100 filters to a sequence of 20- dimensional character embeddings learned dur- ing training and by further applying global max- pooling over the entire character-level sequence. Then we obtain the embedding vector of a given word token, e, by concatenating these word-and character-level embeddings, i.e., e = [e w ; e c ] ∈ R 400 .</p><p>Finally, we obtain the two sets of embedding vectors of question and document token sequences as</p><formula xml:id="formula_9">E q = {e q u } m u=1 ∈ R m×400 and E d = e d t n t=1</formula><p>∈ R n×400 , where m and n represent the sequence length of a question and a document, re- spectively.</p><p>Co-attention layer. Given E q and E d , we feed each of them into the encoder block and obtain their contextual representations as</p><formula xml:id="formula_10">C q = {c q u } m u=1 = EncoderBlock q (E q ) ∈ R m×k C d = {c d t } n t=1 = EncoderBlock d (E d ) ∈ R n×k .</formula><p>These representations are used to calculate the pairwise similarity matrix S ∈ R m×n between to- kens in the question and those in the document by a tri-linear function ( <ref type="bibr" target="#b20">Seo et al., 2017)</ref>, i.e.,</p><formula xml:id="formula_11">s ij = w q · c q i + w d · c d j + w c · (c q i c d j ),</formula><p>where i = 1, . . . , m, j = (1, . . . , n), and rep- resents the element-wise multiplication and w q , w d , and w c are trainable vectors. We apply column-wise softmax to S to obtain the document- to-question attention matrix A. Afterwards, a question-attended document representatioñ C q is calculate as˜C</p><formula xml:id="formula_12">as˜ as˜C q = {˜c{˜c q t } n t=1 = A T C q ∈ R n×k .</formula><p>In addition to this, we obtain vectorãvectorã ∈ R n , cor- responding to the attention of a question to docu- ment tokens, by applying softmax to the column- wise max values of S. Then document-attended question vector is obtained by˜c</p><formula xml:id="formula_12">as˜ as˜C q = {˜c{˜c q t } n t=1 = A T C q ∈ R n×k .</formula><p>The final co-attended representations {d t } n t=1 is obtained by fully connected layer with ReLU as a nonlinear unit, ϕ, as</p><formula xml:id="formula_14">d t = ϕ([c d t ; ˜ c q t ; c d t ˜ c q t ; c d t ˜ c d t ]) ∈ R l .</formula><p>Memory controller. This layer takes the out- put of the co-attention layer {d t } n t=1 as input and refine their representations using our proposed memory controller (Section 2.1). Afterwards, the resulting output vector {o t } n t=1 are given as input to the prediction layer.</p><p>Prediction layer. We feed the output of the memory controller {o t } n t=1 to the prediction layer to predict the start and the end token indices of the answer span. First, it goes through the encoder block followed by the fully connected layer with softmax over the entire sequence to compute the probability distribution of a start index. The prob- ability distribution of the end index is calculated by concatenating the output of the encoder block for the start index with the output of the memory controller and then by feeding them as input to an- other encoder block. These probability distribu- tions are used as part of the negative log-likelihood objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets and preprocessing. We perform ex- tensive experiments with well-known benchmarks such as TriviaQA, QUASAR-T, and SQuAD, as summarized in <ref type="table">Table 1</ref>. In most of these datasets, a question q and a document d are represented as a sequence of words, and the answer span has to be selected from the document words based on the question. SQuAD consists of crowd-sourced ques- tions and paragraphs from Wikipedia articles con- taining the answer to these questions. QUASAR- T is mostly based on factoid questions with their corresponding, large-sized corpus. TriviaQA is composed of question-answer pairs obtained from 14 trivia and quiz-league websites, along with the documents collected later that are likely to contain the answer from either web search or Wikipedia.</p><p>In TriviaQA dataset, we truncate each document to 1,200 words. Even with such truncation, the aver- age word count per document (AWC) of TriviaQA is approximately four times larger than that of SQuAD. In terms of the AWC, documents in Triv- iaQA, QUASAR-T, and SQuAD can be viewed as large-, medium-, and small-length documents, re- spectively. In TriviaQA dataset, because a document is col- lected separately for an already collected question- answer pair, the document does not sometimes have the information to properly infer the answer to the question. In response, Clark and Gard- ner (2017) attempted to solve this problem by ex- posing both relevant and irrelevant paragraphs to- gether separated based on TF-IDF scores. We fol- low this approach in TriviaQA. In QUASAR-T, we follow the same preprocessing steps done by <ref type="bibr" target="#b5">Dhingra et al. (2017</ref>  to build the model and Sonnet 2 to implement the memory interface. NLTK ( <ref type="bibr" target="#b0">Bird and Loper, 2004</ref>) is used for tokenizing words. In the memory con- troller, we use four read heads and one write head, and the memory size is set to 100 × 36, with all initialized as 0. The hidden vector dimension l is set to 200. We use AdaDelta <ref type="bibr" target="#b28">(Zeiler, 2012)</ref> as an optimizer with a learning rate of 0.5. The batch size is set to 20 for TriviaQA ( <ref type="bibr" target="#b12">Joshi et al., 2017)</ref> and 30 for SQuAD ( <ref type="bibr" target="#b18">Rajpurkar et al., 2016)</ref> and QUASAR-T ( <ref type="bibr" target="#b5">Dhingra et al., 2017)</ref>. We use an ex- ponential moving average of weights with a de- caying factor of 0.001. Our model does require more memory than existing methods, but a single GPU (e.g., M40 with 12GB memory) was enough to train model within a reasonable amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Quantitative Results</head><p>For our quantitative comparisons, we use BiDAF with self attention (Clark and Gardner, 2017) as a baseline, which maintains the best results pub- lished on both TriviaQA and SQuAD datasets. In TriviaQA and QUASAR-T dataset, we compare our model with BiDAF ( <ref type="bibr" target="#b20">Seo et al., 2017</ref>) as well as its variant called 'BiDAF + DNC,' which is aug- mented with an existing external memory archi- tecture ( <ref type="bibr" target="#b7">Graves et al., 2016)</ref> just before the answer prediction layer in the BiDAF.</p><p>Overall, in lengthy-document cases such as TriviaQA and QUASAR-T, our model outper- forms all the published results, as seen in <ref type="table" target="#tab_3">Tables  2 and 3</ref>, while in the short-document case such as SQuAD, we mostly achieve the best results, as seen in <ref type="table" target="#tab_6">Table 4</ref>. In the following, we present de- tailed analyses on each dataset.</p><p>TriviaQA. As shown in <ref type="table" target="#tab_3">Table 2</ref>, our model, even without DEBS, outperforms the existing state-of-the-art method such as 'BiDAF + SA + SN' by a large margin in all the cases. Our model with DEBS, which replaces BiGRU en- coder blocks, performs even better than that with- out it in all the cases except for the combina- tion of the 'full' and 'Wikipedia' case, which in- volves documents containing no relevant informa- tion for the answer. Among those methods shown in  <ref type="bibr" target="#b20">Seo et al., 2017)</ref> 37.00 42.50 39.50 44.50  formation as additional features. We note that our method achieves these outstanding results without any additional features.</p><p>QUASAR-T. As shown in <ref type="table" target="#tab_5">Table 3</ref>, our simple baseline 'BiDAF + DNC,' which involves an exist- ing memory architecture, improves performance over BiDAF, indicating the efficacy of incorpo- rating an external memory. Moreover, our model with the proposed memory controller achieves sig- nificantly better results compared to other mod- els. Furthermore, another proposed component, DEBS, gives an additional performance boost to our model. <ref type="table" target="#tab_6">Table 4</ref>, most of the models, if not all, use additional features such as ELMo ( <ref type="bibr">Peters et al.)</ref>, and the self-attention mecha- nism to further improve the performance. We also adopt these mechanisms one by one to show that our model can also benefit from these. First, we adopt ELMo to our model (without DEBS), which uses word embedding as the weighted sum of the hidden layers of a language model with regulariza- tion as an additional feature to our word embed- dings. This improves the F1 score of our model up to 85.13 and EM to 77.44, showing the highest performances among all the methods without us- ing self attention. Due to the relatively short docu- ment length in SQuAD compared to TriviaQA and QUASAR-T, our model without DEBS performs worse than the baseline 'BiDAF + Self Attention + ELMo.' However, after applying DEBS, our model outperforms the baseline, achieving 86.73 F1 and 79.69 EM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SQuAD. As shown in</head><p>Minimum anchor distance. <ref type="bibr" target="#b18">Rajpurkar et al. (2016)</ref> proposed the difficulty measure called syn- tactic divergence, which is computed as the edit distance between syntactic parse trees of the ques- tion and the sentence containing the answer. How- ever, this measure has limitations that the syntac- tic parser does not work properly on incomplete sentences, which are common in web text. It also becomes difficult to compute this measure if the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Test set AF SA EM F1 Our model (with DEBS) + ELMo 79.69 86.73 BiDAF + Self Attention + ELMo ( <ref type="bibr">Peters et al.)</ref> 78.58 85.83 Our model (without DEBS) + ELMo 77.44 85.13 RaSoR + TR + LM <ref type="bibr" target="#b19">(Salant and Berant, 2017)</ref> 77.58 84.16 QANet <ref type="figure" target="#fig_0">(Yu et al., 2018a)</ref> 76.24 84.60 SAN <ref type="figure" target="#fig_0">(Liu et al., 2017b)</ref> 76.83 84.40 FusionNet <ref type="figure" target="#fig_0">(Huang et al., 2017b)</ref> 75.97 83.90 RaSoR + TR <ref type="bibr" target="#b19">(Salant and Berant, 2017)</ref> 75.79 83.26 Conducter-net <ref type="figure" target="#fig_0">(Liu et al., 2017a)</ref> 74.41 82.74 Reinforced Mnemonic Reader <ref type="figure" target="#fig_0">(Hu et al., 2017)</ref> 73.20 81.80 BiDAF + Self Attention (Clark and Gardner, 2017) 72.14 81.05 MEMEN ( <ref type="bibr" target="#b15">Pan et al., 2017)</ref> 70.98 80.36 Our model (without DEBS) 70.99 79.94 r-net ( <ref type="bibr" target="#b21">Wang et al., 2017)</ref> 71.30 79.70 Document Reader ( <ref type="bibr" target="#b2">Chen et al., 2017)</ref> 70.73 <ref type="bibr">79.35 FastQAExt (Weissenborn et al., 2017)</ref> 70.85 78.86 Human Performance 82.30 91.22 answer requires multi-sentence inference. Instead, we develop our own metric called a minimum anchor distance, which is simple and robust to noisy text. To compute this metric, we first identify for all the co-occurring words (an- chor words) between a document and a question while ignoring stop words. Then, we compute the number of words found between the answer and all the possible anchor words and select the mini- mum number from these.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref>, we show F1 scores of our model with DEBS and the baseline with respect to the minimum anchor distance. The scores are obtained from the development set of Trivi- aQA(Web) and SQuAD. The heat map at the bot- tom of the figure indicates the number of samples in each interval of the minimum anchor distance. One can see that our model performs increasingly better than the baseline as the minimum anchor distance gets larger. The examples shown in Ta- ble 5 indicate that documents with long dependen- cies tend to have a large minimum anchor distance. These examples show that our model predicts the remotely placed answer from the anchor word rel- atively well when anaphora resolution and nega- tion are involved.</p><p>Ablation study with an encoder block. We assume that the concatenation of the layer outputs in DEBS helps the memory controller store con- textual representations clearly. To see how DEBS affects the memory controller depending on differ- ent positions in the entire network, we conducted an ablation study by replacing the encoder block with DEBS on SQuAD. As can be seen in <ref type="table">Table 6</ref>, using DEBS in all the places improves the perfor- mance most, and furthermore, the memory con- troller with DEBS gives the largest performance margin. This implies that DEBS can generally work as a better alternative to a BiGRU module, and DEBS is critical in maintaining the high per- formance of our proposed memory controller.</p><p>Adding our proposed modules to other mod- els. To show the wide effectiveness of our pro- posed approaches, we choose two well-known baseline models in SQuAD: R-net ( <ref type="bibr" target="#b21">Wang et al., 2017)</ref> and 'BiDAF + Self Attention' <ref type="bibr" target="#b4">(Clark and Gardner, 2017</ref>). These models have similar archi- tectures where the model first pairs a given ques- tion and document pair using an attention and af- terward applies a self-attention mechanism. We use the publicly available implementation of these models <ref type="bibr">3,</ref><ref type="bibr">4</ref> . In <ref type="table">Table 7</ref>, replacing all the recur- rent units with DEBS and adding our memory controller between the question-document pairing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Example Question : What claimed the life of singer Kathleen Ferrier? Context : (omit) · · · Kathleen Ferrier <ref type="bibr">(22.III.1912</ref><ref type="bibr">Higher Walton, Lancashire-8.X. TriviaQA 1953</ref> was an English contralto singer * who achieved an international (Web) reputation with a repertoire extending from folksong and popular ballads to the classical works. Her death from cancer , at the height of her fame, was a shock to the musical world and particularly to the general public, which was kept in ignorance of · · · (omit) Question : What did Mote think the Yuan class system really represented? Context : The historian Frederick W.Mote wrote that the usage of the term "social SQuAD classes" for this system was misleading and that the position of people within the four -class system * was not an indication of their actual social power and wealth, but just entailed "degrees of privilege" to which they were entitled institutionally and legally, so a person's standing within the classes was not a guarantee of their standing, · · · (omit) * A word with an asterisk indicates an anchor word closest to the ground truth answer.  <ref type="table">Table 6</ref>: Ablation study of replacing an encoder block with DEBS in the co-attention layer (C), the memory controller (M), and the prediction layer (P) in SQuAD. means that DEBS is used. Oth- erwise, BiGRU is used. layer and the self-attention layer increases the F1 score by around 0.5 compared to the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Numerous neural network-based methods have been proposed, pushing the performance nearly up to a human level. Although slight differ- ences exist, ( <ref type="bibr" target="#b21">Wang et al., 2017;</ref><ref type="bibr" target="#b20">Seo et al., 2017;</ref><ref type="bibr" target="#b25">Xiong et al., 2017</ref>) mostly leverage the question- document co-attention based on their pairwise similarity of word-level vector representations. These models currently work as the backbone ar- chitecture for many other models. Furthermore, <ref type="bibr" target="#b21">Wang et al. (2017)</ref> suggest utilizing a self attention mechanism between tokens within a document to refine contextual representations. <ref type="bibr" target="#b19">Salant and Berant (2017)</ref>   Enriching the input representation from pre- trained external models has been shown to be useful in improving RC task performances. <ref type="bibr" target="#b26">Yu et al. (2018a)</ref> have also improved the performance by leveraging self attention for context encoding based on convolutional neural networks. <ref type="bibr" target="#b9">Hu et al. (2017)</ref> refine the contextual representation with multiple hops, and <ref type="bibr" target="#b15">Pan et al. (2017)</ref> use the en-coded query for refining the answer prediction as a memory, which are different from our work in terms of handling long-range dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper proposed two novel, crucial compo- nents for deep neural network-based RC tasks, (1) an advanced memory controller architecture and (2) a densely connected encoder block with self attention. We showed the effectiveness of these approaches in handling long-range dependencies using three benchmark RC datasets such as Triv- iaQA, QUASAR-T, and SQuAD. Our proposed modules are widely applicable to other models to improve their performance. Future work includes developing a scalable read/write accessing mech- anism to handle a large-scale external memory to reason over multiple documents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F1 score on the development set in TriviaQA (Web) and SQuAD with respect to the minimum anchor distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 :</head><label>7</label><figDesc>Effects of our proposed components added to R-net and 'BiDAF + Self Attention (SA)' on SQuAD. The values in parentheses represent the standard deviation from 6 runs. The first row of each base model indicates the result of the original methods. When adding the proposed component, DEBS is used in the place of all the recurrent lay- ers while the memory controller is added between the co-attention and the self-attention layers. word embedding layer to provide rich information. Salant and Berant (2017); Weissenborn (2017); Peters et al. extract and use additional features from other neural models trained for another task or external resources. Chen et al. (2016); Pan et al. (2017) utilize additional syntactic or semantic fea- tures through part-of-speech tagging or named- entity recognition, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>AWC=955) Our model (without DEBS) 64.41 69.60 70.21 75.49 BiDAF + SA + SN (Clark and Gardner, 2017) 63.99 68.93 67.98 72.88 QANet (Yu et al., 2018a) 51.10 56.60 53.30 59.20 Reading Twice for NLU (Weissenborn, 2017) 48.60 55.10 53.40 59.90 M-Reader (Hu et al., 2017) 46.94 52.85 54.45 59.46 BiDAF + DNC 42.57 48.30 46.23 51.61 MEMEN (Pan et al., 2017) 43.16 46.90 49.28 55.83 BiDAF (Seo et al., 2017) 40.32 45.91 44.86 50.71</figDesc><table>). 
Implementation details. We use TensorFlow 1 Domain 
Model 
Full 
Verified 
AF 
EM 
F1 
EM 
F1 
Web 
Our model (with DEBS) 
68.21 73.26 82.57 86.05 
(AWC=631) Our model (without DEBS) 
66.82 71.91 81.01 84.12 
BiDAF + SA + SN (Clark and Gardner, 2017) 66.37 71.32 79.97 83.70 
Reading Twice for NLU (Weissenborn, 2017) 50.56 56.73 63.20 67.97 
M-Reader (Hu et al., 2017) 
46.65 52.89 56.96 61.48 
BiDAF + DNC 
42.34 48.65 51.50 57.17 
MEMEN (Pan et al., 2017) 
44.25 48.34 53.27 57.64 
BiDAF (Seo et al., 2017) 
40.74 47.05 49.54 55.80 
Wikipedia 
Our model (with DEBS) 
64.12 69.44 71.75 76.91 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Single model results on TriviaQA (Web and Wikipedia) dataset. All the results are gathered from their corresponding publications except for our models and 'BiDAF + DNC,' which we implemented on our own. 'Full' represents a complete dataset not guaranteed to contain relevant information to answer the question while 'Verified' corresponds to its subset annotated by humans so that the relevant information for the answer is guaranteed to exist. The last column indicates whether a model uses any additional feature augmentation (AF).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table>Reading Twice for NLU (Weissenborn, 
2017) uses background knowledge from Concept-
Net while both M-Reader (Hu et al., 2017) and 
MEMEN (Pan et al., 2017) use POS and NER in-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance results on QUASAR-T dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Single model results on SQuAD. All the other results than ours are those reported in their own 
publications. The last two column indicate whether a model uses any additional feature augmentation 
(AF) and self attention (SA). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Examples in TriviaQA (Web) and SQuAD. Italic means the ground truth answer, frame in-
dicates the prediction of our model (with DEBS) and underline shows the prediction of 'BiDAF + Self 
Attention' model. 

Adding DEBS 
Dev 
C M 
P 
EM 
F1 
77.22 85.01 

77.31 85.22 


77.75 85.34 

78.70 86.12 


78.93 86.26 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.tensorflow.org</note>

			<note place="foot" n="2"> https://github.com/deepmind/sonnet</note>

			<note place="foot" n="3"> https://github.com/HKUST-KnowComp/R-Net 4 https://github.com/allenai/document-qa</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank to reviewers for helpful feedback. This work is the extended version of <ref type="bibr" target="#b27">Yu et al. (2018b)</ref>. This work is partially supported by the Na-tional Research Foundation of Korea (NRF) grant funded by the Korean government (MSIP) (No. NRF2016R1C1B2015924).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the ACL 2004 Interactive poster and demonstration sessions</title>
		<meeting>the ACL 2004 Interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
	<note>Long Papers) (ACL)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for question answering by search and reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page">471</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tracking the world state with recurrent entity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine Bordes Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03969</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fusionnet: Fusing via fullyaware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsin-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07341</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers) (ACL)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Phase conductor on multi-layered attentions for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Chikina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10504</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Stochastic answer networks for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03556</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Bin Cao, Deng Cai, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
	<note>NAACL-HLT)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03609</idno>
		<title level="m">Contextualized word representations for reading comprehension</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
	<note>Long Papers) (ACL)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02596</idno>
		<title level="m">Dynamic integration of background knowledge in neural nlu systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making neural qa as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Conference on Computational Natural Language Learning</title>
		<meeting>the Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<title level="m">Towards AI-complete question answering: A set of prerequisite toy tasks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A multi-stage memory augmented neural network for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Sathish Reddy Indurthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haejun</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the Workshop on Machine Reading for Question Answering</title>
		<meeting>the Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
