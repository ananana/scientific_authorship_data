<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Decoding as Continuous Optimisation in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Duy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Hoang</surname></persName>
							<email>vhoang2@student.unimelb.edu.au, gholamreza.haffari@monash.edu,</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Melbourne Melbourne</orgName>
								<orgName type="institution" key="instit2">VIC</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Monash University Clayton</orgName>
								<orgName type="institution" key="instit2">VIC</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<email>t.cohn@unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Melbourne Melbourne</orgName>
								<orgName type="institution" key="instit2">VIC</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Decoding as Continuous Optimisation in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="146" to="156"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We refor-mulate decoding, a discrete optimization problem, into a continuous problem, such that optimization can make use of efficient gradient-based techniques. Our powerful decoding framework allows for more accurate decoding for standard neural machine translation models, as well as enabling decoding in intractable models such as intersection of several different NMT models. Our empirical results show that our decoding framework is effective, and can leads to substantial improvements in translations, especially in situations where greedy search and beam search are not feasible. Finally, we show how the technique is highly competitive with, and complementary to, reranking.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence to sequence learning with neural net- works <ref type="bibr" target="#b12">(Graves, 2013;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b17">Lipton et al., 2015</ref>) is typically associated with two phases: training and decoding (a.k.a. inference). Model parameters are learned by optimising the training objective, in order that the model can produce good translations when decoding unseen sentences. The majority of research has focused on the training paradigm or network architec- ture, however effective means of decoding have been under-investigated. Conventional heuristic- based approaches for approximate inference in- clude greedy, beam, and stochastic search. Greedy and beam search have been empirically proved to be adequate for many sequence to sequence tasks, and are the standard methods for NMT decoding.</p><p>However, these inference approaches have sev- eral drawbacks. Firstly, although NMT models use a left-to-right generation which would appear to facilitate efficient search, the models themselves use a recurrent architecture, and accordingly are non-Markov. This prevents exact dynamic pro- gramming solutions, and moreover, limits the po- tential to incorporate additional global features or constraints. Global factors can be highly useful in producing better and more diverse translations. Secondly, the sequential decoding of symbols in the target sequence, the inter-dependencies among the target symbols are not fully exploited. For example, when decoding the words of the target sentence in a left-to-right manner, the right con- text is not exploited leading potentially to inferior performance (see <ref type="bibr" target="#b28">Watanabe and Sumita (2002a)</ref> who apply this idea in traditional statistical MT). A natural way to capture this is to intersect left- to-right and right-to-left models, however the re- sulting model has no natural generation order, and thus standard decoding methods are unsuitable.</p><p>We introduce a novel decoding framework ( § 3) that relaxes this discrete optimisation problem into a continuous optimisation problem. This is akin to linear programming relaxation approach for ap- proximate inference in graphical models with dis- crete random variables, where the exact inference is NP-hard <ref type="bibr" target="#b26">(Sontag, 2010;</ref><ref type="bibr" target="#b2">Belanger and McCallum, 2016</ref>). The resulting continuous optimisation problem is challenging due to the non-linearity and non-convexity of the relaxed decoding ob- jective. We make use of stochastic gradient de- scent (SGD) and exponentiated gradient (EG) al- gorithms for decoding based on our relaxation ap- proach. <ref type="bibr">1</ref> Our decoding framework is powerful and flexible, as it enables us to decode with global con- straints involving intersection of multiple NMT models ( §4). We present experimental results on Chinese-English and German-English translation tasks, confirming the effectiveness of our relaxed optimisation method for decoding ( §5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>We briefly review the attentional neural transla- tion model proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> as a sequence-to-sequence neural model onto which we apply our decoding framework.</p><p>In neural machine translation (NMT), the prob- ability of the target sentence y given a source sen- tence x is written as:</p><formula xml:id="formula_0">P Θ (y|x) = |y| i=1 log P Θ (y i |y &lt;i , x) (1) y i |y &lt;i , x ∼ softmax (f f f (Θ, y &lt;i , x))</formula><p>where f is a non-linear function of the previously generated sequence of words y &lt;i , the source sen- tence x, and the model parameters Θ. In this pa- per, we realise f f f as follows:</p><formula xml:id="formula_1">f f f (Θ, y &lt;i , x) = W o · MLP c i , E E E y i−1 T , g i + b o g i = RNN φ dec c i , E E E y i−1 T , g i−1</formula><p>where MLP is a single hidden layer neural net- work with tanh activation function, and E E E y i−1 T is the embedding of the target word y i−1 in the em- bedding matrix E E E T ∈ R ne×|V T | of the target lan- guage vocabulary V T and n e is the embedding di- mension. The state g i of the decoder RNN is a function of y i−1 , its previous state g i−1 , and the context c i = |x| j=1 α ij h j summarises parts of the source sentence which are attended to, where</p><formula xml:id="formula_2">α i = softmax(e i ) ; e ij = MLP (g i−1 , h j ) h j = biRNN θ enc E E E x j S , − → h j−1 , ← − h j+1</formula><p>In above, − → h i and ← − h i are the states of the left-to- right and right-to-left RNNs encoding the source sentence, and E E E x j S is the embedding of the source word x j in the embedding matrix E E E S ∈ R n e ×|V S | of the source language vocabulary V S and n e is the embedding dimension.</p><p>Given a bilingual corpus D, the model param- eters are learned by maximizing the conditional log-likelihood,</p><formula xml:id="formula_3">Θ * := argmax Θ (x,y)∈D log P Θ (y | x) . (2)</formula><p>The model parameters Θ include the weight ma- trix W o ∈ R |V T |×n h and the bias b o ∈ R |V T | -with n H denoting the hidden dimension size -as well as the RNN encoder biRNN θ enc / de- coder RNN φ dec parameters, word embedding ma- trices, and the parameters of the attention mecha- nism. The model is trained end-to-end by optimis- ing the training objective using stochastic gradient descent (SGD) or its variants. In this paper, we fo- cus on the decoding problem, which we turn to in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoding as Continuous Optimisation</head><p>In decoding, we are interested in finding the high- est probability translation for a given source sen- tence:</p><formula xml:id="formula_4">minimise y − P Θ (y | x) s.t. y ∈ Y x (3)</formula><p>where Y x is the space of possible translations for the source sentence x. In general, search- ing Y x to find the highest probability translation is intractable due to the recurrent nature of eqn (1) which prevents dynamic programming for ef- ficient search. This is problematic, as the space of translations is exponentially large with respect to the output length |y|.</p><p>We now formulate this discrete optimisation problem as a continuous one, and then use stan- dard algorithms for continuous optimisation for decoding. Let us assume that the maximum length of a possible translation for a source sentence is known and denote it as . The best translation for a given source sentence solves the following opti- misation problem:</p><formula xml:id="formula_5">y * = arg min y 1 ,...,y i=1 − log P Θ (y i | y &lt;i , x) (4) s.t. ∀i ∈ {1 . . . } : y i ∈ V T .</formula><p>where we allow the translation to be padded with sentinel symbols to the right, which are ignored in computing the model probability. Equivalently, we can rewrite the above discrete optimisation problem as follows:</p><formula xml:id="formula_6">arg miñ miñ y 1 ,..., ˜ y − i=1˜y i=1˜ i=1˜y i · log softmax (f f f (Θ, ˜ y &lt;i , x)) s.t. ∀i ∈ {1 . . . } : ˜ y i ∈ I |V T |<label>(5)</label></formula><p>where˜ywhere˜ where˜y i are vectors using the one-hot representa- tion of the target words I |V T | .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The EG Algorithm for Decoding by Optimisation</head><p>1: For all i initialisê y 0 i ∈ ∆ |V T | 2: for t = 1, . . . , MaxIter do Q(.) is defined as eqn <ref type="formula">(6)</ref> 3:</p><p>For all i, w :</p><formula xml:id="formula_7">calculate t−1 i,w = ∂Q( ˆ y t−1 1 ,..., ˆ y t−1 ) ∂ ˆ y i (w)</formula><p>using back-propagation <ref type="bibr">4:</ref> For all i, w : updatê</p><formula xml:id="formula_8">y t i (w) ∝ ˆ y t−1 i (w) · exp −η t−1 i,w η is the step size 5: return arg min t Q( ˆ y t 1 , . . . , ˆ y t )</formula><p>We now convert the optimisation problem (5) to a continuous one by dropping the integrality con- straints˜ystraints˜ straints˜y i ∈ I |V | and require the variables to take values from the probability simplex:</p><formula xml:id="formula_9">arg minˆy minˆ minˆy 1 ,..., ˆ y − i=1ˆy i=1ˆ i=1ˆy i · log softmax (f f f (Θ, ˆ y &lt;i , x)) s.t. ∀i ∈ {1 . . . } : ˆ y i ∈ ∆ |V T | where ∆ |V T | is the |V T |-dimensional probability simplex, i.e., { ˆ y i ∈ [0, 1] |V T | : ˆ y i 1 = 1}. Intu- itively, this amounts to replacing E E E y i</formula><p>T with the expected embedding of target language words</p><formula xml:id="formula_10">E ˆ y i (w) [E E E w T ]</formula><p>under the distributionˆydistributionˆ distributionˆy i . After solving the above constrained continuous optimisation problem, there is no guarantee that the resulting solution { ˆ y * i } i=1 will comprise one- hot vectors, i.,e., target language words. Instead it can find fractional solutions, that require 'round- ing' in order to to resolve them to lexical items. To solve this problem, we take the arg max, 2 i.e., take the highest scoring word for each positionˆypositionˆ positionˆy * i . We leave exploration of more elaborate projection techniques to the future work.</p><p>In the context of graphical models, the above relaxation technique gives rise to linear program- ming for approximate inference <ref type="bibr" target="#b26">(Sontag, 2010;</ref><ref type="bibr" target="#b2">Belanger and McCallum, 2016)</ref>. However, our de- coding problem is much harder due to the non- linearity and non-convexity of the objective func- tion operating on high dimensional space for deep models. We now turn our attention to optimisation algorithms to effectively solve the decoding opti- misation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Exponentiated Gradient (EG)</head><p>Exponentiated gradient <ref type="bibr" target="#b14">(Kivinen and Warmuth, 1997</ref>) is an elegant algorithm for solving optimisa- tion problems involving simplex constraints. Re- 2 Ties are broken arbitrarily. call our constrained optimisation problem:</p><formula xml:id="formula_11">arg minˆy minˆ minˆy 1 ,..., ˆ y Q( ˆ y 1 , . . . , ˆ y ) s.t. ∀i ∈ {1 . . . } : ˆ y i ∈ ∆ |V T | where Q( ˆ y 1 , . . . , ˆ y ) is defined as − i=1ˆy i=1ˆ i=1ˆy i · log softmax (f f f (Θ, ˆ y &lt;i , x)) . (6)</formula><p>EG is an iterative algorithm, which updates each distributionˆydistributionˆ distributionˆy t i in the current time-step t based on the distributions of the previous time-step as fol- lows:</p><formula xml:id="formula_12">∀w ∈ V T : ˆ y t i (w) = 1 Z t i ˆ y t−1 i (w) exp −η t−1 i,w</formula><p>where η is the step size,</p><formula xml:id="formula_13">t−1 i,w = ∂Q( ˆ y t−1 1 ,..., ˆ y t−1 ) ∂ ˆ y i (w)</formula><p>and Z t i is the normalisation constant</p><formula xml:id="formula_14">Z t i = w∈V T ˆ y t−1 i (w) exp −η t−1 i,w .</formula><p>The partial derivatives i,w are calculated using the back propagation algorithm treating { ˆ</p><formula xml:id="formula_15">y i } i=1</formula><p>as parameters and the original parameters of the model Θ as constants. Adapting EG to our decod- ing problem leads to Algorithm 1. It can be shown that the EG algorithm is a gra- dient descent algorithm for minimising the follow- ing objective function subject to the simplex con- straints:</p><formula xml:id="formula_16">Q( ˆ y 1 , . . . , ˆ y ) − γ i=1 w∈V T ˆ y i (w) log 1 ˆ y i (w) = Q( ˆ y 1 , . . . , ˆ y ) − γ i=1 Entropy( ˆ y i )<label>(7)</label></formula><p>In other words, the algorithm looks for the max- imum entropy solution which also maximizes the log likelihood under the model. There are intrigu- ing parallels with the maximum entropy formula- tion of log-linear models <ref type="bibr" target="#b3">(Berger et al., 1996</ref>). In our setting, the entropy term acts as a prior which discourages overly-confident estimates in the ab- sence of sufficient evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic Gradient Descent (SGD)</head><p>To be able to apply SGD to our optimisation prob- lem, we need to make sure that the simplex con- straints are enforced. One way to achieve this is by reparameterising using the softmax transfor- mation, i.e. ˆ y i = softmax (ˆ r i ). The resulting unconstrained optimisation problem, now overˆroverˆ overˆr i , becomes arg minˆr</p><formula xml:id="formula_17">minˆ minˆr 1 ,..., ˆ r − i=1 softmax (ˆ ri) · log softmax (f (Θ, ˆ y&lt;i, x))</formula><p>where E E E y i T is replaced with the expected embed- ding of the target words under the distribution re- sulted from the E softmax( ˆ r i ) [E E E w T ] in the model. To apply SGD updates, we need the gradient of the objective function with respect to the new variablesˆrvariablesˆ variablesˆr i which can be derived with the back- propagation algorithm based on the chain rule:</p><formula xml:id="formula_18">∂Q ∂ ˆ r i (w) = w ∈V T ∂Q(.) ∂ ˆ y i (w ) ∂ ˆ y i (w ) ∂ ˆ r i (w)</formula><p>The resulting SGD algorithm is summarized in Al- gorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding in Extended NMT</head><p>Our decoding framework allows us to effectively and flexibly add additional global factors over the output symbols during inference. This en- ables decoding for richer global models, for which there is no effective means of greedy decoding or beam search. We outline several such models, and their corresponding relaxed objective functions for optimisation-based decoding.</p><p>Bidirectional Ensemble. Standard NMT gener- ates the translation in a left-to-right manner, condi- tioning each target word on its left context. How- ever, the joint probability of the translation can be decomposed in a myriad of different orders; one compelling alternative would be to condition each target word on its right context, i.e., generating the target sentence from right-to-left. We would not expect a right-to-left model to outperform a left-to-right, however, as the left-to-right ordering reflects the natural temporal order of spoken lan- guage. However, the right-to-left model is likely to provide a complementary signal in translation, as it will be bringing different biases and making largely independent prediction errors to those of the left-to-right model. For this reason, we pro- pose to use both models, and seek to find trans- lations that have high probability according both models (this mirrors work on bidirectional decod- ing in classical statistical machine translation by <ref type="bibr" target="#b29">Watanabe and Sumita (2002b)</ref>.) Decoding un- der the ensemble of these models leads to an in- tractable search problem, not well suited to tradi- tional greedy or beam search algorithms, which re- quire a fixed generation order of the target words. This ensemble decoding problem can be formu- lated simply in our linear relaxation approach, us- ing the following objective function:</p><formula xml:id="formula_19">C +bidir := − α log P Θ← (y | x) − (1 − α) log P Θ→ (y | x) ;<label>(8)</label></formula><p>where α is an interpolation hyper-parameter, which we set to 0.5; Θ → and Θ ← are the pre- trained left-to-right and right-to-left models, re- spectively. This bidirectional agreement may also lead to improvement in translation diversity, as shown in  in a re-ranking evaluation.</p><p>Bilingual Ensemble. Another source of com- plementary information is in terms of the transla- tion direction, that is forward translation from the source to the target language, and reverse trans- lation in the target to source direction. Decod- ing must find a translation which scores well un- der both the forward and reverse translation mod- els. This is inspired by the direct and reverse feature functions commonly used in classical dis- criminative SMT ( <ref type="bibr" target="#b21">Och and Ney, 2002</ref>) which have been shown to offer some complementary bene- fits (although see <ref type="bibr" target="#b18">Lopez and Resnik (2006)</ref>). More specifically, we decode for the best translation in the intersection of the source-to-target and target- to-source models by minimizing the following ob- jective function:</p><formula xml:id="formula_20">C +biling := − α log P Θs→t (y | x) − (1 − α) log P Θs←t (x | y) ; (9)</formula><p>where α is an interpolation hyper-parameter to be fine-tuned; and Θ s→t and Θ s←t are the pre-trained</p><formula xml:id="formula_21">Algorithm 2</formula><p>The SGD Algorithm for Decoding by Optimisation 1: For all i initialisê r 0 i 2: for t = 1, . . . , MaxIter do Q(.) is defined in eqn (6) andˆyandˆ andˆy i = softmax(ˆ r i )</p><p>3:</p><p>For all i, w :</p><formula xml:id="formula_22">calculate t−1 i,w = w ∈V T ∂Q( ˆ y t−1 1 ,..., ˆ y t−1 ) ∂ ˆ y i (w ) ∂ ˆ y i (w ) ∂ ˆ r i (w) using backpropagation 4:</formula><p>For all i, w : updatê</p><formula xml:id="formula_23">r t i (w) = ˆ r t−1 i (w) − η t−1 i,w</formula><p>η is the step size 5: return arg min t Q(softmax(ˆ r t 1 ), . . . , softmax(ˆ r source-to-target and target-to-source models, re- spectively. Decoding for the best translation under the above objective function leads to an intractable search problem, as the reverse model is global over the target language, meaning there is no obvious means of search with a greedy algorithm or simi- lar.</p><note type="other">t )) # tokens # types # sents BTEC zh→en train 422k / 454k 3k / 3k 44,016 dev 10k / 10k 1k / 1k 1,006 test 5k / 5k 1k / 1k 506 TED Talks de→en train 4m / 4m 26k / 19k 194,181 dev-test2010 33k / 35k 4k / 3k 1,565 test2014 26k / 27k 4k / 3k 1,305 WMT 2016 de→en train 107m / 108m 90k / 78k 4m dev-test2013&amp;14 154k / 152k 20k / 13k 6003 test2015 54k / 54k 10k / 8k 2169</note><p>Discussion. There are two important considera- tions on how best to initialise the relaxed optimisa- tion in the above settings, and how best to choose the step size. As the relaxed optimisation prob- lem is, in general, non-convex, finding a plausible initialisation is likely to be important for avoiding local optima. Furthermore, a proper step size is a key in the success of the EG-based and SGD-based optimisation algorithms, and there is no obvious method how to best choose its value. We may also adaptively change the step size using (scheduled) annealing or via the line search. We return to this considerations in the experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Datasets. We conducted our experiments on datasets with different scales, translating between Chinese→English using the BTEC corpus, and German→English using the IWSLT 2015 TED Talks( <ref type="bibr" target="#b7">Cettolo et al., 2014</ref>) and WMT 2016 3 cor- pora. The statistics of the datasets can be found in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit 4 ( <ref type="bibr" target="#b9">Cohn et al., 2016)</ref>, and using the dynet deep learning library <ref type="bibr">5 (Neubig et al., 2017)</ref>. All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method <ref type="bibr" target="#b24">(Sennrich et al., 2016</ref>) to better handle unknown words. <ref type="bibr">6</ref> For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs.</p><p>Evaluation Metrics. We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or dis- crete), as well as measuring the end transla- tion quality with case-insensitive BLEU <ref type="bibr" target="#b22">(Papineni et al., 2002</ref>). The continuous cost measures − 1 | ˆ y| log P Θ ( ˆ y | x) under the model Θ; the dis- crete model score has the same formulation, al- beit using the discrete rounded solution y (see §3). Note the cost can be used as a tool for selecting the best inference solution, as well as assessing con- vergence, as we illustrate below. q q q qq qqqqq qqqqqq q q q q q q q q q q q qqqqqqq qqqqqq q q q q q q q q 1 10 1 5 20 50 100 200 400 iterations CCost q q q qqqqqqq qqqqqq q q q q q q q q q q q qqqqqqq qqqqqq q q q q q q q q 1 10 1 5 20 50 100 200 400 iterations DCost q q q qqqqqqq qqqqqq q q q q q q q q q q q qqqqqqq qqqqqq q q q q q q q q </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head><p>Initialisation and Step Size. As our relaxed op- timisation problems are non-convex, local optima are likely to be a problem. We test this empiri- cally, focusing on the effect that initialisation and step size, η, have on the inference quality.</p><p>For plausible initialisation states, we evaluate different strategies: uniform in which the relaxed variablesˆyvariablesˆ variablesˆy are initialised to 1 |V T | ; and greedy or beam wherebyˆywherebyˆ wherebyˆy are initialised based on an al- ready good solution produced by a baseline de- coder with greedy (gdec) or beam (bdec). Instead of using the Viterbi outputs as a one-hot represen- tation, we initialise to the probability prediction vectors, 7 which serves to limit attraction of the ini- tialisation condition, which is likely to be a local (but not global) optima. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the effect of initialisation on the EG algorithm, in terms of search error (left and middle) and translation quality (right), as we vary the number of iterations of inference. There is clear evidence of non-convexity: all initialisation methods can be seen to converge using all three measures, however they arrive at highly different solutions. Uniform initialisation is clearly not a viable approach, while greedy and beam initial- isation both yield much better results. The best initialisation, beam, outperforms both greedy and beam decoding in terms of BLEU.</p><p>Note that the EG algorithm has fairly slow con- vergence, requiring at least 100 iterations, irre- spective of the initialisation. To overcome this, <ref type="bibr">7</ref> Here, EG uses softmax normalization whereas SGD uses the pre-softmax vector.</p><p>we use momentum <ref type="bibr" target="#b23">(Qian, 1999</ref>) to accelerate the convergence by modifying the term t i,w in Algo- rithm 1 with a weighted moving average of past gradients:</p><formula xml:id="formula_24">t−1 i,w = γ t−2 i,w + η ∂Q( ˆ y t−1 1 , . . . , ˆ y t−1 ) ∂ ˆ y i (w)</formula><p>where we set the momentum term γ = 0.9. The EG with momentum (EG-MOM) converges after fewer iterations (about 35), and results in marginally better BLEU scores. The momentum technique is usually used for SGD involving addi- tive updates; it is interesting to see it also works in EG with multiplicative updates.</p><p>The step size, η, is another important hyper- parameter for gradient based search. We tune the step size using line search over <ref type="bibr">[10,</ref><ref type="bibr">400]</ref> over the development set. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the effect of changing step size from 50 to <ref type="bibr">400 (compare EG and EG-400 with uniform)</ref>, which results in a marked difference of about 10 BLEU points, un- derlining the importance of tuning this value. We found that EG with momentum had less of a re- liance on step size, with optimal values in <ref type="bibr">[10,</ref><ref type="bibr">50]</ref>; we use this setting hereafter.</p><p>Continuous vs Discrete Costs. Another impor- tant question is whether the assumption behind continuous relaxation is valid, i.e., if we optimise a continuous cost to solve a discrete problem, do we improve the discrete output? Although the continuous cost diminishes with inference itera- tions <ref type="figure" target="#fig_0">(Figure 1 left)</ref>, and appears to converge, it is not clear whether this corresponds to a better dis- crete output (note that the discrete cost and BLEU  scores do show improvements <ref type="figure" target="#fig_0">Figure 1</ref> centre and right.) <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the relation between the two cost measures, showing that in most cases the discrete and continuous costs are identical. Linear relaxation fails only for a handful of cases, where the nearest discrete solution is significantly worse than it would appear using the continuous cost.</p><note type="other">26.69 20.73 filtered rerank 26.84 20.66 EGdec w/ beam init 27.34 20.73 full rerank 27.34 21.76 EGdec w/ rerank init 27.78 21.70</note><p>EG vs SGD. Both the EG and SGD algorithms are iterative methods for solving the relaxed op- timisation problem with simplex constraints. We measure empirically their difference in terms of quality of inference and speed of convergence, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Observe that SGD requires 150 iterations for convergence, whereas EG re- quires many fewer (50). This concurs with pre- vious work on learning structured prediction mod- els with EG ( <ref type="bibr" target="#b10">Globerson et al., 2007)</ref>. Further, the EG algorithm consistently produces better results in terms of both model cost and BLEU.</p><p>EG vs Reranking. Reranking is an alternative method for integrating global factors into the ex- isting NMT systems. We compare our EG decod- ing algorithm against the reranking approach with bidirectional factor where the N-best outputs of a left-to-right decoder is re-scored with the forced decoder operating in a right-to-left fashion. The q q q q q q qqqqq q q q q q q q q q q q q q q q q q q q qqqqq q q q q q q q q q q q q q 37.6</p><p>37.8  <ref type="table" target="#tab_1">Table 2</ref>. Our EG algorithm initialised with the reranked output achieves the best BLEU score. We also compare reranking with EG algorithm initialised with the beam de- coder, where for direct comparison we filter out sentences with length greater than that of the beam output in the k-best lists. These results show that the EG algorithm is capable of effectively exploit- ing the search space. Beyond achieving similar or better translations to re-ranking, note that EG is simpler in imple- mentation, as it does not require kbest lists, weight tuning and so forth. Instead this is replaced with iterative gradient descent. The run-time of the two methods are comparable, when reranking uses modest k, however EG can be considerably faster when k is large, as is typically done to extract the full benefit from re-ranking. This performance dif- ference is a consequence of GPU acceleration of the dense vector operations in EG inference.</p><p>Computational Efficiency. We also quantify the computational efficiency of the proposed de- coding approach. Benchmarking on a GPU Ti- tan X for decoding BTEC zh→en, the average time per sentence is 0.02 secs for greedy, 0.07s for beam=5, 0.11s for beam=10, and 3.1s for relaxed EG decoding, which uses an average of 35 EG it- erations. The majority of time in the EG algorithm is in the forward and backward passes, taking 30% and 67% of the time, respectively. Our imple-BTEC TEDTalks WMT zh → en de → en de → en   mentation was not optimised thoroughly, and it is likely that it could be made significantly faster, which we defer to future research.</p><p>Main Results. <ref type="table" target="#tab_3">Table 3</ref> shows our experimental results across all datasets, evaluating the EG al- gorithm and its variants. 8 For the EG algorithm with greedy initialisation (top), we see small but consistent improvements in terms of BLEU. Beam initialisation led to overall higher BLEU scores, and again demonstrating a similar pattern of im- provements, albeit of a lower magnitude, over the initialisation values. Next we evaluate the capability of our infer- ence method with extended NMT models, where approximate algorithms such as greedy or beam search are infeasible. With the bidirectional en- semble, we obtained the statistically significant BLEU score improvements compared to the uni- directional models, for either greedy or beam ini- tialisation. This is interesting in the sense that the unidirectional right-to-left model always per- forms worse than the left-to-right model. How- ever, our method with bidirectional ensemble is capable of combining their strengths in a unified setting. For the bilingual ensemble, we see similar effects, with better BLEU score improvements in most cases, albeit of a lower magnitude, over the bidirectional one. This is likely to be due to a dis- parity with the training condition for the models,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BTEC zh→en</head><p>Source @@@@@@@@@@@@@@@@@@@@ Reference i am sure that i called the hotel yesterday and made a reservation . beam dec (l2r) i 'm sure i called the hotel reservation and i made a reservation . beam dec (r2l) i 'm sure i made this hotel reservation and made a reservation . rerank +bidir.</p><p>i 'm sure i called the hotel reservation and i made a reservation . rerank +biling.</p><p>i 'm sure i called the hotel reservation and i made a reservation . EGdec i 'm sure i called the hotel yesterday and i made a reservation . +bidir.</p><p>i 'm sure i called the hotel yesterday and i made a reservation . +biling.</p><p>i 'm sure i called the hotel yesterday and i made a reservation .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TED Talks de→en</head><p>Source wir sind doch alle gute bürger der sozialen medien , bei denen die währung neid ist . stimmt ' s ? Reference i mean , we 're all good citizens of social media , are n't we , where the currency is envy ? beam dec <ref type="formula">(l2r)</ref> we 're all great UNK of social media , where the currency is envy . right ? beam dec <ref type="formula">(r2l)</ref> we 're all good citizens in social media , which is where that is envy . right ? rerank +bidir.</p><p>we 're all good citizens of social media , where the currency is envy . right ? rerank +biling.</p><p>we 're all good citizens of social media , where the currency is envy . right ? EGdec we 're all great UNK of social media , where the currency is envy . right ? +bidir.</p><p>we 're all good UNK of social media , where the currency is envy . right ? +biling.</p><p>we 're all good citizens of social media , where the currency is envy . right ?  encoder-decoder. Moreover, we demonstrate the utility of related optimisation for inference over global ensembles of models, resulting in consis- tent improvements in search error and end transla- tion quality. Recently, relaxation techniques have been ap- plied to deep models for training and inference in text classification <ref type="bibr" target="#b2">(Belanger and McCallum, 2016;</ref><ref type="bibr" target="#b1">Belanger et al., 2017)</ref>, and fully differ- entiable training of sequence-to-sequence models with scheduled-sampling ( <ref type="bibr" target="#b11">Goyal et al., 2017)</ref>. Our work has applied the relaxation technique specifi- cally for decoding in NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WMT de→en</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This work presents the first attempt in formulat- ing decoding in NMT as a continuous optimisation problem. The core idea is to drop the integrality (i.e. one-hot vector) constraint from the predic- tion variables and allow them to have soft assign- ments within the probability simplex while min- imising the loss function produced by the neural model. We have provided two optimisation algo- rithms -exponentiated gradient (EG) and stochas- tic gradient descent (SGD) -for optimising the resulting contained optimisation problem, where our findings show the effectiveness of EG com- pared to SGD. Thanks to our framework, we have been able to decode when intersecting left-to-right and right-to-left as well as source-to-target and target-to-source NMT models. Our results show that our decoding framework is effective and leads to substantial improvements in translations gener- ated from the intersected models, where the typi- cal greedy or beam search algorithms are not ap- plicable.</p><p>This work raises several compelling possibili- ties which we intend to address in future work, such as improving decoding speed, integrating ad- ditional constraints such as word coverage and fer- tility into decoding, <ref type="bibr">9</ref> and applying our method to other intractable structured prediction problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Analysis on effects of initialisation states (uniform vs. greedy vs. beam), step size annealing, momentum mechanism from BTEC zh→en translation. EG-400: EG algorithm with step size η = 400 (otherwise η = 50); EG-MOM: EG algorithm with momentum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing discrete vs continuous costs from BTEC zh→en translation, using the EG algorithm with momentum, η = 50. Each point corresponds to a sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis on convergence and performance comparing SOFTMAX and EG algorithms from BTEC zh→en translation. Both algorithms use momentum and step size 50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>gdec</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Source neben dem wm-titel 2007 und dem gewinn der champions league 2014 holte er 2008 ( hsg nordhorn ) und 2010 ( tbv lemgo ) den ehf-pokal . Reference besides the 2007 world championship he also won the champions league in may and the ehf-cup in 2008 ( hsg nordhorn ) and 2010 ( tbv lemgo ) . beam dec (l2r) * in addition to the title 2007 in 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv lemgo ) , he won the ehf cup . beam dec (r2l) in addition to the world championship title 2007 and winning the champions league in 2014 , he won the ehf-cup in 2008 ( hsg nordhorn ) and 2010 ( tbv lemgo ) . EGdec in addition to the title 2007 in 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv lemgo ) , he won the ehf cup . +bidir. in addition to the title championship in 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv lemgo ) , he won the ehf cup . +biling. in addition to the world title title 2007 and the win of the champions league 2014 in 2008 ( hsg nordhorn ) and 2010 ( tbv lemgo ) , he won the ehf cup .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Translation examples generated by the models. * : reranking with bidirectional (+bidir.) and bilingual (+biling.) produced the same translation string.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistics of the training and evalua-
tion sets; token and types are presented for both 
source/target languages. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The BLEU evaluation results with EG al-
gorithm against 100-best reranking on WMT eval-
uation dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The BLEU evaluation results across eval-
uation datasets for EG algorithm variants against 
the baselines; bold: statistically significantly bet-
ter than the best greedy or beam baseline,  † : best 
performance on dataset. 

</table></figure>

			<note place="foot" n="1"> Both methods are mainly used for training in prior work.</note>

			<note place="foot" n="3"> http://www.statmt.org/wmt16/ translation-task.html 4 https://github.com/duyvuleo/Mantidae 5 https://github.com/clab/dynet 6 With BPE, the out of vocabulary rates on heldout data are &lt; 1%.</note>

			<note place="foot" n="8"> Due to the space constraints, we report results for the EG algorithm only. See also translation examples in Figure 4. which were learned independently of one another. Overall, decoding in extended NMT models leads to performance improvements compared to baseline methods. This is one of the main findings in this work, and augurs well for its extension to other global model variants. 6 Related Work Decoding (inference) for neural models is an important task; however, there is limited research in this space perhaps due to the challenging nature of this task, with only a few works exploring some extensions to improve upon them. The most</note>

			<note place="foot" n="9"> These constraints have only been used for training in the previous works (Cohn et al., 2016; Mi et al., 2016).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for valuable feedbacks and discussions. Cong Duy Vu Hoang is supported by Australian Government Research Training Pro-gram Scholarships at the University of Melbourne, Australia. This work was supported by the Aus-tralian Research Council through their Discovery and Future Fellowship programmes.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 3rd International Conference on Learning Representations</title>
		<meeting>of 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-End Learning for Structured Prediction Energy Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="983" to="992" />
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Edinburgh Neural Machine Translation Systems for WMT 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stuker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of The International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>of The International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating Structural Alignment Biases into an Attentional Neural Translation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exponentiated Gradient Algorithms for Log-linear Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><forename type="middle">Y</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Differentiable scheduled sampling for credit assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Bergkirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Short Papers), ACL&apos;17</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating Sequences With Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exponentiated Gradient Versus Gradient Descent for Linear Predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyrki</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Comput</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mutual Information and Diverse Decoding Improve Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Simple, Fast Diverse Decoding Algorithm for Neural Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Critical Review of Recurrent Neural Networks for Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word-based alignment, phrase-based translation: Whats the link</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA)</title>
		<meeting>7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coverage Embedding Models for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<title level="m">The Dynamic Neural Network Toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Decoding neural machine translation using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Snelleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Gothenburg, Sweden</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Chalmers University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Approximate Inference in Graphical Models using LP Relaxations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bidirectional Decoding for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>COLING &apos;02</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional decoding for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Learning as Beam-Search Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
