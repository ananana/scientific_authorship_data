<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale Cloze Test Dataset Created by Teachers</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Large-scale Cloze Test Dataset Created by Teachers</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2344" to="2356"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2344</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cloze tests are widely adopted in language exams to evaluate students&apos; language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH 1 2 , containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Being a classic language exercise, the cloze test <ref type="bibr">(Taylor, 1953)</ref> is an accurate assessment of language proficiency <ref type="bibr" target="#b4">(Fotos, 1991;</ref><ref type="bibr" target="#b8">Jonz, 1991;</ref><ref type="bibr">Tremblay, 2011</ref>) and has been widely employed in language examinations. Under a typical set- ting, a cloze test requires examinees to fill in miss- ing words (or sentences) to best fit the surround- ing context. To facilitate natural language under- standing, automatically-generated cloze datasets are introduced to measure the ability of machines in reading comprehension ( <ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr" target="#b6">Hill et al., 2016;</ref><ref type="bibr">Onishi et al., 2016)</ref>. In these datasets, each cloze question typically consists of a context paragraph and a question sentence. By randomly replacing a particular word in the ques- tion sentence with a blank symbol, a single test case is created. For instance, CNN/Daily Mail datasets ( <ref type="bibr" target="#b5">Hermann et al., 2015</ref>) use news articles as contexts and summary bullet points as the ques- tion sentence. Only named entities are removed when creating the blanks. Similarly, in Children's Books test (CBT) ( <ref type="bibr" target="#b6">Hill et al., 2016)</ref>, cloze ques- tions are obtained by removing a word in the last sentence of every consecutive 21 sentences, with the first 20 sentences being the context. Different from CNN/Daily Mail datasets, CBT also provides each question with a candidate answer set, con- sisting of randomly sampled words with the same part-of-speech tag from the context as that of the correct answer.</p><p>Thanks to the automatic generation process, these datasets can be very large in size, leading to significant research progresses. However, com- pared to how humans would create cloze ques- tions and evaluate reading comprehension ability, the automatic generation process bears some in- evitable issues. Firstly, blanks are chosen uni- formly without considering which aspect of the language phenomenon that questions will test. Hence, quite a portion of automatically-generated questions can be purposeless or even trivial to an- swer. Another issue involves the ambiguity of answers. Given a context and a sentence with a blank, there can be multiple words that fit almost equally well into the blank. A possible solution is to include a candidate option set, as done by CBT, to get rid of the ambiguity. However, automati- cally generating the candidate option set can be problematic since it cannot guarantee the ambigu- ity is removed. More importantly, automatically- generated candidates can be totally irrelevant or simply grammatically unsuitable for the blank, re- sulting in again purposeless or trivial questions.</p><p>Probably due to these unsatisfactory issues, neu- ral models have achieved comparable results to the human-level performance within a very short time <ref type="bibr">(Chen et al., 2016;</ref><ref type="bibr" target="#b2">Dhingra et al., 2016;</ref><ref type="bibr">Seo et al., 2016)</ref>. While there have been works try- ing to incorporate human design into cloze ques- tion generation ( <ref type="bibr">Zweig and Burges, 2011;</ref><ref type="bibr">Paperno et al., 2016)</ref>, due to the expensive labeling process, the MSR Sentence Completion Challenge created by this effort has 1, 040 questions and the LAM- BADA ( <ref type="bibr">Paperno et al., 2016</ref>) dataset has 10, 022 questions, limiting the possibility of developing powerful neural models on it. As a result of the small size, human-created questions are only used to compose development sets and test sets. Motivated by the aforementioned drawbacks, we propose CLOTH, a large-scale cloze test dataset collected from English exams. Questions in the dataset are designed by middle-school and high- school teachers to prepare Chinese students for entrance exams. To design a cloze test, teachers firstly determine the words that can test students' knowledge of vocabulary, reasoning or grammar; then replace those words with blanks and provide other three candidate options for each blank. If a question does not specifically test grammar usage, all of the candidate options would complete the sentence with correct grammar, leading to highly nuanced questions. As a result, human-created questions are usually harder and are a better as- sessment of language proficiency. A general cloze test evaluates several aspects of language profi- ciency including vocabulary, reasoning and gram- mar, which are key components of comprehending natural language.</p><p>To verify if human-created cloze questions are difficult for current models, we train and evaluate the state-of-the-art language model (LM) and ma- chine comprehension models on this dataset, in- cluding a language model trained on the One Bil- lion Word Corpus. We find that the state-of-the- art model lags behind human performance even if the model is trained on a large external corpus. We analyze where the model fails compared to humans who perform well. After conducting er- ror analysis, we assume the performance gap re- sults from the model's inability to use a long-term context. To examine this assumption, we eval- uate human-level performance when the human subjects are only allowed to see one sentence as the context. Our assumption is confirmed by the matched performances of the models and human when given only one sentence. In addition, we demonstrate that human-created data is more dif- ficult than automatically-generated data. Specifi- cally, it is much easier for the same model to per- form well on automatically-generated data.</p><p>We hope that CLOTH provides a valuable testbed for both the language modeling commu- nity and the machine comprehension community. Specifically, the language modeling community can use CLOTH to evaluate their models' abil- ities in modeling long contexts, while the ma- chine comprehension community can use CLOTH to test machine's understanding of language phe- nomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Large-scale automatically-generated cloze tests ( <ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr" target="#b6">Hill et al., 2016;</ref><ref type="bibr">Onishi et al., 2016</ref>) lead to significant research ad- vancements. However, generated questions do not consider language phenomenon to be tested and are relatively easy to solve. Recently proposed reading comprehension datasets are all labeled by humans to ensure a high quality ( <ref type="bibr">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b9">Joshi et al., 2017;</ref><ref type="bibr">Trischler et al., 2016;</ref><ref type="bibr" target="#b14">Nguyen et al., 2016)</ref>.</p><p>Perhaps the closet work to CLOTH is the LAM- BADA dataset ( <ref type="bibr">Paperno et al., 2016)</ref>. LAM- BADA also targets at finding challenging words to test LM's ability in comprehending a longer con- text. However, LAMBADA does not provide a candidate set for each question, which can cause ambiguities when multiple words can fit in. Fur- thermore, only test set and development set are la- beled manually. The provided training set is the unlabeled Book Corpus ( <ref type="bibr">Zhu et al., 2015)</ref>. Such unlabeled data do not emphasize long-dependency questions and have a mismatched distribution with the test set, as showed in Section 5. Further, the Book Corpus is too large to allow rapid algorithm development for researchers who do not have ac- cess to a huge amount of computational power.</p><p>Aiming to evaluate machines under the same conditions that the humans are evaluated, there is a growing interest in obtaining data from exam- inations. NTCIR QA Lab ( <ref type="bibr">Shibuki et al., 2014</ref>) contains a set of real-world college entrance exam questions. The Entrance Exams task at CLEF QA Track ( <ref type="bibr">Peñas et al., 2014;</ref><ref type="bibr">Rodrigo et al., 2015</ref>) evaluates machine's reading comprehension abil-ity. The AI2 Reasoning Challenge ( <ref type="bibr">Clark et al., 2018;</ref><ref type="bibr">Schoenick et al., 2017</ref>) contains approx- imately eight thousand scientific questions used in middle school. <ref type="bibr" target="#b13">Lai et al. (2017)</ref> proposes the first large-scale machine comprehension dataset obtained from exams. They show that questions designed by teachers have a significantly larger proportion of reasoning questions. Our dataset fo- cuses on evaluating both language proficiency and reasoning abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLOTH Dataset</head><p>In this section, we introduce the CLOTH dataset that is collected from English examinations, and study its abilities of assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection and Statistics</head><p>We collect the raw data from three free and pub- lic websites in China that gather exams created by English teachers to prepare students for col- lege/high school entrance exams <ref type="bibr">3</ref> . Before clean- ing, there are 20, 605 passages and 332, 755 ques- tions. We perform the following processes to en- sure the validity of data: Firstly, we remove ques- tions with an inconsistent format such as questions with more than four options. Then we filter all questions whose validity relies on external infor- mation such as pictures or tables. Further, we find that half of the total passages are duplicates and we delete those passages. Lastly, on one of the web- sites, the answers are stored as images. We use two OCR software programs 4 to extract the answers from images. We discard the questions when re- sults from the two software are different. After the cleaning process, we obtain a clean dataset of 7, 131 passages and 99, 433 questions.</p><p>Since high school questions are more diffi- cult than middle school questions, we divide the datasets into CLOTH-M and CLOTH-H, which stand for the middle school part and the high school part. We split 11% of the data for both the test set and the development set. The detailed statistics of the whole dataset and two subsets are presented in <ref type="table">Table 1</ref>. Note that the questions were created to test non-native speakers, hence the vo- cabulary size is not very large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Type Analysis</head><p>In order to evaluate students' mastery of a lan- guage, teachers usually design tests in a way that questions cover different aspects of a language. Specifically, they first identify words in the pas- sage that can examine students' knowledge in vo- cabulary, logic, or grammar. Then, they replace the words with blanks and prepare three incorrect but nuanced candidate options to make the test non-trivial. A sample passage is presented in <ref type="table" target="#tab_2">Ta- ble 2</ref>.</p><p>To understand the abilities of assessment on this dataset, we divide questions into several types and label the proportion of each type. According to English teachers who regularly create cloze test questions for English exams in China, there are largely three types: grammar, vocabulary and rea- soning. Grammar questions are easily differen- tiated from other two categories. However, the teachers themselves cannot specify a clear distinc- tion between reasoning questions and vocabulary questions since all questions require comprehend- ing the words within the context and conducting some level of reasoning by recognizing incom- plete information or conceptual overlap.</p><p>Hence, we divided the questions except gram- mar questions based on the difficulty level for a machine to answer the question, follow- ing works on analyzing machine comprehension datasets ( <ref type="bibr">Chen et al., 2016;</ref><ref type="bibr">Trischler et al., 2016</ref>). In particular, we divide them in terms of their de- pendency ranges, since questions that only involve a single sentence are easier to answer than ques- tions involving evidence distributed in multiple sentences. Further, we divided questions involving long-term dependency into matching/paraphrasing questions and reasoning questions since matching questions are easier. The four types include:</p><p>• Grammar: The question is about grammar us- age, involving tense, preposition usage, ac- tive/passive voices, subjunctive mood and so on.</p><p>• Short-term-reasoning: The question is about content words and can be answered based on the information within the same sentence. Note that the content words can evaluate knowledge of both vocabulary and reasoning.</p><p>• Matching/paraphrasing: The question is an- swered by copying/paraphrasing a word in the context.   <ref type="table">Table 1</ref>: The statistics of the training, development and test sets of CLOTH-M (middle school questions), CLOTH-H (high school questions) and CLOTH</p><p>• Long-term-reasoning: The answer must be inferred from synthesizing information dis- tributed across multiple sentences.</p><p>We sample 100 passages in the high school cat- egory and the middle school category respectively with totally 3, 000 questions. The types of these questions are labeled on Amazon Turk. We pay $1 and $0.5 for high school passages and middle school passages respectively. We refer readers to Appendix A.1 for details of the labeling processes and the labeled sample passage.</p><p>The proportion of different questions is shown in <ref type="table" target="#tab_4">Table 3</ref>. The majority of questions are short- term-reasoning questions while approximately 22.4% of the data needs long-term information, in which the long-term-reasoning questions con- stitute a large proportion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Exploring Models' Limits</head><p>In this section, we investigate if human-created cloze test is a challenging problem for state-of- the-art models. We find that LM trained on the One Billion Word Corpus can achieve a remark- able score but cannot solve the cloze test. After conducting an error analysis, we hypothesize that the model is not able to deal with long-term de- pendencies. We verify the hypothesis by compar- ing the model's performance with the human per- formance when the information humans obtain is limited to one sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Human and Model Performance</head><p>LSTM To test the performance of RNN-based supervised models, we train a bidirectional LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) to predict the missing word given the context with only labeled data. The implementation details are in Appendix A.3.</p><p>Attentive Readers To enable the model to gather information from a longer context, we aug- ment the supervised LSTM model with the at- tention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), so that the representation at the blank is used as a query to find the relevant context in the docu- ment and a blank-specific representation of the document is used to score each candidate an- swer. Specifically, we adapt the Stanford Atten- tive Reader ( <ref type="bibr">Chen et al., 2016</ref>) and the position- aware attention model ( <ref type="bibr">Zhang et al., 2017</ref>) to the cloze test problem. With the position-aware atten- tion model, the attention scores are based on both the context match and the distance from a context to the blank. Both attention models are trained only with human-created blanks just as the LSTM model.</p><p>LM In cloze test, the context on both sides may be enough to determine the correct answer. Sup- pose x i is the missing word and x 1 , · · · , x i−1 , x i+1 , · · · , x n are the context, we choose x i that maximizes the joint probability p(x 1 , · · · , x n ), which essentially maximizes the conditional likelihood p(</p><formula xml:id="formula_0">x i | x 1 , · · · , x i−1 , x i+1 , · · · , x n ).</formula><p>Therefore, LM can be naturally adapted to cloze test.</p><p>In essence, LM treats each word as a possible blank and learns to predict it. As a result, it re- ceives more supervision than the LSTM trained on human-labeled questions. Besides training a neural LM on our dataset, interested in whether the state-of-the-art LM can solve cloze test, we also test the LM trained on the One Billion Word Benchmark ( <ref type="bibr">Chelba et al., 2013</ref>) (referred as 1B- LM) that achieves a perplexity of 30.0 (Jozefow- icz et al., 2016) <ref type="bibr">5</ref> . To make the evaluation time tractable, we limit the context length to one sen- tence or three sentences. Note that the One Billion Word Corpus does not overlap with the CLOTH <ref type="bibr">5</ref> The pre-trained model is obtained from https://github.com/tensorflow/models/tree/master/research/ lm 1b Passage: Nancy had just got a job as a secretary in a com- pany. Monday was the first day she went to work, so she was very 1 and arrived early. She 2 the door open and found nobody there. "I am the 3 to arrive." She thought and came to her desk. She was surprised to find a bunch of 4 on it. They were fresh. She 5 them and they were sweet. She looked around for a 6 to put them in. "Somebody has sent me flowers the very first day!" she thought 7 . " But who could it be?" she began to 8 . The day passed quickly and Nancy did everything with 9 interest. For the follow- ing days of the 10 , the first thing Nancy did was to change water for the followers and then set about her work. Then came another Monday. 11 she came near her desk she was overjoyed to see a(n) 12 bunch of flowers there. She quickly put them in the vase, 13 the old ones. The same thing happened again the next Monday. Nancy began to think of ways to find out the 14 . On Tuesday afternoon, she was sent to hand in a plan to the 15 . She waited for his directives at his secretary's 16 . She happened to see on the desk a half-opened notebook, which 17 : "In order to keep the secretaries in high spirits, the company has decided that every Monday morning a bunch of fresh flowers should be put on each secretarys desk." Later, she was told that their general manager was a business management psychologist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions:</head><p>1  corpus.</p><p>Human performance We measure the perfor- mance of Amazon Mechanical Turkers on 3, 000 sampled questions when the whole passage is given.</p><p>Results The comparison is shown in   The LM achieves much better performance than LSTM. The gap is larger when the LM is trained on the 1 Billion Word Corpus, indicating that more training data results in a better generaliza- tion. Specifically, the accuracy of 1B-LM is 0.695 when one sentence is used as the context. It in- dicates that LM can learn sophisticated language regularities when given sufficient data. The same conclusion can also be drawn from the success of a concurrent work ELMo which uses LM repre- sentations as word vectors and achieves state-of- the-art results on six language tasks ( <ref type="bibr">Peters et al., 2018</ref>). However, if we increase the context length to three sentences, the accuracy of 1B-LM only has a marginal improvement. In contrast, humans outperform 1B-LM by a significant margin, which demonstrates that deliberately designed questions in CLOTH are not completely solved even for state-of-the-art models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analyzing 1B-LM's Strengths and Weaknesses</head><p>In this section, we would like to understand why 1B-LM lags behind human performance. We find that most of the errors involve long-term reason- ing. Additionally, in a lot of cases, the depen- dency is within the context of three sentences. We show several errors made by the 1B-LM in <ref type="table" target="#tab_6">Table  5</ref>. In the first example, the model does not know that Nancy found nobody in the company means that Nancy was the first one to arrive at the com- pany. In the second and third example, the model fails probably because of not recognizing "they" referred to "flowers". The dependency in the last case is longer. It depends on the fact that Nancy was alone in the company. Based on the case study, we hypothesize that the LM is not able to take long-term information into account, although it achieves a surprisingly good overall performance. Additionally, the 1B- LM is trained on the sentence level, which might also result in the inability to track paragraph level information. However, to investigate the differ- ences between training on sentence level and on paragraph level, a prohibitive amount of computa- tional resource is required to train a large model on the 1 Billion Word Corpus.</p><p>On the other hand, a practical comparison is to test the model's performance on different types of questions. We find that the model's accu- racy is 0.591 on long-term-reasoning questions of CLOTH-H while it achieves 0.693 on short-term- reasoning (a comprehensive type-specific perfor- mance is available in Appendix A.3), which par- tially confirms that long-term-reasoning is harder. However, we could not completely rely on the per- formance on specific questions types, partly due to a large variance caused by the small sample size. Another reason is that the reliability of question type labels depends on whether turkers are careful enough. For example, in the error analysis shown in <ref type="table" target="#tab_6">Table 5</ref>, a careless turker would label the second example as short-term-reasoning without noticing that the meaning of "they" relies on a long context.</p><p>To objectively verify if the LM's strengths lie in dealing with short-term information, we obtain the ceiling performance of only utilizing short- term information. Showing only one sentence as the context, we ask the Turkers to select an option based on their best guesses given the insufficient information. By limiting the context span man- ually, the ceiling performance with the access to only a short context is estimated accurately.</p><p>As shown in <ref type="table" target="#tab_7">Table 6</ref>, The performance of 1B- LM using one sentence as the context can almost match the human ceiling performance of only us- ing short-term information. Hence we conclude that the LM can almost perfectly solve all short- term cloze questions. However, the performance of LM is not improved significantly when a long- term context is given, indicating that the perfor- mance gap is due to the inability of long-term rea- soning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparing Human-created Data and Automatically-generated Data</head><p>In this section, we demonstrate that human- created data is a better testbed than automatically- generated cloze test since it results in a larger gap between model's performance and human perfor- mance. A casual observation is that a cloze test can be created by randomly deleting words and ran- domly sampling candidate options. In fact, to generate large-scale data, similar generation pro- cesses have been introduced and widely used in machine comprehension ( <ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr" target="#b6">Hill et al., 2016;</ref><ref type="bibr">Onishi et al., 2016)</ref>. However, research on cloze test design ( <ref type="bibr">Sachs et al., 1997)</ref> shows that tests created by deliberately deleting words are more reliable than tests created by ran- domly or periodically deleting words. To design accurate language proficiency assessment, teach- ers usually deliberately select words in order to ex- amine students' proficiency in grammar, vocabu- lary and reasoning. Moreover, in order to make the question non-trivial, three incorrect options pro- vided by teachers are usually grammatically cor- rect and relevant to the context. For instance, in the fourth problem of the sample passage shown in <ref type="table" target="#tab_2">Table 2</ref>, "grapes", "flowers" and "bananas" all fit the description of being fresh.</p><p>Hence we naturally hypothesize that human- generated data has distinct characteristics when</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Options</head><p>She pushed the door open and found nobody there. "I am the to arrive." She A. last B. second C. third D. first thought and came to her desk.</p><p>They were fresh. She them and they were sweet. She looked around for a vase A. smelled B. ate C. took D. held to put them in.</p><p>She smelled them and they were sweet. She looked around for a to put them in.</p><p>A. vase B. room C. glass D. bottle "Somebody has sent me flowers the very first day!" "But who could it be?" she began to . The day passed quickly and Nancy did A. seek B. wonder C. work D. ask everything with great interest.  compared with automatically-generated data. To verify this assumption, we compare the LSTM model's performance when given different propor- tions of the two types of data. Specifically, to train a model with α percent of automatically-generated data, we randomly replace a percent blanks with blanks at random positions, while keeping the re- maining 1 − α percent questions the same. The candidate options for the generated blanks are ran- dom words sampled from the unigram distribu- tion. We test models obtained with varying α on human-created data and automatically-generated data respectively.  <ref type="table">Table 7</ref>: The model's performance when trained on α percent of automatically-generated data and 100 − α percent of human-created data</p><p>From the comparison in <ref type="table">Table 7</ref>, we have the following observations: (1) human-created data leads to a larger gap between model's perfor- mance and the ceiling/human performance. The model's performance and human's performance on the human-created data are 0.484 and 0.859 re- spectively, as shown in Tab. 4, leading to a gap of 0.376. In comparison, the performance gap on the automatically-generated data is at most 0.185 since the model's performance reaches an accu- racy of 0.815 when fully trained on generated data. (2) Although human-created data may provide more information in distinguishing similar words, the distributional mismatch between two types of data makes it non-trivial to transfer the knowl- edge gained from human-created data to tackle automatically-generated data. Specifically, the model's performance on automatically-generated data monotonically decreases when given a higher ratio of human-created data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Combining Human-created Data with</head><p>Automatically-generated Data</p><p>In Section 4.1, we show that LM is able to take advantage of more supervision since it predicts each word based on the context. At the same time, we also show that human-created data and the automatically-generated data are quite differ- ent in Section 5. In this section, we propose a model that takes advantage of both sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Representative-based Model</head><p>Specifically, for each question, regardless of be- ing human-created or automatically-generated, we can compute the negative log likelihood of the correct answer as the loss function. Suppose J H is the average negative log likelihood loss for human-created questions and J R is the loss func- tion on generated questions, we combine losses on human-created questions and generated questions by simply adding them together, i.e., J R + J H is used as the final loss function. We will introduce the definition of J R in the following paragraphs. Although automatically-generated data has a large quantity and is valuable to the model training, as shown in the previous Section, automatically-generated questions are quite dif- ferent from human-created questions. Ideally, a large amount of human-created questions is more desirable than a large amount of automatically- generated questions. A possible avenue towards having large-scale human-created data is to au- tomatically pick out a large number of generated questions which are representative of or similar to human-created questions. In other words, we train a network to predict whether a question is a gener- ated question or a human-created question. A gen- erated question is representative of human-created questions if it has a high probability of being a human-created question. Then we can give higher weights to questions that resemble human-created question.</p><p>We first introduce our method to obtain the rep- resentativeness information. Let x denote the pas- sage and z denote whether a word is selected as a question by human, i.e., z is 1 if this word is selected to be filled in the original passage or 0 otherwise. Suppose h i is the representation of i-th word given by a bidirectional LSTM. The network computes the probability p i of x i being a human- created question as follows:</p><formula xml:id="formula_1">l i = h T i w x i ; p i = Sigmoid(l i )</formula><p>where l i is the logit which will be used as in the final model and w x i is the the word embedding. We train the network to minimize the binary cross entropy between p and ground-truth labels at each token.</p><p>After obtaining the representativeness informa- tion, we define the representativeness weighted loss function as</p><formula xml:id="formula_2">J R = i ∈H Softmax i ( l 1 α , · · · , l n α )J i</formula><p>where J i denotes the negative log likelihood loss for the i−th question and let l i be the output rep- resentativeness of the i-th question and H is the set of all human-generated questions and α is the temperature of the Softmax function. The model degenerates into assigning a uniform weight to all questions when the temperature is +∞. We set α to 2 based on the performance on the dev set. 6 .   <ref type="table">Table 9</ref>: Ablation study on using the representa- tiveness information (denoted as rep.) and the human-created data (denoted as hum.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We summarize performances of all models in Ta- ble 8. Our representativeness model outperforms all other models that do not use external data on CLOTH, CLOTH-H and CLOTH-M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>In this section, we verify the effectiveness of the representativeness-based averaging by abla- tion studies. When we remove the representative- ness information by setting α to infinity, the ac- curacy drops from 0.583 to 0.566. When we fur- ther remove the human-created data so that only generated data is employed, the accuracy drops to 0.543, similar to the performance of LM. The re- sults further confirm that it is beneficial to incor- porate human-created questions into training. A sample of the predicted representativeness is shown in <ref type="figure" target="#fig_2">Figure 1 7</ref> . Clearly, words that are too ob- vious have low scores, such as punctuation marks, simple words "a" and "the". In contrast, content words whose semantics are directly related to the context have a higher score, e.g., "same", "simi- lar", "difference" have a high score when the dif- ference between two objects is discussed and "se- crets" has a high score since it is related to the sub- sequent sentence "does not want to share with oth- ers". Our prediction model achieves an F1 score of 36.5 on the test set, which is understandable since there are many plausible questions within a pas- sage.</p><p>It has been shown that features such as morphol- ogy information and readability are beneficial in cloze test prediction <ref type="bibr">(Skory and Eskenazi, 2010;</ref><ref type="bibr">Correia et al., 2012</ref><ref type="bibr">Correia et al., , 2010</ref><ref type="bibr" target="#b12">Kurtasov, 2013)</ref>. We leave investigating the advanced approaches of au- tomatically designing cloze test to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Discussion</head><p>In this paper, we propose a large-scale cloze test dataset CLOTH that is designed by teachers. With missing blanks and candidate options carefully created by teachers to test different aspects of lan- guage phenomena, CLOTH requires a deep lan- guage understanding and better captures the com- plexity of human language. We find that hu- man outperforms 1B-LM by a significant mar- gin. After detailed analysis, we find that the performance gap is due to the model's inability to understanding a long context. We also show that, compared to automatically-generated ques- tions, human-created questions are more difficult and lead to a larger margin between human per- formance and the model's performance.</p><p>Despite the excellent performance of 1B-LM when compared with models trained only on CLOTH, it is still important to investigate and cre- ate more effective models and algorithms which provide complementary advantages to having a large amount of data. For rapid algorithm devel- opments, we suggest training models only on the training set of CLOTH and comparing with mod- els that do not utilize external data.</p><p>We hope our dataset provides a valuable testbed to the language modeling community and the machine comprehension community. In partic- ular, the language modeling community can use CLOTH to evaluate their models' abilities in mod- eling a long context. In addition, the machine comprehension community may also find CLOTH useful in evaluating machine's understanding of language phenomena including vocabulary, rea- soning and grammar, which are key components of comprehending natural language.</p><p>In our future work, we would like to design al- gorithms to better model a long context, to utilize external knowledge, and to explore more effec- tive semi-supervised learning approaches. Firstly, we would like to investigate efficient ways of uti- lizing external knowledge such as paraphrasing and semantic concepts like prior works ( <ref type="bibr" target="#b3">Dong et al., 2017;</ref><ref type="bibr" target="#b1">Dasigi et al., 2017)</ref>. In comparison, training on a large external dataset is actually a time-consuming way of utilizing external knowl- edge. Secondly, to use the generated questions more effectively, the representative-based semi- supervised approach might be improved by tech- niques studied in active learning and hard exam- ple mining <ref type="bibr">(Settles, 2009;</ref><ref type="bibr">Shrivastava et al., 2016;</ref><ref type="bibr">Chang et al., 2017</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Representativeness prediction for each word. Lighter color means less representative. The words deleted by human as blanks are in bold text.</figDesc><graphic url="image-1.png" coords="9,132.52,62.81,332.50,120.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>A Sample passage from our dataset. Bold 
faces highlight the correct answers. There is only 
one best answer among four candidates, although 
several candidates may seem correct. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Both attentive readers achieve similar accuracy 
to the LSTM. We hypothesize that the reason of 
the attention model's unsatisfactory performance 
is that the evidence of a question cannot be simply 
found by matching the context. Similarly, on read-
ing comprehension, though attention-based mod-
els (Wang et al., 2017; Seo et al., 2016; Dhingra 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The question type statistics of 3000 sam-
pled questions where GM, STR, MP, LTR and 
O denotes grammar, short-term-reasoning, match-
ing/paraphrasing, long-term-reasoning and others 
respectively. 

Model 
CLOTH CLOTH-M CLOTH-H 

LSTM 
0.484 
0.518 
0.471 
Stanford AR 
0.487 
0.529 
0.471 
Position-aware AR 
0.485 
0.523 
0.471 

LM 
0.548 
0.646 
0.506 
1B-LM (one sent.) 
0.695 
0.723 
0.685 
1B-LM (three sent.) 
0.707 
0.745 
0.693 

Human performance 
0.859 
0.897 
0.845 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Models' performance and human-level 
performance on CLOTH. LSTM, Stanford Atten-
tive Reader and Attentive Reader with position-
aware attention shown in the top part only use 
supervised data labelled by human. LM out-
performs LSTM since it receives more supervi-
sions in learning to predict each word. Training 
on large external corpus further significantly en-
hances LM's accuracy. 

et al., 2016) have reached human performance on 
the SQuAD dataset (Rajpurkar et al., 2016), their 
performance is still not comparable to human per-
formance on datasets that focus more on reason-
ing where the evidence cannot be simply found by 
a matching behavior (Lai et al., 2017; Xu et al., 
2017). Since the focus of this paper is to analyze 
the proposed dataset, we leave the design of rea-
soning oriented attention models for future work. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Error analysis of 1-billion-language-model with three sentences as the context. The questions are sampled from the sample passage shown in Table 2. The correct answer is in bold text. The incorrectly selected options are in italics.</figDesc><table>Model 
CLOTH CLOTH-M CLOTH-H 

Short context 
1B-LM 
0.695 
0.723 
0.685 
Human 
0.713 
0.771 
0.691 

Long context 
1B-LM 
0.707 
0.745 
0.693 
Human 
0.859 
0.897 
0.845 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Humans' performance compared with 1-
billion-language-model. In the short context part, 
both 1B-LM and humans only use information of 
one sentence. In the long context part, humans 
have the whole passage as the context, while 1B-
LM uses contexts of three sentences. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Overall results on CLOTH. Ex. denotes 
external data. 

Model 
CLOTH CLOTH-M CLOTH-H 

Our model 
0.583 
0.673 
0.549 
w.o. rep. 
0.566 
0.662 
0.528 
w.o. hum. 
0.565 
0.665 
0.526 
w.o. rep. or hum. 
0.543 
0.643 
0.505 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and 
Percy Liang. 2016. Squad: 100,000+ questions 
for machine comprehension of text. arXiv preprint 
arXiv:1606.05250. 

´ 
Alvaro Rodrigo, Anselmo Peñas, Yusuke Miyao, Ed-
uard H Hovy, and Noriko Kando. 2015. Overview of 
clef qa entrance exams task 2015. In CLEF (Work-
ing Notes). 

J Sachs, P Tung, and RYH Lam. 1997. How to con-
struct a cloze test: Lessons from testing measure-
ment theory models. Perspectives. 

Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter 
Turney, and Oren Etzioni. 2017. Moving beyond the 
turing test with the allen ai science challenge. Com-
munications of the ACM, 60(9):60-64. 

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and 
Hannaneh Hajishirzi. 2016. Bidirectional attention 
flow for machine comprehension. arXiv preprint 
arXiv:1611.01603. 

Burr Settles. 2009. Active learning literature survey. 

Hideyuki Shibuki, Kotaro Sakamoto, Yoshinobu Kano, 
Teruko Mitamura, Madoka Ishioroshi, Kelly Y 
Itakura, Di Wang, Tatsunori Mori, and Noriko 
Kando. 2014. Overview of the ntcir-11 qa-lab task. 
In NTCIR. 

Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-
shick. 2016. Training region-based object detectors 
with online hard example mining. In Proceedings of 
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 761-769. 

Adam Skory and Maxine Eskenazi. 2010. Predicting 
cloze task quality for vocabulary training. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop 
on Innovative Use of NLP for Building Educational 
Applications, pages 49-56. Association for Compu-
tational Linguistics. 

Wilson L Taylor. 1953. cloze procedure: a new 
tool for measuring readability. Journalism Bulletin, 
30(4):415-433. 

Annie Tremblay. 2011. Proficiency assessment stan-
dards in second language acquisition research. Stud-
ies in Second Language Acquisition, 33(3):339-372. 

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830. 

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, 
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics 
(Volume 1: Long Papers), volume 1, pages 189-198. 

Yichong Xu, Jingjing Liu, Jianfeng Gao, Yelong Shen, 
and Xiaodong Liu. 2017. Towards human-level ma-
chine reading comprehension: Reasoning and in-
ference with multiple strategies. arXiv preprint 
arXiv:1711.04964. 

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D Manning. 2017. Position-
aware attention and supervised data improve slot fill-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing, 
pages 35-45. 

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja 
Fidler. 2015. Aligning books and movies: Towards 
story-like visual explanations by watching movies 
and reading books. In Proceedings of the IEEE 
international conference on computer vision, pages 
19-27. 

Geoffrey Zweig and Christopher JC Burges. 2011. The 
microsoft research sentence completion challenge. 
Technical report, Technical Report MSR-TR-2011-
129, Microsoft. </table></figure>

			<note place="foot">* Equal contribution. 1 CLOTH (CLOze test by TeacHers) is available at http://www.cs.cmu.edu/ ˜ glai1/data/cloth/. 2 The leaderboard is available at http://www. qizhexie.com/data/CLOTH_leaderboard.html</note>

			<note place="foot" n="3"> The three websites include http://www.21cnjy.com/; http://5utk.ks5u.com/; http://zujuan.xkw.com/. We checked that CLOTH does not contain sentence completion example questions from GRE, SAT and PSAT. 4 tesseract: https://github.com/tesseract-ocr; ABBYY FineReader: https://www.abbyy.com/en-us/finereader/</note>

			<note place="foot" n="6"> The code is available at https://github.com/qizhex/Largescale-Cloze-Test-Dataset-Created-by-Teachers</note>

			<note place="foot" n="7"> The script to generate the Figure is obtained at https://gist.github.com/ihsgnef/ f13c35cd46624c8f458a4d23589ac768</note>

			<note place="foot" n="8"> https://github.com/pytorch/examples/tree/master/word language model</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Yulun Du, Kaiyu Shi and Zhilin Yang for insightful discussions and suggestions on the draft. We thank Shi Feng for the script to high-light representative words. This research was sup-ported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Question Type Labeling</head><p>To label the questions, we provided the definition and an example for each question category to the Amazon Mechanical Turkers. To ensure quality, we limited the workers to master Turkers who are expe- rienced and maintain a high acceptance rate. However, we did not restrict the backgrounds of the Turkers since master Turkers should have a reasonable amount of knowledge about English to conduct previous tasks. In addition, the vocabulary used in CLOTH are usually not difficult since they are constructed to test non-native speakers in middle school or high school. To get a concrete idea of the nature of question types, please refer to examples shown in Tab. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Type-specific Performance Analysis</head><p>We can also further verify the strengths and weaknesses of the 1B-LM by studying the performance of models and human on different question categories. Note that the performance presented here may be subject to a high variance due to the limited number of samples in each category. From the comparison shown in <ref type="figure">Figure 2</ref>, we see that 1B-LM is indeed good at short-term questions. Specifically, when the human only has access to the context of one sentence, 1B-LM is close to human's performance on almost all categories. Further, comparing LM and 1B-LM, we find that training on the large corpus leads to improvements on all categories, showing that training on a large amount of data leads to a substantial improvement in learning complex language regularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details</head><p>We implement our models using PyTorch ( <ref type="bibr">Paszke et al., 2017)</ref>. We train our model on all questions in CLOTH and test it on CLOTH-M and CLOTH-H separately. For our final model, we use Adam ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) with the learning rate of 0.001. The hidden dimension is set to 650 and we initialize the word embedding by 300-dimensional Glove word vector ( <ref type="bibr">Pennington et al., 2014</ref>). The temperature α is set to 2. We tried to increase the dimensionality of the model but do not observe performance improvement.</p><p>When we train the small LM on CLOTH, we largely follow the recommended hyperparameters in the Pytorch LM example 8 . Specifically, we employ a 2-layer LSTM with hidden dimension as 1024. The input embedding and output weight matrix are tied. We set the dropout rate to 0.5. The initial learning rate is set to 10 and divided by 4 whenever the PPL stops improving on the dev set.</p><p>We predict the answer for each blank independently for all of the models mentioned in this paper, since we do not observe significant performance improvements in our preliminary experiments when an auto-regressive approach is employed, i.e., when we fill all previous blanks with predicted answers. We hypothesize that, regardless of whether there exist inter-blank dependencies, since blanks are usually Passage: Nancy had just got a job as a secretary in a company. Monday was the first day she went to work, so she was very 1 and arrived early. She 2 the door open and found nobody there. "I am the 3 to arrive." She thought and came to her desk. She was surprised to find a bunch of 4 on it. They were fresh. She 5 them and they were sweet. She looked around for a 6 to put them in. "Somebody has sent me flowers the very first day!" she thought 7 . " But who could it be?" she began to 8 . The day passed quickly and Nancy did everything with 9 interest. For the following days of the 10 , the first thing Nancy did was to change water for the followers and then set about her work. Then came another Monday. 11 she came near her desk she was overjoyed to see a(n) 12 bunch of flowers there. She quickly put them in the vase, 13 the old ones. The same thing happened again the next Monday. Nancy began to think of ways to find out the 14 . On Tuesday afternoon, she was sent to hand in a plan to the 15 . She waited for his directives at his secretary's 16 . She happened to see on the desk a half-opened notebook, which 17 : "In order to keep the secretaries in high spirits, the company has decided that every Monday morning a bunch of fresh flowers should be put on each secretarys desk." Later, she was told that their general manager was a business management psychologist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Questions</head><p>Question type 1.</p><p>A  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ontology-aware token embeddings for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02925</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to paraphrase for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06022</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The cloze test as an integrative measure of efl proficiency: A substitute for essays on college entrance examinations? Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandra S Fotos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="313" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cloze item types and second language comprehension. Language testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Jonz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring the limits of lming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A system for generating cloze test items from russian-language text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kurtasov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop associated with RANLP 2013</title>
		<meeting>the Student Research Workshop associated with RANLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">S ho rt-t er m-r ea so ni ng G ra m m ar M at ch in g/ pa ra ph ra si ng Lo ng-t er m-r ea so ni ng LM Our model 1B-LM</title>
		<imprint/>
	</monogr>
	<note>one sentence) 1B-LM (three sentences</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Human (one sentence) Human (whole passage)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">S ho rt-t er m-r ea so ni ng G ra m m ar M at ch in g/ pa ra ph ra si ng Lo ng-t er m-r ea so ni ng LM Our model 1B-LM</title>
		<imprint/>
	</monogr>
	<note>one sentence) 1B-LM (three sentences</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Human (one sentence) Human (whole passage)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">High school group</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Our model will be introduced in Sec. 6. distributed far away from each other, LSTM is not able to capture such long dependencies</title>
		<imprint/>
	</monogr>
	<note>Figure 2: Model and human&apos;s performance on questions with different types. When testing language models, we use the longest text spans that do not contain blanks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
