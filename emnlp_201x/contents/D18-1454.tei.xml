<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Commonsense for Generative Multi-Hop Question Answering Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mohit Bansal UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mohit Bansal UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Commonsense for Generative Multi-Hop Question Answering Tasks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4220" to="4230"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4220</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong genera-tive baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop re-lational commonsense information from Con-ceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted com-monsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model&apos;s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we explore the task of machine reading comprehension (MRC) based QA. This task tests a model's natural language understand- ing capabilities by asking it to answer a question * Equal contribution (published at EMNLP 2018).</p><p>We publicly release all our code, models, and data at:</p><p>https://github.com/yicheng-w/CommonSenseMultiHopQA based on a passage of relevant content. Much progress has been made in reasoning-based MRC- QA on the bAbI dataset ( <ref type="bibr">Weston et al., 2016)</ref>, which contains questions that require the combi- nation of multiple disjoint pieces of evidence in the context. However, due to its synthetic nature, bAbI evidences have smaller lexicons and sim- pler passage structures when compared to human- generated text. There also have been several attempts at the MRC-QA task on human-generated text. Large scale datasets such as CNN/DM ( <ref type="bibr" target="#b14">Hermann et al., 2015</ref>) and SQuAD ( <ref type="bibr" target="#b25">Rajpurkar et al., 2016)</ref> have made the training of end-to-end neural models possible. However, these datasets are fact-based and do not place heavy emphasis on multi-hop rea- soning capabilities. More recent datasets such as QAngaroo ( <ref type="bibr">Welbl et al., 2018)</ref> have prompted a strong focus on multi-hop reasoning in very long texts. However, QAngaroo is an extractive dataset where answers are guaranteed to be spans within the context; hence, this is more focused on fact finding and linking, and does not require models to synthesize and generate new information.</p><p>We focus on the recently published Narra- tiveQA generative dataset <ref type="bibr" target="#b16">(Kočisk`Kočisk`y et al., 2018</ref>) that contains questions requiring multi-hop rea- soning for long, complex stories and other nar- ratives, which requires the model to go beyond fact linking and to synthesize non-span answers. Hence, models that perform well on previous rea- soning tasks ( <ref type="bibr" target="#b10">Dhingra et al., 2018</ref>) have had lim- ited success on this dataset. In this paper, we first propose the Multi-Hop Pointer-Generator Model (MHPGM), a strong baseline model that uses mul- tiple hops of bidirectional attention, self-attention, and a pointer-generator decoder to effectively read and reason within a long passage and synthesize a coherent response. Our model achieves 41.49 Rouge-L and 17.33 METEOR on the summary subtask of NarrativeQA, substantially better than the performance of previous generative models.</p><p>Next, to address the issue that understand- ing human-generated text and performing long- distance reasoning on it often involves intermittent access to missing hops of external commonsense (background) knowledge, we present an algorithm for selecting useful, grounded multi-hop relational knowledge paths from ConceptNet ( <ref type="bibr" target="#b28">Speer and Havasi, 2012</ref>) via a pointwise mutual information (PMI) and term-frequency-based scoring func- tion. We then present a novel method of insert- ing these selected commonsense paths between the hops of document-context reasoning within our model, via the Necessary and Optional Infor- mation Cell (NOIC), which employs a selectively- gated attention mechanism that utilizes common- sense information to effectively fill in gaps of in- ference. With these additions, we further improve performance on the NarrativeQA dataset, achiev- ing 44.16 Rouge-L and 19.03 METEOR (also ver- ified via human evaluation). We also provide man- ual analysis on the effectiveness of our common- sense selection algorithm.</p><p>Finally, to show the effectiveness and gener- alizability of our multi-hop reasoning and com- monsense methods, we also tested our MH- PGM and MHPGM+NOIC models on QAngaroo- WikiHop ( <ref type="bibr">Welbl et al., 2018)</ref>, which is an extrac- tive dataset for multi-hop reasoning on human- generated documents. We found that our back- ground commonsense knowledge enhanced model achieved 1.5% higher accuracy than our strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine Reading Comprehension: MRC has long been a task used to assess a model's ability to understand and reason about language. Large scale datasets such as CNN/Daily Mail <ref type="bibr" target="#b14">(Hermann et al., 2015</ref>) and SQuAD ( <ref type="bibr" target="#b25">Rajpurkar et al., 2016)</ref> have encouraged the development of many advanced, high performing attention-based neural models ( <ref type="bibr" target="#b27">Seo et al., 2017;</ref><ref type="bibr" target="#b11">Dhingra et al., 2017)</ref>. Concurrently, datasets such as bAbI ( <ref type="bibr">Weston et al., 2016)</ref> have focused specifically on multi-step rea- soning by requiring the model to reason with disjoint pieces of information. On this task, it has been shown that iteratively updating the query representation with information from the context can effectively emulate multi-step reason- ing ( <ref type="bibr" target="#b29">Sukhbaatar et al., 2015</ref>).</p><p>More recently, there has been an increase in multi-paragraph, multi-hop inference QA datasets such as QAngaroo ( <ref type="bibr">Welbl et al., 2018)</ref> and Narra- tiveQA <ref type="bibr" target="#b16">(Kočisk`Kočisk`y et al., 2018)</ref>. These datasets have much longer contexts than previous datasets, and answering a question often requires the synthesis of multiple discontiguous pieces of evidence. It has been shown that models designed for previ- ous tasks ( <ref type="bibr" target="#b27">Seo et al., 2017;</ref><ref type="bibr" target="#b15">Kadlec et al., 2016)</ref> have limited success on these new datasets. In our work, we expand upon Gated Attention Net- work ( <ref type="bibr" target="#b11">Dhingra et al., 2017)</ref> to create a baseline model better suited for complex MRC datasets such as NarrativeQA by improving its attention and gating mechanisms, expanding its generation capabilities, and allowing access to external com- monsense for connecting implicit relations.</p><p>Commonsense/Background Knowledge: Com- monsense or background knowledge has been used for several tasks including opinion min- ing ( <ref type="bibr" target="#b6">Cambria et al., 2010)</ref>, sentiment analy- sis ( <ref type="bibr" target="#b23">Poria et al., 2015</ref><ref type="bibr" target="#b24">Poria et al., , 2016</ref>, handwritten text recognition ( <ref type="bibr" target="#b32">Wang et al., 2013)</ref>, and more re- cently, dialogue <ref type="bibr">(Young et al., 2018;</ref><ref type="bibr" target="#b13">Ghazvininejad et al., 2018)</ref>. These approaches add com- monsense knowledge as relation triples or fea- tures from external databases. Recently, large- scale graphical commonsense databases such as ConceptNet ( <ref type="bibr" target="#b28">Speer and Havasi, 2012</ref>) use graph- ical structure to express intricate relations be- tween concepts, but effective goal-oriented graph traversal has not been extensively used in previous commonsense incorporation efforts. Knowledge- base QA is a task in which systems are asked to find answers to questions by traversing knowledge graphs ( <ref type="bibr" target="#b3">Bollacker et al., 2008)</ref>. Knowledge path extraction has been shown to be effective at the task ( <ref type="bibr" target="#b4">Bordes et al., 2014;</ref><ref type="bibr" target="#b2">Bao et al., 2016)</ref>. We ap- ply these techniques to MRC-QA by using them to extract useful commonsense knowledge paths that fully utilize the graphical nature of databases such as ConceptNet <ref type="bibr" target="#b28">(Speer and Havasi, 2012</ref>).</p><p>Incorporation of External Knowledge: There have been several attempts at using external knowledge to boost model performance on a vari- ety of tasks: <ref type="bibr" target="#b7">Chen et al. (2018)</ref> showed that adding lexical information from semantic databases such as WordNet improves performance on NLI; <ref type="bibr">Xu et al. (2017)</ref> used a gated recall-LSTM mechanism to incorporate commonsense information into to-ken representations in dialogue.</p><p>In MRC, <ref type="bibr">Weissenborn et al. (2017)</ref> integrated external background knowledge into an NLU model by using contextually-refined word em- beddings which integrated information from Con- ceptNet (single-hop relations mapped to unstruc- tured text) via a single layer bidirectional LSTM. Concurrently to our work, <ref type="bibr" target="#b19">Mihaylov and Frank (2018)</ref> showed improvements on a cloze-style task by incorporating commonsense knowledge via a context-to-commonsense attention, where com- monsense relations were extracted as triples. This work represented commonsense relations as key- value pairs and combined context representation and commonsense via a static gate.</p><p>Differing from previous works, we employ multi-hop commonsense paths (multiple con- nected edges within ConceptNet graph that give us information beyond a single relationship triple) to help with our MRC model. Moreover, we use this in tandem with our multi-hop reasoning archi- tecture to incorporate different aspects of the com- monsense relationship path at each hop, in order to bridge different inference gaps in the multi-hop QA task. Additionally, our model performs syn- thesis with its external, background knowledge as it generates, rather than extracts, its answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Hop Pointer-Generator Baseline</head><p>We first rigorously state the problem of genera- tive QA as follows: given two sequences of input tokens: the context, X C = {w C 1 , w C 2 , . . . , w C n } and the query, X Q = {w Q 1 , w Q 2 , . . . , w Q m }, the system should generate a series of answer tokens X a = {w a 1 , w a 2 , . . . , w a p }. As outlined in previous sections, an effective generative QA model needs to be able to perform several hops of reasoning over long and complex passages. It would also need to be able to generate coherent statements to answer complex questions while having the abil- ity to copy rare words such as specific entities from the reading context. With these in mind, we propose the Multi-Hop Pointer-Generator Model (MHPGM) baseline, a novel combination of previ- ous works with the following major components:</p><p>• Embedding Layer: The tokens are embedded into both learned word embeddings and pre- trained context-aware embeddings (ELMo (Pe- ters et al., 2018)).</p><p>• Reasoning Layer: The embedded context is then passed through k reasoning cells, each of which iteratively updates the context repre- sentation with information from the query via BiDAF attention ( <ref type="bibr" target="#b27">Seo et al., 2017)</ref>, emulating a single reasoning step within the multi-step rea- soning process.</p><p>• Self-Attention Layer: The context representa- tion is passed through a layer of self-attention ( <ref type="bibr" target="#b8">Cheng et al., 2016</ref>) to resolve long-term depen- dencies and co-reference within the context.</p><p>• Pointer-Generator Decoding Layer:</p><p>A attention-pointer-generator decoder ( <ref type="bibr" target="#b26">See et al., 2017</ref>) that attends on and potentially copies from the context is used to create the answer.</p><p>The overall model is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, and the layers are described in further detail below. Embedding layer: We embed each word from the context and question with a learned embedding space of dimension d. We also obtain context- aware embeddings for each word via the pre- trained embedding from language models (ELMo) (1024 dimensions). The embedded representation for each word in the context or question,</p><formula xml:id="formula_0">e C i or e Q i ∈ R d+1024</formula><p>, is the concatenation of its learned word embedding and ELMo embedding. Reasoning layer: Our reasoning layer is com- posed of k reasoning cells (see <ref type="figure" target="#fig_0">Fig. 1</ref>), where each incrementally updates the context representation. The t th reasoning cell's inputs are the previous step's output ({c t−1 i } n i=1 ) and the embedded ques-</p><formula xml:id="formula_1">tion ({e Q i } m i=1</formula><p>). It first creates step-specific con- text and query encodings via cell-specific bidirec- tional LSTMs:</p><formula xml:id="formula_2">u t = BiLSTM(c t−1 ); v t = BiLSTM(e Q )</formula><p>Then, we use bidirectional attention ( <ref type="bibr" target="#b27">Seo et al., 2017)</ref> to emulate a hop of reasoning by focusing on relevant aspects of the context. Specifically, we first compute context-to-query attention:</p><formula xml:id="formula_3">S t ij = W t 1 u t i + W t 2 v t j + W t 3 (u t i v t j ) p t ij = exp(S t ij ) m k=1 exp(S t ik ) (c q ) t i = m j=1 p t ij v t j</formula><p>where W t 1 , W t 2 , W t 3 are trainable parameters, and is elementwise multiplication. We then com- pute a query-to-context attention vector:</p><formula xml:id="formula_4">m t i = max 1≤j≤m S t ij ... + p sel</formula><p>Attention Distribution  </p><formula xml:id="formula_5">x t-</formula><formula xml:id="formula_6">p t i = exp(m t i ) n j=1 exp(m t j ) q c t = n i=1 p t i u t i</formula><p>We then obtain the updated context representation:</p><formula xml:id="formula_7">c t i = [u t i ; (c q ) t i ; u t i (c q ) t i ; q c t (c q ) t i ]</formula><p>where ; is concatenation, c t is the cell's output. The initial input of the reasoning layer is the embedded context representation, i.e., c 0 = e C , and the final output of the reasoning layer is the output of the last cell, c k . Self-Attention Layer: As the final layer before answer generation, we utilize a residual static self- attention mechanism ) to help the model process long contexts with long- term dependencies. The input of this layer is the output of the last reasoning cell, c k . We first pass this representation through a fully-connected layer and then a bi-directional LSTM to obtain another representation of the context c SA . We obtain the self attention representation c :</p><formula xml:id="formula_8">S SA ij = W 4 c SA i + W 5 c SA j + W 6 (c SA i c SA j ) p SA ij = exp(S SA ij ) n k=1 exp(S SA ik ) c i = n j=1 p SA ij c SA j</formula><p>where W 4 , W 5 , and W 6 are trainable parameters. The output of the self-attention layer is gener- ated by another layer of bidirectional LSTM.</p><formula xml:id="formula_9">c = BiLSTM([c ; c SA ; c c SA ]</formula><p>Finally, we add this residually to c k to obtain the encoded context c = c k + c . Pointer-Generator Decoding Layer: Similar to the work of <ref type="bibr" target="#b26">See et al. (2017)</ref>, we use a pointer- generator model attending on (and potentially copying from) the context. At decoding step t, the decoder receives the in- put x t (embedded representation of last timestep's output), the last time step's hidden state s t−1 and context vector a t−1 . The decoder computes the current hidden state s t as:</p><formula xml:id="formula_10">s t = LSTM([x t ; a t−1 ], s t−1 )</formula><p>This hidden state is then used to compute a proba- bility distribution over the generative vocabulary:</p><formula xml:id="formula_11">P gen = softmax(W gen s t + b gen )</formula><p>We employ Bahdanau attention mecha- nism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) to attend over the context (c being the output of self-attention layer):</p><formula xml:id="formula_12">α i = v tanh(W c c i + W s s t + b attn )</formula><p>"What is the connection between Esther and Lady Dedlock?" "Mother and daughter." </p><formula xml:id="formula_13">ˆ α i = exp(α i ) n j=1 exp(α j ) a t = n i=1ˆα i=1ˆ i=1ˆα i c i</formula><p>We utilize a pointer mechanism that allows the decoder to directly copy tokens from the context based onˆαonˆ onˆα i . We calculate a selection distribution p sel ∈ R 2 , where p sel 1 is the probability of gener- ating a token from P gen and p sel 2 is the probability of copying a word from the context:</p><formula xml:id="formula_14">o = σ(W a a t + W x x t + W s s t + b ptr ) p sel = softmax(o)</formula><p>Our final output distribution at timestep t is a weighted sum of the generative distribution and the copy distribution:</p><formula xml:id="formula_15">P t (w) = p sel 1 P gen (w) + p sel 2 i:w C i =wˆα =wˆ =wˆα i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Commonsense Selection and Representation</head><p>In QA tasks that require multiple hops of reason- ing, the model often needs knowledge of relations not directly stated in the context to reach the cor- rect conclusion. In the datasets we consider, man- ual analysis shows that external knowledge is fre- quently needed for inference (see <ref type="table" target="#tab_1">Table 1</ref>). Even with a large amount of training data, it is very unlikely that a model is able to learn ev- ery nuanced relation between concepts and ap- ply the correct ones (as in <ref type="figure" target="#fig_1">Fig. 2</ref>) when reasoning   about a question. We remedy this issue by intro- ducing grounded commonsense (background) in- formation using relations between concepts from ConceptNet (Speer and Havasi, 2012) 1 that help inference by introducing useful connections be- tween concepts in the context and question. Due to the size of the semantic network and the large amount of unnecessary information, we need an effective way of selecting relations which provides novel information while being grounded by the context-query pair. Our commonsense se- lection strategy is twofold: (1) collect potentially relevant concepts via a tree construction method aimed at selecting with high recall candidate rea- soning paths, and (2) rank and filter these paths to ensure both the quality and variety of added infor- mation via a 3-step scoring strategy (initial node scoring, cumulative node scoring, and path selec- tion). We will refer to <ref type="figure" target="#fig_1">Fig. 2</ref> as a running example throughout this section. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Tree Construction</head><p>Given context C and question Q, we want to con- struct paths grounded in the pair that emulate rea- soning steps required to answer the question. In this section, we build 'prototype' paths by con- structing trees rooted in concepts in the query with the following branching steps 3 to emulate multi- hop reasoning process. For each concept c 1 in the question, we do: Direct Interaction: In the first level, we select re- lations r 1 from ConceptNet that directly link c 1 to a concept within the context, c 2 ∈ C, e.g., in <ref type="figure" target="#fig_1">Fig. 2</ref>, we have lady → church, lady → mother, lady → person. Multi-Hop: We then select relations in Concept- Net r 2 that link c 2 to another concept in the con- text, c 3 ∈ C. This emulates a potential reason-ing hop within the context of the MRC task, e.g., church → house, mother → daughter, person → lover. Outside Knowledge: We then allow an uncon- strained hop into c 3 's neighbors in ConceptNet, getting to c 4 ∈ nbh(c 3 ) via r 3 (nbh(v) is the set of nodes that can be reached from v in one hop). This emulates the gathering of useful external in- formation to complete paths within the context, e.g., house → child, daughter → child. Context-Grounding: To ensure that the exter- nal knowledge is indeed helpful to the task, and also to explicitly link 2nd degree neighbor con- cepts within the context, we finish the process by grounding it again into context by connecting c 4 to c 5 ∈ C via r 4 , e.g., child → their.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Rank and Filter</head><p>This tree building process collects a large number of potentially relevant and useful paths. However, this step also introduces a large amount of noise. For example, given the question and full context (not depicted in the figure) in <ref type="figure" target="#fig_1">Fig. 2</ref>, we obtain the path "between → hard → being → cottage → country" using our tree building method, which is not relevant to our question. Therefore, to improve the precision of useful concepts, we rank these knowledge paths by their relevance and filter out noise using the following 3-step scoring method: Initial Node Scoring: We want to select paths with nodes that are important to the context, in order to provide the most useful common- sense relations. We approximate importance and saliency for concepts in the context by their term- frequency, under the heuristic that important con- cepts occur more frequently. Thus we score c ∈ {c 2 , c 3 , c 5 } by: score(c) = count(c)/|C|, where |C| is the context length and count() is the num- ber of times a concept appears in the context. In <ref type="figure" target="#fig_1">Fig. 2</ref>, this ensures that concepts like daughter are scored highly due to their frequency in the context.</p><p>For c 4 , we use a special scoring function as it is an unconstrained hop into ConceptNet. We want c 4 to be a logically consistent next step in reason- ing following the path of c 1 to c 3 , e.g., in <ref type="figure" target="#fig_1">Fig. 2</ref>, we see that child is a logically consistent next step af- ter the partial path of mother → daughter. We ap- proximate this based on the heuristic that logically consistent paths occur more frequently. Therefore, we score this node via Pointwise Mutual Informa- tion (PMI) between the partial path c 1−3 and c 4 : PMI(c 4 , c 1−3 ) = log(P(c 4 , c 1−3 )/P(c 4 )P(c 1−3 )), where P(c 4 , c 1−3 ) = # of paths connecting c 1 , c 2 , c 3 , c 4 # of distinct paths of length 4 P(c 4 ) = # of nodes that can reach c 4 |ConceptNet| Since the branching at each juncture represents a hop in the multi-hop reasoning process, and hops at different levels or with different parent nodes do not 'compete' with each other, we normalize each node's score against its siblings: n-score(c) = softmax siblings(c) (score(c)).</p><formula xml:id="formula_16">P</formula><p>Cumulative Node Scoring: We want to add com- monsense paths consisting of multiple hops of relevant information, thus we re-score each node based not only on its relevance and saliency but also that of its tree descendants.</p><p>We do this by computing a cumulative node score from the bottom up, where at the leaf nodes, we have c-score = n-score, and for c l not a leaf node, we have c-score(c l ) = n-score(c l ) + f (c l ) where f of a node is the average of the c-scores of its top 2 highest scoring children.</p><p>For example, given the paths lady → mother → daughter, lady → mother → married, and lady → mother → book, we start the cumulative scoring at the leaf nodes, which in this case are daugh- ter, married, and book, where daughter and mar- ried are scored much higher than book due to their more frequent occurrences. Then, to cumulatively score mother , we would take the average score of its two highest scoring children (in this case mar- ried and daughter) and compound that with the score of mother itself. Note that the poor scoring of the irrelevant concept book does not affect the scoring of mother, which is quite high due to the concept's frequent occurrence and the relevance of its top scoring children. Path Selection: We select paths in a top-down breath-first fashion in order to add information rel- evant to different parts of the context. Starting at the root, we recursively take two of its children with the highest cumulative scores until we reach a leaf, selecting up to 2 4 = 16 paths. For example, if we were at node mother, this allows us to se- lect the child node daughter and married over the child node book. These selected paths, as well as their partial sub-paths, are what we add as exter- nal information to the QA model, i.e., we add the complete path lady, AtLocation, church, Relat- edTo, house, RelatedTo, child, RelatedTo, their, but also truncated versions of the path, including lady, AtLocation, church, RelatedTo, house, Re- latedTo, child. We directly give these paths to the model as sequences of tokens. <ref type="bibr">4</ref> Overall, our sampling strategy provides the knowledge that a lady can be a mother and that mother is connected to daughter. This creates a logical connection between lady and daughter which helps highlight the importance of our sec- ond piece of evidence (see <ref type="figure" target="#fig_1">Fig. 2)</ref>. Likewise, the commonsense information we extracted cre- ate a similar connection in our third piece of ev- idence, which states the explicit connection be- tween daughter and Esther. We also successfully extract a more story context-centric connection, in which commonsense provides the knowledge that a lady is at the location church, which directs to another piece of evidence in the context. Addition- ally, this path also encodes a relation between lady and child, by way of church, which is how lady and Esther are explicitly connected in the story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Commonsense Model Incorporation</head><p>Given the list of commonsense logic paths as se- quences of words:</p><formula xml:id="formula_17">X CS = {w CS 1 , w CS 2 , . . . , w CS l } where w CS i</formula><p>represents the list of tokens that make up a single path, we first embed these commonsense tokens into the learned embedding space used by the model, giving us the embedded commonsense tokens, e CS ij ∈ R d . We want to use these commonsense paths to fill in the gaps of reasoning between hops of inference. Thus, we propose Necessary and Optional Information Cell (NOIC), a variation of our base reasoning cell used in the reasoning layer that is capable of incorporating optional helpful information.</p><p>NOIC This cell is an extension to the base rea- soning cell that allows the model to use common- sense information to fill in gaps of reasoning. An example of this is on the bottom left of <ref type="figure" target="#fig_0">Fig. 1</ref>, where we see that the cell first performs the op- erations done in the base reasoning cell and then adds optional, commonsense information.</p><p>At reasoning step t, after obtaining the out- put of the base reasoning cell, c t , we create a cell-specific representation for commonsense in- formation by concatenating the embedded com- monsense paths so that each path has a single vec- tor representation, u CS i . We then project it to the same dimension as c t i :</p><formula xml:id="formula_18">v CS i = ReLU(W u CS i + b)</formula><note type="other">where W and b are trainable parameters.</note><p>We use an attention layer to model the interac- tion between commonsense and the context:</p><formula xml:id="formula_19">S CS ij = W CS 1 c t i + W CS 2 v CS j + W CS 3 (c t i v CS j ) p CS ij = exp(S CS ij ) l k=1 exp(S CS ij ) c CS i = l j=1 p CS ij v CS j</formula><p>Finally, we combine this commonsense-aware context representation with the original c t i via a sigmoid gate, since commonsense information is often not necessary at every step of inference:</p><formula xml:id="formula_20">z i = σ(W z [c CS i ; c t i ] + b z ) (c o ) t i = z i c t i + (1 − z i ) c CS i</formula><p>We use c o t as the output of the current reasoning step instead of c t . As we replace each base rea- soning cell with NOIC, we selectively incorporate commonsense at every step of inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets: We report results on two multi-hop rea- soning datasets: generative NarrativeQA <ref type="bibr" target="#b16">(Kočisk`Kočisk`y et al., 2018</ref>) (summary subtask) and extractive QAngaroo WikiHop ( <ref type="bibr">Welbl et al., 2018</ref>). For multiple-choice WikiHop, we rank candidate re- sponses by their generation probability. Similar to previous works ( <ref type="bibr" target="#b10">Dhingra et al., 2018)</ref>, we use the non-oracle, unmasked and not-validated dataset. Evaluation Metrics: We evaluate NarrativeQA on the metrics proposed by its original authors: Bleu-1, Bleu-4 ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>), ME- TEOR ( <ref type="bibr" target="#b1">Banerjee and Lavie, 2005</ref>) and Rouge- L <ref type="bibr" target="#b18">(Lin, 2004</ref>). We also evaluate on CIDEr <ref type="bibr" target="#b31">(Vedantam et al., 2015</ref>) which emphasizes annotator con- sensus. For WikiHop, we evaluate on accuracy. <ref type="bibr">5</ref> More dataset, metric, and all other training de- tails are in the supplementary.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Experiment</head><p>The results of our model on both NarrativeQA and WikiHop with and without commonsense incorpo- ration are shown in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_0">Table 3</ref>. We see empirically that our model outperforms all gener- ative models on NarrativeQA, and is competitive with the top span prediction models. Furthermore, with the NOIC commonsense integration, we were able to further improve performance (p &lt; 0.001 on all metrics 6 ), establishing a new state-of-the-art for the task. We also see that our model performs well on WikiHop, 7 and is further improved via the addition of commonsense (p &lt; 0.001), demon- strating the generalizability of both our model and commonsense addition techniques. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Ablations</head><p>We also tested the effectiveness of each compo- nent of our architecture as well as the effective- ness of adding commonsense information on the NarrativeQA validation set, with results shown in <ref type="table" target="#tab_7">Table 4</ref>. Experiment 1 and 5 are our models pre- (500 examples) held-out part of the training set, and test on the original validation set (by treating it as an unseen test set). We will promptly include the non-public test set results in the next version and at: https://github.com/yicheng-w/ CommonSenseMultiHopQA 6 Stat. significance computed using bootstrap test with 100K iterations <ref type="bibr" target="#b20">(Noreen, 1989;</ref><ref type="bibr" target="#b12">Efron and Tibshirani, 1994)</ref>. <ref type="bibr">7</ref> Note that we compare our model's performance to other models' tuned performance on the development set and ours is still equal or better.</p><p>8 All results here are for the standard (non-oracle) un- masked and not-validated dataset. <ref type="bibr">Welbl et al. (2018)</ref>   <ref type="bibr">et al., 2018)</ref> were also important for the model's performance and that self-attention is able to con- tribute significantly to performance on top of other components of the model. Finally, we see that ef- fectively introducing external knowledge via our commonsense selection algorithm and NOIC can improve performance even further on top of our strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Commonsense Ablations</head><p>We also conducted experiments testing the effec- tiveness of our commonsense selection and incor- poration techniques. We first tried to naively add ConceptNet information by initializing the word embeddings with the ConceptNet-trained embed- dings, NumberBatch (Speer and Havasi, 2012) (we also change embedding size from 256 to 300). Then, to verify the effectiveness of our com- monsense selection and grounding algorithm, we test our best model on in-domain noise by giv- ing each context-query pair a set of random rela- tions grounded in other context-query pairs. This should teach the model about general common- sense relations present in the domain of Narra- tiveQA but does not provide grounding that fills in specific hops of inference. We also experi- mented with a simpler commonsense extraction method of using a single hop from the query to the context. The results of these are shown in <ref type="table" target="#tab_8">Table 5</ref>, where we see that neither NumberBatch nor random-relationships nor single-hop common- sense offer statistically significant improvements 9 ,   whereas our commonsense selection and incorpo- ration mechanism improves performance signifi- cantly across all metrics. We also present several examples of extracted commonsense and its model attention visualization in the supplementary.</p><formula xml:id="formula_21"># Ablation B-1 B-4 M R C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Human Evaluation Analysis</head><p>We also conduct human evaluation analysis on both the quality of the selected commonsense re- lations, as well as the performance of our final model. Commonsense Selection: We conducted manual analysis on a 50 sample subset of the NarrativeQA test set to check the effectiveness of our common- sense selection algorithm. Specifically, given a context-query pair, as well as the commonsense selected by our algorithm, we conduct two inde- pendent evaluations: (1) was any external com- monsense knowledge necessary for answering the question?; (2) were the commonsense relations provided by our algorithm relevant to the ques- tion? The result for these two evaluations as well as how they overlap with each other are shown in <ref type="table" target="#tab_9">Table 6</ref>, where we see that 50% of the cases re- quired external commonsense knowledge, and on a majority (34%) of those cases our algorithm was able to select the correct/relevant commonsense information to fill in gaps of inference. We also see that in general, our algorithm was able to pro- vide useful commonsense 48% of the time.</p><p>Model Performance: We also conduct human evaluation to verify that our commonsense incor- porated model was indeed better than MHPGM. We randomly selected 100 examples from the Nar- rativeQA test set, along with both models' pre- dicted answers, and for each datapoint, we asked</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Commonsense Required Yes No</head><p>Relevant CS Extracted 34% 14% Irrelevant CS Extracted 16% 36%  3 external human evaluators (fluent English speak- ers) to decide (without knowing which model pro- duced each response) if one is strictly better than the other, or that they were similar in quality (both- good or both-bad). As shown in <ref type="table" target="#tab_10">Table 7</ref>, we see that the human evaluation results are in agreement with that of the automatic evaluation metrics: our commonsense incorporation has a reasonable im- pact on the overall correctness of the model. The inter-annotator agreement had a Fleiss κ = 0.831, indicating 'almost-perfect' agreement between the annotators ( <ref type="bibr" target="#b17">Landis and Koch, 1977)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present an effective reasoning-generative QA architecture that is a novel combination of previ- ous work, which uses multiple hops of bidirec- tional attention and a pointer-generator decoder to effectively perform multi-hop reasoning and syn- thesize a coherent and correct answer. Further, we introduce an algorithm to select grounded, use- ful paths of commonsense knowledge to fill in the gaps of inference required for QA, as well a Necessary and Optional Information Cell (NOIC) which successfully incorporates this information during multi-hop reasoning to achieve the new state-of-the-art on NarrativeQA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture for our Multi-Hop Pointer-Generator Model, and the NOIC commonsense reasoning cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"Figure 2 :</head><label>2</label><figDesc>Figure 2: Commonsense selection approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(c 1−3 ) = # of paths connecting c 1 , c 2 , c 3 # of paths of length 3 Further, it is well known that PMI has high sensitivity to low-frequency values, thus we use normalized PMI (NPMI) (Bouma, 2009): score(c 4 ) = PMI(c 4 , c 1−3 )/(− log P(c 4 , c 1−3 )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>3 k Reasoning Cells</head><label>3</label><figDesc></figDesc><table>Query 

Embedding Layer 

Reasoning Layer 

Self-Attention Layer 

Decoding Layer 

Context Representation 

x t-2 
x t-1 

Context Vector 

Generative 
Distribution 
Final 
Distribution 

Context 
Embed 

w 1 

C 

w 2 

C 

w n 

C 

... 

w 1 

Q 

w m 

Q 

... 

Embed 

Self-Attention 

... 

BiDAF 

Bi-LSTM 
Context 

Bi-LSTM 

Query 

σ 

; 

BiDAF 
Attention 

Bi-LSTM 

; 
; 

NOIC Reasoning Cell 

Context 

Bi-LSTM 

Commonsense Relations 

Query 

w1 

CS 

, 
..., wl 

CS 

w2 

CS 

, 

Baseline Reasoning Cell 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Qualitative analysis of commonsense require-
ments. WikiHop results are from Welbl et al. (2018); 
NarrativeQA results are from our manual analysis (on 
the validation set). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results across different metrics on the test set of NarrativeQA-summaries task.  † indicates span prediction 
models trained on the Rouge-L retrieval oracle. 

Model 
Acc (%) 

BiDAF (Welbl et al., 2018) 
42.09 
Coref-GRU (Dhingra et al., 2018) 
56.00 

MHPGM 
56.74 
MHPGM+ NOIC 
58.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Results of our models on WikiHop dataset.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>has reported higher numbers on different data settings which are not comparable to our results. sented in Table 2. Experiment 2 demonstrates the importance of multi-hop attention by showing that if we only allow one hop of attention (even with all other components of the model, including ELMo embeddings) the model's performance decreases by over 12 Rouge-L points. Experiment 3 and 4 demonstrate the effectiveness of other parts of our model. We see that ELMo embeddings (Peters</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model ablations on NarrativeQA val-set. 

Commonsense B-1 
B-4 
M 
R 
C 

None 
42.3 18.9 18.3 44.9 151.6 
NumberBatch 
42.6 19.6 18.6 44.4 148.1 
Random Rel. 
43.3 19.3 18.6 45.2 151.2 
Single Hop 
42.1 19.9 18.2 44.0 148.6 
Grounded Rel. 
45.9 21.9 20.7 48.0 166.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Commonsense ablations on NarrativeQA val-
set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 : NarrativeQA's commonsense requirements and effectiveness of commonsense selection algorithm.</head><label>6</label><figDesc></figDesc><table>MHPGM+NOIC better 
23% 
MHPGM better 
15% 
Indistinguishable (Both-good) 41% 
Indistinguishable (Both-bad) 
21% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Human evaluation on the output quality of the 
MHPGM+NOIC vs. MHPGM in terms of correctness. 

</table></figure>

			<note place="foot" n="1"> A semantic network where the nodes are individual concepts (words or phrases) and the edges describe directed relations between them (e.g., island, UsedFor, vacation). 2 We release all our commonsense extraction code and the extracted commonsense data at: https://github.com/ yicheng-w/CommonSenseMultiHopQA 3 If we are unable to find a relation that satisfies the condition, we keep the steps up to and including the node.</note>

			<note place="foot" n="4"> In cases where more than one relation can be used to make a hop, we pick one at random.</note>

			<note place="foot" n="5"> Due to the 2-week evaluation wait-time on the nonpublic test set, we instead train our model on a sub-section of the training set, pick hyperparameters based on a small</note>

			<note place="foot" n="9"> The improvement in Rouge-L and METEOR for all three ablation approaches have p ≥ 0.15 with the bootstrap test.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful com-ments. This work was supported by DARPA (YFA17-D17AP00022), Google Faculty Research Award, Bloomberg Data Science Research Grant, and NVidia GPU awards. The views contained in this article are those of the authors and not of the funding agency.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constraint-based question answering with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2503" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>AcM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Normalized (pointwise) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of GSCL</title>
		<meeting>GSCL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentic computing for patient centered applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tariq</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eckl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing (ICSP), 2010 IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1279" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural natural language inference models enhanced with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2406" to="2417" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="845" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural models for reasoning over multiple mentions using coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gatedattention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A knowledge-grounded neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Wen-tau Yih, and Michel Galley</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="908" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The narrativeqa reading comprehension challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gáabor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The measurement of observer agreement for categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary G</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="page" from="159" to="174" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledgeable reader: enhancing cloze-style reading comprehension with external commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="821" to="832" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computer-intensive methods for testing hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sentiment data flow analysis by means of dynamic linguistic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="26" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentic LDA: Improving on LDA with semantic similarity for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bisio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks (IJCNN), 2016 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4465" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representing general relational knowledge in ConceptNet 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3679" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-range reasoning for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09074</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Common sense knowledge for handwritten chinese text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
