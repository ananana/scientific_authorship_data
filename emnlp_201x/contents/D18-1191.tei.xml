<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Span Selection Model for Semantic Role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
							<email>hiroki.ouchi@riken.jp, { shindo, matsu }@is.naist.jp</email>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project (AIP)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project (AIP)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">RIKEN Center for Advanced Intelligence Project (AIP)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Span Selection Model for Semantic Role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1630" to="1642"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use span-level features, that are difficult to use in token-based BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic Role Labeling (SRL) is a shallow se- mantic parsing task whose goal is to recognize the predicate-argument structure of each predicate. Given a sentence and a target predicate, SRL sys- tems have to predict semantic arguments of the predicate. Each argument is a span, a unit that consists of one or more words. A key to the ar- gument span prediction is how to represent and model spans.</p><p>One popular approach to it is based on BIO tagging schemes. State-of-the-art neural SRL models adopt this approach ( <ref type="bibr">Zhou and Xu, 2015;</ref><ref type="bibr">He et al., 2017;</ref><ref type="bibr">Tan et al., 2018</ref>). Using features induced by neural networks, they predict a BIO tag for each word. Words at the beginning and inside of argument spans have the "B" and "I" tags, and words outside argument spans have the tag "O." While yielding high accuracies, this approach re- constructs argument spans from the predicted BIO tags instead of directly predicting the spans.</p><p>Another approach is based on labeled span pre- diction <ref type="bibr">(Täckström et al., 2015;</ref><ref type="bibr">FitzGerald et al., 2015)</ref>. This approach scores each span with its la- bel. One advantage of this approach is to allow us to design and use span-level features, that are difficult to use in BIO tagging approaches. How- ever, the performance has lagged behind that of the state-of-the-art BIO-based neural models.</p><p>To fill this gap, this paper presents a simple and accurate span-based model. Inspired by recent span-based models in syntactic parsing and coref- erence resolution <ref type="bibr">(Stern et al., 2017;</ref><ref type="bibr">Lee et al., 2017)</ref>, our model directly scores all possible la- beled spans based on span representations induced from neural networks. At decoding time, we greedily select higher scoring labeled spans. The model parameters are learned by optimizing log- likelihood of correct labeled spans.</p><p>We evaluate the performance of our span-based model on the <ref type="bibr">CoNLL-2005</ref><ref type="bibr">and 2012</ref><ref type="bibr">datasets (Carreras and M` arquez, 2005</ref><ref type="bibr">Pradhan et al., 2012)</ref>. Experimental results show that the span- based model outperforms the BiLSTM-CRF model. In addition, by using contextualized word representations, <ref type="bibr">ELMo (Peters et al., 2018</ref>), our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the <ref type="bibr">CoNLL-2005</ref> and 2012 datasets, respectively. Empirical analy- sis on these results shows that the label prediction ability of our span-based model is better than that of the CRF-based model. Another finding is that ELMo improves the model performance for span boundary identification.</p><p>In summary, our main contributions include:</p><p>• A simple span-based model that achieves the state-of-the-art results.</p><p>• Quantitative and qualitative analysis on strengths and weaknesses of the span-based model.</p><p>• Empirical analysis on the performance gains by ELMo.</p><p>Our code and scripts are publicly available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We treat SRL as span selection, in which we select appropriate spans from a set of possible spans for each label. This section formalizes the problem and provides our span selection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Span Selection Problem</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Setting</head><p>Given a sentence that consists of T words w 1:T = w 1 , · · · , w T and the target predicate position in- dex p, the goal is to predict a set of labeled spans Y = {⟨i, j, r⟩ k } |Y | k=1 .</p><p>Input : X = {w 1:T , p},</p><formula xml:id="formula_0">Output : Y = {⟨i, j, r⟩ k } |Y | k=1 .</formula><p>Each labeled span ⟨i, j, r⟩ consists of word indices i and j in the sentence (1 ≤ i ≤ j ≤ T ) and a semantic role label r ∈ R.</p><p>One simple method to predict Y is to select the highest scoring span (i, j) from all possible spans S for each label r,</p><formula xml:id="formula_1">argmax (i,j)∈S SCORE r (i, j), r ∈ R .<label>(1)</label></formula><p>Function SCORE r (i, j) returns a real value for each span (i, j) ∈ S (described in Section 2.2 in more detail). The number of possible spans S in the input sentence w 1:T is T (T +1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, and S is defined as follows,</p><formula xml:id="formula_2">S = {(i, j) | i, j ∈ {1, · · · , T }, i ≤ j} .</formula><p>Note that some semantic roles may not appear in the sentence. To deal with the absence of some labels, we define the predicate position span (p, p) as a NULL span and train a model to select the NULL span when there is no span for the label. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Consider the following sentence with the set of correct labeled spans Y .</p><formula xml:id="formula_3">She 1 kept 2 a 3 cat 4 [ A0 ] [ A1 ] Y = { ⟨1, 1, A0⟩, ⟨3, 4, A1⟩, ⟨2, 2, A2⟩, · · · , ⟨2, 2, TMP⟩ } 2</formula><p>Since the predicate itself can never be an argument of its own, we define the position as the NULL span.</p><p>The input sentence is w 1:4 = "She kept a cat", and the target predicate position is p = 2. The correct labeled span ⟨1, 1, A0⟩ indicates that the A0 ar- gument is "She", and ⟨3, 4, A1⟩ indicates that the A1 argument is "a cat". The other labeled spans ⟨2, 2, * ⟩ indicate there are no arguments.</p><p>All the possible spans in this sentence are as fol- lows, S w 1:4 = {(1, 1), (1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 3), (3, 4), (4, 4)} , where the predicate span (2, 2) is treated as the NULL span. Among these candidates, we select the highest scoring span for each label. As a result, we can obtain correct labeled spans Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scoring Function</head><p>As the scoring function for each span in Eq. 1, we model normalized distribution over all possi- ble spans S for each label r,</p><formula xml:id="formula_4">SCORE r (i, j) = P θ (i, j | r) = exp(F θ (i, j, r)) ∑ (i ′ ,j ′ )∈S exp(F θ (i ′ , j ′ , r)) ,<label>(2)</label></formula><p>where function F θ returns a real value. We train the parameters θ of F θ on a training set,</p><formula xml:id="formula_5">D = {(X (n) , Y (n) )} |D| n=1 , X = {w 1:T , p} , Y = {⟨i, j, r⟩ k } |Y | k=1 .</formula><p>To train the parameters θ of F θ , we minimize the cross-entropy loss function,</p><formula xml:id="formula_6">L(θ) = ∑ (X,Y )∈D ℓ θ (X, Y ) ,<label>(3)</label></formula><formula xml:id="formula_7">ℓ θ (X, Y ) = ∑ ⟨i,j,r⟩∈Y log P θ (i, j|r) ,</formula><p>where function ℓ θ (X, Y ) is a loss for each sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Function F θ</head><p>Function F θ in Eq. 2 consists of three types of functions; the base feature function f base , the span feature function f span and the labeling function f label as follows,</p><formula xml:id="formula_8">h 1:T = f base (w 1:T , p) ,<label>(4)</label></formula><formula xml:id="formula_9">h s = f span (h 1:T , s) ,<label>(5)</label></formula><formula xml:id="formula_10">F θ (i, j, r) = f label (h s , r) .<label>(6)</label></formula><p>Firstly, f base calculates a base feature vector h t for each word w t ∈ w 1:T . Then, from a sequence of the base feature vectors h 1:T , f span calculates a span feature vector h s for a span s = (i, j). Fi- nally, using h s , f label calculates the score for the span s = (i, j) with a label r. Each function in Eqs. 4, 5 and 6 can arbitrarily be defined. In Section 3, we describe our functions used in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Inference</head><p>The simple argmax inference (Eq. 1) selects one span for each label. While this argmax inference is computationally efficient, it faces the following two problematic issues.</p><p>(a) The argmax inference sometimes selects spans that overlap with each other. (b) The argmax inference cannot select multiple spans for one label.</p><p>In terms of (a), for example, when ⟨1, 3, A0⟩ and ⟨2, 4, A1⟩ are selected, a part of these two spans overlaps. In terms of (b), consider the following sentence.</p><p>He came to the U.S. yesterday at 5 p.m.</p><formula xml:id="formula_11">[A0] [ A4 ] [ TMP ] [ TMP ]</formula><p>In this example, the label TMP is assigned to the two spans ("yesterday" and "at 5 p.m."). Semantic role labels are mainly categorized into (i) core la- bels or (ii) adjunct labels. In the above example, the labels A0 and A4 are regarded as core labels, which indicate obligatory arguments for the pred- icate. In contrast, the labels like TMP are regarded as adjunct labels, which indicate optional argu- ments for the predicate. As the example shows, adjunct labels can be assigned to multiple spans.</p><p>To deal with these issues, we use a greedy search that keeps the consistency among spans and can return multiple spans for adjunct labels. Specifically, we greedily select higher scoring la- beled spans subject to two constraints.</p><p>Overlap Constraint: Any spans that overlap with the selected spans cannot be selected. Number Constraint: While multiple spans can be selected for each adjunct label, at most one span can be selected for each core label.</p><p>As a precise description of this algorithm, we de- scribe the pseudo code and its explanation in Ap- pendix A.   <ref type="figure">Figure 1</ref>: Overall architecture of our BiLSTM-span model.</p><formula xml:id="formula_12">46664724A ) ) , ) .. ( - .. ,<label>( 301 -</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network Architecture</head><p>To compute the score for each span, we have intro- duced three functions (f base , f span , f label ) in Sec- tion 2.3. As an instantiation of each function, we use neural networks. This section describes our neural networks for each function and the overall network architecture. <ref type="figure">Figure 1</ref> illustrates the overall architecture of our model. The first component f base uses bidirec- tional LSTMs (BiLSTMs) <ref type="bibr">(Schuster and Paliwal, 1997;</ref><ref type="bibr">Graves et al., 2005</ref><ref type="bibr">Graves et al., , 2013</ref> to calculate the base features. From the base features, the second component f span extracts span features. Based on them, the final component f label calculates the score for each labeled span. In the following, we describe these three components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BiLSTM-Span Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Feature Function</head><p>As the base feature function f base , we use BiL- STMs,</p><formula xml:id="formula_13">f base (w 1:T , p) = BILSTM(w 1:T , p) .</formula><p>There are some variants of BiLSTMs. Following the deep SRL models proposed by <ref type="bibr">Zhou and Xu (2015)</ref> and <ref type="bibr">He et al. (2017)</ref>, we stack BiLSTMs in an interleaving fashion. The stacked BiLSTMs process an input sequence in a left-to-right man- ner at odd-numbered layers and in a right-to-left manner at even-numbered layers.</p><p>The first layer of the stacked BiLSTMs receives word embeddings x word ∈ R d word and predicate mark embeddings x mark ∈ R d mark . As the word embeddings, we can use existing word embed- dings. The mark embeddings are created from the mark feature which has a binary value. The value is 1 if the word is the target predicate and 0 other- wise. For example, at the bottom part of <ref type="figure">Figure 1</ref>, the word "bought" is the target predicate and as- signed 1 as its mark feature.</p><p>Receiving these inputs, the stacked BiLSTMs calculates the hidden states until the top layer. We use these hidden states as the input feature vectors h 1:T for the span feature function f span (Eq. 5). Each vector h t ∈ h 1:T has d hidden dimensions. We provide a detailed description of the stacked BiLSTMs in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Span Feature Function</head><p>From the base features induced by the BiLSTMs, we create the span feature representations,</p><formula xml:id="formula_14">f span (h 1:T , s) = [h i + h j ; h i − h j ] , (7)</formula><p>where the addition and subtraction features of the i-th and j-th hidden states are concatenated and used as the feature for a span s = (i, j). The re- sulting vector h s is a 2d hidden dimensional vector.</p><p>The middle part of <ref type="figure">Figure 1</ref> shows an example of this process. For the span (3, 5), the span fea- ture function f span receives the 3rd and 5th fea- tures (h 3 and h 5 ). Then, these two vectors are added, and the 5th vector is subtracted from the 3rd vector. The resulting vectors are concatenated and given to the labeling function f label .</p><p>Our design of the span features is inspired by the span (or segment) features used in syntac- tic parsing ( <ref type="bibr">Wang and Chang, 2016;</ref><ref type="bibr">Stern et al., 2017;</ref><ref type="bibr">Teranishi et al., 2017)</ref>. While these neural span features cannot be used in BIO-based SRL models, they can easily be incorporated into span- based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labeling Function</head><p>Taking a span representation h s as input, the la- beling function f label returns the score for the span s = (i, j) with a label r. Specifically, we use the following labeling function,</p><formula xml:id="formula_15">f label (h s , r) = W[r] · h s ,<label>(8)</label></formula><p>where W ∈ R |R|×2d hidden has a row vector associ- ated with each label r, and W[r] denotes the r-th row vector. As the result of the inner product of W[r] and h s , we obtain the score for a span (i, j) with a label r. The upper part of <ref type="figure">Figure 1</ref> shows an example of this process. The span representation h s for the span s = (3, 5) is created from addition and sub- traction of h 3 and h 5 . Then, we calculate the inner product of h s and W[r]. The score for the label A0 is 2.1, and the score for the label A1 is 3.7. In the same manner, by calculating the scores for all the spans S and labels R, we can obtain the score matrix (at the top part of <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ensembling</head><p>We propose an ensemble model that uses span representations from multiple models. Each base model trained with different random initializations has variance in span representations. To take ad- vantage of it, we introduce a variant of a mixture of experts (MoE) <ref type="bibr">(Shazeer et al., 2017)</ref></p><formula xml:id="formula_16">, 3 h moe s = W moe s · M ∑ m=1 α m h (m) s , (9) f moe label (h moe s , r) = W moe [r] · h moe s .<label>(10)</label></formula><p>Firstly, we combine span representations h</p><formula xml:id="formula_17">(m) s from each model m ∈ {1, · · · , M }. W moe s</formula><p>is a parameter matrix and {α m } M m=1 are trainable, softmax-normalized parameters. Then, using the combined span representation h moe s , we calculate the score in the same way as Eq. 8. We use the same greedy search algorithm used for our base model (Section 2.4).</p><p>During training, we update only the parameters of the ensemble model, i.e.,</p><formula xml:id="formula_18">{W moe s , W moe , {α m } M m=1 }.</formula><p>That is, we fix the parameters of each trained model m. As the loss function, we use the cross-entropy (Eq. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the CoNLL-2005 and 2012 datasets <ref type="bibr">4</ref> . We follow the standard train-development-test split and use the official evaluation script 5 from the CoNLL-2005 shared task on both datasets.</p><p>3 One popular ensemble model for SRL is the product of experts (PoE) model ( <ref type="bibr">FitzGerald et al., 2015;</ref><ref type="bibr">He et al., 2017;</ref><ref type="bibr">Tan et al., 2018</ref>). In our preliminary experiments, we tried the PoE model but it did not improve the performance. <ref type="bibr">4</ref> We use the version of OntoNotes downloaded at: http://cemantix.org/data/ontonotes.html. <ref type="bibr">5</ref> The   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Model</head><p>For comparison, as a model based on BIO tag- ging approaches, we use the BiLSTM-CRF model proposed by <ref type="bibr">Zhou and Xu (2015)</ref>. The BiLSTMs for the base feature function f base are the same as those used in our BiLSTM-span model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Setup</head><p>As the base function f base , we use 4 BiLSTM layers with 300 dimensional hidden units. To optimize the model parameters, we use Adam ( <ref type="bibr">Kingma and Ba, 2014</ref>). Other hyperparameters are described in Appendix C in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Embeddings</head><p>Word embeddings have a great influence on SRL models. To validate the model performance, we use two types of word embeddings.</p><p>• Typical word embeddings, SENNA and ELMo can be regarded as different types of embeddings in terms of the context sensi- tivity. SENNA and other typical word embeddings always assign an identical vector to each word re- gardless of the input context. In contrast, ELMo assigns different vectors to each word depending on the input context. In this work, we use these word embeddings that have different properties. 8 These embeddings are fixed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head><p>As the objective function, we use the cross- entropy L θ in Eq. 3 with L2 weight decay,</p><formula xml:id="formula_19">L θ = ∑ (X,Y )∈D ℓ θ (X, Y ) + λ 2 ||θ|| 2 ,<label>(11)</label></formula><p>where the hyperparameter λ is the coefficient gov- erning the L2 weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We report averaged scores across five different runs of the model training. <ref type="table" target="#tab_2">Tables 1 and 2</ref> show the experimental results on the <ref type="bibr">CoNLL-2005 and</ref><ref type="bibr">2012</ref> datasets. Over- all, our span-based ensemble model using ELMo achieved the best F1 scores, 87.4 F1 and 87.0 F1 on the <ref type="bibr">CoNLL-2005 and</ref><ref type="bibr">CoNLL-2012 datasets, respectively.</ref> In comparison with the CRF-based single model, our span-based single model con- sistently yielded better F1 scores regardless of the word embeddings, SENNA and ELMO. Al- though the performance difference was small be- tween these models using ELMO, it seems natural because both models got much better results and approached to the performance upper bound. <ref type="table" target="#tab_3">Table 3</ref> shows the comparison with existing models in F1 scores. Our single and ensemble models using ELMO achieved the best F1 scores on all the test sets except the Brown test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>To better understand our span-based model, we ad- dressed the following questions and obtained the following findings. In addition, we have conducted qualitative analy- sis on span and label representations learned in the span-based model (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance for Span Boundary Identification</head><p>We analyze the results predicted by the single models. We evaluate F1 scores only for the span boundary match, shown by   On both datasets, the CRF-based models achieved better F1 than that of the span-based models. Also, compared with SENNA, ELMO yielded much better F1 by over 3.0. This suggests that a factor of the overall SRL performance gain by ELMO is the improvement of the model ability to identify span boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance for Label Prediction</head><p>We analyze labels of the predicted results. For la- beled spans whose boundaries match the gold an- notation, we evaluate the label accuracies. As Ta- ble 5 shows, the span-based models outperformed the CRF-based models. Also, interestingly, the performance gap between SENNA and ELMO was not so big as that for span boundary identification.</p><p>Label-wise Performance <ref type="table" target="#tab_9">Table 6</ref> shows F1 scores for frequent labels on the CoNLL-2005 and 2012 datasets. For A0 and A1, the performances of the CRF-based and span- based models were almost the same. For A2, the span-based models outperformed the CRF-based model by about 1.0 F1 on the both datasets. 9 <ref type="figure" target="#fig_4">Figure 2</ref> shows a confusion matrix for labeling er- rors of the span-based model using ELMo. 10 Fol- lowing <ref type="bibr">He et al. (2017)</ref>, we only count predicted arguments that match the gold span boundaries. <ref type="bibr">9</ref> The PNC label got low scores on the CoNLL-2012 dataset in <ref type="table" target="#tab_9">Table 6</ref>. Almost all the gold PNC (purpose) la- bels are assigned to only the news article domain texts of the CoNLL-2012 dataset. The other 6 domain texts have no or very few PNC labels. This can lead to the low performance. <ref type="bibr">10</ref> We have observed the same tendency of labeling confu- sions between the models using ELMo and SENNA. <ref type="bibr">CoNLL-2005</ref> CoNLL <ref type="table" target="#tab_2">-2012  SENNA  ELMO  SENNA  ELMO  Label  CRF SPAN CRF SPAN CRF SPAN CRF SPAN  A0</ref> 89     The span-based model confused A0 and A1 ar- guments the most. In particular, the model con- fused them for ergative verbs. Consider the fol- lowing two sentences:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label Confusion Matrix</head><note type="other">43.2 47.3 56.6 54.5 54.1 52.0 61.1 59.7 LOC 58.2 60.5 68.1 68.3 65.8 65.0 72.0 72.0 MNR 61.4 61.3 66.5 67.7 64.4 65.7 70.5 71.1 PNC 57.3 60.2 68.8 67.7 18.5 13.7 20.2 16.1 TMP 81.8 82.7 86.1 86.0 82.2 82.3 86.1 86.2 Overall 81.</note><p>People start their own business ... <ref type="bibr">[ A0 ]</ref> .. Congress has started to jump on ...</p><p>[ A1 ]</p><p>where the constituents located at the syntactic sub- jective position fulfill a different role A0 or A1 ac- cording to their semantic properties, such as ani- macy. Such arguments are difficult for SRL mod- els to correctly identify.</p><p>Another point is the confusions of A2 with DIR and LOC. As <ref type="bibr">He et al. (2017)</ref> pointed out, A2 in a lot of verb frames represents semantic relations such as direction or location, which can cause the confusions of A2 with such location-related ad- juncts. To remedy these two problematic issues, it can be a promising approach to incorporate frame knowledge into SRL models by using verb frame dictionaries.  <ref type="table">Table 7</ref>: Example of the CoNLL-2005 development set, in which our model misclassified the label for the span "across the border". We collect 10 nearest neigh- bors of this span from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis on Our Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On Span Representations</head><p>Our span-based model computes and uses span representations (Eq. 7) for label prediction. To investigate a relation between the span represen- tations and predicted labels, we qualitatively ana- lyze nearest neighbors of each span representation with its predicted label. Specifically, for each pre- dicted span in the development set, we collect 10 nearest neighbor spans with their gold labels from the training set. <ref type="table">Table 7</ref> shows 10 nearest neighbors of a span "across the border" for the predicate "move". The label of this span was misclassified, i.e., the pre- dicted label is DIR but the gold is A2. Looking at its nearest neighbor spans, they have different gold labels, such as DIR, A2 and A3. Like this case, we have observed that spans with a misclas- sified label often have their nearest neighbors with inconsistent labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On Label Embeddings</head><p>We analyze the label embeddings in the labeling function (Eq. 8). <ref type="figure" target="#fig_5">Figure 3</ref> shows the distribution of the learned label embeddings. The adjunct la- bels are close to each other, which are likely to be less discriminative. Also, the core label A2 is close to the adjunct label DIR, which are often confused by the model. To enhance the discrim- inative power, it is promising to apply techniques that keep label representations far away from each other ( <ref type="bibr">Wen et al., 2016;</ref><ref type="bibr">Luo et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Semantic Role Labeling Tasks</head><p>Automatic SRL has been widely studied ( <ref type="bibr">Gildea and Jurafsky, 2002</ref>). There have been two main styles of SRL.</p><p>• FrameNet-style SRL ( <ref type="bibr" target="#b0">Baker et al., 1998)</ref> • PropBank-style SRL ( <ref type="bibr">Palmer et al., 2005)</ref> In this paper, we have tackled PropBank-style SRL. <ref type="bibr">11</ref> In PropBank-style SRL, there have been two main task settings.</p><p>• Span-based SRL: <ref type="bibr">CoNLL-2004 and</ref><ref type="bibr">2005</ref> shared tasks ( <ref type="bibr">Carreras and Marquez, 2004;</ref><ref type="bibr">Carreras and M` arquez, 2005)</ref> • Dependency-based SRL: CoNLL-2008 and 2009 shared tasks ( <ref type="bibr">Surdeanu et al., 2008;</ref><ref type="bibr">Hajič et al., 2009)</ref> 11 Detailed descriptions on FrameNet-style and PropBank- style SRL can be found in <ref type="bibr" target="#b0">Baker et al. (1998)</ref>; <ref type="bibr">Das et al. (2014)</ref>; <ref type="bibr">Kingsbury and Palmer (2002)</ref>; <ref type="bibr">Palmer et al. (2005)</ref>.</p><p>He hit the ball with the bat A0 A1 A2 A0 A1 A2 <ref type="figure">Figure 4</ref>: Example of dependency-based SRL (the up- per part) and span-based SRL (the lower part). <ref type="figure">Figure 4</ref> illustrates an example of span-based and dependency-based SRL. In dependency-based SRL (at the upper part of <ref type="figure">Figure 4</ref>), the correct A2 argument for the predicate "hit" is the word "with". On one hand, in span-based SRL (at the lower part of <ref type="figure">Figure 4</ref>), the correct A2 argument is the span "with the bat". For span-based SRL, the CoNLL-2004 and 2005 shared tasks ( <ref type="bibr">Carreras and Marquez, 2004;</ref><ref type="bibr">Carreras and M` arquez, 2005</ref>) provided the task settings and datasets. In the task settings, various SRL models, from traditional pipeline mod- els to recent neural ones, have been proposed and competed with each other ( <ref type="bibr">Pradhan et al., 2005;</ref><ref type="bibr">He et al., 2017;</ref><ref type="bibr">Tan et al., 2018</ref>). For dependency-based SRL, the CoNLL-2008 and 2009 shared tasks ( <ref type="bibr">Surdeanu et al., 2008;</ref><ref type="bibr">Hajič et al., 2009</ref>) provided the task settings and datasets. As in span-based SRL, recent neural models achieved high-performance in dependency-based SRL ( <ref type="bibr">Marcheggiani et al., 2017;</ref><ref type="bibr">Marcheggiani and Titov, 2017;</ref><ref type="bibr">He et al., 2018b;</ref><ref type="bibr">Cai et al., 2018)</ref>. This paper focuses on span-based SRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">BIO-based SRL Models</head><p>Span-based SRL can be solved as BIO sequen- tial tagging ( <ref type="bibr">Hacioglu et al., 2004;</ref><ref type="bibr">Pradhan et al., 2005;</ref><ref type="bibr">M` arquez et al., 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural models</head><p>State-of-the-art SRL models use neural networks based on the BIO tagging approach. The pioneering neural SRL model was proposed by <ref type="bibr">Collobert et al. (2011)</ref>. They use convolutional neural networks (CNNs) and CRFs. Instead of CNNs, <ref type="bibr">Zhou and Xu (2015)</ref> and <ref type="bibr">He et al. (2017)</ref> used stacked BiLSTMs and achieved strong performance without syntactic in- puts. <ref type="bibr">Tan et al. (2018)</ref> replaced stacked BiLSTMs with self-attention architectures. <ref type="bibr">Strubell et al. (2018)</ref> improved the self-attention SRL model by incorporating syntactic information.</p><p>Word representations Typical word represen- tations, such as SENNA <ref type="bibr">(Collobert et al., 2011</ref>) and GloVe ( <ref type="bibr">Pennington et al., 2014</ref>), have been used and contributed to the performance im- provement <ref type="bibr">(Collobert et al., 2011;</ref><ref type="bibr">Zhou and Xu, 2015;</ref><ref type="bibr">He et al., 2017)</ref>. Recently, <ref type="bibr">Peters et al. (2018)</ref> integrated contextualized word represen- tation, ELMo, into the model of <ref type="bibr">He et al. (2017)</ref> and improved the performance by 3.2 F1 score. <ref type="bibr">Strubell and McCallum (2018)</ref> also integrated ELMo into the model of <ref type="bibr">Strubell et al. (2018)</ref> and reported the performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Span-based SRL Models</head><p>Another line of approaches to SRL is labeled span modeling ( <ref type="bibr">Xue and Palmer, 2004;</ref><ref type="bibr">Koomen et al., 2005;</ref><ref type="bibr">Toutanova et al., 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typical models</head><p>Typically, in this approach, models firstly identify candidate argument spans (argument identification) and then classify each span into one of the semantic role labels (argu- ment classification). For inference, several ef- fective methods have been proposed, such as structural constraint inference by using integer linear programming <ref type="bibr">(Punyakanok et al., 2008</ref>) or dynamic programming <ref type="bibr">(Täckström et al., 2015;</ref><ref type="bibr">FitzGerald et al., 2015</ref>).</p><p>Recent span-based model A very recent work, <ref type="bibr">He et al. (2018a)</ref>, proposed a span-based SRL model similar to our model. They also used BiL- STMs to induce span representations in an end- to-end fashion. A main difference is that while they model P(r|i, j), we model P(i, j|r). In other words, while their model seeks to select an ap- propriate label for each span (label selection), our model seeks to select appropriate spans for each label (span selection). This point distinguishes be- tween their model and ours.</p><p>FrameNet span-based model For FrameNet- style SRL, <ref type="bibr">Swayamdipta et al. (2017)</ref> used a segmental RNN ( <ref type="bibr">Kong et al., 2016)</ref>, combin- ing bidirectional RNNs with semi-Markov CRFs ( <ref type="bibr">Sarawagi and Cohen, 2004</ref>). Their model com- putes span representations using BiLSTMs and learns a conditional distribution over all possible labeled spans of an input sequence. Although we cannot compare our results with theirs, we can re- gard that our model is simpler and effective for PropBank-style SRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Span-based Models in Other NLP Tasks</head><p>In syntactic parsing, <ref type="bibr">Wang and Chang (2016)</ref> pro- posed an LSTM-based sentence segment embed- ding method named LSTM-Minus. <ref type="bibr">Stern et al. (2017)</ref>; <ref type="bibr">Kitaev and Klein (2018)</ref> incorporated the LSTM Minus into their parsing model and achieved the best results in constituency pars- ing. In coreference resolution, <ref type="bibr">Lee et al. (2017</ref><ref type="bibr">Lee et al. ( , 2018</ref> presented an end-to-end coreference reso- lution model, which considers all spans in a docu- ment as potential mentions and learn distributions over possible antecedents for each. Our model can be regarded as an extension of their model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented a simple and accurate span- based model. We treat SRL as span selection and our model seeks to select appropriate spans for each label. Experimental results have demon- strated that despite the simplicity, the model out- performs a strong BiLSTM-CRF model. Also, our span-based ensemble model using ELMo achieves the state-of-the-art results on the <ref type="bibr">CoNLL-2005 and</ref><ref type="bibr">2012</ref> datasets. Through empirical analysis, we have obtained some interesting findings. One of them is that the span-based model is better at label prediction compared with the CRF-based model. Another one is that ELMo improves the model performance for span boundary identification.</p><p>An interesting direction for future work con- cerns evaluating span representations from our span-based model. Since the investigation on the characteristics of the representations can lead to interesting findings, it is worthwhile evaluat- ing them intrinsically and extrinsically. Another promising direction is to explore methods of incor- porating frame knowledge into SRL models. We have observed that a lot of label confusions arise due to the lack of such knowledge. The use of frame knowledge to reduce these confusions is a straightforward approach. <ref type="bibr">Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014</ref>. Glove: Global vectors for word representation. if r ∈ R (core) then 13:</p><p>used cores ← used cores ∪ {r} 14: return spans Algorithm 1 describes the pseudo code of the greedy search algorithm introduced in Section 2.4. This algorithm receives the three inputs (line 1-3). M is the score matrix illustrated at the top part of <ref type="figure">Figure 1</ref> in Section 3. Each cell of the matrix rep- resents the score of each span. p is a target predi- cate position index. R (core) is the set of core labels. At line 4, the variable "spans" is initialized. This variable stores the selected spans to be returned as the output. At line 5, the variable "used cores" is initialized. This variable keeps track of the already selected core labels.</p><p>At line 6, the score matrix M is converted to tuples, (i, j, r, score), by the function f latten(·). These tuples are stored in the variable U. At line 7, from U, we remove the tuples that fall into any one of the followings, (i) the tuples whose bound- ary (i, j) overlaps with the predicate position p or (ii) the tuples whose score is lower than that of the predicate span tuples. In terms of (i), since spans whose boundary (i, j) overlaps with the predi- cate position, i ≤ p ≤ j, can never be a cor- rect argument, we remove such tuples. In terms of (ii), we remove the tuples ( * , * , r, score) whose score is lower than that of the predicate span tuple (p, p, r, score). In Section 2, we define the predi- cate span (p, p) as the NULL span, implying that we can regard the spans whose score is lower than that of the NULL span as an inappropriate argu- ment. Thus, we remove such tuples from the set of the candidates U.</p><p>The main processing starts from line 8. Based on the scores, the function sort(·) sorts the tuples (i, j, r, score) in a descending order. At line 9-10, there are constraints for output spans. At line 9, "r / ∈ used cores" represents the constraint that at most one span can be selected for each core label. At line 10, the function is overlap(·) takes as input a span (i, j) and the set of the selected spans, and returns the boolean value ("True" or "False") that represents whether the span overlaps with any one of the selected spans or not. At line 11, the span is added to the set of the selected spans. At line 12-13, if the label r is in- cluded in the core labels R (core) , the label is added to "used cores". At line 14, as the final output, the set of the selected spans "spans" is returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BiLSTMs</head><p>As the base feature function f base (Eq. 4 in Sec- tion 2.3), we use BiLSTMs,</p><formula xml:id="formula_20">f base (w 1:T , p) = BILSTM(w 1:T , p) .</formula><p>In particular, we use the stacked BiLSTMs in an interleaving fashion ( <ref type="bibr">Zhou and Xu, 2015;</ref><ref type="bibr">He et al., 2017</ref>). The stacked BiLSTMs process an input sequence in a left-to-right manner for odd- numbered layers and in a right-to-left manner for even-numbered layers.</p><p>The stacked BiLSTMs consist of L layers. The hidden state in each layer ℓ ∈ {1, · · · , L} is cal- culated as follows, create this vector by concatenating a word embed- ding and predicate mark embedding,</p><formula xml:id="formula_21">x (1) t = [x word t ; x mark t ] ,</formula><p>where x word ∈ R d word and x mark ∈ R d mark . The mark embedding is created from the binary mark feature. The value is 1 if the word is the target predicate and 0 otherwise.</p><p>After the L-th LSTM layer runs, we obtain x   <ref type="table" target="#tab_13">Table 8</ref>: Hyperparameters for our span-based model.  <ref type="bibr">Glorot and Bengio (2010)</ref>, and bias parameters are initialized with zero vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Span-based Model</head><p>Regularization We set the coefficient λ for the L2 weight decay (Eq. 11 in Section 4.3) to 0.0001. We apply dropout ( <ref type="bibr">Srivastava et al., 2014</ref>) to the input vectors of each LSTM with dropout ratio of 0.1 and the ELMo embeddings with dropout ratio of 0.5.</p><p>Training To optimize the parameters, we use Adam ( <ref type="bibr">Kingma and Ba, 2014</ref>) with β 1 = 0.9 and β 2 = 0.999. The learning rate is initialized to 0.001. After training 50 epochs, we halve the learning rate every 25 epochs. Parameter updates are performed in mini-batches of 32. The num- ber of training epochs is set to 100. We save the parameters that achieve the best F1 score on the development set and evaluate them on the test set. Training our model on the CoNLL-2005 training set takes about one day and on the CoNLL-2012 training set takes about two days on a single GPU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ensemble Model</head><p>Our ensemble model uses span representations h Training To optimize the parameters, we use Adam with β 1 = 0.9 and β 2 = 0.999. The learn- ing rate is set to 0.0001. Parameter updates are performed in mini-batches of 8. The number of training epochs is set to 20. We save the parame- ters that achieve the best F1 score on the develop- ment set and evaluate them on the test set. Train- ing one ensemble model on the <ref type="bibr">CoNLL-2005 and</ref><ref type="bibr">2012</ref> training sets takes about one day on a single GPU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>SENNA 6 (</head><label>6</label><figDesc>Collobert et al., 2011) • Contextualized word embeddings, ELMo 7 (Peters et al., 2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>What are strengths and weaknesses of our span-based model compared with the CRF- based model? (b) What aspect of SRL does ELMo improve? Findings (a) While the CRF-based model is better at span boundary identification (Section 5.1), the span-based model is better at label prediction, especially for A2 (Section 5.2). (b) ELMo improves the model performance for span boundary identification (Section 5.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Confusion matrix for labeling errors of our span-based model using ELMo. Each cell shows the percentage of predicted labels for each gold label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Label embedding distribution of our spanbased model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>,</head><label></label><figDesc>· · · , x (L+1) T . We use them as the input of the span feature function f span (Eq. 5 in Section 2.3), i.e., h 1:T = x (L+1) 1:T . Each vector h t ∈ h 1:T has d hidden dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>from base models m ∈ {1, · · · , M } (Sec- tion 3.2). We use 5 base models (M = 5) learned over different runs. Note that, during training, we fix the parameters of the five base models and up- date only the parameters of the ensemble model. Network setup The parameter matrix W moe s (Eq. 9 in Section 3.2) is initialized with the iden- tity matrix. The scalar parameters {α m } M m=1 (Eq. 9) are initialized with 0. Each row vector W moe [r] of the parameter matrix W moe (Eq. 10) is initialized with the averaged vector over the row vectors W (m) [r] of each model m, i.e., 1 M ∑ M m=1 W (m) [r].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Experimental results on the CoNLL-2012 dataset.</head><label>2</label><figDesc></figDesc><table>CoNLL-05 
CoNLL12 
WSJ Brown ALL 
SINGLE MODEL 
ELMO-SPAN 
87.6 
78.7 
86.4 
86.2 
He+ 18 
87.4 
80.4 
-
85.5 
Peters+ 18 
-
-
-
84.6 
Strubell+ 18 
83.9 
72.6 
-
-
Tan+ 18 
84.8 
74.1 
83.4 
82.7 
He+ 17 
83.1 
72.1 
81.6 
81.7 
Zhou+ 15 
82.8 
69.4 
81.1 
81.3 
FitzGerald+ 15 
79.4 
71.2 
-
79.6 
Täckström+ 15 
79.9 
71.3 
-
79.4 
Toutanova+ 08 
79.7 
67.8 
-
-
Punyakanok+ 08 79.4 
67.8 
77.9 
-

ENSEMBLE MODEL 
ELMO-SPAN 
88.5 
79.6 
87.4 
87.0 
Tan+ 18 
86.1 
74.8 
84.6 
83.9 
He+ 17 
84.6 
73.6 
83.2 
83.4 
FitzGerald+ 15 
80.3 
72.2 
-
80.1 
Toutanova+ 08 
80.3 
68.8 
-
-
Punyakanok+ 08 79.4 
67.8 
77.9 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with existing models. The num-
bers denote F1 scores on each test set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>We regard a 
predicted boundary ⟨i, j,  * ⟩ as correct if it matches 
the gold annotation regardless of its label. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : F1 scores only for span boundary match.</head><label>4</label><figDesc></figDesc><table>CoNLL-05 
CoNLL-12 
EMB 
MODEL Acc. 
diff 
Acc. 
diff 

SENNA 
SPAN 
95.3 +1.5 
95.1 +1.5 
CRF 
93.8 
93.6 

ELMO 
SPAN 
96.1 +0.9 
95.7 +1.3 
CRF 
95.2 
94.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 : Accuracies only for semantic role labels.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc>F1 Scores for frequent labels on the development set of the CoNLL-2005 and 2012 datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>" · · · toy makers to move</head><label>"</label><figDesc></figDesc><table>[ across the border ] ." 
GOLD:A2 
PRED:DIR 
Nearest neighbors of "across the border" 
1 DIR across the Hudson 
2 DIR outside their traditional tony circle 
3 DIR across the floor 
4 DIR through this congress 
5 
A2 
off their foundations 
6 DIR off its foundation 
7 DIR off the center field wall 
8 
A3 
out of bed 
9 
A2 
through cottage rooftops 
10 DIR through San Francisco 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>In Proceedings of EMNLP, pages 1532-1543.Span-Consistent Greedy Search 1: Input: Score Matrix M ∈ R |R|×|S| , 2: Predicate Position Index p 3:spans ∪ {⟨i, j, r⟩} 12:</head><label></label><figDesc></figDesc><table>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt 
Gardner, Christopher Clark, Kenton Lee, and Luke 
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT, pages 
2227-2237. 

Sameer Pradhan, Kadri Hacioglu, Wayne Ward, 
James H Martin, and Daniel Jurafsky. 2005. Seman-
tic role chunking combining complementary syntac-
tic views. In Proceedings of CoNLL, pages 217-
220. 

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, 
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Proceedings of 
EMNLP-CoNLL, pages 1-40. 

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. 
The importance of syntactic parsing and inference in 
semantic role labeling. Computational Linguistics, 
34(2):257-287. 

Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information 
extraction. In Proceedings of NIPS, pages 1185-
1192. 

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2013. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv 
preprint arXiv:1312.6120. 

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions 
on Signal Processing, pages 2673-2681. 

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, 
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff 
Dean. 2017. Outrageously large neural networks: 
The sparsely-gated mixture-of-experts layer. arXiv 
preprint arXiv:1701.06538. 

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, 
Ilya Sutskever, and Ruslan Salakhutdinov. 2014. 
Dropout: A simple way to prevent neural networks 
from overfitting. The Journal of Machine Learning 
Research, 15(1):1929-1958. 

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A 
minimal span-based neural constituency parser. In 
Proceedings of ACL, pages 818-827. 

Emma Strubell and Andrew McCallum. 2018. Syntax 
helps elmo understand semantics: Is syntax still rel-
evant in a deep neural architecture for srl? In Pro-
ceedings of the Workshop on the Relevance of Lin-
guistic Structure in Neural Architectures for NLP, 
pages 19-27. 

Emma Strubell, Patrick Verga, Daniel Andor, 
David Weiss, and Andrew McCallum. 2018. 
Linguistically-informed self-attention for semantic 
role labeling. arXiv preprint arXiv:1804.08199. 

Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluís M` arquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. In Proceedings of CoNLL, 
pages 159-177. 

Swabha Swayamdipta, Sam Thomson, Chris Dyer, and 
Noah A Smith. 2017. Frame-semantic parsing with 
softmax-margin segmental rnns and a syntactic scaf-
fold. arXiv preprint arXiv:1706.09528. 

Oscar Täckström, Kuzman Ganchev, and Dipanjan 
Das. 2015. Efficient inference and structured learn-
ing for semantic role labeling. Transactions of ACL, 
3:29-41. 

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen, 
and Xiaodong Shi. 2018. Deep semantic role label-
ing with self-attention. In Proceedings of AAAI. 

Hiroki Teranishi, Hiroyuki Shindo, and Yuji Mat-
sumoto. 2017. Coordination boundary identification 
with similarity and replaceability. In Proceedings of 
IJCNLP, pages 264-272. 

Kristina Toutanova, Aria Haghighi, and Christopher D 
Manning. 2005. Joint learning improves semantic 
role labeling. In Proceedings of ACL, pages 589-
596. 

Wenhui Wang and Baobao Chang. 2016. Graph-based 
dependency parsing with bidirectional lstm. In Pro-
ceedings of ACL, pages 2306-2315. 

Yandong Wen, Kaipeng Zhang, Zhifeng Li, and 
Yu Qiao. 2016. A discriminative feature learning 
approach for deep face recognition. In Proceedings 
of ECCV, pages 499-515. 

Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Proceedings 
of EMNLP. 

Jie Zhou and Wei Xu. 2015. End-to-end learning of 
semantic role labeling using recurrent neural net-
works. In Proceedings of ACL-IJCNLP, pages 
1127-1137. A Span-Consistent Greedy Search 

Algorithm 1 Core Label Set R (core) 
4: spans ← ϕ 
5: used cores ← ϕ 
6: U ← {(i, j, r, score) ∈ f latten(M)} 
7: U ← f ilter(U, p) 
8: for (i, j, r, score) ∈ sort(U) do 

9: 

if r / 
∈ used cores and 

10: 

is overlap((i, j), spans) is False then 

11: 

spans ← </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 lists</head><label>8</label><figDesc></figDesc><table>the hyperparameters used for our 
span-based model. 

Word representation setup As word embed-
dings x word , we use two types of embeddings, (i) 
SENNA (Collobert et al., 2011), 50-dimensional 
word vectors (d word = 50), and (ii) ELMo 
(Peters et al., 2018), 1024-dimensional vectors 
(d word = 1024). During training, we fix these 
word embeddings (not update them). As predi-
cate mark embeddings x mark , we use randomly 
initialized 50-dimensional vectors (d mark = 50). 
During training, we update them. 

Network setup As the base feature function 
f base , we use 4 stacked BiLSTMs (2 forward and 
2 backward LSTMs) with 300-dimensional hid-
den units (d hidden = 300). Following He et al. 
(2017), we initialize all the parameter matrices 
in BiLSTMs with random orthonormal matrices 
(Saxe et al., 2013). Other parameters are initial-
ized following </table></figure>

			<note place="foot" n="1"> https://github.com/hiroki13/span-based-srl.</note>

			<note place="foot" n="6"> http://ronan.collobert.com/senna/ 7 http://allennlp.org/elmo 8 In our preliminary experiments, we also used the GloVe embeddings (Pennington et al., 2014), but the performance was worse than SENNA.</note>

			<note place="foot">t. Following He et al. (2017), we</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by JST CREST Grant Number JPMJCR1513 and JSPS KAK-ENHI Grant Number 18K18109. We are grateful to the members of the NAIST Computational Lin-guistics Laboratory, the members of Tohoku Uni-versity Inui-Suzuki Laboratory, Kentaro Inui, Jun Suzuki, Yuichiro Matsubayashi, and the anony-mous reviewers for their insightful comments.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-COLING</title>
		<meeting>ACL-COLING</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
